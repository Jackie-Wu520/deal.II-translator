 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26] 

[1.x.27][1.x.28] [1.x.29] 


This program grew out of a student project by Ivan Christov at Texas A&amp;M University. Most of the work for this program is by him. 

The goal of this program is to solve the sine-Gordon soliton equation in 1, 2 or 3 spatial dimensions. The motivation for solving this equation is that very little is known about the nature of the solutions in 2D and 3D, even though the 1D case has been studied extensively. 

Rather facetiously, the sine-Gordon equation's moniker is a pun on the so-called Klein-Gordon equation, which is a relativistic version of the Schr√∂dinger equation for particles with non-zero mass. The resemblance is not just superficial, the sine-Gordon equation has been shown to model some unified-field phenomena such as interaction of subatomic particles (see, e.g., Perring &amp; Skyrme in Nuclear %Physics [1.x.30]) and the Josephson (quantum) effect in superconductor junctions (see, e.g., [1.x.31]). Furthermore, from the mathematical standpoint, since the sine-Gordon equation is "completely integrable," it is a candidate for study using the usual methods such as the inverse scattering transform. Consequently, over the years, many interesting solitary-wave, and even stationary, solutions to the sine-Gordon equation have been found. In these solutions, particles correspond to localized features. For more on the sine-Gordon equation, the inverse scattering transform and other methods for finding analytical soliton equations, the reader should consult the following "classical" references on the subject: G. L. Lamb's [1.x.32] (Chapter 5, Section 2) and G. B. Whitham's [1.x.33] (Chapter 17, Sections 10-13). 

 [2.x.3]  We will cover a separate nonlinear equation from quantum   mechanics, the Nonlinear Schr&ouml;dinger Equation, in  [2.x.4] . 

[1.x.34][1.x.35] 

The sine-Gordon initial-boundary-value problem (IBVP) we wish to solve consists of the following equations: [1.x.36] It is a nonlinear equation similar to the wave equation we discussed in  [2.x.5]  and  [2.x.6] . We have chosen to enforce zero Neumann boundary conditions in order for waves to reflect off the boundaries of our domain. It should be noted, however, that Dirichlet boundary conditions are not appropriate for this problem. Even though the solutions to the sine-Gordon equation are localized, it only makes sense to specify (Dirichlet) boundary conditions at  [2.x.7] , otherwise either a solution does not exist or only the trivial solution  [2.x.8]  exists. 

However, the form of the equation above is not ideal for numerical discretization. If we were to discretize the second-order time derivative directly and accurately, then  we would need a large stencil (i.e., several time steps would need to be kept in the memory), which could become expensive. Therefore, in complete analogy to what we did in  [2.x.9]  and  [2.x.10] , we split the second-order (in time) sine-Gordon equation into a system of two first-order (in time) equations, which we call the split, or velocity, formulation. To this end, by setting  [2.x.11] , it is easy to see that the sine-Gordon equation is equivalent to [1.x.37] 

[1.x.38][1.x.39] 

Now, we can discretize the split formulation in time using the  [2.x.12] -method, which has a stencil of only two time steps. By choosing a  [2.x.13] , the latter discretization allows us to choose from a continuum of schemes. In particular, if we pick  [2.x.14]  or  [2.x.15] , we obtain the first-order accurate explicit or implicit Euler method, respectively. Another important choice is  [2.x.16] , which gives the second-order accurate Crank-Nicolson scheme. Henceforth, a superscript  [2.x.17]  denotes the values of the variables at the  [2.x.18]  time step, i.e. at  [2.x.19] , where  [2.x.20]  is the (fixed) time step size. Thus, the split formulation of the time-discretized sine-Gordon equation becomes [1.x.40] 

We can simplify the latter via a bit of algebra. Eliminating  [2.x.21]  from the first equation and rearranging, we obtain [1.x.41] 

It may seem as though we can just proceed to discretize the equations in space at this point. While this is true for the second equation (which is linear in  [2.x.22] ), this would not work for all  [2.x.23]  since the first equation above is nonlinear. Therefore, a nonlinear solver must be implemented, then the equations can be discretized in space and solved. 

To this end, we can use Newton's method. Given the nonlinear equation  [2.x.24] , we produce successive approximations to  [2.x.25]  as follows: [1.x.42] The iteration can be initialized with the old time step, i.e.  [2.x.26] , and eventually it will produce a solution to the first equation of the split formulation (see above). For the time discretization of the sine-Gordon equation under consideration here, we have that [1.x.43] Notice that while  [2.x.27]  is a function,  [2.x.28]  is an operator. 

[1.x.44][1.x.45] 

With hindsight, we choose both the solution and the test space to be  [2.x.29] . Hence, multiplying by a test function  [2.x.30]  and integrating, we obtain the following variational (or weak) formulation of the split formulation (including the nonlinear solver for the first equation) at each time step: [1.x.46] Note that the we have used integration by parts and the zero Neumann boundary conditions on all terms involving the Laplacian operator. Moreover,  [2.x.31]  and  [2.x.32]  are as defined above, and  [2.x.33]  denotes the usual  [2.x.34]  inner product over the domain  [2.x.35] , i.e.  [2.x.36] . Finally, notice that the first equation is, in fact, the definition of an iterative procedure, so it is solved multiple times during each time step until a stopping criterion is met. 

[1.x.47][1.x.48] 

Using the Finite Element Method, we discretize the variational formulation in space. To this end, let  [2.x.37]  be a finite-dimensional  [2.x.38] -conforming finite element space ( [2.x.39] ) with nodal basis  [2.x.40] . Now, we can expand all functions in the weak formulation (see above) in terms of the nodal basis. Henceforth, we shall denote by a capital letter the vector of coefficients (in the nodal basis) of a function denoted by the same letter in lower case; e.g.,  [2.x.41]  where  [2.x.42]  and  [2.x.43] . Thus, the finite-dimensional version of the variational formulation requires that we solve the following matrix equations at each time step: [1.x.49] 

Above, the matrix  [2.x.44]  and the vector  [2.x.45]  denote the discrete versions of the gadgets discussed above, i.e., [1.x.50] Again, note that the first matrix equation above is, in fact, the definition of an iterative procedure, so it is solved multiple times until a stopping criterion is met. Moreover,  [2.x.46]  is the mass matrix, i.e.  [2.x.47] ,  [2.x.48]  is the Laplace matrix, i.e.  [2.x.49] ,  [2.x.50]  is the nonlinear term in the equation that defines our auxiliary velocity variable, i.e.  [2.x.51] , and  [2.x.52]  is the nonlinear term in the Jacobian matrix of  [2.x.53] , i.e.  [2.x.54] . 

What solvers can we use for the first equation? Let's look at the matrix we have to invert: [1.x.51] for some  [2.x.55]  that depends on the present and previous solution. First, note that the matrix is symmetric. In addition, if the time step  [2.x.56]  is small enough, i.e. if  [2.x.57] , then the matrix is also going to be positive definite. In the program below, this will always be the case, so we will use the Conjugate Gradient method together with the SSOR method as preconditioner. We should keep in mind, however, that this will fail if we happen to use a bigger time step. Fortunately, in that case the solver will just throw an exception indicating a failure to converge, rather than silently producing a wrong result. If that happens, then we can simply replace the CG method by something that can handle indefinite symmetric systems. The GMRES solver is typically the standard method for all "bad" linear systems, but it is also a slow one. Possibly better would be a solver that utilizes the symmetry, such as, for example, SymmLQ, which is also implemented in deal.II. 

This program uses a clever optimization over  [2.x.58]  and  [2.x.59]  " [2.x.60] ": If you read the above formulas closely, it becomes clear that the velocity  [2.x.61]  only ever appears in products with the mass matrix. In  [2.x.62]  and  [2.x.63] , we were, therefore, a bit wasteful: in each time step, we would solve a linear system with the mass matrix, only to multiply the solution of that system by  [2.x.64]  again in the next time step. This can, of course, be avoided, and we do so in this program. 


[1.x.52][1.x.53] 


There are a few analytical solutions for the sine-Gordon equation, both in 1D and 2D. In particular, the program as is computes the solution to a problem with a single kink-like solitary wave initial condition.  This solution is given by Leibbrandt in \e Phys. \e Rev. \e Lett. \b 41(7), and is implemented in the  [2.x.65]  class. 

It should be noted that this closed-form solution, strictly speaking, only holds for the infinite-space initial-value problem (not the Neumann initial-boundary-value problem under consideration here). However, given that we impose \e zero Neumann boundary conditions, we expect that the solution to our initial-boundary-value problem would be close to the solution of the infinite-space initial-value problem, if reflections of waves off the boundaries of our domain do \e not occur. In practice, this is of course not the case, but we can at least assume that this were so. 

The constants  [2.x.66]  and  [2.x.67]  in the 2D solution and  [2.x.68] ,  [2.x.69]  and  [2.x.70]  in the 3D solution are called the B&auml;cklund transformation parameters. They control such things as the orientation and steepness of the kink. For the purposes of testing the code against the exact solution, one should choose the parameters so that the kink is aligned with the grid. 

The solutions that we implement in the  [2.x.71]  class are these:  [2.x.72]     [2.x.73] In 1D:   [1.x.54]   where we choose  [2.x.74] . 

  In 1D, more interesting analytical solutions are known. Many of them are   listed on http://mathworld.wolfram.com/Sine-GordonEquation.html . 

   [2.x.75] In 2D:   [1.x.55]   where  [2.x.76]  is defined as   [1.x.56]   and where we choose  [2.x.77] . 

   [2.x.78] In 3D:   [1.x.57]   where  [2.x.79]  is defined as   [1.x.58]   and where we choose  [2.x.80] .  [2.x.81]  


Since it makes it easier to play around, the  [2.x.82]  class that is used to set &mdash; surprise! &mdash; the initial values of our simulation simply queries the class that describes the exact solution for the value at the initial time, rather than duplicating the effort to implement a solution function. [1.x.59] [1.x.60] 


[1.x.61]  [1.x.62] 




For an explanation of the include files, the reader should refer to the example programs  [2.x.83]  through  [2.x.84] . They are in the standard order, which is  [2.x.85]  --  [2.x.86]  (since each of these categories roughly builds upon previous ones), then a few C++ headers for file input/output and string streams. 

[1.x.63] 



The last step is as in all previous programs: 

[1.x.64] 




[1.x.65]  [1.x.66] 




The entire algorithm for solving the problem is encapsulated in this class. As in previous example programs, the class is declared with a template parameter, which is the spatial dimension, so that we can solve the sine-Gordon equation in one, two or three spatial dimensions. For more on the dimension-independent class-encapsulation of the problem, the reader should consult  [2.x.87]  and  [2.x.88] .    


Compared to  [2.x.89]  and  [2.x.90] , there isn't anything newsworthy in the general structure of the program (though there is of course in the inner workings of the various functions!). The most notable difference is the presence of the two new functions  [2.x.91]  and  [2.x.92]  that compute the nonlinear contributions to the system matrix and right-hand side of the first equation, as discussed in the Introduction. In addition, we have to have a vector  [2.x.93]  that contains the nonlinear update to the solution vector in each Newton step.    


As also mentioned in the introduction, we do not store the velocity variable in this program, but the mass matrix times the velocity. This is done in the  [2.x.94]  variable (the "x" is intended to stand for "times").    


Finally, the  [2.x.95]  variable stores the number of time steps to be taken each time before graphical output is to be generated. This is of importance when using fine meshes (and consequently small time steps) where we would run lots of time steps and create lots of output files of solutions that look almost the same in subsequent files. This only clogs up our visualization procedures and we should avoid creating more output than we are really interested in. Therefore, if this variable is set to a value  [2.x.96]  bigger than one, output is generated only every  [2.x.97] th time step. 

[1.x.67] 




[1.x.68]  [1.x.69] 




In the following two classes, we first implement the exact solution for 1D, 2D, and 3D mentioned in the introduction to this program. This space-time solution may be of independent interest if one wanted to test the accuracy of the program by comparing the numerical against the analytic solution (note however that the program uses a finite domain, whereas these are analytic solutions for an unbounded domain). This may, for example, be done using the  [2.x.98]  function. Note, again (as was already discussed in  [2.x.99] ), how we describe space-time functions as spatial functions that depend on a time variable that can be set and queried using the  [2.x.100]  and  [2.x.101]  member functions of the FunctionTime base class of the Function class. 

[1.x.70] 



In the second part of this section, we provide the initial conditions. We are lazy (and cautious) and don't want to implement the same functions as above a second time. Rather, if we are queried for initial conditions, we create an object  [2.x.102] , set it to the correct time, and let it compute whatever values the exact solution has at that time: 

[1.x.71] 




[1.x.72]  [1.x.73] 




Let's move on to the implementation of the main class, as it implements the algorithm outlined in the introduction. 





[1.x.74]  [1.x.75] 




This is the constructor of the  [2.x.103]  class. It specifies the desired polynomial degree of the finite elements, associates a  [2.x.104]  object (just as in the example programs  [2.x.105]  and  [2.x.106] ), initializes the current or initial time, the final time, the time step size, and the value of  [2.x.107]  for the time stepping scheme. Since the solutions we compute here are time-periodic, the actual value of the start-time doesn't matter, and we choose it so that we start at an interesting time.    


Note that if we were to chose the explicit Euler time stepping scheme ( [2.x.108] ), then we must pick a time step  [2.x.109] , otherwise the scheme is not stable and oscillations might arise in the solution. The Crank-Nicolson scheme ( [2.x.110] ) and the implicit Euler scheme ( [2.x.111] ) do not suffer from this deficiency, since they are unconditionally stable. However, even then the time step should be chosen to be on the order of  [2.x.112]  in order to obtain a good solution. Since we know that our mesh results from the uniform subdivision of a rectangle, we can compute that time step easily; if we had a different domain, the technique in  [2.x.113]  using  [2.x.114]  would work as well. 

[1.x.76] 




[1.x.77]  [1.x.78] 




This function creates a rectangular grid in  [2.x.115]  dimensions and refines it several times. Also, all matrix and vector members of the  [2.x.116]  class are initialized to their appropriate sizes once the degrees of freedom have been assembled. Like  [2.x.117] , we use  [2.x.118]  functions to generate a mass matrix  [2.x.119]  and a Laplace matrix  [2.x.120]  and store them in the appropriate variables for the remainder of the program's life. 

[1.x.79] 




[1.x.80]  [1.x.81] 




This function assembles the system matrix and right-hand side vector for each iteration of Newton's method. The reader should refer to the Introduction for the explicit formulas for the system matrix and right-hand side.    


Note that during each time step, we have to add up the various contributions to the matrix and right hand sides. In contrast to  [2.x.121]  and  [2.x.122] , this requires assembling a few more terms, since they depend on the solution of the previous time step or previous nonlinear step. We use the functions  [2.x.123]  and  [2.x.124]  to do this, while the present function provides the top-level logic. 

[1.x.82] 



First we assemble the Jacobian matrix  [2.x.125] , where  [2.x.126]  is stored in the vector  [2.x.127]  for convenience. 

[1.x.83] 



Next we compute the right-hand side vector. This is just the combination of matrix-vector products implied by the description of  [2.x.128]  in the introduction. 

[1.x.84] 




[1.x.85]  [1.x.86] 




This function computes the vector  [2.x.129] , which appears in the nonlinear term in both equations of the split formulation. This function not only simplifies the repeated computation of this term, but it is also a fundamental part of the nonlinear iterative solver that we use when the time stepping is implicit (i.e.  [2.x.130] ). Moreover, we must allow the function to receive as input an "old" and a "new" solution. These may not be the actual solutions of the problem stored in  [2.x.131] , but are simply the two functions we linearize about. For the purposes of this function, let us call the first two arguments  [2.x.132]  and  [2.x.133]  in the documentation of this class below, respectively.    


As a side-note, it is perhaps worth investigating what order quadrature formula is best suited for this type of integration. Since  [2.x.134]  is not a polynomial, there are probably no quadrature formulas that can integrate these terms exactly. It is usually sufficient to just make sure that the right hand side is integrated up to the same order of accuracy as the discretization scheme is, but it may be possible to improve on the constant in the asymptotic statement of convergence by choosing a more accurate quadrature formula. 

[1.x.87] 



Once we re-initialize our  [2.x.135]  instantiation to the current cell, we make use of the  [2.x.136]  routine to get the values of the "old" data (presumably at  [2.x.137] ) and the "new" data (presumably at  [2.x.138] ) at the nodes of the chosen quadrature formula. 

[1.x.88] 



Now, we can evaluate  [2.x.139]  using the desired quadrature formula. 

[1.x.89] 



We conclude by adding up the contributions of the integrals over the cells to the global integral. 

[1.x.90] 




[1.x.91]  [1.x.92] 




This is the second function dealing with the nonlinear scheme. It computes the matrix  [2.x.140] , which appears in the nonlinear term in the Jacobian of  [2.x.141] . Just as  [2.x.142] , we must allow this function to receive as input an "old" and a "new" solution, which we again call  [2.x.143]  and  [2.x.144]  below, respectively. 

[1.x.93] 



Again, first we re-initialize our  [2.x.145]  instantiation to the current cell. 

[1.x.94] 



Then, we evaluate  [2.x.146]  using the desired quadrature formula. 

[1.x.95] 



Finally, we add up the contributions of the integrals over the cells to the global integral. 

[1.x.96] 




[1.x.97]  [1.x.98] 




As discussed in the Introduction, this function uses the CG iterative solver on the linear system of equations resulting from the finite element spatial discretization of each iteration of Newton's method for the (nonlinear) first equation of the split formulation. The solution to the system is, in fact,  [2.x.147]  so it is stored in  [2.x.148]  in the  [2.x.149]  function.    


Note that we re-set the solution update to zero before solving for it. This is not necessary: iterative solvers can start from any point and converge to the correct solution. If one has a good estimate about the solution of a linear system, it may be worthwhile to start from that vector, but as a general observation it is a fact that the starting point doesn't matter very much: it has to be a very, very good guess to reduce the number of iterations by more than a few. It turns out that for this problem, using the previous nonlinear update as a starting point actually hurts convergence and increases the number of iterations needed, so we simply set it to zero.    


The function returns the number of iterations it took to converge to a solution. This number will later be used to generate output on the screen showing how many iterations were needed in each nonlinear iteration. 

[1.x.99] 




[1.x.100]  [1.x.101] 




This function outputs the results to a file. It is pretty much identical to the respective functions in  [2.x.150]  and  [2.x.151] : 

[1.x.102] 




[1.x.103]  [1.x.104] 




This function has the top-level control over everything: it runs the (outer) time-stepping loop, the (inner) nonlinear-solver loop, and outputs the solution after each time step. 

[1.x.105] 



To acknowledge the initial condition, we must use the function  [2.x.152]  to compute  [2.x.153] . To this end, below we will create an object of type  [2.x.154] ; note that when we create this object (which is derived from the  [2.x.155]  class), we set its internal time variable to  [2.x.156] , to indicate that the initial condition is a function of space and time evaluated at  [2.x.157] .      


Then we produce  [2.x.158]  by projecting  [2.x.159]  onto the grid using  [2.x.160] . We have to use the same construct using hanging node constraints as in  [2.x.161] : the  [2.x.162]  function requires a hanging node constraints object, but to be used we first need to close it: 

[1.x.106] 



For completeness, we output the zeroth time step to a file just like any other time step. 

[1.x.107] 



Now we perform the time stepping: at every time step we solve the matrix equation(s) corresponding to the finite element discretization of the problem, and then advance our solution according to the time stepping formulas we discussed in the Introduction. 

[1.x.108] 



At the beginning of each time step we must solve the nonlinear equation in the split formulation via Newton's method --- i.e. solve for  [2.x.163]  then compute  [2.x.164]  and so on. The stopping criterion for this nonlinear iteration is that  [2.x.165] . Consequently, we need to record the norm of the residual in the first iteration.          


At the end of each iteration, we output to the console how many linear solver iterations it took us. When the loop below is done, we have (an approximation of)  [2.x.166] . 

[1.x.109] 



Upon obtaining the solution to the first equation of the problem at  [2.x.167] , we must update the auxiliary velocity variable  [2.x.168] . However, we do not compute and store  [2.x.169]  since it is not a quantity we use directly in the problem. Hence, for simplicity, we update  [2.x.170]  directly: 

[1.x.110] 



Oftentimes, in particular for fine meshes, we must pick the time step to be quite small in order for the scheme to be stable. Therefore, there are a lot of time steps during which "nothing interesting happens" in the solution. To improve overall efficiency -- in particular, speed up the program and save disk space -- we only output the solution every  [2.x.171]  time steps: 

[1.x.111] 




[1.x.112]  [1.x.113] 




This is the main function of the program. It creates an object of top-level class and calls its principal function. If exceptions are thrown during the execution of the run method of the  [2.x.172]  class, we catch and report them here. For more information about exceptions the reader should consult  [2.x.173] . 

[1.x.114] 

[1.x.115][1.x.116] 

The explicit Euler time stepping scheme  ( [2.x.174] ) performs adequately for the problems we wish to solve. Unfortunately, a rather small time step has to be chosen due to stability issues ---  [2.x.175]  appears to work for most the simulations we performed. On the other hand, the Crank-Nicolson scheme ( [2.x.176] ) is unconditionally stable, and (at least for the case of the 1D breather) we can pick the time step to be as large as  [2.x.177]  without any ill effects on the solution. The implicit Euler scheme ( [2.x.178] ) is "exponentially damped," so it is not a good choice for solving the sine-Gordon equation, which is conservative. However, some of the damped schemes in the continuum that is offered by the  [2.x.179] -method were useful for eliminating spurious oscillations due to boundary effects. 

In the simulations below, we solve the sine-Gordon equation on the interval  [2.x.180]  in 1D and on the square  [2.x.181]  in 2D. In each case, the respective grid is refined uniformly 6 times, i.e.  [2.x.182] . 

[1.x.117][1.x.118] 

The first example we discuss is the so-called 1D (stationary) breather solution of the sine-Gordon equation. The breather has the following closed-form expression, as mentioned in the Introduction: [1.x.119] where  [2.x.183] ,  [2.x.184]  and  [2.x.185]  are constants. In the simulation below, we have chosen  [2.x.186] ,  [2.x.187] ,  [2.x.188] . Moreover, it is know that the period of oscillation of the breather is  [2.x.189] , hence we have chosen  [2.x.190]  and  [2.x.191]  so that we can observe three oscillations of the solution. Then, taking  [2.x.192] ,  [2.x.193]  and  [2.x.194] , the program computed the following solution. 

 [2.x.195]  

Though not shown how to do this in the program, another way to visualize the (1+1)-d solution is to use output generated by the DataOutStack class; it allows to "stack" the solutions of individual time steps, so that we get 2D space-time graphs from 1D time-dependent solutions. This produces the space-time plot below instead of the animation above. 

 [2.x.196]  

Furthermore, since the breather is an analytical solution of the sine-Gordon equation, we can use it to validate our code, although we have to assume that the error introduced by our choice of Neumann boundary conditions is small compared to the numerical error. Under this assumption, one could use the  [2.x.197]  function to compute the difference between the numerical solution and the function described by the  [2.x.198]  class of this program. For the simulation shown in the two images above, the  [2.x.199]  norm of the error in the finite element solution at each time step remained on the order of  [2.x.200] . Hence, we can conclude that the numerical method has been implemented correctly in the program. 


[1.x.120][1.x.121] 


The only analytical solution to the sine-Gordon equation in (2+1)D that can be found in the literature is the so-called kink solitary wave. It has the following closed-form expression:   [1.x.122] with   [1.x.123] where  [2.x.201] ,  [2.x.202]  and  [2.x.203]  are constants. In the simulation below we have chosen  [2.x.204] . Notice that if  [2.x.205]  the kink is stationary, hence it would make a good solution against which we can validate the program in 2D because no reflections off the boundary of the domain occur. 

The simulation shown below was performed with  [2.x.206] ,  [2.x.207] ,  [2.x.208] ,  [2.x.209]  and  [2.x.210] . The  [2.x.211]  norm of the error of the finite element solution at each time step remained on the order of  [2.x.212] , showing that the program is working correctly in 2D, as well as 1D. Unfortunately, the solution is not very interesting, nonetheless we have included a snapshot of it below for completeness. 

 [2.x.213]  

Now that we have validated the code in 1D and 2D, we move to a problem where the analytical solution is unknown. 

To this end, we rotate the kink solution discussed above about the  [2.x.214]  axis: we let   [2.x.215] . The latter results in a solitary wave that is not aligned with the grid, so reflections occur at the boundaries of the domain immediately. For the simulation shown below, we have taken  [2.x.216] ,  [2.x.217] ,  [2.x.218] ,  [2.x.219]  and  [2.x.220] . Moreover, we had to pick  [2.x.221]  because for any  [2.x.222]  oscillations arose at the boundary, which are likely due to the scheme and not the equation, thus picking a value of  [2.x.223]  a good bit into the "exponentially damped" spectrum of the time stepping schemes assures these oscillations are not created. 

 [2.x.224]  

Another interesting solution to the sine-Gordon equation (which cannot be obtained analytically) can be produced by using two 1D breathers to construct the following separable 2D initial condition: [1.x.124] where  [2.x.225] ,  [2.x.226]  as in the 1D case we discussed above. For the simulation shown below, we have chosen  [2.x.227] ,  [2.x.228] ,  [2.x.229]  and  [2.x.230] . The solution is pretty interesting 

--- it acts like a breather (as far as the pictures are concerned); however, it appears to break up and reassemble, rather than just oscillate. 

 [2.x.231]  


[1.x.125] [1.x.126][1.x.127] 


It is instructive to change the initial conditions. Most choices will not lead to solutions that stay localized (in the soliton community, such solutions are called "stationary", though the solution does change with time), but lead to solutions where the wave-like character of the equation dominates and a wave travels away from the location of a localized initial condition. For example, it is worth playing around with the  [2.x.232]  class, by replacing the call to the  [2.x.233]  class by something like this function: [1.x.128] if  [2.x.234] , and  [2.x.235]  outside this region. 

A second area would be to investigate whether the scheme is energy-preserving. For the pure wave equation, discussed in  [2.x.236]  " [2.x.237] ", this is the case if we choose the time stepping parameter such that we get the Crank-Nicolson scheme. One could do a similar thing here, noting that the energy in the sine-Gordon solution is defined as [1.x.129] (We use  [2.x.238]  instead of  [2.x.239]  in the formula to ensure that all contributions to the energy are positive, and so that decaying solutions have finite energy on unbounded domains.) 

Beyond this, there are two obvious areas: 

- Clearly, adaptivity (i.e. time-adaptive grids) would be of interest   to problems like these. Their complexity leads us to leave this out   of this program again, though the general comments in the   introduction of  [2.x.240]  " [2.x.241] " remain true. 

- Faster schemes to solve this problem. While computers today are   plenty fast enough to solve 2d and, frequently, even 3d stationary   problems within not too much time, time dependent problems present   an entirely different class of problems. We address this topic in    [2.x.242]  where we show how to solve this problem in parallel and   without assembling or inverting any matrix at all. [1.x.130] [1.x.131]  [2.x.243]  

 [2.x.244] 
