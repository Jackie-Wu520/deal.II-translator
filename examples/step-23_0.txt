 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22] 

[1.x.23] [1.x.24][1.x.25] 


 [2.x.3]  

This is the first of a number of tutorial programs that will finally cover "real" time-dependent problems, not the slightly odd form of time dependence found in  [2.x.4]  or the DAE model of  [2.x.5] . In particular, this program introduces the wave equation in a bounded domain. Later,  [2.x.6]  will consider an example of absorbing boundary conditions, and  [2.x.7]  " [2.x.8] " a kind of nonlinear wave equation producing solutions called solitons. 

The wave equation in its prototypical form reads as follows: find  [2.x.9]  that satisfies [1.x.26] 

Note that since this is an equation with second-order time derivatives, we need to pose two initial conditions, one for the value and one for the time derivative of the solution. 

Physically, the equation describes the motion of an elastic medium. In 2-d, one can think of how a membrane moves if subjected to a force. The Dirichlet boundary conditions above indicate that the membrane is clamped at the boundary at a height  [2.x.10]  (this height might be moving as well &mdash; think of people holding a blanket and shaking it up and down). The first initial condition equals the initial deflection of the membrane, whereas the second one gives its velocity. For example, one could think of pushing the membrane down with a finger and then letting it go at  [2.x.11]  (nonzero deflection but zero initial velocity), or hitting it with a hammer at  [2.x.12]  (zero deflection but nonzero velocity). Both cases would induce motion in the membrane. 


[1.x.27][1.x.28] 


[1.x.29][1.x.30] 

There is a long-standing debate in the numerical analysis community over whether a discretization of time dependent equations should involve first discretizing the time variable leading to a stationary PDE at each time step that is then solved using standard finite element techniques (this is called the Rothe method), or whether one should first discretize the spatial variables, leading to a large system of ordinary differential equations that can then be handled by one of the usual ODE solvers (this is called the method of lines). 

Both of these methods have advantages and disadvantages. Traditionally, people have preferred the method of lines, since it allows to use the very well developed machinery of high-order ODE solvers available for the rather stiff ODEs resulting from this approach, including step length control and estimation of the temporal error. 

On the other hand, Rothe's method becomes awkward when using higher-order time stepping method, since one then has to write down a PDE that couples the solution of the present time step not only with that at the previous time step, but possibly also even earlier solutions, leading to a significant number of terms. 

For these reasons, the method of lines was the method of choice for a long time. However, it has one big drawback: if we discretize the spatial variable first, leading to a large ODE system, we have to choose a mesh once and for all. If we are willing to do this, then this is a legitimate and probably superior approach. 

If, on the other hand, we are looking at the wave equation and many other time dependent problems, we find that the character of a solution changes as time progresses. For example, for the wave equation, we may have a single wave travelling through the domain, where the solution is smooth or even constant in front of and behind the wave &mdash; adaptivity would be really useful for such cases, but the key is that the area where we need to refine the mesh changes from time step to time step! 

If we intend to go that way, i.e. choose a different mesh for each time step (or set of time steps), then the method of lines is not appropriate any more: instead of getting one ODE system with a number of variables equal to the number of unknowns in the finite element mesh, our number of unknowns now changes all the time, a fact that standard ODE solvers are certainly not prepared to deal with at all. On the other hand, for the Rothe method, we just get a PDE for each time step that we may choose to discretize independently of the mesh used for the previous time step; this approach is not without perils and difficulties, but at least is a sensible and well-defined procedure. 

For all these reasons, for the present program, we choose to use the Rothe method for discretization, i.e. we first discretize in time and then in space. We will not actually use adaptive meshes at all, since this involves a large amount of additional code, but we will comment on this some more in the [1.x.31]. 


[1.x.32][1.x.33] 


Given these considerations, here is how we will proceed: let us first define a simple time stepping method for this second order problem, and then in a second step do the spatial discretization, i.e. we will follow Rothe's approach. 

For the first step, let us take a little detour first: in order to discretize a second time derivative, we can either discretize it directly, or we can introduce an additional variable and transform the system into a first order system. In many cases, this turns out to be equivalent, but dealing with first order systems is often simpler. To this end, let us introduce [1.x.34] and call this variable the [1.x.35] for obvious reasons. We can then reformulate the original wave equation as follows: [1.x.36] 

The advantage of this formulation is that it now only contains first time derivatives for both variables, for which it is simple to write down time stepping schemes. Note that we do not have boundary conditions for  [2.x.13]  at first. However, we could enforce  [2.x.14]  on the boundary. It turns out in numerical examples that this is actually necessary: without doing so the solution doesn't look particularly wrong, but the Crank-Nicolson scheme does not conserve energy if one doesn't enforce these boundary conditions. 

With this formulation, let us introduce the following time discretization where a superscript  [2.x.15]  indicates the number of a time step and  [2.x.16]  is the length of the present time step: [1.x.37] Note how we introduced a parameter  [2.x.17]  here. If we chose  [2.x.18] , for example, the first equation would reduce to  [2.x.19] , which is well-known as the forward or explicit Euler method. On the other hand, if we set  [2.x.20] , then we would get  [2.x.21] , which corresponds to the backward or implicit Euler method. Both these methods are first order accurate methods. They are simple to implement, but they are not really very accurate. 

The third case would be to choose  [2.x.22] . The first of the equations above would then read  [2.x.23] . This method is known as the Crank-Nicolson method and has the advantage that it is second order accurate. In addition, it has the nice property that it preserves the energy in the solution (physically, the energy is the sum of the kinetic energy of the particles in the membrane plus the potential energy present due to the fact that it is locally stretched; this quantity is a conserved one in the continuous equation, but most time stepping schemes do not conserve it after time discretization). Since  [2.x.24]  also appears in the equation for  [2.x.25] , the Crank-Nicolson scheme is also implicit. 

In the program, we will leave  [2.x.26]  as a parameter, so that it will be easy to play with it. The results section will show some numerical evidence comparing the different schemes. 

The equations above (called the [1.x.38] equations because we have only discretized the time, but not space), can be simplified a bit by eliminating  [2.x.27]  from the first equation and rearranging terms. We then get [1.x.39] In this form, we see that if we are given the solution  [2.x.28]  of the previous timestep, that we can then solve for the variables  [2.x.29]  separately, i.e. one at a time. This is convenient. In addition, we recognize that the operator in the first equation is positive definite, and the second equation looks particularly simple. 


[1.x.40][1.x.41] 


We have now derived equations that relate the approximate (semi-discrete) solution  [2.x.30]  and its time derivative  [2.x.31]  at time  [2.x.32]  with the solutions  [2.x.33]  of the previous time step at  [2.x.34] . The next step is to also discretize the spatial variable using the usual finite element methodology. To this end, we multiply each equation with a test function, integrate over the entire domain, and integrate by parts where necessary. This leads to [1.x.42] 

It is then customary to approximate  [2.x.35] , where  [2.x.36]  are the shape functions used for the discretization of the  [2.x.37] -th time step and  [2.x.38]  are the unknown nodal values of the solution. Similarly,  [2.x.39] . Finally, we have the solutions of the previous time step,  [2.x.40]  and  [2.x.41] . Note that since the solution of the previous time step has already been computed by the time we get to time step  [2.x.42] ,  [2.x.43]  are known. Furthermore, note that the solutions of the previous step may have been computed on a different mesh, so we have to use shape functions  [2.x.44] . 

If we plug these expansions into above equations and test with the test functions from the present mesh, we get the following linear system: [1.x.43] where [1.x.44] 



If we solve these two equations, we can move the solution one step forward and go on to the next time step. 

It is worth noting that if we choose the same mesh on each time step (as we will in fact do in the program below), then we have the same shape functions on time step  [2.x.45]  and  [2.x.46] , i.e.  [2.x.47] . Consequently, we get  [2.x.48]  and  [2.x.49] . On the other hand, if we had used different shape functions, then we would have to compute integrals that contain shape functions defined on two meshes. This is a somewhat messy process that we omit here, but that is treated in some detail in  [2.x.50] . 

Under these conditions (i.e. a mesh that doesn't change), one can optimize the solution procedure a bit by basically eliminating the solution of the second linear system. We will discuss this in the introduction of the  [2.x.51]  " [2.x.52] " program. 

[1.x.45][1.x.46] 


One way to compare the quality of a time stepping scheme is to see whether the numerical approximation preserves conservation properties of the continuous equation. For the wave equation, the natural quantity to look at is the energy. By multiplying the wave equation by  [2.x.53] , integrating over  [2.x.54] , and integrating by parts where necessary, we find that [1.x.47] By consequence, in absence of body forces and constant boundary values, we get that [1.x.48] is a conserved quantity, i.e. one that doesn't change with time. We will compute this quantity after each time step. It is straightforward to see that if we replace  [2.x.55]  by its finite element approximation, and  [2.x.56]  by the finite element approximation of the velocity  [2.x.57] , then [1.x.49] As we will see in the results section, the Crank-Nicolson scheme does indeed conserve the energy, whereas neither the forward nor the backward Euler scheme do. 


[1.x.50][1.x.51] 


One of the reasons why the wave equation is nasty to solve numerically is that explicit time discretizations are only stable if the time step is small enough. In particular, it is coupled to the spatial mesh width  [2.x.58] . For the lowest order discretization we use here, the relationship reads [1.x.52] where  [2.x.59]  is the wave speed, which in our formulation of the wave equation has been normalized to one. Consequently, unless we use the implicit schemes with  [2.x.60] , our solutions will not be numerically stable if we violate this restriction. Implicit schemes do not have this restriction for stability, but they become inaccurate if the time step is too large. 

This condition was first recognized by Courant, Friedrichs, and Lewy &mdash; in 1928, long before computers became available for numerical computations! (This result appeared in the German language article R. Courant, K. Friedrichs and H. Lewy: [1.x.53], Mathematische Annalen, vol. 100, no. 1, pages 32-74, 1928.) This condition on the time step is most frequently just referred to as the [1.x.54] condition. Intuitively, the CFL condition says that the time step must not be larger than the time it takes a wave to cross a single cell. 

In the program, we will refine the square  [2.x.61]  seven times uniformly, giving a mesh size of  [2.x.62] , which is what we set the time step to. The fact that we set the time step and mesh size individually in two different places is error prone: it is too easy to refine the mesh once more but forget to also adjust the time step.  [2.x.63]  " [2.x.64] " shows a better way how to keep these things in sync. 


[1.x.55][1.x.56] 


Although the program has all the hooks to deal with nonzero initial and boundary conditions and body forces, we take a simple case where the domain is a square  [2.x.65]  and [1.x.57] 

This corresponds to a membrane initially at rest and clamped all around, where someone is waving a part of the clamped boundary once up and down, thereby shooting a wave into the domain. [1.x.58] [1.x.59] 


[1.x.60]  [1.x.61] 




We start with the usual assortment of include files that we've seen in so many of the previous tests: 

[1.x.62] 



Here are the only three include files of some new interest: The first one is already used, for example, for the  [2.x.66]  and  [2.x.67]  functions. However, we here use another function in that class,  [2.x.68]  to compute our initial values as the  [2.x.69]  projection of the continuous initial values. Furthermore, we use  [2.x.70]  to generate the integrals  [2.x.71] . These were previously always generated by hand in  [2.x.72]  or similar functions in application code. However, we're too lazy to do that here, so simply use a library function: 

[1.x.63] 



In a very similar vein, we are also too lazy to write the code to assemble mass and Laplace matrices, although it would have only taken copying the relevant code from any number of previous tutorial programs. Rather, we want to focus on the things that are truly new to this program and therefore use the  [2.x.73]  and  [2.x.74]  functions. They are declared here: 

[1.x.64] 



Finally, here is an include file that contains all sorts of tool functions that one sometimes needs. In particular, we need the  [2.x.75]  class that, given an integer argument, returns a string representation of it. It is particularly useful since it allows for a second parameter indicating the number of digits to which we want the result padded with leading zeros. We will use this to write output files that have the form  [2.x.76]  denotes the number of the time step and always consists of three digits even if we are still in the single or double digit time steps. 

[1.x.65] 



The last step is as in all previous programs: 

[1.x.66] 




[1.x.67]  [1.x.68] 




Next comes the declaration of the main class. It's public interface of functions is like in most of the other tutorial programs. Worth mentioning is that we now have to store four matrices instead of one: the mass matrix  [2.x.77] , the Laplace matrix  [2.x.78] , the matrix  [2.x.79]  used for solving for  [2.x.80] , and a copy of the mass matrix with boundary conditions applied used for solving for  [2.x.81] . Note that it is a bit wasteful to have an additional copy of the mass matrix around. We will discuss strategies for how to avoid this in the section on possible improvements.    


Likewise, we need solution vectors for  [2.x.82]  as well as for the corresponding vectors at the previous time step,  [2.x.83] . The  [2.x.84]  will be used for whatever right hand side vector we have when solving one of the two linear systems in each time step. These will be solved in the two functions  [2.x.85]  and  [2.x.86] .    


Finally, the variable  [2.x.87]  is used to indicate the parameter  [2.x.88]  that is used to define which time stepping scheme to use, as explained in the introduction. The rest is self-explanatory. 

[1.x.69] 




[1.x.70]  [1.x.71] 




Before we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial and boundary values for both the solution  [2.x.89]  and its time derivative  [2.x.90] , as well as a right hand side class. We do so using classes derived from the Function class template that has been used many times before, so the following should not be a surprise.    


Let's start with initial values and choose zero for both the value  [2.x.91]  as well as its time derivative, the velocity  [2.x.92] : 

[1.x.72] 



Secondly, we have the right hand side forcing term. Boring as we are, we choose zero here as well: 

[1.x.73] 



Finally, we have boundary values for  [2.x.93]  and  [2.x.94] . They are as described in the introduction, one being the time derivative of the other: 

[1.x.74] 




[1.x.75]  [1.x.76] 




The implementation of the actual logic is actually fairly short, since we relegate things like assembling the matrices and right hand side vectors to the library. The rest boils down to not much more than 130 lines of actual code, a significant fraction of which is boilerplate code that can be taken from previous example programs (e.g. the functions that solve linear systems, or that generate output).    


Let's start with the constructor (for an explanation of the choice of time step, see the section on Courant, Friedrichs, and Lewy in the introduction): 

[1.x.77] 




[1.x.78]  [1.x.79] 




The next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to  [2.x.95] : 

[1.x.80] 



Then comes a block where we have to initialize the 3 matrices we need in the course of the program: the mass matrix, the Laplace matrix, and the matrix  [2.x.96]  used when solving for  [2.x.97]  in each time step.      


When setting up these matrices, note that they all make use of the same sparsity pattern object. Finally, the reason why matrices and sparsity patterns are separate objects in deal.II (unlike in many other finite element or linear algebra classes) becomes clear: in a significant fraction of applications, one has to hold several matrices that happen to have the same sparsity pattern, and there is no reason for them not to share this information, rather than re-building and wasting memory on it several times.      


After initializing all of these matrices, we call library functions that build the Laplace and mass matrices. All they need is a DoFHandler object and a quadrature formula object that is to be used for numerical integration. Note that in many respects these functions are better than what we would usually do in application programs, for example because they automatically parallelize building the matrices if multiple processors are available in a machine: for more information see the documentation of WorkStream or the  [2.x.98]  "Parallel computing with multiple processors" module. The matrices for solving linear systems will be filled in the run() method because we need to re-apply boundary conditions every time step. 

[1.x.81] 



The rest of the function is spent on setting vector sizes to the correct value. The final line closes the hanging node constraints object. Since we work on a uniformly refined mesh, no constraints exist or have been computed (i.e. there was no need to call  [2.x.99]  as in other programs), but we need a constraints object in one place further down below anyway. 

[1.x.82] 




[1.x.83]  [1.x.84] 




The next two functions deal with solving the linear systems associated with the equations for  [2.x.100]  and  [2.x.101] . Both are not particularly interesting as they pretty much follow the scheme used in all the previous tutorial programs.    


One can make little experiments with preconditioners for the two matrices we have to invert. As it turns out, however, for the matrices at hand here, using Jacobi or SSOR preconditioners reduces the number of iterations necessary to solve the linear system slightly, but due to the cost of applying the preconditioner it is no win in terms of run-time. It is not much of a loss either, but let's keep it simple and just do without: 

[1.x.85] 




[1.x.86]  [1.x.87] 




Likewise, the following function is pretty much what we've done before. The only thing worth mentioning is how here we generate a string representation of the time step number padded with leading zeros to 3 character length using the  [2.x.102]  function's second argument. 

[1.x.88] 



Like  [2.x.103] , since we write output at every time step (and the system we have to solve is relatively easy), we instruct DataOut to use the zlib compression algorithm that is optimized for speed instead of disk usage since otherwise plotting the output becomes a bottleneck: 

[1.x.89] 




[1.x.90]  [1.x.91] 




The following is really the only interesting function of the program. It contains the loop over all time steps, but before we get to that we have to set up the grid, DoFHandler, and matrices. In addition, we have to somehow get started with initial values. To this end, we use the  [2.x.104]  function that takes an object that describes a continuous function and computes the  [2.x.105]  projection of this function onto the finite element space described by the DoFHandler object. Can't be any simpler than that: 

[1.x.92] 



The next thing is to loop over all the time steps until we reach the end time ( [2.x.106]  in this case). In each time step, we first have to solve for  [2.x.107] , using the equation  [2.x.108]   [2.x.109]   [2.x.110] . Note that we use the same mesh for all time steps, so that  [2.x.111]  and  [2.x.112] . What we therefore have to do first is to add up  [2.x.113]  and the forcing terms, and put the result into the  [2.x.114]  vector. (For these additions, we need a temporary vector that we declare before the loop to avoid repeated memory allocations in each time step.)      


The one thing to realize here is how we communicate the time variable to the object describing the right hand side: each object derived from the Function class has a time field that can be set using the  [2.x.115]  and read by  [2.x.116]  In essence, using this mechanism, all functions of space and time are therefore considered functions of space evaluated at a particular time. This matches well what we typically need in finite element programs, where we almost always work on a single time step at a time, and where it never happens that, for example, one would like to evaluate a space-time function for all times at any given spatial location. 

[1.x.93] 



After so constructing the right hand side vector of the first equation, all we have to do is apply the correct boundary values. As for the right hand side, this is a space-time function evaluated at a particular time, which we interpolate at boundary nodes and then use the result to apply boundary values as we usually do. The result is then handed off to the solve_u() function: 

[1.x.94] 



The matrix for solve_u() is the same in every time steps, so one could think that it is enough to do this only once at the beginning of the simulation. However, since we need to apply boundary values to the linear system (which eliminate some matrix rows and columns and give contributions to the right hand side), we have to refill the matrix in every time steps before we actually apply boundary data. The actual content is very simple: it is the sum of the mass matrix and a weighted Laplace matrix: 

[1.x.95] 



The second step, i.e. solving for  [2.x.117] , works similarly, except that this time the matrix on the left is the mass matrix (which we copy again in order to be able to apply boundary conditions, and the right hand side is  [2.x.118]  plus forcing terms. Boundary values are applied in the same way as before, except that now we have to use the BoundaryValuesV class: 

[1.x.96] 



Finally, after both solution components have been computed, we output the result, compute the energy in the solution, and go on to the next time step after shifting the present solution into the vectors that hold the solution at the previous time step. Note the function  [2.x.119]  that can compute  [2.x.120]  and  [2.x.121]  in one step, saving us the expense of a temporary vector and several lines of code: 

[1.x.97] 




[1.x.98]  [1.x.99] 




What remains is the main function of the program. There is nothing here that hasn't been shown in several of the previous programs: 

[1.x.100] 

[1.x.101][1.x.102] 


When the program is run, it produces the following output: 

[1.x.103] 



What we see immediately is that the energy is a constant at least after  [2.x.122]  (until which the boundary source term  [2.x.123]  is nonzero, injecting energy into the system). 

In addition to the screen output, the program writes the solution of each time step to an output file. If we process them adequately and paste them into a movie, we get the following: 

 [2.x.124]  

The movie shows the generated wave nice traveling through the domain and back, being reflected at the clamped boundary. Some numerical noise is trailing the wave, an artifact of a too-large mesh size that can be reduced by reducing the mesh width and the time step. 


[1.x.104] [1.x.105][1.x.106] 


If you want to explore a bit, try out some of the following things:  [2.x.125]     [2.x.126] Varying  [2.x.127] . This gives different time stepping schemes, some of   which are stable while others are not. Take a look at how the energy   evolves. 

   [2.x.128] Different initial and boundary conditions, right hand sides. 

   [2.x.129] More complicated domains or more refined meshes. Remember that the time   step needs to be bounded by the mesh width, so changing the mesh should   always involve also changing the time step. We will come back to this issue   in  [2.x.130] . 

   [2.x.131] Variable coefficients: In real media, the wave speed is often   variable. In particular, the "real" wave equation in realistic media would   read   [1.x.107]   where  [2.x.132]  is the density of the material, and  [2.x.133]  is related to the   stiffness coefficient. The wave speed is then  [2.x.134] . 

  To make such a change, we would have to compute the mass and Laplace   matrices with a variable coefficient. Fortunately, this isn't too hard: the   functions  [2.x.135]  and    [2.x.136]  have additional default parameters that can   be used to pass non-constant coefficient functions to them. The required   changes are therefore relatively small. On the other hand, care must be   taken again to make sure the time step is within the allowed range. 

   [2.x.137] In the in-code comments, we discussed the fact that the matrices for   solving for  [2.x.138]  and  [2.x.139]  need to be reset in every time because of   boundary conditions, even though the actual content does not change. It is   possible to avoid copying by not eliminating columns in the linear systems,   which is implemented by appending a  [2.x.140]  argument to the call:   [1.x.108] 



   [2.x.141] deal.II being a library that supports adaptive meshes it would of course be   nice if this program supported change the mesh every few time steps. Given the   structure of the solution &mdash; a wave that travels through the domain &mdash;   it would seem appropriate if we only refined the mesh where the wave currently is,   and not simply everywhere. It is intuitively clear that we should be able to   save a significant amount of cells this way. (Though upon further thought one   realizes that this is really only the case in the initial stages of the simulation.   After some time, for wave phenomena, the domain is filled with reflections of   the initial wave going in every direction and filling every corner of the domain.   At this point, there is in general little one can gain using local mesh   refinement.) 

  To make adaptively changing meshes possible, there are basically two routes.   The "correct" way would be to go back to the weak form we get using Rothe's   method. For example, the first of the two equations to be solved in each time   step looked like this:   [1.x.109]   Now, note that we solve for  [2.x.142]  on mesh  [2.x.143] , and   consequently the test functions  [2.x.144]  have to be from the space    [2.x.145]  as well. As discussed in the introduction, terms like    [2.x.146]  then require us to integrate the solution of the   previous step (which may have been computed on a different mesh    [2.x.147] ) against the test functions of the current mesh,   leading to a matrix  [2.x.148] . This process of integrating shape   functions from different meshes is, at best, awkward. It can be done   but because it is difficult to ensure that  [2.x.149]  and    [2.x.150]  differ by at most one level of refinement, one   has to recursively match cells from both meshes. It is feasible to   do this, but it leads to lengthy and not entirely obvious code. 

  The second approach is the following: whenever we change the mesh,   we simply interpolate the solution from the last time step on the old   mesh to the new mesh, using the SolutionTransfer class. In other words,   instead of the equation above, we would solve   [1.x.110]   where  [2.x.151]  interpolates a given function onto mesh  [2.x.152] .   This is a much simpler approach because, in each time step, we no   longer have to worry whether  [2.x.153]  were computed on the   same mesh as we are using now or on a different mesh. Consequently,   the only changes to the code necessary are the addition of a function   that computes the error, marks cells for refinement, sets up a   SolutionTransfer object, transfers the solution to the new mesh, and   rebuilds matrices and right hand side vectors on the new mesh. Neither   the functions building the matrices and right hand sides, nor the   solvers need to be changed. 

  While this second approach is, strictly speaking,   not quite correct in the Rothe framework (it introduces an addition source   of error, namely the interpolation), it is nevertheless what   almost everyone solving time dependent equations does. We will use this   method in  [2.x.154] , for example.  [2.x.155]  [1.x.111] [1.x.112]  [2.x.156]  

 [2.x.157] 
