 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33] 

[1.x.34] [1.x.35][1.x.36] 


 [2.x.3]  

This program is devoted to two aspects: the use of mixed finite elements -- in particular Raviart-Thomas elements -- and using block matrices to define solvers, preconditioners, and nested versions of those that use the substructure of the system matrix. The equation we are going to solve is again the Poisson equation, though with a matrix-valued coefficient: [1.x.37] 

 [2.x.4]  is assumed to be uniformly positive definite, i.e., there is  [2.x.5]  such that the eigenvalues  [2.x.6]  of  [2.x.7]  satisfy  [2.x.8] . The use of the symbol  [2.x.9]  instead of the usual  [2.x.10]  for the solution variable will become clear in the next section. 

After discussing the equation and the formulation we are going to use to solve it, this introduction will cover the use of block matrices and vectors, the definition of solvers and preconditioners, and finally the actual test case we are going to solve. 

We are going to extend this tutorial program in  [2.x.11]  to solve not only the mixed Laplace equation, but add another equation that describes the transport of a mixture of two fluids. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.12]  module. 


[1.x.38][1.x.39] 


In the form above, the Poisson equation (i.e., the Laplace equation with a nonzero right hand side) is generally considered a good model equation for fluid flow in porous media. Of course, one typically models fluid flow through the [1.x.40] or, if fluid velocities are slow or the viscosity is large, the [1.x.41] (which we cover in  [2.x.13] ). In the first of these two models, the forces that act are inertia and viscous friction, whereas in the second it is only viscous friction -- i.e., forces that one fluid particle exerts on a nearby one. This is appropriate if you have free flow in a large domain, say a pipe, a river, or in the air. On the other hand, if the fluid is confined in pores, then friction forces exerted by the pore walls on the fluid become more and more important and internal viscous friction becomes less and less important. Modeling this then first leads to the [1.x.42] if both effects are important, and in the limit of very small pores to the [1.x.43]. The latter is just a different name for the Poisson or Laplace equation, connotating it with the area to which one wants to apply it: slow flow in a porous medium. In essence it says that the velocity is proportional to the negative pressure gradient that drives the fluid through the porous medium. 

The Darcy equation models this pressure that drives the flow. (Because the solution variable is a pressure, we here use the name  [2.x.14]  instead of the name  [2.x.15]  more commonly used for the solution of partial differential equations.) Typical applications of this view of the Laplace equation are then modeling groundwater flow, or the flow of hydrocarbons in oil reservoirs. In these applications,  [2.x.16]  is the permeability tensor, i.e., a measure for how much resistance the soil or rock matrix asserts on the fluid flow. 

In the applications named above, a desirable feature for a numerical scheme is that it should be locally conservative, i.e., that whatever flows into a cell also flows out of it (or the difference is equal to the integral over the source terms over each cell, if the sources are nonzero). However, as it turns out, the usual discretizations of the Laplace equation (such as those used in  [2.x.17] ,  [2.x.18] , or  [2.x.19] ) do not satisfy this property. But, one can achieve this by choosing a different formulation of the problem and a particular combination of finite element spaces. 


[1.x.44][1.x.45] 


To this end, one first introduces a second variable, called the velocity,  [2.x.20] . By its definition, the velocity is a vector in the negative direction of the pressure gradient, multiplied by the permeability tensor. If the permeability tensor is proportional to the unit matrix, this equation is easy to understand and intuitive: the higher the permeability, the higher the velocity; and the velocity is proportional to the gradient of the pressure, going from areas of high pressure to areas of low pressure (thus the negative sign). 

With this second variable, one then finds an alternative version of the Laplace equation, called the [1.x.46]: [1.x.47] 

Here, we have multiplied the equation defining the velocity  [2.x.21]  by  [2.x.22]  because this makes the set of equations symmetric: one of the equations has the gradient, the second the negative divergence, and these two are of course adjoints of each other, resulting in a symmetric bilinear form and a consequently symmetric system matrix under the common assumption that  [2.x.23]  is a symmetric tensor. 

The weak formulation of this problem is found by multiplying the two equations with test functions and integrating some terms by parts: [1.x.48] 

where [1.x.49] 

Here,  [2.x.24]  is the outward normal vector at the boundary. Note how in this formulation, Dirichlet boundary values of the original problem are incorporated in the weak form. 

To be well-posed, we have to look for solutions and test functions in the space  [2.x.25]  for  [2.x.26] , [2.x.27] , and  [2.x.28]  for  [2.x.29] . It is a well-known fact stated in almost every book on finite element theory that if one chooses discrete finite element spaces for the approximation of  [2.x.30]  inappropriately, then the resulting discrete problem is instable and the discrete solution will not converge to the exact solution. (Some details on the problem considered here -- which falls in the class of "saddle-point problems" 

-- can be found on the Wikipedia page on the [1.x.50].) 

To overcome this, a number of different finite element pairs for  [2.x.31]  have been developed that lead to a stable discrete problem. One such pair is to use the Raviart-Thomas spaces  [2.x.32]  for the velocity  [2.x.33]  and discontinuous elements of class  [2.x.34]  for the pressure  [2.x.35] . For details about these spaces, we refer in particular to the book on mixed finite element methods by Brezzi and Fortin, but many other books on the theory of finite elements, for example the classic book by Brenner and Scott, also state the relevant results. In any case, with appropriate choices of function spaces, the discrete formulation reads as follows: Find  [2.x.36]  so that [1.x.51] 




Before continuing, let us briefly pause and show that the choice of function spaces above provides us with the desired local conservation property. In particular, because the pressure space consists of discontinuous piecewise polynomials, we can choose the test function  [2.x.37]  as the function that is equal to one on any given cell  [2.x.38]  and zero everywhere else. If we also choose  [2.x.39]  everywhere (remember that the weak form above has to hold for [1.x.52] discrete test functions  [2.x.40] ), then putting these choices of test functions into the weak formulation above implies in particular that [1.x.53] 

which we can of course write in more explicit form as [1.x.54] 

Applying the divergence theorem results in the fact that  [2.x.41]  has to satisfy, for every choice of cell  [2.x.42] , the relationship [1.x.55] 

If you now recall that  [2.x.43]  was the velocity, then the integral on the left is exactly the (discrete) flux across the boundary of the cell  [2.x.44] . The statement is then that the flux must be equal to the integral over the sources within  [2.x.45] . In particular, if there are no sources (i.e.,  [2.x.46]  in  [2.x.47] ), then the statement is that [1.x.56] flux is zero, i.e., whatever flows into a cell must flow out of it through some other part of the cell boundary. This is what we call [1.x.57] because it holds for every cell. 

On the other hand, the usual continuous  [2.x.48]  elements would not result in this kind of property when used for the pressure (as, for example, we do in  [2.x.49] ) because one can not choose a discrete test function  [2.x.50]  that is one on a cell  [2.x.51]  and zero everywhere else: It would be discontinuous and consequently not in the finite element space. (Strictly speaking, all we can say is that the proof above would not work for continuous elements. Whether these elements might still result in local conservation is a different question as one could think that a different kind of proof might still work; in reality, however, the property really does not hold.) 




[1.x.58][1.x.59] 


The deal.II library (of course) implements Raviart-Thomas elements  [2.x.52]  of arbitrary order  [2.x.53] , as well as discontinuous elements  [2.x.54] . If we forget about their particular properties for a second, we then have to solve a discrete problem [1.x.60] 

with the bilinear form and right hand side as stated above, and  [2.x.55] ,  [2.x.56] . Both  [2.x.57]  and  [2.x.58]  are from the space  [2.x.59] , where  [2.x.60]  is itself a space of  [2.x.61] -dimensional functions to accommodate for the fact that the flow velocity is vector-valued. The necessary question then is: how do we do this in a program? 

Vector-valued elements have already been discussed in previous tutorial programs, the first time and in detail in  [2.x.62] . The main difference there was that the vector-valued space  [2.x.63]  is uniform in all its components: the  [2.x.64]  components of the displacement vector are all equal and from the same function space. What we could therefore do was to build  [2.x.65]  as the outer product of the  [2.x.66]  times the usual  [2.x.67]  finite element space, and by this make sure that all our shape functions have only a single non-zero vector component. Instead of dealing with vector-valued shape functions, all we did in  [2.x.68]  was therefore to look at the (scalar) only non-zero component and use the  [2.x.69]  call to figure out which component this actually is. 

This doesn't work with Raviart-Thomas elements: following from their construction to satisfy certain regularity properties of the space  [2.x.70] , the shape functions of  [2.x.71]  are usually nonzero in all their vector components at once. For this reason, were  [2.x.72]  applied to determine the only nonzero component of shape function  [2.x.73] , an exception would be generated. What we really need to do is to get at  [2.x.74] all [2.x.75]  vector components of a shape function. In deal.II diction, we call such finite elements  [2.x.76] non-primitive [2.x.77] , whereas finite elements that are either scalar or for which every vector-valued shape function is nonzero only in a single vector component are called  [2.x.78] primitive [2.x.79] . 

So what do we have to do for non-primitive elements? To figure this out, let us go back in the tutorial programs, almost to the very beginnings. There, we learned that we use the  [2.x.80]  class to determine the values and gradients of shape functions at quadrature points. For example, we would call  [2.x.81]  to obtain the value of the  [2.x.82] th shape function on the quadrature point with number  [2.x.83] . Later, in  [2.x.84]  and other tutorial programs, we learned that this function call also works for vector-valued shape functions (of primitive finite elements), and that it returned the value of the only non-zero component of shape function  [2.x.85]  at quadrature point  [2.x.86] . 

For non-primitive shape functions, this is clearly not going to work: there is no single non-zero vector component of shape function  [2.x.87] , and the call to  [2.x.88]  would consequently not make much sense. However, deal.II offers a second function call,  [2.x.89]  that returns the value of the  [2.x.90]  at quadrature point  [2.x.91]  is an index between zero and the number of vector components of the present finite element; for example, the element we will use to describe velocities and pressures is going to have  [2.x.92]  components. It is worth noting that this function call can also be used for primitive shape functions: it will simply return zero for all components except one; for non-primitive shape functions, it will in general return a non-zero value for more than just one component. 

We could now attempt to rewrite the bilinear form above in terms of vector components. For example, in 2d, the first term could be rewritten like this (note that  [2.x.93] ): [1.x.61] 

If we implemented this, we would get code like this: 

[1.x.62] 



This is, at best, tedious, error prone, and not dimension independent. There are obvious ways to make things dimension independent, but in the end, the code is simply not pretty. What would be much nicer is if we could simply extract the  [2.x.94]  and  [2.x.95]  components of a shape function  [2.x.96] . In the program we do that in the following way: 

[1.x.63] 



This is, in fact, not only the first term of the bilinear form, but the whole thing (sans boundary contributions). 

What this piece of code does is, given an  [2.x.97]  object, to extract the values of the first  [2.x.98]  components of shape function  [2.x.99]  at quadrature points  [2.x.100] , that is the velocity components of that shape function. Put differently, if we write shape functions  [2.x.101]  as the tuple  [2.x.102] , then the function returns the velocity part of this tuple. Note that the velocity is of course a  [2.x.103] -dimensional tensor, and that the function returns a corresponding object. Similarly, where we subscript with the pressure extractor, we extract the scalar pressure component. The whole mechanism is described in more detail in the  [2.x.104]  module. 

In practice, it turns out that we can do a bit better if we evaluate the shape functions, their gradients and divergences only once per outermost loop, and store the result, as this saves us a few otherwise repeated computations (it is possible to save even more repeated operations by calculating all relevant quantities in advance and then only inserting the results in the actual loop, see  [2.x.105]  for a realization of that approach). The final result then looks like this, working in every space dimension: 

[1.x.64] 



This very closely resembles the form in which we have originally written down the bilinear form and right hand side. 

There is one final term that we have to take care of: the right hand side contained the term  [2.x.106] , constituting the weak enforcement of pressure boundary conditions. We have already seen in  [2.x.107]  how to deal with face integrals: essentially exactly the same as with domain integrals, except that we have to use the FEFaceValues class instead of  [2.x.108] . To compute the boundary term we then simply have to loop over all boundary faces and integrate there. The mechanism works in the same way as above, i.e. the extractor classes also work on FEFaceValues objects: 

[1.x.65] 



You will find the exact same code as above in the sources for the present program. We will therefore not comment much on it below. 


[1.x.66][1.x.67] 


After assembling the linear system we are faced with the task of solving it. The problem here is that the matrix possesses two undesirable properties: 

- It is [1.x.68],   i.e., it has both positive and negative eigenvalues.   We don't want to prove this property here, but note that this is true   for all matrices of the form    [2.x.109]    such as the one here where  [2.x.110]  is positive definite. 

- The matrix has a zero block at the bottom right (there is no term in   the bilinear form that couples the pressure  [2.x.111]  with the   pressure test function  [2.x.112] ). 

At least it is symmetric, but the first issue above still means that the Conjugate Gradient method is not going to work since it is only applicable to problems in which the matrix is symmetric and positive definite. We would have to resort to other iterative solvers instead, such as MinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, then the next problem immediately surfaces: Due to the zero block, there are zeros on the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR) will work as they require division by diagonal elements. 

For the matrix sizes we expect to run with this program, the by far simplest approach would be to just use a direct solver (in particular, the SparseDirectUMFPACK class that is bundled with deal.II).  [2.x.113]  goes this route and shows that solving [1.x.69] linear system can be done in just 3 or 4 lines of code. 

But then, this is a tutorial: We teach how to do things. Consequently, in the following, we will introduce some techniques that can be used in cases like these. Namely, we will consider the linear system as not consisting of one large matrix and vectors, but we will want to decompose matrices into [1.x.70] that correspond to the individual operators that appear in the system. We note that the resulting solver is not optimal -- there are much better ways to efficiently compute the system, for example those explained in the results section of  [2.x.114]  or the one we use in  [2.x.115]  for a problem similar to the current one. Here, our goal is simply to introduce new solution techniques and how they can be implemented in deal.II. 


[1.x.71][1.x.72] 


In view of the difficulties using standard solvers and preconditioners mentioned above, let us take another look at the matrix. If we sort our degrees of freedom so that all velocity come before all pressure variables, then we can subdivide the linear system  [2.x.116]  into the following blocks: [1.x.73] 

where  [2.x.117]  are the values of velocity and pressure degrees of freedom, respectively,  [2.x.118]  is the mass matrix on the velocity space,  [2.x.119]  corresponds to the negative divergence operator, and  [2.x.120]  is its transpose and corresponds to the gradient. 

By block elimination, we can then re-order this system in the following way (multiply the first row of the system by  [2.x.121]  and then subtract the second row from it): [1.x.74] 

Here, the matrix  [2.x.122]  (called the [1.x.75] of  [2.x.123] ) is obviously symmetric and, owing to the positive definiteness of  [2.x.124]  and the fact that  [2.x.125]  has full column rank,  [2.x.126]  is also positive definite. 

Consequently, if we could compute  [2.x.127] , we could apply the Conjugate Gradient method to it. However, computing  [2.x.128]  is expensive because it requires us to compute the inverse of the (possibly large) matrix  [2.x.129] ; and  [2.x.130]  is in fact also a full matrix because even though  [2.x.131]  is sparse, its inverse  [2.x.132]  will generally be a dense matrix. On the other hand, the CG algorithm doesn't require us to actually have a representation of  [2.x.133] : It is sufficient to form matrix-vector products with it. We can do so in steps, using the fact that matrix products are associative (i.e., we can set parentheses in such a way that the product is more convenient to compute): To compute  [2.x.134] , we  [2.x.135]    [2.x.136]  compute  [2.x.137] ;   [2.x.138]  solve  [2.x.139]  for  [2.x.140] , using the CG method applied to the   positive definite and symmetric mass matrix  [2.x.141] ;   [2.x.142]  compute  [2.x.143]  to obtain  [2.x.144] .  [2.x.145]  Note how we evaluate the expression  [2.x.146]  right to left to avoid matrix-matrix products; this way, all we have to do is evaluate matrix-vector products. 

In the following, we will then have to come up with ways to represent the matrix  [2.x.147]  so that it can be used in a Conjugate Gradient solver, as well as to define ways in which we can precondition the solution of the linear system involving  [2.x.148] , and deal with solving linear systems with the matrix  [2.x.149]  (the second step above). 

 [2.x.150]  The key point in this consideration is to recognize that to implement an iterative solver such as CG or GMRES, we never actually need the actual [1.x.76] of a matrix! All that is required is that we can form matrix-vector products. The same is true for preconditioners. In deal.II we encode this requirement by only requiring that matrices and preconditioners given to solver classes have a  [2.x.151]  member function that does the matrix-vector product. How a class chooses to implement this function is not important to the solver. Consequently, classes can implement it by, for example, doing a sequence of products and linear solves as discussed above. 


[1.x.77][1.x.78] 


deal.II includes support for describing such linear operations in a very general way. This is done with the LinearOperator class that, like  [2.x.152]  "the MatrixType concept", defines a minimal interface for [1.x.79] a linear operation to a vector: 

[1.x.80] 

The key difference between a LinearOperator and an ordinary matrix is however that a LinearOperator does not allow any further access to the underlying object. All you can do with a LinearOperator is to apply its "action" to a vector! We take the opportunity to introduce the LinearOperator concept at this point because it is a very useful tool that allows you to construct complex solvers and preconditioners in a very intuitive manner. 

As a first example let us construct a LinearOperator object that represents  [2.x.153] . This means that whenever the  [2.x.154]  function of this operator is called it has to solve a linear system. This requires us to specify a solver (and corresponding) preconditioner. Assuming that  [2.x.155]  is a reference to the upper left block of the system matrix we can write: 

[1.x.81] 

Rather than using a SolverControl we use the ReductionControl class here that stops iterations when either an absolute tolerance is reached (for which we choose  [2.x.156] ) or when the residual is reduced by a certain factor (here,  [2.x.157] ). In contrast the SolverControl class only checks for absolute tolerances. We have to use ReductionControl in our case to work around a minor issue: The right hand sides that we  will feed to  [2.x.158]  are essentially formed by residuals that naturally decrease vastly in norm as the outer iterations progress. This makes control by an absolute tolerance very error prone. 

We now have a LinearOperator  [2.x.159]  that we can use to construct more complicated operators such as the Schur complement  [2.x.160] . Assuming that  [2.x.161]  is a reference to the upper right block constructing a LinearOperator  [2.x.162]  is a matter of two lines: 

[1.x.82] 

Here, the multiplication of three LinearOperator objects yields a composite object  [2.x.163]  function first applies  [2.x.164] , then  [2.x.165]  (i.e. solving an equation with  [2.x.166] ), and finally  [2.x.167]  to any given input vector. In that sense  [2.x.168]  is similar to the following code: 

[1.x.83] 

( [2.x.169]  are two temporary vectors). The key point behind this approach is the fact that we never actually create an inner product of matrices. Instead, whenever we have to perform a matrix vector multiplication with  [2.x.170]  we simply run all individual  [2.x.171]  operations in above sequence. 

 [2.x.172]  We could have achieved the same goal of creating a "matrix like" object by implementing a specialized class  [2.x.173]  that provides a suitable  [2.x.174]  function. Skipping over some details this might have looked like the following: 

[1.x.84] 

Even though both approaches are exactly equivalent, the LinearOperator class has a big advantage over this manual approach. It provides so-called [1.x.85][1.x.86]: Mathematically, we think about  [2.x.175]  as being the composite matrix  [2.x.176]  and the LinearOperator class allows you to write this out more or less verbatim, 

[1.x.87] 

The manual approach on the other hand obscures this fact. 

All that is left for us to do now is to form the right hand sides of the two equations defining  [2.x.177]  and  [2.x.178] , and then solve them with the Schur complement matrix and the mass matrix, respectively. For example the right hand side of the first equation reads  [2.x.179] . This could be implemented as follows: 

[1.x.88] 

Again, this is a perfectly valid approach, but the fact that deal.II requires us to manually resize the final and temporary vector, and that every operation takes up a new line makes this hard to read. This is the point where a second class in the linear operator framework can will help us. Similarly in spirit to LinearOperator, a PackagedOperation stores a "computation": 

[1.x.89] 

The class allows [1.x.90] of expressions involving vectors and linear operators. This is done by storing the computational expression and only performing the computation when either the object is converted to a vector object, or  [2.x.180]  (or  [2.x.181]  is invoked by hand. Assuming that  [2.x.182]  are the two vectors of the right hand side we can simply write: 

[1.x.91] 

Here,  [2.x.183]  is a PackagedOperation that [1.x.92] the computation we specified. It does not create a vector with the actual result immediately. 

With these prerequisites at hand, solving for  [2.x.184]  and  [2.x.185]  is a matter of creating another solver and inverse: 

[1.x.93] 



 [2.x.186]  The functionality that we developed in this example step by hand is already readily available in the library. Have a look at schur_complement(), condense_schur_rhs(), and postprocess_schur_solution(). 


[1.x.94][1.x.95] 


One may ask whether it would help if we had a preconditioner for the Schur complement  [2.x.187] . The general answer, as usual, is: of course. The problem is only, we don't know anything about this Schur complement matrix. We do not know its entries, all we know is its action. On the other hand, we have to realize that our solver is expensive since in each iteration we have to do one matrix-vector product with the Schur complement, which means that we have to do invert the mass matrix once in each iteration. 

There are different approaches to preconditioning such a matrix. On the one extreme is to use something that is cheap to apply and therefore has no real impact on the work done in each iteration. The other extreme is a preconditioner that is itself very expensive, but in return really brings down the number of iterations required to solve with  [2.x.188] . 

We will try something along the second approach, as much to improve the performance of the program as to demonstrate some techniques. To this end, let us recall that the ideal preconditioner is, of course,  [2.x.189] , but that is unattainable. However, how about [1.x.96] 

as a preconditioner? That would mean that every time we have to do one preconditioning step, we actually have to solve with  [2.x.190] . At first, this looks almost as expensive as solving with  [2.x.191]  right away. However, note that in the inner iteration, we do not have to calculate  [2.x.192] , but only the inverse of its diagonal, which is cheap. 

Thankfully, the LinearOperator framework makes this very easy to write out. We already used a Jacobi preconditioner ( [2.x.193] ) for the  [2.x.194]  matrix earlier. So all that is left to do is to write out how the approximate Schur complement should look like: 

[1.x.97] 

Note how this operator differs in simply doing one Jacobi sweep (i.e. multiplying with the inverses of the diagonal) instead of multiplying with the full  [2.x.195] . (This is how a single Jacobi preconditioner step with  [2.x.196]  is defined: it is the multiplication with the inverse of the diagonal of  [2.x.197] ; in other words, the operation  [2.x.198]  on a vector  [2.x.199]  is exactly what PreconditionJacobi does.) 

With all this we almost have the preconditioner completed: it should be the inverse of the approximate Schur complement. We implement this again by creating a linear operator with inverse_operator() function. This time however we would like to choose a relatively modest tolerance for the CG solver (that inverts  [2.x.200] ). The reasoning is that  [2.x.201] , so we actually do not need to invert it exactly. This, however creates a subtle problem:  [2.x.202]  will be used in the final outer CG iteration to create an orthogonal basis. But for this to work, it must be precisely the same linear operation for every invocation. We ensure this by using an IterationNumberControl that allows us to fix the number of CG iterations that are performed to a fixed small number (in our case 30): 

[1.x.98] 



That's all! 

Obviously, applying this inverse of the approximate Schur complement is a very expensive preconditioner, almost as expensive as inverting the Schur complement itself. We can expect it to significantly reduce the number of outer iterations required for the Schur complement. In fact it does: in a typical run on 7 times refined meshes using elements of order 0, the number of outer iterations drops from 592 to 39. On the other hand, we now have to apply a very expensive preconditioner 25 times. A better measure is therefore simply the run-time of the program: on a current laptop (as of January 2019), it drops from 3.57 to 2.05 seconds for this test case. That doesn't seem too impressive, but the savings become more pronounced on finer meshes and with elements of higher order. For example, an seven times refined mesh and using elements of order 2 (which amounts to about 0.4 million degrees of freedom) yields an improvement of 1134 to 83 outer iterations, at a runtime of 168 seconds to 40 seconds. Not earth shattering, but significant. 


[1.x.99][1.x.100] 


In this tutorial program, we will solve the Laplace equation in mixed formulation as stated above. Since we want to monitor convergence of the solution inside the program, we choose right hand side, boundary conditions, and the coefficient so that we recover a solution function known to us. In particular, we choose the pressure solution [1.x.101] 

and for the coefficient we choose the unit matrix  [2.x.203]  for simplicity. Consequently, the exact velocity satisfies [1.x.102] 

This solution was chosen since it is exactly divergence free, making it a realistic test case for incompressible fluid flow. By consequence, the right hand side equals  [2.x.204] , and as boundary values we have to choose  [2.x.205] . 

For the computations in this program, we choose  [2.x.206] . You can find the resulting solution in the [1.x.103], after the commented program. [1.x.104] [1.x.105] 


[1.x.106]  [1.x.107] 




Since this program is only an adaptation of  [2.x.207] , there is not much new stuff in terms of header files. In deal.II, we usually list include files in the order base-lac-grid-dofs-fe-numerics, followed by C++ standard include files: 

[1.x.108] 



The only two new header files that deserve some attention are those for the LinearOperator and PackagedOperation classes: 

[1.x.109] 



This is the only significant new header, namely the one in which the Raviart-Thomas finite element is declared: 

[1.x.110] 



Finally, as a bonus in this program, we will use a tensorial coefficient. Since it may have a spatial dependence, we consider it a tensor-valued function. The following include file provides the  [2.x.208]  class that offers such functionality: 

[1.x.111] 



The last step is as in all previous programs: We put all of the code relevant to this program into a namespace. (This idea was first introduced in  [2.x.209] .) 

[1.x.112] 




[1.x.113]  [1.x.114] 




Again, since this is an adaptation of  [2.x.210] , the main class is almost the same as the one in that tutorial program. In terms of member functions, the main differences are that the constructor takes the degree of the Raviart-Thomas element as an argument (and that there is a corresponding member variable to store this value) and the addition of the  [2.x.211]  function in which, no surprise, we will compute the difference between the exact and the numerical solution to determine convergence of our computations: 

[1.x.115] 



The second difference is that the sparsity pattern, the system matrix, and solution and right hand side vectors are now blocked. What this means and what one can do with such objects is explained in the introduction to this program as well as further down below when we explain the linear solvers and preconditioners for this problem: 

[1.x.116] 




[1.x.117]  [1.x.118] 




Our next task is to define the right hand side of our problem (i.e., the scalar right hand side for the pressure in the original Laplace equation), boundary values for the pressure, and a function that describes both the pressure and the velocity of the exact solution for later computations of the error. Note that these functions have one, one, and  [2.x.212]  components, respectively, and that we pass the number of components down to the  [2.x.213]  base class. For the exact solution, we only declare the function that actually returns the entire solution vector (i.e. all components of it) at once. Here are the respective declarations: 

[1.x.119] 



And then we also have to define these respective functions, of course. Given our discussion in the introduction of how the solution should look, the following computations should be straightforward: 

[1.x.120] 




[1.x.121]  [1.x.122] 




In addition to the other equation data, we also want to use a permeability tensor, or better -- because this is all that appears in the weak form -- the inverse of the permeability tensor,  [2.x.214] . For the purpose of verifying the exactness of the solution and determining convergence orders, this tensor is more in the way than helpful. We will therefore simply set it to the identity matrix.      


However, a spatially varying permeability tensor is indispensable in real-life porous media flow simulations, and we would like to use the opportunity to demonstrate the technique to use tensor valued functions.      


Possibly unsurprisingly, deal.II also has a base class not only for scalar and generally vector-valued functions (the  [2.x.215]  base class) but also for functions that return tensors of fixed dimension and rank, the  [2.x.216]  template. Here, the function under consideration returns a dim-by-dim matrix, i.e. a tensor of rank 2 and dimension  [2.x.217] . We then choose the template arguments of the base class appropriately.      


The interface that the  [2.x.218]  class provides is essentially equivalent to the  [2.x.219]  class. In particular, there exists a  [2.x.220]  function that takes a list of points at which to evaluate the function, and returns the values of the function in the second argument, a list of tensors: 

[1.x.123] 



The implementation is less interesting. As in previous examples, we add a check to the beginning of the class to make sure that the sizes of input and output parameters are the same (see  [2.x.221]  for a discussion of this technique). Then we loop over all evaluation points, and for each one set the output tensor to the identity matrix.      


There is an oddity at the top of the function (the `(void)points;` statement) that is worth discussing. The values we put into the output `values` array does not actually depend on the `points` arrays of coordinates at which the function is evaluated. In other words, the `points` argument is in fact unused, and we could have just not given it a name if we had wanted. But we want to use the `points` object for checking that the `values` object has the correct size. The problem is that in release mode, `AssertDimension` is defined as a macro that expands to nothing; the compiler will then complain that the `points` object is unused. The idiomatic approach to silencing this warning is to have a statement that evaluates (reads) variable but doesn't actually do anything: That's what `(void)points;` does: It reads from `points`, and then casts the result of the read to `void`, i.e., nothing. This statement is, in other words, completely pointless and implies no actual action except to explain to the compiler that yes, this variable is in fact used even in release mode. (In debug mode, the `AssertDimension` macro expands to something that reads from the variable, and so the funny statement would not be necessary in debug mode.) 

[1.x.124] 




[1.x.125]  [1.x.126] 





[1.x.127]  [1.x.128] 




In the constructor of this class, we first store the value that was passed in concerning the degree of the finite elements we shall use (a degree of zero, for example, means to use RT(0) and DG(0)), and then construct the vector valued element belonging to the space  [2.x.222]  described in the introduction. The rest of the constructor is as in the early tutorial programs.    


The only thing worth describing here is the constructor call of the  [2.x.223]  class to which this variable belongs has a number of different constructors that all refer to binding simpler elements together into one larger element. In the present case, we want to couple a single RT(degree) element with a single DQ(degree) element. The constructor to  [2.x.224]  that does this requires us to specify first the first base element (the  [2.x.225]  object of given degree) and then the number of copies for this base element, and then similarly the kind and number of  [2.x.226]  elements. Note that the Raviart-Thomas element already has  [2.x.227]  vector components, so that the coupled element will have  [2.x.228]  vector components, the first  [2.x.229]  of which correspond to the velocity variable whereas the last one corresponds to the pressure.    


It is also worth comparing the way we constructed this element from its base elements, with the way we have done so in  [2.x.230] : there, we have built it as  [2.x.231] , i.e. we have simply used  [2.x.232]  element, one copy for the displacement in each coordinate direction. 

[1.x.129] 




[1.x.130]  [1.x.131] 




This next function starts out with well-known functions calls that create and refine a mesh, and then associate degrees of freedom with it: 

[1.x.132] 



However, then things become different. As mentioned in the introduction, we want to subdivide the matrix into blocks corresponding to the two different kinds of variables, velocity and pressure. To this end, we first have to make sure that the indices corresponding to velocities and pressures are not intermingled: First all velocity degrees of freedom, then all pressure DoFs. This way, the global matrix separates nicely into a  [2.x.233]  system. To achieve this, we have to renumber degrees of freedom based on their vector component, an operation that conveniently is already implemented: 

[1.x.133] 



The next thing is that we want to figure out the sizes of these blocks so that we can allocate an appropriate amount of space. To this end, we call the  [2.x.234]  function that counts how many shape functions are non-zero for a particular vector component. We have  [2.x.235]  vector components, and  [2.x.236]  will count how many shape functions belong to each of these components.      


There is one problem here. As described in the documentation of that function, it [1.x.134] to put the number of  [2.x.237] -velocity shape functions into  [2.x.238] , the number of  [2.x.239] -velocity shape functions into  [2.x.240]  (and similar in 3d), and the number of pressure shape functions into  [2.x.241] . But, the Raviart-Thomas element is special in that it is non- [2.x.242]  "primitive", i.e., for Raviart-Thomas elements all velocity shape functions are nonzero in all components. In other words, the function cannot distinguish between  [2.x.243]  and  [2.x.244]  velocity functions because there [1.x.135] no such distinction. It therefore puts the overall number of velocity into each of  [2.x.245] ,  [2.x.246] . On the other hand, the number of pressure variables equals the number of shape functions that are nonzero in the dim-th component.      


Using this knowledge, we can get the number of velocity shape functions from any of the first  [2.x.247]  elements of  [2.x.248] , and then use this below to initialize the vector and matrix block sizes, as well as create output.      




 [2.x.249]  If you find this concept difficult to understand, you may want to consider using the function  [2.x.250]  instead, as we do in the corresponding piece of code in  [2.x.251] . You might also want to read up on the difference between  [2.x.252]  "blocks" and  [2.x.253]  "components" in the glossary. 

[1.x.136] 



The next task is to allocate a sparsity pattern for the matrix that we will create. We use a compressed sparsity pattern like in the previous steps, but as  [2.x.254]  is a block matrix we use the class  [2.x.255]  instead of just  [2.x.256] . This block sparsity pattern has four blocks in a  [2.x.257]  pattern. The blocks' sizes depend on  [2.x.258] , which hold the number of velocity and pressure variables. In the second step we have to instruct the block system to update its knowledge about the sizes of the blocks it manages; this happens with the  [2.x.259]  call. 

[1.x.137] 



We use the compressed block sparsity pattern in the same way as the non-block version to create the sparsity pattern and then the system matrix: 

[1.x.138] 



Then we have to resize the solution and right hand side vectors in exactly the same way as the block compressed sparsity pattern: 

[1.x.139] 




[1.x.140]  [1.x.141] 




Similarly, the function that assembles the linear system has mostly been discussed already in the introduction to this example. At its top, what happens are all the usual steps, with the addition that we do not only allocate quadrature and  [2.x.260]  objects for the cell terms, but also for face terms. After that, we define the usual abbreviations for variables, and the allocate space for the local matrix and right hand side contributions, and the array that holds the global numbers of the degrees of freedom local to the present cell. 

[1.x.142] 



The next step is to declare objects that represent the source term, pressure boundary value, and coefficient in the equation. In addition to these objects that represent continuous functions, we also need arrays to hold their values at the quadrature points of individual cells (or faces, for the boundary values). Note that in the case of the coefficient, the array has to be one of matrices. 

[1.x.143] 



Finally, we need a couple of extractors that we will use to get at the velocity and pressure components of vector-valued shape functions. Their function and use is described in detail in the  [2.x.261]  report. Essentially, we will use them as subscripts on the FEValues objects below: the FEValues object describes all vector components of shape functions, while after subscription, it will only refer to the velocities (a set of  [2.x.262]  components starting at component zero) or the pressure (a scalar component located at position  [2.x.263] ): 

[1.x.144] 



With all this in place, we can go on with the loop over all cells. The body of this loop has been discussed in the introduction, and will not be commented any further here: 

[1.x.145] 



The final step in the loop over all cells is to transfer local contributions into the global matrix and right hand side vector. Note that we use exactly the same interface as in previous examples, although we now use block matrices and vectors instead of the regular ones. In other words, to the outside world, block objects have the same interface as matrices and vectors, but they additionally allow to access individual blocks. 

[1.x.146] 




[1.x.147]  [1.x.148] 




The linear solvers and preconditioners we use in this example have been discussed in significant detail already in the introduction. We will therefore not discuss the rationale for our approach here any more, but rather only comment on some remaining implementational aspects. 





[1.x.149]  [1.x.150] 




As already outlined in the introduction, the solve function consists essentially of two steps. First, we have to form the first equation involving the Schur complement and solve for the pressure (component 1 of the solution). Then, we can reconstruct the velocities from the second equation (component 0 of the solution). 

[1.x.151] 



As a first step we declare references to all block components of the matrix, the right hand side and the solution vector that we will need. 

[1.x.152] 



Then, we will create corresponding LinearOperator objects and create the  [2.x.264]  operator: 

[1.x.153] 



This allows us to declare the Schur complement  [2.x.265]  and the approximate Schur complement  [2.x.266] : 

[1.x.154] 



We now create a preconditioner out of  [2.x.267]  that applies a fixed number of 30 (inexpensive) CG iterations: 

[1.x.155] 



Now on to the first equation. The right hand side of it is  [2.x.268] , which is what we compute in the first few lines. We then solve the first equation with a CG solver and the preconditioner we just declared. 

[1.x.156] 



After we have the pressure, we can compute the velocity. The equation reads  [2.x.269] , and we solve it by first computing the right hand side, and then multiplying it with the object that represents the inverse of the mass matrix: 

[1.x.157] 




[1.x.158]  [1.x.159] 





[1.x.160]  [1.x.161] 




After we have dealt with the linear solver and preconditioners, we continue with the implementation of our main class. In particular, the next task is to compute the errors in our numerical solution, in both the pressures as well as velocities.    


To compute errors in the solution, we have already introduced the  [2.x.270]  function in  [2.x.271]  and  [2.x.272] . However, there we only dealt with scalar solutions, whereas here we have a vector-valued solution with components that even denote different quantities and may have different orders of convergence (this isn't the case here, by choice of the used finite elements, but is frequently the case in mixed finite element applications). What we therefore have to do is to `mask' the components that we are interested in. This is easily done: the  [2.x.273]  function takes as one of its arguments a pointer to a weight function (the parameter defaults to the null pointer, meaning unit weights). What we have to do is to pass a function object that equals one in the components we are interested in, and zero in the other ones. For example, to compute the pressure error, we should pass a function that represents the constant vector with a unit value in component  [2.x.274] , whereas for the velocity the constant vector should be one in the first  [2.x.275]  components, and zero in the location of the pressure.    


In deal.II, the  [2.x.276]  does exactly this: it wants to know how many vector components the function it is to represent should have (in our case this would be  [2.x.277] , for the joint velocity-pressure space) and which individual or range of components should be equal to one. We therefore define two such masks at the beginning of the function, following by an object representing the exact solution and a vector in which we will store the cellwise errors as computed by  [2.x.278] : 

[1.x.162] 



As already discussed in  [2.x.279] , we have to realize that it is impossible to integrate the errors exactly. All we can do is approximate this integral using quadrature. This actually presents a slight twist here: if we naively chose an object of type  [2.x.280]  as one may be inclined to do (this is what we used for integrating the linear system), one realizes that the error is very small and does not follow the expected convergence curves at all. What is happening is that for the mixed finite elements used here, the Gauss points happen to be superconvergence points in which the pointwise error is much smaller (and converges with higher order) than anywhere else. These are therefore not particularly good points for integration. To avoid this problem, we simply use a trapezoidal rule and iterate it  [2.x.281]  times in each coordinate direction (again as explained in  [2.x.282] ): 

[1.x.163] 



With this, we can then let the library compute the errors and output them to the screen: 

[1.x.164] 




[1.x.165]  [1.x.166] 




The last interesting function is the one in which we generate graphical output. Note that all velocity components get the same solution name "u". Together with using  [2.x.283]  this will cause  [2.x.284]  to generate a vector representation of the individual velocity components, see  [2.x.285]  or the  [2.x.286]  "Generating graphical output" section of the  [2.x.287]  module for more information. Finally, it seems inappropriate for higher order elements to only show a single bilinear quadrilateral per cell in the graphical output. We therefore generate patches of size (degree+1)x(degree+1) to capture the full information content of the solution. See the  [2.x.288]  tutorial program for more information on this. 

[1.x.167] 




[1.x.168]  [1.x.169] 




This is the final function of our main class. It's only job is to call the other functions in their natural order: 

[1.x.170] 




[1.x.171]  [1.x.172] 




The main function we stole from  [2.x.289]  instead of  [2.x.290] . It is almost equal to the one in  [2.x.291]  (apart from the changed class names, of course), the only exception is that we pass the degree of the finite element space to the constructor of the mixed Laplace problem (here, we use zero-th order elements). 

[1.x.173] 

[1.x.174][1.x.175] 


[1.x.176][1.x.177] 




If we run the program as is, we get this output for the  [2.x.292]  mesh we use (for a total of 1024 cells with 1024 pressure degrees of freedom since we use piecewise constants, and 2112 velocities because the Raviart-Thomas element defines one degree per freedom per face and there are  [2.x.293]  faces parallel to the  [2.x.294] -axis and the same number parallel to the  [2.x.295] -axis): 

[1.x.178] 



The fact that the number of iterations is so small, of course, is due to the good (but expensive!) preconditioner we have developed. To get confidence in the solution, let us take a look at it. The following three images show (from left to right) the x-velocity, the y-velocity, and the pressure: 

 [2.x.296]  




Let us start with the pressure: it is highest at the left and lowest at the right, so flow will be from left to right. In addition, though hardly visible in the graph, we have chosen the pressure field such that the flow left-right flow first channels towards the center and then outward again. Consequently, the x-velocity has to increase to get the flow through the narrow part, something that can easily be seen in the left image. The middle image represents inward flow in y-direction at the left end of the domain, and outward flow in y-direction at the right end of the domain. 




As an additional remark, note how the x-velocity in the left image is only continuous in x-direction, whereas the y-velocity is continuous in y-direction. The flow fields are discontinuous in the other directions. This very obviously reflects the continuity properties of the Raviart-Thomas elements, which are, in fact, only in the space H(div) and not in the space  [2.x.297] . Finally, the pressure field is completely discontinuous, but that should not surprise given that we have chosen  [2.x.298]  as the finite element for that solution component. 




[1.x.179][1.x.180] 




The program offers two obvious places where playing and observing convergence is in order: the degree of the finite elements used (passed to the constructor of the  [2.x.299] ), and the refinement level (determined in  [2.x.300] ). What one can do is to change these values and observe the errors computed later on in the course of the program run. 




If one does this, one finds the following pattern for the  [2.x.301]  error in the pressure variable:  [2.x.302]  

The theoretically expected convergence orders are very nicely reflected by the experimentally observed ones indicated in the last row of the table. 




One can make the same experiment with the  [2.x.303]  error in the velocity variables:  [2.x.304]  The result concerning the convergence order is the same here. 




[1.x.181] [1.x.182][1.x.183] 


[1.x.184][1.x.185] 


Realistic flow computations for ground water or oil reservoir simulations will not use a constant permeability. Here's a first, rather simple way to change this situation: we use a permeability that decays very rapidly away from a central flowline until it hits a background value of 0.001. This is to mimic the behavior of fluids in sandstone: in most of the domain, the sandstone is homogeneous and, while permeable to fluids, not overly so; on the other stone, the stone has cracked, or faulted, along one line, and the fluids flow much easier along this large crack. Here is how we could implement something like this: 

[1.x.186] 

Remember that the function returns the inverse of the permeability tensor. 




With a significantly higher mesh resolution, we can visualize this, here with x- and y-velocity: 

 [2.x.305]  

It is obvious how fluids flow essentially only along the middle line, and not anywhere else. 




Another possibility would be to use a random permeability field. A simple way to achieve this would be to scatter a number of centers around the domain and then use a permeability field that is the sum of (negative) exponentials for each of these centers. Flow would then try to hop from one center of high permeability to the next one. This is an entirely unscientific attempt at describing a random medium, but one possibility to implement this behavior would look like this: 

[1.x.187] 



A piecewise constant interpolation of the diagonal elements of the inverse of this tensor (i.e., of  [2.x.306] ) looks as follows: 

 [2.x.307]  


With a permeability field like this, we would get x-velocities and pressures as follows: 

 [2.x.308]  

We will use these permeability fields again in  [2.x.309]  and  [2.x.310] . 


[1.x.188][1.x.189] 


As mentioned in the introduction, the Schur complement solver used here is not the best one conceivable (nor is it intended to be a particularly good one). Better ones can be found in the literature and can be built using the same block matrix techniques that were introduced here. We pick up on this theme again in  [2.x.311] , where we first build a Schur complement solver for the Stokes equation as we did here, and then in the [1.x.190] section discuss better ways based on solving the system as a whole but preconditioning based on individual blocks. We will also come back to this in  [2.x.312] . [1.x.191] [1.x.192]  [2.x.313]  

 [2.x.314] 
