 [2.x.0] [2.x.1]  

本教程取决于 [2.x.2] , [2.x.3] 。

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15] 

 [2.x.4]  

[1.x.16] 


[1.x.17][1.x.18]。


这个例子展示了如何使用CUDA在GPU上实现一个无矩阵的方法，用于超立方体上系数可变的亥姆霍兹方程。该线性系统将使用共轭梯度法进行求解，并使用MPI进行并行化。

在过去的几年里，异构计算，特别是GPU，已经获得了很大的普及。这是因为在给定的功率预算下，GPU比CPU提供更好的计算能力和内存带宽。在2019年初的架构中，对于PDE相关的任务，GPU的功率效率约为服务器CPU的2-3倍，宽[1.x.19]。GPU也是机器学习中最受欢迎的架构。另一方面，GPU并不容易编程。这个程序探索了deal.II的能力，看看这样的程序可以如何有效地实现。

虽然我们试图让CPU和GPU的无矩阵类的界面尽可能接近，但还是有一些区别。当在GPU上使用无矩阵框架时，人们必须编写一些CUDA代码。然而，数量相当少，而且对CUDA的使用仅限于几个关键词。


[1.x.20][1.x.21] 。


在这个例子中，我们考虑Helmholtz问题[1.x.22] 。

其中[2.x.5]是一个可变系数。

我们选择 [2.x.6] 和 [2.x.7] 作为域。由于系数是围绕原点对称的，但域不是，我们最终会得到一个非对称的解决方案。

如果你在本教程中读到这里，你就会知道这个问题的弱表述是什么样子的，以及原则上如何为它组装线性系统。当然，在这个程序中，我们实际上不会形成矩阵，而只是表示它与之相乘时的作用。


[1.x.23][1.x.24] 


GPU（我们从现在开始用 "设备 "一词来指代GPU）有自己的内存，与CPU（我们从现在开始用 "主机 "一词）可访问的内存分开。设备上的正常计算可以分为三个独立的步骤。

 -# 数据从主机移到设备上。

 -# 计算在设备上完成。

 -# 结果从设备上移回主机。

数据移动可以由用户代码明确完成，也可以使用UVM（统一虚拟内存）自动完成。在deal.II中，只支持第一种方法。虽然这意味着用户有额外的负担，但这可以更好地控制数据移动，更重要的是可以避免在主机上而不是设备上错误地运行重要的内核。

deal.II中的数据移动是通过[2.x.8]完成的，这些向量可以被看作是主机上的缓冲区，用来存储从设备上收到的数据或发送数据到设备上。有两种类型的向量可以在设备上使用。

- [2.x.9] 它类似于更常见的Vector<Number>，和 

- [2.x.10] [2.x.11] 这是一个普通的[2.x.12]，我们指定了要使用哪个内存空间。

如果没有指定内存空间，默认是[2.x.13]。 

接下来，我们展示如何使用[2.x.14]将数据移入/移出设备。 

[1.x.25] 

这里使用的两个向量类都只在一台机器上工作，也就是说，一个内存空间在主机上，一个在设备上。

但在有些情况下，人们希望在若干台机器上的多个MPI进程之间运行并行计算，而每台机器都配备有GPU。在这种情况下，人们希望使用[2.x.15]，它是类似的，但`import()`阶段可能涉及MPI通信。

[1.x.26] 

`relevant_rw_vector`是一个存储向量所有元素的子集的对象。通常，这些是[2.x.16]"本地相关的DoF"，这意味着它们在不同的MPI进程之间是重叠的。因此，一台机器上存储在该向量中的元素可能与该机器上的GPU存储的元素不一致，需要MPI通信来导入它们。

在所有这些情况下，在导入一个向量时，可以插入数值（使用[2.x.17]或添加到向量的先前内容中（使用[2.x.18  


[1.x.27][1.x.28] 。


在设备上评估无矩阵算子所需的代码与主机上的代码非常相似。然而，也有一些区别，主要是[2.x.19]中的`local_apply()`函数和正交点的循环都需要封装在自己的函数中。[1.x.29] [1.x.30]。

首先包括从以前的教程中知道的deal.II库中的必要文件。

[1.x.31] 



下面的包括在GPU上实现无矩阵方法的数据结构。

[1.x.32] 



像往常一样，我们把所有的东西都包围在一个自己的命名空间中。

[1.x.33] 




[1.x.34] [1.x.35]。




接下来，我们定义一个类，实现我们想在亥姆霍兹算子中使用的变化系数。后来，我们想把这个类型的对象传递给一个[2.x.20]对象，该对象希望该类有一个`运算器()`，为给定的单元填充构造器中提供的值。这个操作符需要在设备上运行，所以它需要为编译器标记为`__device__`。

[1.x.36] 



由于[2.x.21]不知道其数组的大小，我们需要在这个类中存储正交点的数量和自由度的数量，以进行必要的索引转换。

[1.x.37] 



下面的函数实现了这个系数。记得在介绍中，我们曾将其定义为[2.x.22]  

[1.x.38] 




[1.x.39] [1.x.40]。




类`HelmholtzOperatorQuad`实现了Helmholtz算子在每个正交点的评估。它使用了类似于 [2.x.23] 中介绍的 MatrixFree 框架的机制。与那里不同的是，实际的正交点索引是通过转换当前线程索引隐式处理的。和以前一样，这个类的函数需要在设备上运行，所以需要为编译器标记为`__device__`。

[1.x.41] 



我们在这里要解决的Helmholtz问题以弱的形式写成如下。[1.x.42] 

如果你看过 [2.x.24] ，那么很明显，左边的两个项对应着这里的两个函数调用。

[1.x.43] 




[1.x.44] [1.x.45] 。




最后，我们需要定义一个类，实现整个运算符的评估，在基于矩阵的方法中对应于矩阵-向量积。

[1.x.46] 



同样，[2.x.25]对象不知道自由度的数量和正交点的数量，所以我们需要在调用算子中存储这些用于索引计算。

[1.x.47] 



这是一个调用算子，在给定的单元上执行亥姆霍兹算子的评估，类似于CPU上的MatrixFree框架。特别是，我们需要访问源向量的值和梯度，我们将值和梯度信息写入目标向量。

[1.x.48] 




[1.x.49] [1.x.50] 




HelmholtzOperator "类作为 "LocalHelmholtzOperator "的封装器，定义了一个可以与SolverCG等线性求解器一起使用的接口。特别是，像每一个实现线性算子接口的类一样，它需要有一个`vmult()'函数来执行线性算子对源向量的操作。

[1.x.51] 



下面是这个类的构造函数的实现。在第一部分，我们初始化`mf_data`成员变量，该变量将在评估算子时为我们提供必要的信息。   


在第二部分中，我们需要在每个活动的、本地拥有的单元中存储每个正交点的系数值。我们可以向平行三角法询问活动的、本地拥有的单元的数量，但手头只有一个DoFHandler对象。由于[2.x.26]返回的是Triangulation对象，而不是[2.x.27]对象，我们必须对返回值进行下移。在这里这样做是安全的，因为我们知道三角形实际上是一个[2.x.28]对象。

[1.x.52] 



然后，关键的一步是使用前面所有的类来循环所有的单元格来执行矩阵-向量乘积。我们在下一个函数中实现这一点。   


在应用亥姆霍兹算子时，我们必须注意正确处理边界条件。因为本地算子不知道约束条件，所以我们必须在事后将正确的值从源头复制到目的向量上。

[1.x.53] 




[1.x.54][1.x.55] 。




这是本程序的主类。它定义了我们用于教程程序的通常框架。唯一值得评论的一点是`solve()`函数和向量类型的选择。

[1.x.56] 



由于`solve()`函数中的所有操作都是在显卡上执行的，因此所使用的向量也有必要在GPU上存储其值。 [2.x.29]可以被告知要使用哪个内存空间。还有[2.x.30]，总是使用GPU内存存储，但不与MPI一起工作。值得注意的是，如果MPI实现是CUDA感知的，并且配置标志`DEAL_II_MPI_WITH_CUDA_SUPPORT`被启用，不同MPI进程之间的通信可以得到改善。(这个标志的值需要在安装deal.II时调用`cmake`时设置)。     


此外，我们还保留了一个带有CPU存储的解决方案向量，这样我们就可以像往常一样查看和显示解决方案。

[1.x.57] 



除了[2.x.31]之外，这个类的所有其余函数的实现并不包含任何新的内容，我们不会对整体方法进一步发表过多的评论。

[1.x.58] 



与[2.x.32]或[2.x.33]等程序不同，我们不必组装整个线性系统，只需组装右手边的向量。这在本质上与我们在[2.x.34]中所做的一样，例如，但我们必须注意在将局部贡献复制到全局矢量时使用正确的约束对象。特别是，我们需要确保对应于边界节点的条目被正确地清零了。这对于CG的收敛是必要的。 另一个解决方案是修改上面的`vmult()`函数，我们假装源向量的条目为零，在矩阵-向量乘积中不考虑它们。但这里使用的方法更简单）。)    


在函数的最后，我们不能直接将数值从主机复制到设备上，而是需要使用一个类型为[2.x.35]的中间对象来构建必要的正确通信模式。

[1.x.59] 



这个solve()函数最后包含了对之前讨论的新类的调用。这里我们不使用任何预处理程序，即通过身份矩阵进行预处理，以只关注[2.x.36]框架的特殊性。当然，在实际应用中，选择一个合适的预处理程序是至关重要的，但我们至少有与[2.x.37]中相同的限制，因为矩阵条目是即时计算而不是存储的。   


在函数的第一部分解出线性系统后，我们将解决方案从设备上复制到主机上，以便能够查看其值并在`output_results()`中显示。这种转移的工作方式与前一个函数的结尾相同。

[1.x.60] 



输出结果函数和平时一样，因为我们已经将数值从GPU复制回CPU。   


既然我们已经在用这个函数做事情了，我们不妨计算一下解决方案的[2.x.38]规范。我们通过调用[2.x.39]来做到这一点，该函数旨在通过评估数值解（由自由度值的向量给出）和代表精确解的对象之间的差异来计算误差。但是我们可以通过传入一个零函数来轻松地计算解决方案的[2.x.40]准则。也就是说，我们不是在评估误差[2.x.41]，而是在评估[2.x.42]。

[1.x.61] 



`run()`函数中也没有什么令人惊讶的地方。我们只是在一系列（全局）细化的网格上计算解决方案。

[1.x.62] 




[1.x.63][1.x.64] 。




最后为`main()`函数。 默认情况下，所有的MPI等级将尝试访问编号为0的设备，我们假设它是与某一MPI等级运行的CPU相关的GPU设备。这是可行的，但是如果我们在运行MPI支持时，可能会有多个MPI进程在同一台机器上运行（例如，每个CPU核心一个），然后它们都想访问该机器上的同一个GPU。如果机器上只有一个GPU，我们对此无能为力。该机器上的所有MPI行列都需要共享它。但是如果有不止一个GPU，那么最好为不同的进程解决不同的显卡。下面的选择是基于MPI进程的ID，通过将GPU轮流分配给GPU行列。(为了正确地工作，这个方案假定一台机器上的MPI等级是连续的。如果不是这样的话，那么等级与GPU的关联可能就不是最佳的了）。) 为了使其正常工作，在使用这个函数之前，需要对MPI进行初始化。

[1.x.65] 

[1.x.66][1.x.67]。


由于本教程的主要目的是演示如何使用[2.x.43]接口，而不是计算任何有用的东西本身，我们只是在这里显示预期的输出。

[1.x.68] 



在这里，人们可以提出两个看法。首先，数值解的规范收敛了，大概是收敛到精确（但未知）解的规范。其次，每次细化网格时，迭代次数大约增加一倍。这与CG迭代次数随矩阵条件数的平方根增长的预期一致；而且我们知道二阶微分运算的矩阵条件数的增长方式为[2.x.44] 。这当然是相当低效的，因为一个最佳解算器的迭代次数与问题的大小无关。但是要有这样一个求解器，就需要使用比我们在这里使用的身份矩阵更好的预处理程序。


[1.x.69] [1.x.70][1.x.71] 。


目前，这个程序完全没有使用预处理程序。这主要是因为构造一个高效的无矩阵预处理程序是不容易的。 然而，只需要相应矩阵的对角线的简单选择是很好的选择，这些也可以用无矩阵的方式计算。另外，也许更好的是，我们可以扩展教程，使用类似于 [2.x.45] 的切比雪夫平滑器的多网格。[1.x.72] [1.x.73] [2.x.46] 。 

 [2.x.47] 
