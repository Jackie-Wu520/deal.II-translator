 [2.x.0]   [2.x.1]  

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18] 

[1.x.19] [1.x.20][1.x.21] 


[1.x.22][1.x.23] 


Since this is the first tutorial program, let us comment first on how this tutorial and the rest of the deal.II documentation is supposed to work. The documentation for deal.II comes essentially at three different levels: 

- The tutorial: This is a collection of programs that shows how   deal.II is used in practice. It doesn't typically discuss individual   functions at the level of individual arguments, but rather wants to   give the big picture of how things work together. In other words, it   discusses "concepts": what are the building blocks of deal.II and   how are they used together in finite element programs. 

- The manual: This is the documentation of every single class and   every single (member) function in deal.II. You get there if, for   example, you click on the "Main page" or "Classes" tab at the top of   this page. This is the place where you would look up what the second   argument of  [2.x.2]  means,   to give just one slightly obscure example. You need this level of   documentation for when you know what you want to do, but forgot how   exactly the function was named, what its arguments are, or what it   returns. Note that you also get into the manual whenever you read   through the tutorial and click on any of the class or function   names, i.e. the tutorial contains a great many links into the manual   for whenever you need a more detailed description of a function or   class. On the other hand, the manual is not a good place to learn   deal.II since it gives you a microscopic view of things without   telling you how a function might fit into the bigger picture. 

- Modules: These are groups of classes and functions that work   together or have related functionality. If you click on the   "Modules" tab at the top of this page, you end up on a page that   lists a number of such groups. Each module discusses the underlying   principles of these classes; for example, the  [2.x.3]  module   talks about all sorts of different issues related to storing   sparsity patterns of matrices. This is documentation at an   intermediate level: they give you an overview of what's there in a   particular area. For example when you wonder what finite element   classes exist, you would take a look at the  [2.x.4]  module. The   modules are, of course, also cross-linked to the manual (and, at   times, to the tutorial); if you click on a class name, say on   Triangulation, would will also at the very top right under the class   name get a link to the modules this class is a member of if you want   to learn more about its context. 

Let's come back to the tutorial, since you are looking at the first program (or "step") of it. Each tutorial program is subdivided into the following sections:  [2.x.5]     [2.x.6]  [1.x.24] This is a discussion of what the program        does, including the mathematical model, and        what programming techniques are new compared to previous        tutorial programs.    [2.x.7]  [1.x.25] An extensively documented listing of the        source code. Here, we often document individual lines, or        blocks of code, and discuss what they do, how they do it, and        why. The comments frequently reference the introduction,        i.e. you have to understand [1.x.26] the program wants to achieve        (a goal discussed in the introduction) before you can        understand [1.x.27] it intends to get there.    [2.x.8]  [1.x.28] The output of the program, with comments and        interpretation. This section also frequently has a subsection        that gives suggestions on how to extend the program in various        direction; in the earlier programs, this is intended to give        you directions for little experiments designed to make your        familiar with deal.II, while in later programs it is more about        how to use more advanced numerical techniques.    [2.x.9]  [1.x.29] The source code stripped of        all comments. This is useful if you want to see the "big        picture" of the code, since the commented version of the        program has so much text in between that it is often difficult        to see the entire code of a single function on the screen at        once.  [2.x.10]  

The tutorials are not only meant to be static documentation, but you should play with them. To this end, go to the  [2.x.11]  directory (or whatever the number of the tutorial is that you're interested in) and type 

[1.x.30] 

The first command sets up the files that describe which include files this tutorial program depends on, how to compile it and how to run it. This command should find the installed deal.II libraries as well that were generated when you compiled and installed everything as described in the [1.x.31] file. If this command should fail to find the deal.II library, then you need to provide the path to the installation using the command 

[1.x.32] 

instead. 

The second of the commands above compiles the sources into an executable, while the last one executes it (strictly speaking,  [2.x.12]  will also compile the code if the executable doesn't exist yet, so you could have skipped the second command if you wanted). This is all that's needed to run the code and produce the output that is discussed in the "Results" section of the tutorial programs. This sequence needs to be repeated in all of the tutorial directories you want to play with. 

When learning the library, you need to play with it and see what happens. To this end, open the  [2.x.13]  source file with your favorite editor and modify it in some way, save it and run it as above. A few suggestions for possibly modifications are given at the end of the results section of this program, where we also provide a few links to other useful pieces of information. 


[1.x.33][1.x.34] 


This and several of the other tutorial programs are also discussed and demonstrated in [1.x.35] on deal.II and computational science. In particular, you can see the steps he executes to run this and other programs, and you will get a much better idea of the tools that can be used to work with deal.II. In particular, lectures 2 and 4 give an overview of deal.II and of the building blocks of any finite element code. ( [2.x.14]  

If you are not yet familiar with using Linux and running things on the command line, you may be interested in watching lectures 2.9 and 2.91. ( [2.x.15]  line and on what happens when compiling programs, respectively. 

Note that deal.II is actively developed, and in the course of this development we occasionally rename or deprecate functions or classes that are still referenced in these video lectures.  For example, the  [2.x.16]  code shown in video lecture 5 uses a class HyperShellBoundary which was replaced with SphericalManifold class later on. Additionally, as of deal.II version 9.0,  [2.x.17]  now automatically attaches a SphericalManifold to the Triangulation. Otherwise the rest of the lecture material is relevant. 

[1.x.36][1.x.37] 


Let's come back to  [2.x.18] , the current program. In this first example, we don't actually do very much, but show two techniques: what is the syntax to generate triangulation objects, and some elements of simple loops over all cells. We create two grids, one which is a regularly refined square (not very exciting, but a common starting grid for some problems), and one more geometric attempt: a ring-shaped domain, which is refined towards the inner edge. Through this, you will get to know three things every finite element program will have to have somewhere: An object of type Triangulation for the mesh; a call to the GridGenerator functions to generate a mesh; and loops over all cells that involve iterators (iterators are a generalization of pointers and are frequently used in the C++ standard library; in the context of deal.II, the  [2.x.19]  module talks about them). 

The program is otherwise small enough that it doesn't need a whole lot of introduction. 

 [2.x.20]  


[1.x.38][1.x.39] 


If you are reading through this tutorial program, chances are that you are interested in continuing to use deal.II for your own projects. Thus, you are about to embark on an exercise in programming using a large-scale scientific computing library. Unless you are already an experienced user of large-scale programming methods, this may be new territory for you &mdash; with all the new rules that go along with it such as the fact that you will have to deal with code written by others, that you may have to think about documenting your own code because you may not remember what exactly it is doing a year down the road (or because others will be using it as well), or coming up with ways to test that your program is doing the right thing. None of this is something that we typically train mathematicians, engineers, or scientists in but that is important when you start writing software of more than a few hundred lines. Remember: Producing software is not the same as just writing code. 

To make your life easier on this journey let us point to some resources that are worthwhile browsing through before you start any large-scale programming: 

- The [1.x.40] has a good number of answers to questions about   particular aspects of deal.II, but also to more general questions such as "How   do I debug scientific computing codes?" or "Can I train myself to write code   that has fewer bugs?". 

- You will benefit from becoming a better programmer. An excellent   resource to this end is the book   [Code Complete](https://en.wikipedia.org/wiki/Code_Complete)   by Steve McConnell  [2.x.21]  . It's already   a few years old, with the last edition published in 2004, but it has   lost none of its appeal as a guide to good programming practices,   and some of the principal developers use it as a group reading   project with every generation of their research group members. 

- The [1.x.41]   that provides introductions to many topics that are important to dealing   with software, such as version control, make files, testing, etc. It is   specifically written for scientists and engineers, not for computer   scientists, and has a focus on short, practical lessons. 

- The [1.x.42] has a lot of resources (and interesting blog posts) that   cover many aspects of writing scientific software. 

- The [1.x.43] also has resources on software development, in   particular for parallel computing. In the "Events" section on   that site are recorded tutorials and webinars that cover many   interesting topics. 

- An article on [1.x.44] that gives an introduction to   many of the ways by which you can make sure you are an efficient   programmer writing programs that work. 

As a general recommendation: If you expect to spend more than a few days writing software in the future, do yourself the favor of learning tools that can make your life more productive, in particular debuggers and integrated development environments. ( [2.x.22]  You will find that you will get the time spent learning these tools back severalfold soon by being more productive! Several of the video lectures referenced above show how to use tools such as integrated development environments or debuggers. [1.x.45] [1.x.46] 


[1.x.47]  [1.x.48] 




The most fundamental class in the library is the Triangulation class, which is declared here: 

[1.x.49] 



Here are some functions to generate standard grids: 

[1.x.50] 



Output of grids in various graphics formats: 

[1.x.51] 



This is needed for C++ output: 

[1.x.52] 



And this for the declarations of the  [2.x.23]  and  [2.x.24]  functions: 

[1.x.53] 



The final step in importing deal.II is this: All deal.II functions and classes are in a namespace  [2.x.25] , to make sure they don't clash with symbols from other libraries you may want to use in conjunction with deal.II. One could use these functions and classes by prefixing every use of these names by  [2.x.26] , but that would quickly become cumbersome and annoying. Rather, we simply import the entire deal.II namespace for general use: 

[1.x.54] 




[1.x.55]  [1.x.56] 




In the following, first function, we simply use the unit square as domain and produce a globally refined grid from it. 

[1.x.57] 



The first thing to do is to define an object for a triangulation of a two-dimensional domain: 

[1.x.58] 



Here and in many following cases, the string "<2>" after a class name indicates that this is an object that shall work in two space dimensions. Likewise, there are versions of the triangulation class that are working in one ("<1>") and three ("<3>") space dimensions. The way this works is through some template magic that we will investigate in some more detail in later example programs; there, we will also see how to write programs in an essentially dimension independent way. 




Next, we want to fill the triangulation with a single cell for a square domain. The triangulation is the refined four times, to yield  [2.x.27]  cells in total: 

[1.x.59] 



Now we want to write a graphical representation of the mesh to an output file. The GridOut class of deal.II can do that in a number of different output formats; here, we choose scalable vector graphics (SVG) format that you can visualize using the web browser of your choice: 

[1.x.60] 




[1.x.61]  [1.x.62] 




The grid in the following, second function is slightly more complicated in that we use a ring domain and refine the result once globally. 

[1.x.63] 



We start again by defining an object for a triangulation of a two-dimensional domain: 

[1.x.64] 



We then fill it with a ring domain. The center of the ring shall be the point (1,0), and inner and outer radius shall be 0.5 and 1. The number of circumferential cells could be adjusted automatically by this function, but we choose to set it explicitly to 10 as the last argument: 

[1.x.65] 



By default, the triangulation assumes that all boundaries are straight lines, and all cells are bi-linear quads or tri-linear hexes, and that they are defined by the cells of the coarse grid (which we just created). Unless we do something special, when new points need to be introduced the domain is assumed to be delineated by the straight lines of the coarse mesh, and new points will simply be in the middle of the surrounding ones. Here, however, we know that the domain is curved, and we would like to have the Triangulation place new points according to the underlying geometry. Fortunately, some good soul implemented an object which describes a spherical domain, of which the ring is a section; it only needs the center of the ring and automatically figures out how to instruct the Triangulation where to place the new points. The way this works in deal.II is that you tag parts of the triangulation you want to be curved with a number that is usually referred to as "manifold indicator" and then tell the triangulation to use a particular "manifold object" for all places with this manifold indicator. How exactly this works is not important at this point (you can read up on it in  [2.x.28]  and  [2.x.29] ). The functions in GridGenerator handle this for us in most circumstances: they attach the correct manifold to a domain so that when the triangulation is refined new cells are placed in the correct places. In the present case  [2.x.30]  attaches a SphericalManifold to all cells: this causes cells to be refined with calculations in spherical coordinates (so new cells have edges that are either radial or lie along concentric circles around the origin).    


By default (i.e., for a Triangulation created by hand or without a call to a GridGenerator function like  [2.x.31]  or  [2.x.32]  all cells and faces of the Triangulation have their manifold_id set to  [2.x.33]  which is the default if you want a manifold that produces straight edges, but you can change this number for individual cells and faces. In that case, the curved manifold thus associated with number zero will not apply to those parts with a non-zero manifold indicator, but other manifold description objects can be associated with those non-zero indicators. If no manifold description is associated with a particular manifold indicator, a manifold that produces straight edges is implied. (Manifold indicators are a slightly complicated topic; if you're confused about what exactly is happening here, you may want to look at the  [2.x.34]  "glossary entry on this topic".) Since the default chosen by  [2.x.35]  is reasonable we leave things alone.    


In order to demonstrate how to write a loop over all cells, we will refine the grid in five steps towards the inner circle of the domain: 

[1.x.66] 



Next, we need to loop over the active cells of the triangulation. You can think of a triangulation as a collection of cells. If it were an array, you would just get a pointer that you increment from one element to the next using the operator `++`. The cells of a triangulation aren't stored as a simple array, but the concept of an [1.x.67] generalizes how pointers work to arbitrary collections of objects (see [1.x.68] for more information). Typically, any container type in C++ will return an iterator pointing to the start of the collection with a method called `begin`, and an iterator point to 1 past the end of the collection with a method called `end`. We can increment an iterator `it` with the operator `++it`, dereference it to get the underlying data with `*it`, and check to see if we're done by comparing `it != collection.end()`.        


The second important piece is that we only need the active cells. Active cells are those that are not further refined, and the only ones that can be marked for further refinement. deal.II provides iterator categories that allow us to iterate over [1.x.69] cells (including the parent cells of active ones) or only over the active cells. Because we want the latter, we need to call the method  [2.x.36]         


Putting all of this together, we can loop over all the active cells of a triangulation with  [2.x.37]  In the initializer of this loop, we've used the `auto` keyword for the type of the iterator `it`. The `auto` keyword means that the type of the object being declared will be inferred from the context. This keyword is useful when the actual type names are long or possibly even redundant. If you're unsure of what the type is and want to look up what operations the result supports, you can go to the documentation for the method  [2.x.38]  In this case, the type of `it` is  [2.x.39]         


While the `auto` keyword can save us from having to type out long names of data types, we still have to type a lot of redundant declarations about the start and end iterator and how to increment it. Instead of doing that, we'll use [1.x.71], which wrap up all of the syntax shown above into a much shorter form: 

[1.x.72] 



 [2.x.40]  See  [2.x.41]  for more information about the iterator classes used in deal.II, and  [2.x.42]  for more information about range-based for loops and the `auto` keyword.            


Next, we loop over all vertices of the cells. For that purpose we query an iterator over the vertex indices (in 2d, this is an array that contains the elements `{0,1,2,3}`, but since `cell->vertex_indices()` knows the dimension the cell lives in, the array so returned is correct in all dimensions and this enables this code to be correct whether we run it in 2d or 3d, i.e., it enables "dimension-independent programming" -- a big part of what we will discuss in  [2.x.43] ). 

[1.x.73] 



If this cell is at the inner boundary, then at least one of its vertices must sit on the inner ring and therefore have a radial distance from the center of exactly 0.5, up to floating point accuracy. So we compute this distance, and if we find a vertex with this property, we flag this cell for later refinement. We can then also break the loop over all vertices and move on to the next cell.                


Because the distance from the center is computed as a floating point number, we have to expect that whatever we compute is only accurate to within [round-off](https://en.wikipedia.org/wiki/Round-off_error). As a consequence, we can never expect to compare the distance with the inner radius by equality: A statement such as `if (distance_from_center == inner_radius)` will fail unless we get exceptionally lucky. Rather, we need to do this comparison with a certain tolerance, and the usual way to do this is to write it as `if  [2.x.44]  - inner_radius) <= tolerance)` where `tolerance` is some small number larger than round-off. The question is how to choose it: We could just pick, say, `1e-10`, but this is only appropriate if the objects we compare are of size one. If we had created a mesh with cells of size `1e+10`, then `1e-10` would be far lower than round-off and, as before, the comparison will only succeed if we get exceptionally lucky. Rather, it is almost always useful to make the tolerance *relative* to a typical "scale" of the objects being compared. Here, the "scale" would be the inner radius, or maybe the diameter of cells. We choose the former and set the tolerance equal to  [2.x.45]  times the inner radius of the annulus. 

[1.x.74] 



Now that we have marked all the cells that we want refined, we let the triangulation actually do this refinement. The function that does so owes its long name to the fact that one can also mark cells for coarsening, and the function does coarsening and refinement all at once: 

[1.x.75] 



Finally, after these five iterations of refinement, we want to again write the resulting mesh to a file, again in SVG format. This works just as above: 

[1.x.76] 




[1.x.77]  [1.x.78] 




Finally, the main function. There isn't much to do here, only to call the two subfunctions, which produce the two grids. 

[1.x.79] 

[1.x.80][1.x.81] 


Running the program produces graphics of two grids (grid-1.svg and grid-2.svg). You can open these with most every web browser -- in the simplest case, just open the current directory in your file system explorer and click on the file. If you like working on the command line, you call your web browser with the file: `firefox grid-1.svg`, `google-chrome grid-1.svg`, or whatever the name of your browser is. If you do this, the two meshes should look like this: 

 [2.x.46]  

The left one, well, is not very exciting. The right one is &mdash; at least &mdash; unconventional. The pictures color-code the "refinement level" of each cell: How many times did a coarse mesh cell have to be subdivided to obtain the given cell. In the left image, this is boring since the mesh was refined globally a number of times, i.e., [1.x.82] cell was refined the same number of times. 

(While the second mesh is entirely artificial and made-up, and certainly not very practical in applications, to everyone's surprise it has found its way into the literature: see  [2.x.47] . Apparently it is good for some things at least.) 


[1.x.83][1.x.84] 


[1.x.85][1.x.86] 


This program obviously does not have a whole lot of functionality, but in particular the  [2.x.48]  function has a bunch of places where you can play with it. For example, you could modify the criterion by which we decide which cells to refine. An example would be to change the condition to this: 

[1.x.87] 

This would refine all cells for which the  [2.x.49] -coordinate of the cell's center is greater than zero (the  [2.x.50]  function that we call by dereferencing the  [2.x.51]  iterator returns a Point<2> object; subscripting  [2.x.52]  would give the  [2.x.53] -coordinate, subscripting  [2.x.54]  the  [2.x.55] -coordinate). By looking at the functions that TriaAccessor provides, you can also use more complicated criteria for refinement. 

In general, what you can do with operations of the form `cell->something()` is a bit difficult to find in the documentation because `cell` is not a pointer but an iterator. The functions you can call on a cell can be found in the documentation of the classes `TriaAccessor` (which has functions that can also be called on faces of cells or, more generally, all sorts of geometric objects that appear in a triangulation), and `CellAccessor` (which adds a few functions that are specific to *cells*). 

A more thorough description of the whole iterator concept can be found in the  [2.x.56]  documentation module. 


[1.x.88][1.x.89] 


Another possibility would be to generate meshes of entirely different geometries altogether. While for complex geometries there is no way around using meshes obtained from mesh generators, there is a good number of geometries for which deal.II can create meshes using the functions in the GridGenerator namespace. Many of these geometries (such as the one used in this example program) contain cells with curved faces: put another way, we expect the new vertices placed on the boundary to lie along a circle. deal.II handles complex geometries with the Manifold class (and classes inheriting from it); in particular, the functions in GridGenerator corresponding to non-Cartesian grids (such as  [2.x.57]  or  [2.x.58]  attach a Manifold object to the part of the triangulation that should be curved (SphericalManifold and CylindricalManifold, respectively) and use another manifold on the parts that should be flat (FlatManifold). See the documentation of Manifold or the  [2.x.59]  "manifold module" for descriptions of the design philosophy and interfaces of these classes. Take a look at what they provide and see how they could be used in a program like this. 

We also discuss a variety of other ways to create and manipulate meshes (and describe the process of attaching Manifolds) in  [2.x.60] . 


[1.x.90][1.x.91] 


We close with a comment about modifying or writing programs with deal.II in general. When you start working with tutorial programs or your own applications, you will find that mistakes happen: your program will contain code that either aborts the program right away or bugs that simply lead to wrong results. In either case, you will find it extremely helpful to know how to work with a debugger: you may get by for a while by just putting debug output into your program, compiling it, and running it, but ultimately finding bugs with a debugger is much faster, much more convenient, and more reliable because you don't have to recompile the program all the time and because you can inspect the values of variables and how they change. 

Rather than postponing learning how to use a debugger till you really can't see any other way to find a bug, here's the one piece of advice we will provide in this program: learn how to use a debugger as soon as possible. It will be time well invested. ( [2.x.61]  Questions (FAQ) page linked to from the top-level [1.x.92] also provides a good number of hints on debugging deal.II programs. 


[1.x.93][1.x.94] 


It is often useful to include meshes into your theses or publications. For this, it may not be very useful to color-code the cells by refinement level, and to print the cell number onto each cell. But it doesn't have to be that way -- the GridOut class allows setting flags for each possible output format (see the classes in the GridOutFlags namespace) that control how exactly a mesh is plotted. You can of course also choose other output file formats such as VTK or VTU; this is particularly useful for 3d meshes where a 2d format such as SVG is not particular useful because it fixes a particular viewpoint onto the 3d object. As a consequence, you might want to explore other options in the GridOut class. [1.x.95] [1.x.96]  [2.x.62]  

 [2.x.63] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12] 

[1.x.13] [1.x.14][1.x.15] 


 [2.x.3]  

After we have created a grid in the previous example, we now show how to define degrees of freedom on this mesh. For this example, we will use the lowest order ( [2.x.4] ) finite elements, for which the degrees of freedom are associated with the vertices of the mesh. Later examples will demonstrate higher order elements where degrees of freedom are not necessarily associated with vertices any more, but can be associated with edges, faces, or cells. 

The term "degree of freedom" is commonly used in the finite element community to indicate two slightly different, but related things. The first is that we'd like to represent the finite element solution as a linear combination of shape functions, in the form  [2.x.5] . Here,  [2.x.6]  is a vector of expansion coefficients. Because we don't know their values yet (we will compute them as the solution of a linear or nonlinear system), they are called "unknowns" or "degrees of freedom". The second meaning of the term can be explained as follows: A mathematical description of finite element problems is often to say that we are looking for a finite dimensional function  [2.x.7]  that satisfies some set of equations (e.g.  [2.x.8]  for all test functions  [2.x.9] ). In other words, all we say here that the solution needs to lie in some space  [2.x.10] . However, to actually solve this problem on a computer we need to choose a basis of this space; this is the set of shape functions  [2.x.11]  we have used above in the expansion of  [2.x.12]  with coefficients  [2.x.13] . There are of course many bases of the space  [2.x.14] , but we will specifically choose the one that is described by the finite element functions that are traditionally defined locally on the cells of the mesh. Describing "degrees of freedom" in this context requires us to simply [1.x.16] the basis functions of the space  [2.x.15] . For  [2.x.16]  elements this means simply enumerating the vertices of the mesh in some way, but for higher order elements, one also has to enumerate the shape functions that are associated with edges, faces, or cell interiors of the mesh. In other words, the enumeration of degrees of freedom is an entirely separate thing from the indices we use for vertices. The class that provides this enumeration of the basis functions of  [2.x.17]  is called DoFHandler. 

Defining degrees of freedom ("DoF"s in short) on a mesh is a rather simple task, since the library does all the work for you. Essentially, all you have to do is create a finite element object (from one of the many finite element classes deal.II already has, see for example the  [2.x.18]  documentation) and give it to a DoFHandler object through the  [2.x.19]  function ("distributing DoFs" is the term we use to describe the process of [1.x.17] the basis functions as discussed above). The DoFHandler is a class that knows which degrees of freedom live where, i.e., it can answer questions like "how many degrees of freedom are there globally" and "on this cell, give me the global indices of the shape functions that live here". This is the sort of information you need when determining how big your system matrix should be, and when copying the contributions of a single cell into the global matrix. 

[1.x.18][1.x.19] 


The next step would then be to compute a matrix and right hand side corresponding to a particular differential equation using this finite element and mesh. We will keep this step for the  [2.x.20]  program and rather talk about one practical aspect of a finite element program, namely that finite element matrices are always very sparse: almost all entries in these matrices are zero. 

To be more precise, we say that a matrix is sparse if the number of nonzero entries [1.x.20] in the matrix is bounded by a number that is independent of the overall number of degrees of freedom. For example, the simple 5-point stencil of a finite difference approximation of the Laplace equation leads to a sparse matrix since the number of nonzero entries per row is five, and therefore independent of the total size of the matrix. For more complicated problems -- say, the Stokes problem of  [2.x.21]  -- and in particular in 3d, the number of entries per row may be several hundred. But the important point is that this number is independent of the overall size of the problem: If you refine the mesh, the maximal number of unknowns per row remains the same. 

Sparsity is one of the distinguishing feature of the finite element method compared to, say, approximating the solution of a partial differential equation using a Taylor expansion and matching coefficients, or using a Fourier basis. 

In practical terms, it is the sparsity of matrices that enables us to solve problems with millions or billions of unknowns. To understand this, note that a matrix with  [2.x.22]  rows, each with a fixed upper bound for the number of nonzero entries, requires  [2.x.23]  memory locations for storage, and a matrix-vector multiplication also requires only  [2.x.24]  operations. Consequently, if we had a linear solver that requires only a fixed number of matrix-vector multiplications to come up with the solution of a linear system with this matrix, then we would have a solver that can find the values of all  [2.x.25]  unknowns with optimal complexity, i.e., with a total of  [2.x.26]  operations. It is clear that this wouldn't be possible if the matrix were not sparse (because then the number of entries in the matrix would have to be  [2.x.27]  with some  [2.x.28] , and doing a fixed number of matrix-vector products would take  [2.x.29]  operations), but it also requires very specialized solvers such as multigrid methods to satisfy the requirement that the solution requires only a fixed number of matrix-vector multiplications. We will frequently look at the question of what solver to use in the remaining programs of this tutorial. 

The sparsity is generated by the fact that finite element shape functions are defined [1.x.21] on individual cells, rather than globally, and that the local differential operators in the bilinear form only couple shape functions whose support overlaps. (The "support" of a function is the area where it is nonzero. For the finite element method, the support of a shape function is generally the cells adjacent to the vertex, edge, or face it is defined on.) In other words, degrees of freedom  [2.x.30]  and  [2.x.31]  that are not defined on the same cell do not overlap, and consequently the matrix entry  [2.x.32]  will be zero.  (In some cases such as the Discontinuous Galerkin method, shape functions may also connect to neighboring cells through face integrals. But finite element methods do not generally couple shape functions beyond the immediate neighbors of a cell on which the function is defined.) 


[1.x.22][1.x.23] 


By default, the DoFHandler class enumerates degrees of freedom on a mesh in a rather random way; consequently, the sparsity pattern is also not optimized for any particular purpose. To show this, the code below will demonstrate a simple way to output the "sparsity pattern" that corresponds to a DoFHandler, i.e., an object that represents all of the potentially nonzero elements of a matrix one may build when discretizing a partial differential equation on a mesh and its DoFHandler. This lack of structure in the sparsity pattern will be apparent from the pictures we show below. 

For most applications and algorithms, the exact way in which degrees of freedom are numbered does not matter. For example, the Conjugate Gradient method we use to solve linear systems does not care. On the other hand, some algorithms do care: in particular, some preconditioners such as SSOR will work better if they can walk through degrees of freedom in a particular order, and it would be nice if we could just sort them in such a way that SSOR can iterate through them from zero to  [2.x.33]  in this order. Other examples include computing incomplete LU or Cholesky factorizations, or if we care about the block structure of matrices (see  [2.x.34]  for an example). deal.II therefore has algorithms that can re-enumerate degrees of freedom in particular ways in namespace DoFRenumbering. Renumbering can be thought of as choosing a different, permuted basis of the finite element space. The sparsity pattern and matrices that result from this renumbering are therefore also simply a permutation of rows and columns compared to the ones we would get without explicit renumbering. 

In the program below, we will use the algorithm of Cuthill and McKee to do so. We will show the sparsity pattern for both the original enumeration of degrees of freedom and of the renumbered version below, in the [1.x.24]. [1.x.25] [1.x.26] 

The first few includes are just like in the previous program, so do not require additional comments: 

[1.x.27] 



However, the next file is new. We need this include file for the association of degrees of freedom ("DoF"s) to vertices, lines, and cells: 

[1.x.28] 



The following include contains the description of the bilinear finite element, including the facts that it has one degree of freedom on each vertex of the triangulation, but none on faces and none in the interior of the cells. 




(In fact, the file contains the description of Lagrange elements in general, i.e. also the quadratic, cubic, etc versions, and not only for 2d but also 1d and 3d.) 

[1.x.29] 



In the following file, several tools for manipulating degrees of freedom can be found: 

[1.x.30] 



We will use a sparse matrix to visualize the pattern of nonzero entries resulting from the distribution of degrees of freedom on the grid. That class can be found here: 

[1.x.31] 



We will also need to use an intermediate sparsity pattern structure, which is found in this file: 

[1.x.32] 



We will want to use a special algorithm to renumber degrees of freedom. It is declared here: 

[1.x.33] 



And this is again needed for C++ output: 

[1.x.34] 



Finally, as in  [2.x.35] , we import the deal.II namespace into the global scope: 

[1.x.35] 




[1.x.36]  [1.x.37] 




This is the function that produced the circular grid in the previous  [2.x.36]  example program with fewer refinements steps. The sole difference is that it returns the grid it produces via its argument. 

[1.x.38] 




[1.x.39]  [1.x.40] 




Up to now, we only have a grid, i.e. some geometrical (the position of the vertices) and some topological information (how vertices are connected to lines, and lines to cells, as well as which cells neighbor which other cells). To use numerical algorithms, one needs some logic information in addition to that: we would like to associate degree of freedom numbers to each vertex (or line, or cell, in case we were using higher order elements) to later generate matrices and vectors which describe a finite element field on the triangulation. 




This function shows how to do this. The object to consider is the  [2.x.37]  class template.  Before we do so, however, we first need something that describes how many degrees of freedom are to be associated to each of these objects. Since this is one aspect of the definition of a finite element space, the finite element base class stores this information. In the present context, we therefore create an object of the derived class  [2.x.38]  that describes Lagrange elements. Its constructor takes one argument that states the polynomial degree of the element, which here is one (indicating a bi-linear element); this then corresponds to one degree of freedom for each vertex, while there are none on lines and inside the quadrilateral. A value of, say, three given to the constructor would instead give us a bi-cubic element with one degree of freedom per vertex, two per line, and four inside the cell. In general,  [2.x.39]  denotes the family of continuous elements with complete polynomials (i.e. tensor-product polynomials) up to the specified order. 




We first need to create an object of this class and then pass it on to the  [2.x.40]  object to allocate storage for the degrees of freedom (in deal.II lingo: we [1.x.41]). 

[1.x.42] 



Now that we have associated a degree of freedom with a global number to each vertex, we wonder how to visualize this?  There is no simple way to directly visualize the DoF number associated with each vertex. However, such information would hardly ever be truly important, since the numbering itself is more or less arbitrary. There are more important factors, of which we will demonstrate one in the following.    


Associated with each vertex of the triangulation is a shape function. Assume we want to solve something like Laplace's equation, then the different matrix entries will be the integrals over the gradient of each pair of such shape functions. Obviously, since the shape functions are nonzero only on the cells adjacent to the vertex they are associated with, matrix entries will be nonzero only if the supports of the shape functions associated to that column and row %numbers intersect. This is only the case for adjacent shape functions, and therefore only for adjacent vertices. Now, since the vertices are numbered more or less randomly by the above function  [2.x.41]  the pattern of nonzero entries in the matrix will be somewhat ragged, and we will take a look at it now.    


First we have to create a structure which we use to store the places of nonzero elements. This can then later be used by one or more sparse matrix objects that store the values of the entries in the locations stored by this sparsity pattern. The class that stores the locations is the SparsityPattern class. As it turns out, however, this class has some drawbacks when we try to fill it right away: its data structures are set up in such a way that we need to have an estimate for the maximal number of entries we may wish to have in each row. In two space dimensions, reasonable values for this estimate are available through the  [2.x.42]  function, but in three dimensions the function almost always severely overestimates the true number, leading to a lot of wasted memory, sometimes too much for the machine used, even if the unused memory can be released immediately after computing the sparsity pattern. In order to avoid this, we use an intermediate object of type DynamicSparsityPattern that uses a different %internal data structure and that we can later copy into the SparsityPattern object without much overhead. (Some more information on these data structures can be found in the  [2.x.43]  module.) In order to initialize this intermediate data structure, we have to give it the size of the matrix, which in our case will be square with as many rows and columns as there are degrees of freedom on the grid: 

[1.x.43] 



We then fill this object with the places where nonzero elements will be located given the present numbering of degrees of freedom: 

[1.x.44] 



Now we are ready to create the actual sparsity pattern that we could later use for our matrix. It will just contain the data already assembled in the DynamicSparsityPattern. 

[1.x.45] 



With this, we can now write the results to a file: 

[1.x.46] 



The result is stored in an  [2.x.44]  file, where each nonzero entry in the matrix corresponds with a red square in the image. The output will be shown below.    


If you look at it, you will note that the sparsity pattern is symmetric. This should not come as a surprise, since we have not given the  [2.x.45]  any information that would indicate that our bilinear form may couple shape functions in a non-symmetric way. You will also note that it has several distinct region, which stem from the fact that the numbering starts from the coarsest cells and moves on to the finer ones; since they are all distributed symmetrically around the origin, this shows up again in the sparsity pattern. 

[1.x.47] 




[1.x.48]  [1.x.49] 




In the sparsity pattern produced above, the nonzero entries extended quite far off from the diagonal. For some algorithms, for example for incomplete LU decompositions or Gauss-Seidel preconditioners, this is unfavorable, and we will show a simple way how to improve this situation. 




Remember that for an entry  [2.x.46]  in the matrix to be nonzero, the supports of the shape functions i and j needed to intersect (otherwise in the integral, the integrand would be zero everywhere since either the one or the other shape function is zero at some point). However, the supports of shape functions intersected only if they were adjacent to each other, so in order to have the nonzero entries clustered around the diagonal (where  [2.x.47]  equals  [2.x.48] ), we would like to have adjacent shape functions to be numbered with indices (DoF numbers) that differ not too much. 




This can be accomplished by a simple front marching algorithm, where one starts at a given vertex and gives it the index zero. Then, its neighbors are numbered successively, making their indices close to the original one. Then, their neighbors, if not yet numbered, are numbered, and so on. 




One algorithm that adds a little bit of sophistication along these lines is the one by Cuthill and McKee. We will use it in the following function to renumber the degrees of freedom such that the resulting sparsity pattern is more localized around the diagonal. The only interesting part of the function is the first call to  [2.x.49] , the rest is essentially as before: 

[1.x.50] 



Again, the output is shown below. Note that the nonzero entries are clustered far better around the diagonal than before. This effect is even more distinguished for larger matrices (the present one has 1260 rows and columns, but large matrices often have several 100,000s). 




It is worth noting that the  [2.x.50]  class offers a number of other algorithms as well to renumber degrees of freedom. For example, it would of course be ideal if all couplings were in the lower or upper triangular part of a matrix, since then solving the linear system would amount to only forward or backward substitution. This is of course unachievable for symmetric sparsity patterns, but in some special situations involving transport equations, this is possible by enumerating degrees of freedom from the inflow boundary along streamlines to the outflow boundary. Not surprisingly,  [2.x.51]  also has algorithms for this. 










[1.x.51]  [1.x.52] 




Finally, this is the main program. The only thing it does is to allocate and create the triangulation, then create a  [2.x.52]  object and associate it to the triangulation, and finally call above two functions on it: 

[1.x.53] 

[1.x.54][1.x.55] 


The program has, after having been run, produced two sparsity patterns. We can visualize them by opening the  [2.x.53]  files in a web browser. 

The results then look like this (every point denotes an entry which might be nonzero; of course the fact whether the entry actually is zero or not depends on the equation under consideration, but the indicated positions in the matrix tell us which shape functions can and which can't couple when discretizing a local, i.e. differential, equation):  [2.x.54]  

The different regions in the left picture, indicated by kinks in the lines and single dots on the left and top, represent the degrees of freedom on the different refinement levels of the triangulation.  As can be seen in the right picture, the sparsity pattern is much better clustered around the main diagonal of the matrix after renumbering. Although this might not be apparent, the number of nonzero entries is the same in both pictures, of course. 




[1.x.56][1.x.57] 


Just as with  [2.x.55] , you may want to play with the program a bit to familiarize yourself with deal.II. For example, in the  [2.x.56]  function, we use linear finite elements (that's what the argument "1" to the FE_Q object is). Explore how the sparsity pattern changes if you use higher order elements, for example cubic or quintic ones (by using 3 and 5 as the respective arguments). 

You could also explore how the sparsity pattern changes by refining the mesh. You will see that not only the size of the matrix changes, but also its bandwidth (the distance from the diagonal of those nonzero elements of the matrix that are farthest away from the diagonal), though the ratio of bandwidth to size typically shrinks, i.e. the matrix clusters more around the diagonal. 

Another idea of experiments would be to try other renumbering strategies than Cuthill-McKee from the DoFRenumbering namespace and see how they affect the sparsity pattern. 

You can also visualize the output using [1.x.58] (one of the simpler visualization programs; maybe not the easiest to use since it is command line driven, but also universally available on all Linux and other Unix-like systems) by changing from  [2.x.57] : 

[1.x.59] 



Another practice based on [1.x.60] is trying to print out the mesh with locations and numbering of the support points. For that, you need to include header files for GridOut and MappingQ1. The code for this is: 

[1.x.61] 

After we run the code, we get a file called gnuplot.gpl. To view this file, we can run the following code in the command line: 

[1.x.62]. With that, you will get a picture similar to  [2.x.58]  depending on the mesh you are looking at. For more information, see  [2.x.59]  [1.x.63] [1.x.64]  [2.x.60]  

 [2.x.61] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25] 

[1.x.26] [1.x.27][1.x.28] 


 [2.x.3]  

[1.x.29][1.x.30] 


This is the first example where we actually use finite elements to compute something. We will solve a simple version of Poisson's equation with zero boundary values, but a nonzero right hand side: 

[1.x.31] 

We will solve this equation on the square,  [2.x.4] , for which you've already learned how to generate a mesh in  [2.x.5]  and  [2.x.6] . In this program, we will also only consider the particular case  [2.x.7]  and come back to how to implement the more general case in the next tutorial program,  [2.x.8] . 

If you've learned about the basics of the finite element method, you will remember the steps we need to take to approximate the solution  [2.x.9]  by a finite dimensional approximation. Specifically, we first need to derive the weak form of the equation above, which we obtain by multiplying the equation by a test function  [2.x.10]  [1.x.32] (we will come back to the reason for multiplying from the left and not from the right below) and integrating over the domain  [2.x.11] : 

[1.x.33] 

This can be integrated by parts: 

[1.x.34] 

The test function  [2.x.12]  has to satisfy the same kind of boundary conditions (in mathematical terms: it needs to come from the tangent space of the set in which we seek the solution), so on the boundary  [2.x.13]  and consequently the weak form we are looking for reads 

[1.x.35] 

where we have used the common notation  [2.x.14] . The problem then asks for a function  [2.x.15]  for which this statement is true for all test functions  [2.x.16]  from the appropriate space (which here is the space  [2.x.17] ). 

Of course we can't find such a function on a computer in the general case, and instead we seek an approximation  [2.x.18] , where the  [2.x.19]  are unknown expansion coefficients we need to determine (the "degrees of freedom" of this problem), and  [2.x.20]  are the finite element shape functions we will use. To define these shape functions, we need the following: 

- A mesh on which to define shape functions. You have already seen how to   generate and manipulate the objects that describe meshes in  [2.x.21]  and    [2.x.22] . 

- A finite element that describes the shape functions we want to use on the   reference cell (which in deal.II is always the unit interval  [2.x.23] , the   unit square  [2.x.24]  or the unit cube  [2.x.25] , depending on which space   dimension you work in). In  [2.x.26] , we had already used an object of type   FE_Q<2>, which denotes the usual Lagrange elements that define shape   functions by interpolation on support points. The simplest one is   FE_Q<2>(1), which uses polynomial degree 1. In 2d, these are often referred   to as [1.x.36], since they are linear in each of the two coordinates   of the reference cell. (In 1d, they would be [1.x.37] and in 3d   [1.x.38]; however, in the deal.II documentation, we will frequently   not make this distinction and simply always call these functions "linear".) 

- A DoFHandler object that enumerates all the degrees of freedom on the mesh,   taking the reference cell description the finite element object provides as   the basis. You've also already seen how to do this in  [2.x.27] . 

- A mapping that tells how the shape functions on the real cell are obtained   from the shape functions defined by the finite element class on the   reference cell. By default, unless you explicitly say otherwise, deal.II   will use a (bi-, tri-)linear mapping for this, so in most cases you don't   have to worry about this step. 

Through these steps, we now have a set of functions  [2.x.28] , and we can define the weak form of the discrete problem: Find a function  [2.x.29] , i.e., find the expansion coefficients  [2.x.30]  mentioned above, so that 

[1.x.39] 

Note that we here follow the convention that everything is counted starting at zero, as common in C and C++. This equation can be rewritten as a linear system if you insert the representation  [2.x.31]  and then observe that 

[1.x.40] 

With this, the problem reads: Find a vector  [2.x.32]  so that 

[1.x.41] 

where the matrix  [2.x.33]  and the right hand side  [2.x.34]  are defined as 

[1.x.42] 




[1.x.43][1.x.44] 


Before we move on with describing how these quantities can be computed, note that if we had multiplied the original equation from the [1.x.45] by a test function rather than from the left, then we would have obtained a linear system of the form 

[1.x.46] 

with a row vector  [2.x.35] . By transposing this system, this is of course equivalent to solving 

[1.x.47] 

which here is the same as above since  [2.x.36] . But in general is not, and in order to avoid any sort of confusion, experience has shown that simply getting into the habit of multiplying the equation from the left rather than from the right (as is often done in the mathematical literature) avoids a common class of errors as the matrix is automatically correct and does not need to be transposed when comparing theory and implementation. See  [2.x.37]  for the first example in this tutorial where we have a non-symmetric bilinear form for which it makes a difference whether we multiply from the right or from the left. 


[1.x.48][1.x.49] 


Now we know what we need (namely: objects that hold the matrix and vectors, as well as ways to compute  [2.x.38] ), and we can look at what it takes to make that happen: 

- The object for  [2.x.39]  is of type SparseMatrix while those for  [2.x.40]  and  [2.x.41]  are of   type Vector. We will see in the program below what classes are used to solve   linear systems. 

- We need a way to form the integrals. In the finite element method, this is   most commonly done using quadrature, i.e. the integrals are replaced by a   weighted sum over a set of points on each cell. That is, we first split the   integral over  [2.x.42]  into integrals over all cells,   [1.x.50] 

  and then approximate each cell's contribution by quadrature:   [1.x.51] 

  where  [2.x.43]  is the  [2.x.44] th quadrature point on cell  [2.x.45] , and  [2.x.46]    the  [2.x.47] th quadrature weight. There are different parts to what is needed in   doing this, and we will discuss them in turn next. 

- First, we need a way to describe the location  [2.x.48]  of quadrature   points and their weights  [2.x.49] . They are usually mapped from the reference   cell in the same way as shape functions, i.e., implicitly using the   MappingQ1 class or, if you explicitly say so, through one of the other   classes derived from Mapping. The locations and weights on the reference   cell are described by objects derived from the Quadrature base   class. Typically, one chooses a quadrature formula (i.e. a set of points and   weights) so that the quadrature exactly equals the integral in the matrix;   this can be achieved because all factors in the integral are polynomial, and   is done by Gaussian quadrature formulas, implemented in the QGauss class. 

- We then need something that can help us evaluate  [2.x.50]    on cell  [2.x.51] . This is what the FEValues class does: it takes a finite element   objects to describe  [2.x.52]  on the reference cell, a quadrature object to   describe the quadrature points and weights, and a mapping object (or   implicitly takes the MappingQ1 class) and provides values and derivatives of   the shape functions on the real cell  [2.x.53]  as well as all sorts of other   information needed for integration, at the quadrature points located on  [2.x.54] . 

FEValues really is the central class in the assembly process. One way you can view it is as follows: The FiniteElement and derived classes describe shape [1.x.52], i.e., infinite dimensional objects: functions have values at every point. We need this for theoretical reasons because we want to perform our analysis with integrals over functions. However, for a computer, this is a very difficult concept, since they can in general only deal with a finite amount of information, and so we replace integrals by sums over quadrature points that we obtain by mapping (the Mapping object) using  points defined on a reference cell (the Quadrature object) onto points on the real cell. In essence, we reduce the problem to one where we only need a finite amount of information, namely shape function values and derivatives, quadrature weights, normal vectors, etc, exclusively at a finite set of points. The FEValues class is the one that brings the three components together and provides this finite set of information on a particular cell  [2.x.55] . You will see it in action when we assemble the linear system below. 

It is noteworthy that all of this could also be achieved if you simply created these three objects yourself in an application program, and juggled the information yourself. However, this would neither be simpler (the FEValues class provides exactly the kind of information you actually need) nor faster: the FEValues class is highly optimized to only compute on each cell the particular information you need; if anything can be re-used from the previous cell, then it will do so, and there is a lot of code in that class to make sure things are cached wherever this is advantageous. 

The final piece of this introduction is to mention that after a linear system is obtained, it is solved using an iterative solver and then postprocessed: we create an output file using the DataOut class that can then be visualized using one of the common visualization programs. 

 [2.x.56]  The preceding overview of all the important steps of any finite element implementation has its counterpart in deal.II: The library can naturally be grouped into a number of "modules" that cover the basic concepts just outlined. You can access these modules through the tab at the top of this page. An overview of the most fundamental groups of concepts is also available on the [1.x.53]. 


[1.x.54][1.x.55] 


Although this is the simplest possible equation you can solve using the finite element method, this program shows the basic structure of most finite element programs and also serves as the template that almost all of the following programs will essentially follow. Specifically, the main class of this program looks like this: 

[1.x.56] 



This follows the object oriented programming mantra of [1.x.57], i.e. we do our best to hide almost all internal details of this class in private members that are not accessible to the outside. 

Let's start with the member variables: These follow the building blocks we have outlined above in the bullet points, namely we need a Triangulation and a DoFHandler object, and a finite element object that describes the kinds of shape functions we want to use. The second group of objects relate to the linear algebra: the system matrix and right hand side as well as the solution vector, and an object that describes the sparsity pattern of the matrix. This is all this class needs (and the essentials that any solver for a stationary PDE requires) and that needs to survive throughout the entire program. In contrast to this, the FEValues object we need for assembly is only required throughout assembly, and so we create it as a local object in the function that does that and destroy it again at its end. 

Secondly, let's look at the member functions. These, as well, already form the common structure that almost all following tutorial programs will use:  [2.x.57]     [2.x.58]   [2.x.59] : This is what one could call a        [1.x.58]. As its name suggests, it sets up the        object that stores the triangulation. In later examples, it could also        deal with boundary conditions, geometries, etc.    [2.x.60]   [2.x.61] : This then is the function in which all the        other data structures are set up that are needed to solve the        problem. In particular, it will initialize the DoFHandler object and        correctly size the various objects that have to do with the linear        algebra. This function is often separated from the preprocessing        function above because, in a time dependent program, it may be called        at least every few time steps whenever the mesh        is adaptively refined (something we will see how to do in  [2.x.62] ). On        the other hand, setting up the mesh itself in the preprocessing        function above is done only once at the beginning of the program and        is, therefore, separated into its own function.    [2.x.63]   [2.x.64] : This, then is where the contents of the        matrix and right hand side are computed, as discussed at length in the        introduction above. Since doing something with this linear system is        conceptually very different from computing its entries, we separate it        from the following function.    [2.x.65]   [2.x.66] : This then is the function in which we compute the        solution  [2.x.67]  of the linear system  [2.x.68] . In the current program, this        is a simple task since the matrix is so simple, but it will become a        significant part of a program's size whenever the problem is not so        trivial any more (see, for example,  [2.x.69] ,  [2.x.70] , or  [2.x.71]  once        you've learned a bit more about the library).    [2.x.72]   [2.x.73] : Finally, when you have computed a        solution, you probably want to do something with it. For example, you        may want to output it in a format that can be visualized, or you may        want to compute quantities you are interested in: say, heat fluxes in a        heat exchanger, air friction coefficients of a wing, maximum bridge        loads, or simply the value of the numerical solution at a point. This        function is therefore the place for postprocessing your solution.  [2.x.74]  All of this is held together by the single public function (other than the constructor), namely the  [2.x.75]  function. It is the one that is called from the place where an object of this type is created, and it is the one that calls all the other functions in their proper order. Encapsulating this operation into the  [2.x.76]  function, rather than calling all the other functions from  [2.x.77]  makes sure that you can change how the separation of concerns within this class is implemented. For example, if one of the functions becomes too big, you can split it up into two, and the only places you have to be concerned about changing as a consequence are within this very same class, and not anywhere else. 

As mentioned above, you will see this general structure &mdash; sometimes with variants in spelling of the functions' names, but in essentially this order of separation of functionality &mdash; again in many of the following tutorial programs. 


[1.x.59][1.x.60] 


deal.II defines a number of integral %types via alias in namespace  [2.x.78]  (In the previous sentence, the word "integral" is used as the [1.x.61] that corresponds to the noun "integer". It shouldn't be confused with the [1.x.62] "integral" that represents the area or volume under a curve or surface. The adjective "integral" is widely used in the C++ world in contexts such as "integral type", "integral constant", etc.) In particular, in this program you will see  [2.x.79]  in a couple of places: an integer type that is used to denote the [1.x.63] index of a degree of freedom, i.e., the index of a particular degree of freedom within the DoFHandler object that is defined on top of a triangulation (as opposed to the index of a particular degree of freedom within a particular cell). For the current program (as well as almost all of the tutorial programs), you will have a few thousand to maybe a few million unknowns globally (and, for  [2.x.80]  elements, you will have 4 [1.x.64] in 2d and 8 in 3d). Consequently, a data type that allows to store sufficiently large numbers for global DoF indices is  [2.x.81]  given that it allows to store numbers between 0 and slightly more than 4 billion (on most systems, where integers are 32-bit). In fact, this is what  [2.x.82]  is. 

So, why not just use  [2.x.83]  right away? deal.II used to do this until version 7.3. However, deal.II supports very large computations (via the framework discussed in  [2.x.84] ) that may have more than 4 billion unknowns when spread across a few thousand processors. Consequently, there are situations where  [2.x.85]  is not sufficiently large and we need a 64-bit unsigned integral type. To make this possible, we introduced  [2.x.86]  which by default is defined as simply <code>unsigned int</code> whereas it is possible to define it as <code>unsigned long long int</code> if necessary, by passing a particular flag during configuration (see the ReadMe file). 

This covers the technical aspect. But there is also a documentation purpose: everywhere in the library and codes that are built on it, if you see a place using the data type  [2.x.87]  you immediately know that the quantity that is being referenced is, in fact, a global dof index. No such meaning would be apparent if we had just used  [2.x.88]  (which may also be a local index, a boundary indicator, a material id, etc.). Immediately knowing what a variable refers to also helps avoid errors: it's quite clear that there must be a bug if you see an object of type  [2.x.89]  being assigned to variable of type  [2.x.90]  even though they are both represented by unsigned integers and the compiler will, consequently, not complain. 

In more practical terms what the presence of this type means is that during assembly, we create a  [2.x.91]  matrix (in 2d, using a  [2.x.92]  element) of the contributions of the cell we are currently sitting on, and then we need to add the elements of this matrix to the appropriate elements of the global (system) matrix. For this, we need to get at the global indices of the degrees of freedom that are local to the current cell, for which we will always use the following piece of the code: 

[1.x.65] 

where  [2.x.93]  is declared as 

[1.x.66] 

The name of this variable might be a bit of a misnomer -- it stands for "the global indices of those degrees of freedom locally defined on the current cell" -- but variables that hold this information are universally named this way throughout the library. 

 [2.x.94]   [2.x.95]  is not the only type defined in this namespace. Rather, there is a whole family, including  [2.x.96]   [2.x.97]  and  [2.x.98]  All of these are alias for integer data types but, as explained above, they are used throughout the library so that (i) the intent of a variable becomes more easily discerned, and (ii) so that it becomes possible to change the actual type to a larger one if necessary without having to go through the entire library and figure out whether a particular use of  [2.x.99]  corresponds to, say, a material indicator. [1.x.67] [1.x.68] 


[1.x.69]  [1.x.70] 




These include files are already known to you. They declare the classes which handle triangulations and enumeration of degrees of freedom: 

[1.x.71] 



And this is the file in which the functions are declared that create grids: 

[1.x.72] 



This file contains the description of the Lagrange interpolation finite element: 

[1.x.73] 



And this file is needed for the creation of sparsity patterns of sparse matrices, as shown in previous examples: 

[1.x.74] 



The next two files are needed for assembling the matrix using quadrature on each cell. The classes declared in them will be explained below: 

[1.x.75] 



The following three include files we need for the treatment of boundary values: 

[1.x.76] 



We're now almost to the end. The second to last group of include files is for the linear algebra which we employ to solve the system of equations arising from the finite element discretization of the Laplace equation. We will use vectors and full matrices for assembling the system of equations locally on each cell, and transfer the results into a sparse matrix. We will then use a Conjugate Gradient solver to solve the problem, for which we need a preconditioner (in this program, we use the identity preconditioner which does nothing, but we need to include the file anyway): 

[1.x.77] 



Finally, this is for output to a file and to the console: 

[1.x.78] 



...and this is to import the deal.II namespace into the global scope: 

[1.x.79] 




[1.x.80]  [1.x.81] 




Instead of the procedural programming of previous examples, we encapsulate everything into a class for this program. The class consists of functions which each perform certain aspects of a finite element program, a `main` function which controls what is done first and what is done next, and a list of member variables. 




The public part of the class is rather short: it has a constructor and a function `run` that is called from the outside and acts as something like the `main` function: it coordinates which operations of this class shall be run in which order. Everything else in the class, i.e. all the functions that actually do anything, are in the private section of the class: 

[1.x.82] 



Then there are the member functions that mostly do what their names suggest and whose have been discussed in the introduction already. Since they do not need to be called from outside, they are made private to this class. 







[1.x.83] 



And finally we have some member variables. There are variables describing the triangulation and the global numbering of the degrees of freedom (we will specify the exact polynomial degree of the finite element in the constructor of this class)... 

[1.x.84] 



...variables for the sparsity pattern and values of the system matrix resulting from the discretization of the Laplace equation... 

[1.x.85] 



...and variables which will hold the right hand side and solution vectors. 

[1.x.86] 




[1.x.87]  [1.x.88] 




Here comes the constructor. It does not much more than first to specify that we want bi-linear elements (denoted by the parameter to the finite element object, which indicates the polynomial degree), and to associate the dof_handler variable to the triangulation we use. (Note that the triangulation isn't set up with a mesh at all at the present time, but the DoFHandler doesn't care: it only wants to know which triangulation it will be associated with, and it only starts to care about an actual mesh once you try to distribute degree of freedom on the mesh using the distribute_dofs() function.) All the other member variables of the Step3 class have a default constructor which does all we want. 

[1.x.89] 




[1.x.90]  [1.x.91] 




Now, the first thing we've got to do is to generate the triangulation on which we would like to do our computation and number each vertex with a degree of freedom. We have seen these two steps in  [2.x.100]  and  [2.x.101]  before, respectively. 




This function does the first part, creating the mesh.  We create the grid and refine all cells five times. Since the initial grid (which is the square  [2.x.102] ) consists of only one cell, the final grid has 32 times 32 cells, for a total of 1024. 




Unsure that 1024 is the correct number? We can check that by outputting the number of cells using the  [2.x.103]  function on the triangulation. 

[1.x.92] 



 [2.x.104]  We call the  [2.x.105]  function, rather than  [2.x.106]  Here, [1.x.93] means the cells that aren't refined any further. We stress the adjective "active" since there are more cells, namely the parent cells of the finest cells, their parents, etc, up to the one cell which made up the initial grid. Of course, on the next coarser level, the number of cells is one quarter that of the cells on the finest level, i.e. 256, then 64, 16, 4, and 1. If you called  [2.x.107]  instead in the code above, you would consequently get a value of 1365 instead. On the other hand, the number of cells (as opposed to the number of active cells) is not typically of much interest, so there is no good reason to print it. 










[1.x.94]  [1.x.95] 




Next we enumerate all the degrees of freedom and set up matrix and vector objects to hold the system data. Enumerating is done by using  [2.x.108]  as we have seen in the  [2.x.109]  example. Since we use the FE_Q class and have set the polynomial degree to 1 in the constructor, i.e. bilinear elements, this associates one degree of freedom with each vertex. While we're at generating output, let us also take a look at how many degrees of freedom are generated: 

[1.x.96] 



There should be one DoF for each vertex. Since we have a 32 times 32 grid, the number of DoFs should be 33 times 33, or 1089. 




As we have seen in the previous example, we set up a sparsity pattern by first creating a temporary structure, tagging those entries that might be nonzero, and then copying the data over to the SparsityPattern object that can then be used by the system matrix. 

[1.x.97] 



Note that the SparsityPattern object does not hold the values of the matrix, it only stores the places where entries are. The entries themselves are stored in objects of type SparseMatrix, of which our variable system_matrix is one.    


The distinction between sparsity pattern and matrix was made to allow several matrices to use the same sparsity pattern. This may not seem relevant here, but when you consider the size which matrices can have, and that it may take some time to build the sparsity pattern, this becomes important in large-scale problems if you have to store several matrices in your program. 

[1.x.98] 



The last thing to do in this function is to set the sizes of the right hand side vector and the solution vector to the right values: 

[1.x.99] 




[1.x.100]  [1.x.101] 








The next step is to compute the entries of the matrix and right hand side that form the linear system from which we compute the solution. This is the central function of each finite element program and we have discussed the primary steps in the introduction already. 




The general approach to assemble matrices and vectors is to loop over all cells, and on each cell compute the contribution of that cell to the global matrix and right hand side by quadrature. The point to realize now is that we need the values of the shape functions at the locations of quadrature points on the real cell. However, both the finite element shape functions as well as the quadrature points are only defined on the reference cell. They are therefore of little help to us, and we will in fact hardly ever query information about finite element shape functions or quadrature points from these objects directly. 




Rather, what is required is a way to map this data from the reference cell to the real cell. Classes that can do that are derived from the Mapping class, though one again often does not have to deal with them directly: many functions in the library can take a mapping object as argument, but when it is omitted they simply resort to the standard bilinear Q1 mapping. We will go this route, and not bother with it for the moment (we come back to this in  [2.x.110] ,  [2.x.111] , and  [2.x.112] ). 




So what we now have is a collection of three classes to deal with: finite element, quadrature, and mapping objects. That's too much, so there is one type of class that orchestrates information exchange between these three: the FEValues class. If given one instance of each three of these objects (or two, and an implicit linear mapping), it will be able to provide you with information about values and gradients of shape functions at quadrature points on a real cell. 




Using all this, we will assemble the linear system for this problem in the following function: 

[1.x.102] 



Ok, let's start: we need a quadrature formula for the evaluation of the integrals on each cell. Let's take a Gauss formula with two quadrature points in each direction, i.e. a total of four points since we are in 2D. This quadrature formula integrates polynomials of degrees up to three exactly (in 1D). It is easy to check that this is sufficient for the present problem: 

[1.x.103] 



And we initialize the object which we have briefly talked about above. It needs to be told which finite element we want to use, and the quadrature points and their weights (jointly described by a Quadrature object). As mentioned, we use the implied Q1 mapping, rather than specifying one ourselves explicitly. Finally, we have to tell it what we want it to compute on each cell: we need the values of the shape functions at the quadrature points (for the right hand side  [2.x.113] ), their gradients (for the matrix entries  [2.x.114] ), and also the weights of the quadrature points and the determinants of the Jacobian transformations from the reference cell to the real cells.    


This list of what kind of information we actually need is given as a collection of flags as the third argument to the constructor of FEValues. Since these values have to be recomputed, or updated, every time we go to a new cell, all of these flags start with the prefix  [2.x.115]  and then indicate what it actually is that we want updated. The flag to give if we want the values of the shape functions computed is #update_values; for the gradients it is #update_gradients. The determinants of the Jacobians and the quadrature weights are always used together, so only the products (Jacobians times weights, or short  [2.x.116] ) are computed; since we need them, we have to list #update_JxW_values as well: 

[1.x.104] 



The advantage of this approach is that we can specify what kind of information we actually need on each cell. It is easily understandable that this approach can significantly speed up finite element computations, compared to approaches where everything, including second derivatives, normal vectors to cells, etc are computed on each cell, regardless of whether they are needed or not.    




 [2.x.117]  The syntax <code>update_values | update_gradients | update_JxW_values</code> is not immediately obvious to anyone not used to programming bit operations in C for years already. First,  [2.x.118]  is the [1.x.105], i.e., it takes two integer arguments that are interpreted as bit patterns and returns an integer in which every bit is set for which the corresponding bit is set in at least one of the two arguments. For example, consider the operation  [2.x.119]  (where the prefix  [2.x.120]  indicates that the number is to be interpreted as a binary number) and  [2.x.121] . Going through each bit and seeing whether it is set in one of the argument, we arrive at  [2.x.122]  or, in decimal notation,  [2.x.123] . The second piece of information you need to know is that the various  [2.x.124]  flags are all integers that have [1.x.106]. For example, assume that  [2.x.125] ,  [2.x.126] ,  [2.x.127] . Then <code>update_values | update_gradients | update_JxW_values = 0b10011 = 19</code>. In other words, we obtain a number that [1.x.107], where each operation corresponds to exactly one bit in the integer that, if equal to one, means that a particular piece should be updated on each cell and, if it is zero, means that we need not compute it. In other words, even though  [2.x.128]  is the [1.x.108], what it really represents is [1.x.109]. Such binary masks are quite common in C programming, but maybe not so in higher level languages like C++, but serve the current purpose quite well. 




For use further down below, we define a shortcut for a value that will be used very frequently. Namely, an abbreviation for the number of degrees of freedom on each cell (since we are in 2D and degrees of freedom are associated with vertices only, this number is four, but we rather want to write the definition of this variable in a way that does not preclude us from later choosing a different finite element that has a different number of degrees of freedom per cell, or work in a different space dimension).    


In general, it is a good idea to use a symbolic name instead of hard-coding these numbers even if you know them, since for example, you may want to change the finite element at some time. Changing the element would have to be done in a different function and it is easy to forget to make a corresponding change in another part of the program. It is better to not rely on your own calculations, but instead ask the right object for the information: Here, we ask the finite element to tell us about the number of degrees of freedom per cell and we will get the correct number regardless of the space dimension or polynomial degree we may have chosen elsewhere in the program.    


The shortcut here, defined primarily to discuss the basic concept and not because it saves a lot of typing, will then make the following loops a bit more readable. You will see such shortcuts in many places in larger programs, and `dofs_per_cell` is one that is more or less the conventional name for this kind of object. 

[1.x.110] 



Now, we said that we wanted to assemble the global matrix and vector cell-by-cell. We could write the results directly into the global matrix, but this is not very efficient since access to the elements of a sparse matrix is slow. Rather, we first compute the contribution of each cell in a small matrix with the degrees of freedom on the present cell, and only transfer them to the global matrix when the computations are finished for this cell. We do the same for the right hand side vector. So let's first allocate these objects (these being local objects, all degrees of freedom are coupling with all others, and we should use a full matrix object rather than a sparse one for the local operations; everything will be transferred to a global sparse matrix later on): 

[1.x.111] 



When assembling the contributions of each cell, we do this with the local numbering of the degrees of freedom (i.e. the number running from zero through dofs_per_cell-1). However, when we transfer the result into the global matrix, we have to know the global numbers of the degrees of freedom. When we query them, we need a scratch (temporary) array for these numbers (see the discussion at the end of the introduction for the type,  [2.x.129]  used here): 

[1.x.112] 



Now for the loop over all cells. We have seen before how this works for a triangulation. A DoFHandler has cell iterators that are exactly analogous to those of a Triangulation, but with extra information about the degrees of freedom for the finite element you're using. Looping over the active cells of a degree-of-freedom handler works the same as for a triangulation.    


Note that we declare the type of the cell as `const auto &` instead of `auto` this time around. In step 1, we were modifying the cells of the triangulation by flagging them with refinement indicators. Here we're only examining the cells without modifying them, so it's good practice to declare `cell` as `const` in order to enforce this invariant. 

[1.x.113] 



We are now sitting on one cell, and we would like the values and gradients of the shape functions be computed, as well as the determinants of the Jacobian matrices of the mapping between reference cell and true cell, at the quadrature points. Since all these values depend on the geometry of the cell, we have to have the FEValues object re-compute them on each cell: 

[1.x.114] 



Next, reset the local cell's contributions to global matrix and global right hand side to zero, before we fill them: 

[1.x.115] 



Now it is time to start integration over the cell, which we do by looping over all quadrature points, which we will number by q_index. 

[1.x.116] 



First assemble the matrix: For the Laplace problem, the matrix on each cell is the integral over the gradients of shape function i and j. Since we do not integrate, but rather use quadrature, this is the sum over all quadrature points of the integrands times the determinant of the Jacobian matrix at the quadrature point times the weight of this quadrature point. You can get the gradient of shape function  [2.x.130]  at quadrature point with number q_index by using  [2.x.131] ; this gradient is a 2-dimensional vector (in fact it is of type Tensor@<1,dim@>, with here dim=2) and the product of two such vectors is the scalar product, i.e. the product of the two shape_grad function calls is the dot product. This is in turn multiplied by the Jacobian determinant and the quadrature point weight (that one gets together by the call to  [2.x.132]  ). Finally, this is repeated for all shape functions  [2.x.133]  and  [2.x.134] : 

[1.x.117] 



We then do the same thing for the right hand side. Here, the integral is over the shape function i times the right hand side function, which we choose to be the function with constant value one (more interesting examples will be considered in the following programs). 

[1.x.118] 



Now that we have the contribution of this cell, we have to transfer it to the global matrix and right hand side. To this end, we first have to find out which global numbers the degrees of freedom on this cell have. Let's simply ask the cell for that information: 

[1.x.119] 



Then again loop over all shape functions i and j and transfer the local elements to the global matrix. The global numbers can be obtained using local_dof_indices[i]: 

[1.x.120] 



And again, we do the same thing for the right hand side vector. 

[1.x.121] 



Now almost everything is set up for the solution of the discrete system. However, we have not yet taken care of boundary values (in fact, Laplace's equation without Dirichlet boundary values is not even uniquely solvable, since you can add an arbitrary constant to the discrete solution). We therefore have to do something about the situation.    


For this, we first obtain a list of the degrees of freedom on the boundary and the value the shape function shall have there. For simplicity, we only interpolate the boundary value function, rather than projecting it onto the boundary. There is a function in the library which does exactly this:  [2.x.135]  Its parameters are (omitting parameters for which default values exist and that we don't care about): the DoFHandler object to get the global numbers of the degrees of freedom on the boundary; the component of the boundary where the boundary values shall be interpolated; the boundary value function itself; and the output object.    


The component of the boundary is meant as follows: in many cases, you may want to impose certain boundary values only on parts of the boundary. For example, you may have inflow and outflow boundaries in fluid dynamics, or clamped and free parts of bodies in deformation computations of bodies. Then you will want to denote these different parts of the boundary by indicators, and tell the interpolate_boundary_values function to only compute the boundary values on a certain part of the boundary (e.g. the clamped part, or the inflow boundary). By default, all boundaries have a 0 boundary indicator, unless otherwise specified. If sections of the boundary have different boundary conditions, you have to number those parts with different boundary indicators. The function call below will then only determine boundary values for those parts of the boundary for which the boundary indicator is in fact the zero specified as the second argument.    


The function describing the boundary values is an object of type Function or of a derived class. One of the derived classes is  [2.x.136]  which describes (not unexpectedly) a function which is zero everywhere. We create such an object in-place and pass it to the  [2.x.137]  function.    


Finally, the output object is a list of pairs of global degree of freedom numbers (i.e. the number of the degrees of freedom on the boundary) and their boundary values (which are zero here for all entries). This mapping of DoF numbers to boundary values is done by the  [2.x.138]  class. 

[1.x.122] 



Now that we got the list of boundary DoFs and their respective boundary values, let's use them to modify the system of equations accordingly. This is done by the following function call: 

[1.x.123] 




[1.x.124]  [1.x.125] 




The following function simply solves the discretized equation. As the system is quite a large one for direct solvers such as Gauss elimination or LU decomposition, we use a Conjugate Gradient algorithm. You should remember that the number of variables here (only 1089) is a very small number for finite element computations, where 100.000 is a more usual number.  For this number of variables, direct methods are no longer usable and you are forced to use methods like CG. 

[1.x.126] 



First, we need to have an object that knows how to tell the CG algorithm when to stop. This is done by using a SolverControl object, and as stopping criterion we say: stop after a maximum of 1000 iterations (which is far more than is needed for 1089 variables; see the results section to find out how many were really used), and stop if the norm of the residual is below  [2.x.139] . In practice, the latter criterion will be the one which stops the iteration: 

[1.x.127] 



Then we need the solver itself. The template parameter to the SolverCG class is the type of the vectors, and leaving the empty angle brackets would indicate that we are taking the default argument (which is  [2.x.140] ). However, we explicitly mention the template argument: 

[1.x.128] 



Now solve the system of equations. The CG solver takes a preconditioner as its fourth argument. We don't feel ready to delve into this yet, so we tell it to use the identity operation as preconditioner: 

[1.x.129] 



Now that the solver has done its job, the solution variable contains the nodal values of the solution function. 

[1.x.130] 




[1.x.131]  [1.x.132] 




The last part of a typical finite element program is to output the results and maybe do some postprocessing (for example compute the maximal stress values at the boundary, or the average flux across the outflow, etc). We have no such postprocessing here, but we would like to write the solution to a file. 

[1.x.133] 



To write the output to a file, we need an object which knows about output formats and the like. This is the DataOut class, and we need an object of that type: 

[1.x.134] 



Now we have to tell it where to take the values from which it shall write. We tell it which DoFHandler object to use, and the solution vector (and the name by which the solution variable shall appear in the output file). If we had more than one vector which we would like to look at in the output (for example right hand sides, errors per cell, etc) we would add them as well: 

[1.x.135] 



After the DataOut object knows which data it is to work on, we have to tell it to process them into something the back ends can handle. The reason is that we have separated the frontend (which knows about how to treat DoFHandler objects and data vectors) from the back end (which knows many different output formats) and use an intermediate data format to transfer data from the front- to the backend. The data is transformed into this intermediate format by the following function: 

[1.x.136] 



Now we have everything in place for the actual output. Just open a file and write the data into it, using VTK format (there are many other functions in the DataOut class we are using here that can write the data in postscript, AVS, GMV, Gnuplot, or some other file formats): 

[1.x.137] 




[1.x.138]  [1.x.139] 




Finally, the last function of this class is the main function which calls all the other functions of the  [2.x.141]  class. The order in which this is done resembles the order in which most finite element programs work. Since the names are mostly self-explanatory, there is not much to comment about: 

[1.x.140] 




[1.x.141]  [1.x.142] 




This is the main function of the program. Since the concept of a main function is mostly a remnant from the pre-object oriented era before C++ programming, it often does not do much more than creating an object of the top-level class and calling its principle function. 




Finally, the first line of the function is used to enable output of some diagnostics that deal.II can generate.  The  [2.x.142]  variable (which stands for deal-log, not de-allog) represents a stream to which some parts of the library write output. For example, iterative solvers will generate diagnostics (starting residual, number of solver steps, final residual) as can be seen when running this tutorial program. 




The output of  [2.x.143]  can be written to the console, to a file, or both. Both are disabled by default since over the years we have learned that a program should only generate output when a user explicitly asks for it. But this can be changed, and to explain how this can be done, we need to explain how  [2.x.144]  works: When individual parts of the library want to log output, they open a "context" or "section" into which this output will be placed. At the end of the part that wants to write output, one exits this section again. Since a function may call another one from within the scope where this output section is open, output may in fact be nested hierarchically into these sections. The LogStream class of which  [2.x.145]  is a variable calls each of these sections a "prefix" because all output is printed with this prefix at the left end of the line, with prefixes separated by colons. There is always a default prefix called "DEAL" (a hint at deal.II's history as the successor of a previous library called "DEAL" and from which the LogStream class is one of the few pieces of code that were taken into deal.II). 




By default,  [2.x.146]  only outputs lines with zero prefixes -- i.e., all output is disabled because the default "DEAL" prefix is always there. But one can set a different maximal number of prefixes for lines that should be output to something larger, and indeed here we set it to two by calling  [2.x.147]  This means that for all screen output, a context that has pushed one additional prefix beyond the default "DEAL" is allowed to print its output to the screen ("console"), whereas all further nested sections that would have three or more prefixes active would write to  [2.x.148]  but  [2.x.149]  does not forward this output to the screen. Thus, running this example (or looking at the "Results" section), you will see the solver statistics prefixed with "DEAL:CG", which is two prefixes. This is sufficient for the context of the current program, but you will see examples later on (e.g., in  [2.x.150] ) where solvers are nested more deeply and where you may get useful information by setting the depth even higher. 

[1.x.143] 

[1.x.144][1.x.145] 


The output of the program looks as follows: 

[1.x.146] 



The first two lines is what we wrote to  [2.x.151] . The last two lines were generated without our intervention by the CG solver. The first two lines state the residual at the start of the iteration, while the last line tells us that the solver needed 47 iterations to bring the norm of the residual to 5.3e-13, i.e. below the threshold 1e-12 which we have set in the `solve' function. We will show in the next program how to suppress this output, which is sometimes useful for debugging purposes, but often clutters up the screen display. 

Apart from the output shown above, the program generated the file  [2.x.152] , which is in the VTK format that is widely used by many visualization programs today -- including the two heavy-weights [1.x.147] and [1.x.148] that are the most commonly used programs for this purpose today. 

Using VisIt, it is not very difficult to generate a picture of the solution like this:  [2.x.153]  It shows both the solution and the mesh, elevated above the  [2.x.154] - [2.x.155]  plane based on the value of the solution at each point. Of course the solution here is not particularly exciting, but that is a result of both what the Laplace equation represents and the right hand side  [2.x.156]  we have chosen for this program: The Laplace equation describes (among many other uses) the vertical deformation of a membrane subject to an external (also vertical) force. In the current example, the membrane's borders are clamped to a square frame with no vertical variation; a constant force density will therefore intuitively lead to a membrane that simply bulges upward -- like the one shown above. 

VisIt and Paraview both allow playing with various kinds of visualizations of the solution. Several video lectures show how to use these programs.  [2.x.157]  




[1.x.149] [1.x.150][1.x.151] 


If you want to play around a little bit with this program, here are a few suggestions: </p> 

 [2.x.158]     [2.x.159]    Change the geometry and mesh: In the program, we have generated a square   domain and mesh by using the  [2.x.160]    function. However, the  [2.x.161]  has a good number of other   functions as well. Try an L-shaped domain, a ring, or other domains you find   there.    [2.x.162]  

   [2.x.163]    Change the boundary condition: The code uses the  [2.x.164]    function to generate zero boundary conditions. However, you may want to try   non-zero constant boundary values using    [2.x.165]  instead of    [2.x.166]  to have unit Dirichlet boundary   values. More exotic functions are described in the documentation of the   Functions namespace, and you may pick one to describe your particular boundary   values.    [2.x.167]  

   [2.x.168]  Modify the type of boundary condition: Presently, what happens   is that we use Dirichlet boundary values all around, since the   default is that all boundary parts have boundary indicator zero, and   then we tell the    [2.x.169]  function to   interpolate boundary values to zero on all boundary components with   indicator zero.  <p> We can change this behavior if we assign parts   of the boundary different indicators. For example, try this   immediately after calling  [2.x.170]    [1.x.152] 



  What this does is it first asks the triangulation to   return an iterator that points to the first active cell. Of course,   this being the coarse mesh for the triangulation of a square, the   triangulation has only a single cell at this moment, and it is   active. Next, we ask the cell to return an iterator to its first   face, and then we ask the face to reset the boundary indicator of   that face to 1. What then follows is this: When the mesh is refined,   faces of child cells inherit the boundary indicator of their   parents, i.e. even on the finest mesh, the faces on one side of the   square have boundary indicator 1. Later, when we get to   interpolating boundary conditions, the    [2.x.171]  call will only produce boundary   values for those faces that have zero boundary indicator, and leave   those faces alone that have a different boundary indicator. What   this then does is to impose Dirichlet boundary conditions on the   former, and homogeneous Neumann conditions on the latter (i.e. zero   normal derivative of the solution, unless one adds additional terms   to the right hand side of the variational equality that deal with   potentially non-zero Neumann conditions). You will see this if you   run the program. 

  An alternative way to change the boundary indicator is to label   the boundaries based on the Cartesian coordinates of the face centers.   For example, we can label all of the cells along the top and   bottom boundaries with a boundary indicator 1 by checking to   see if the cell centers' y-coordinates are within a tolerance   (here 1e-12) of -1 and 1. Try this immediately after calling    [2.x.172]  as before:   [1.x.153] 

  Although this code is a bit longer than before, it is useful for   complex geometries, as it does not require knowledge of face labels. 

   [2.x.173]    A slight variation of the last point would be to set different boundary   values as above, but then use a different boundary value function for   boundary indicator one. In practice, what you have to do is to add a second   call to  [2.x.174]  for boundary indicator one:   [1.x.154] 

  If you have this call immediately after the first one to this function, then   it will interpolate boundary values on faces with boundary indicator 1 to the   unit value, and merge these interpolated values with those previously   computed for boundary indicator 0. The result will be that we will get   discontinuous boundary values, zero on three sides of the square, and one on   the fourth. 

   [2.x.175]    Observe convergence: We will only discuss computing errors in norms in    [2.x.176] , but it is easy to check that computations converge   already here. For example, we could evaluate the value of the solution in a   single point and compare the value for different %numbers of global   refinement (the number of global refinement steps is set in    [2.x.177]  above). To evaluate the   solution at a point, say at  [2.x.178] , we could add the   following code to the  [2.x.179]  function:   [1.x.155] 

  For 1 through 9 global refinement steps, we then get the following sequence   of point values:    [2.x.180]    By noticing that the difference between each two consecutive values reduces   by about a factor of 4, we can conjecture that the "correct" value may be    [2.x.181] . In fact, if we assumed this to be   the correct value, we could show that the sequence above indeed shows  [2.x.182]  convergence &mdash; theoretically, the convergence order should be    [2.x.183]  but the symmetry of the domain and the mesh may lead   to the better convergence order observed. 

  A slight variant of this would be to repeat the test with quadratic   elements. All you need to do is to set the polynomial degree of the finite   element to two in the constructor    [2.x.184] . 

   [2.x.185] Convergence of the mean: A different way to see that the solution   actually converges (to something &mdash; we can't tell whether it's really   the correct value!) is to compute the mean of the solution. To this end, add   the following code to  [2.x.186] :   [1.x.156] 

  The documentation of the function explains what the second and fourth   parameters mean, while the first and third should be obvious. Doing the same   study again where we change the number of global refinement steps, we get   the following result:    [2.x.187]    Again, the difference between two adjacent values goes down by about a   factor of four, indicating convergence as  [2.x.188] .  [2.x.189]  




[1.x.157][1.x.158] 


%HDF5 is a commonly used format that can be read by many scripting languages (e.g. R or Python). It is not difficult to get deal.II to produce some %HDF5 files that can then be used in external scripts to postprocess some of the data generated by this program. Here are some ideas on what is possible. 


[1.x.159][1.x.160] 


To fully make use of the automation we first need to introduce a private variable for the number of global refinement steps  [2.x.190] , which will be used for the output filename. In  [2.x.191]  with 

[1.x.161] 

The deal.II library has two different %HDF5 bindings, one in the HDF5 namespace (for interfacing to general-purpose data files) and another one in DataOut (specifically for writing files for the visualization of solutions). Although the HDF5 deal.II binding supports both serial and MPI, the %HDF5 DataOut binding only supports parallel output. For this reason we need to initialize an MPI communicator with only one processor. This is done by adding the following code. 

[1.x.162] 

Next we change the  [2.x.192]  output routine as described in the DataOutBase namespace documentation: 

[1.x.163] 

The resulting file can then be visualized just like the VTK file that the original version of the tutorial produces; but, since %HDF5 is a more general file format, it can also easily be processed in scripting languages for other purposes. 


[1.x.164][1.x.165] 


After outputting the solution, the file can be opened again to include more datasets.  This allows us to keep all the necessary information of our experiment in a single result file, which can then be read and processed by some postprocessing script. (Have a look at  [2.x.193]  for further information on the possible output options.) 

To make this happen, we first include the necessary header into our file: 

[1.x.166] 

Adding the following lines to the end of our output routine adds the information about the value of the solution at a particular point, as well as the mean value of the solution, to our %HDF5 file: 

[1.x.167] 






[1.x.168][1.x.169] 


The data put into %HDF5 files above can then be used from scripting languages for further postprocessing. In the following, let us show how this can, in particular, be done with the [1.x.170], a widely used language in statistical data analysis. (Similar things can also be done in Python, for example.) If you are unfamiliar with R and ggplot2 you could check out the data carpentry course on R [1.x.171]. Furthermore, since most search engines struggle with searches of the form "R + topic", we recommend using the specializes service [1.x.172] instead. 

The most prominent difference between R and other languages is that the assignment operator (`a = 5`) is typically written as `a <- 5`. As the latter is considered standard we will use it in our examples as well. To open the `.h5` file in R you have to install the [1.x.173] package, which is a part of the Bioconductor package. 

First we will include all necessary packages and have a look at how the data is structured in our file. 

[1.x.174] 

This gives the following output 

[1.x.175] 

The datasets can be accessed by  [2.x.194] . The function  [2.x.195]  gives us the dimensions of the matrix that is used to store our cells. We can see the following three matrices, as well as the two additional data points we added.  [2.x.196]   [2.x.197]   [2.x.198] : a 4x1024 matrix that stores the  (C++) vertex indices for each cell  [2.x.199]   [2.x.200] : a 2x1089 matrix storing the position values (x,y) for our cell vertices  [2.x.201]   [2.x.202] : a 1x1089 matrix storing the values of our solution at each vertex  [2.x.203]  Now we can use this data to generate various plots. Plotting with ggplot2 usually splits into two steps. At first the data needs to be manipulated and added to a  [2.x.204] . After that, a  [2.x.205]  object is constructed and manipulated by adding plot elements to it. 

 [2.x.206]  contain all the information we need to plot our grid. The following code wraps all the data into one dataframe for plotting our grid: 

[1.x.176] 



With the finished dataframe we have everything we need to plot our grid: 

[1.x.177] 



The contents of this file then look as follows (not very exciting, but you get the idea):  [2.x.207]  

We can also visualize the solution itself, and this is going to look more interesting. To make a 2D pseudocolor plot of our solution we will use  [2.x.208] . This function needs a structured grid, i.e. uniform in x and y directions. Luckily our data at this point is structured in the right way. The following code plots a pseudocolor representation of our surface into a new PDF: 

[1.x.178] 

This is now going to look as follows:  [2.x.209]  

For plotting the converge curves we need to re-run the C++ code multiple times with different values for  [2.x.210]  starting from 1. Since every file only contains a single data point we need to loop over them and concatenate the results into a single vector. 

[1.x.179] 

As we are not interested in the values themselves but rather in the error compared to a "exact" solution we will assume our highest refinement level to be that solution and omit it from the data. 

[1.x.180] 

Now we have all the data available to generate our plots. It is often useful to plot errors on a log-log scale, which is accomplished in the following code: 

[1.x.181] 

This results in the following plot that shows how the errors in the mean value and the solution value at the chosen point nicely converge to zero:  [2.x.211]  [1.x.182] [1.x.183]  [2.x.212]  

 [2.x.213] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18] 

[1.x.19] [1.x.20][1.x.21] 


 [2.x.3]  

deal.II has a unique feature which we call ``dimension independent programming''. You may have noticed in the previous examples that many classes had a number in angle brackets suffixed to them. This is to indicate that for example the triangulation in two and three space dimensions are different, but related data %types. We could as well have called them  [2.x.4]  instead of  [2.x.5]  and  [2.x.6]  to name the two classes, but this has an important drawback: assume you have a function which does exactly the same functionality, but on 2d or 3d triangulations, depending on which dimension we would like to solve the equation in presently (if you don't believe that it is the common case that a function does something that is the same in all dimensions, just take a look at the code below - there are almost no distinctions between 2d and 3d!). We would have to write the same function twice, once working on  [2.x.7]  and once working with a  [2.x.8] . This is an unnecessary obstacle in programming and leads to a nuisance to keep the two function in sync (at best) or difficult to find errors if the two versions get out of sync (at worst; this would probably the more common case). 





Such obstacles can be circumvented by using some template magic as provided by the C++ language: templatized classes and functions are not really classes or functions but only a pattern depending on an as-yet undefined data type parameter or on a numerical value which is also unknown at the point of definition. However, the compiler can build proper classes or functions from these templates if you provide it with the information that is needed for that. Of course, parts of the template can depend on the template parameters, and they will be resolved at the time of compilation for a specific template parameter. For example, consider the following piece of code: 

[1.x.22] 






At the point where the compiler sees this function, it does not know anything about the actual value of  [2.x.9] . The only thing the compiler has is a template, i.e. a blueprint, to generate functions  [2.x.10]  if given a particular value of  [2.x.11]  has an unknown value, there is no code the compiler can generate for the moment. 




However, if later down the compiler would encounter code that looks, for example, like this, 

[1.x.23] 

then the compiler will deduce that the function  [2.x.12]  for  [2.x.13]  was requested and will compile the template above into a function with dim replaced by 2 everywhere, i.e. it will compile the function as if it were defined as 

[1.x.24] 






However, it is worth to note that the function  [2.x.14]  depends on the dimension as well, so in this case, the compiler will call the function  [2.x.15]  while if dim were 3, it would call  [2.x.16]  which might be (and actually is) a totally unrelated  function. 




The same can be done with member variables. Consider the following function, which might in turn call the above one: 

[1.x.25] 

This function has a member variable of type  [2.x.17] . Again, the compiler can't compile this function until it knows for which dimension. If you call this function for a specific dimension as above, the compiler will take the template, replace all occurrences of dim by the dimension for which it was called, and compile it. If you call the function several times for different dimensions, it will compile it several times, each time calling the right  [2.x.18]  function and reserving the right amount of memory for the member variable; note that the size of a  [2.x.19]  might, and indeed does, depend on the space dimension. 




The deal.II library is built around this concept of dimension-independent programming, and therefore allows you to program in a way that will not need to distinguish between the space dimensions. It should be noted that in only a very few places is it necessary to actually compare the dimension using  [2.x.20] es. However, since the compiler has to compile each function for each dimension separately, even there it knows the value of  [2.x.21]  at the time of compilation and will therefore be able to optimize away the  [2.x.22]  statement along with the unused branch. 




In this example program, we will show how to program dimension independently (which in fact is even simpler than if you had to take care about the dimension) and we will extend the Laplace problem of the last example to a program that runs in two and three space dimensions at the same time. Other extensions are the use of a non-constant right hand side function and of non-zero boundary values. 




 [2.x.23]  When using templates, C++ imposes all sorts of syntax constraints that make it sometimes a bit difficult to understand why exactly something has to be written this way. A typical example is the need to use the keyword  [2.x.24]  in so many places. If you are not entirely familiar with this already, then several of these difficulties are explained in the deal.II Frequently Asked Questions (FAQ) linked to from the [1.x.26]. 

<!--We need a blank line to end the above block properly.--> [1.x.27] [1.x.28] 


[1.x.29]  [1.x.30] 




The first few (many?) include files have already been used in the previous example, so we will not explain their meaning here again. 

[1.x.31] 



This is new, however: in the previous example we got some unwanted output from the linear solvers. If we want to suppress it, we have to include this file and add a single line somewhere to the program (see the main() function below for that): 

[1.x.32] 



The final step, as in previous programs, is to import all the deal.II class and function names into the global namespace: 

[1.x.33] 




[1.x.34]  [1.x.35] 




This is again the same  [2.x.25]  class as in the previous example. The only difference is that we have now declared it as a class with a template parameter, and the template parameter is of course the spatial dimension in which we would like to solve the Laplace equation. Of course, several of the member variables depend on this dimension as well, in particular the Triangulation class, which has to represent quadrilaterals or hexahedra, respectively. Apart from this, everything is as before. 

[1.x.36] 




[1.x.37]  [1.x.38] 




In the following, we declare two more classes denoting the right hand side and the non-homogeneous Dirichlet boundary values. Both are functions of a dim-dimensional space variable, so we declare them as templates as well. 




Each of these classes is derived from a common, abstract base class Function, which declares the common interface which all functions have to follow. In particular, concrete classes have to overload the  [2.x.26]  function, which takes a point in dim-dimensional space as parameters and returns the value at that point as a  [2.x.27]  variable. 




The  [2.x.28]  function takes a second argument, which we have here named  [2.x.29] : This is only meant for vector-valued functions, where you may want to access a certain component of the vector at the point  [2.x.30] . However, our functions are scalar, so we need not worry about this parameter and we will not use it in the implementation of the functions. Inside the library's header files, the Function base class's declaration of the  [2.x.31]  function has a default value of zero for the component, so we will access the  [2.x.32]  function of the right hand side with only one parameter, namely the point where we want to evaluate the function. A value for the component can then simply be omitted for scalar functions. 




Function objects are used in lots of places in the library (for example, in  [2.x.33]  we used a  [2.x.34]  instance as an argument to  [2.x.35]  and this is the first tutorial where we define a new class that inherits from Function. Since we only ever call  [2.x.36]  we could get away with just a plain function (and this is what is done in  [2.x.37] ), but since this is a tutorial we inherit from Function for the sake of example. 

[1.x.39] 



If you are not familiar with what the keywords `virtual` and `override` in the function declarations above mean, you will probably want to take a look at your favorite C++ book or an online tutorial such as http://www.cplusplus.com/doc/tutorial/polymorphism/ . In essence, what is happening here is that Function<dim> is an "abstract" base class that declares a certain "interface" -- a set of functions one can call on objects of this kind. But it does not actually *implement* these functions: it just says "this is how Function objects look like", but what kind of function it actually is, is left to derived classes that implement the `value()` function. 




Deriving one class from another is often called an "is-a" relationship function. Here, the `RightHandSide` class "is a" Function class because it implements the interface described by the Function base class. (The actual implementation of the `value()` function is in the code block below.) The `virtual` keyword then means "Yes, the function here is one that can be overridden by derived classes", and the `override` keyword means "Yes, this is in fact a function we know has been declared as part of the base class". The `override` keyword is not strictly necessary, but is an insurance against typos: If we get the name of the function or the type of one argument wrong, the compiler will warn us by stating "You say that this function overrides one in a base class, but I don't actually know any such function with this name and these arguments." 




But back to the concrete case here: For this tutorial, we choose as right hand side the function  [2.x.38]  in 2D, or  [2.x.39]  in 3D. We could write this distinction using an if-statement on the space dimension, but here is a simple way that also allows us to use the same function in 1D (or in 4D, if you should desire to do so), by using a short loop.  Fortunately, the compiler knows the size of the loop at compile time (remember that at the time when you define the template, the compiler doesn't know the value of  [2.x.40] , but when it later encounters a statement or declaration  [2.x.41] , it will take the template, replace all occurrences of dim by 2 and compile the resulting function).  In other words, at the time of compiling this function, the number of times the body will be executed is known, and the compiler can minimize the overhead needed for the loop; the result will be as fast as if we had used the formulas above right away. 




The last thing to note is that a  [2.x.42]  denotes a point in dim-dimensional space, and its individual components (i.e.  [2.x.43] ,  [2.x.44] , ... coordinates) can be accessed using the () operator (in fact, the [] operator will work just as well) with indices starting at zero as usual in C and C++. 

[1.x.40] 



As boundary values, we choose  [2.x.45]  in 2D, and  [2.x.46]  in 3D. This happens to be equal to the square of the vector from the origin to the point at which we would like to evaluate the function, irrespective of the dimension. So that is what we return: 

[1.x.41] 




[1.x.42]  [1.x.43] 




Next for the implementation of the class template that makes use of the functions above. As before, we will write everything as templates that have a formal parameter  [2.x.47]  that we assume unknown at the time we define the template functions. Only later, the compiler will find a declaration of  [2.x.48]  function, actually) and compile the entire class with  [2.x.49]  replaced by 2, a process referred to as `instantiation of a template'. When doing so, it will also replace instances of  [2.x.50]  by  [2.x.51]  and instantiate the latter class from the class template. 




In fact, the compiler will also find a declaration  [2.x.52]  in  [2.x.53] . This will cause it to again go back to the general  [2.x.54]  template, replace all occurrences of  [2.x.55] , this time by 3, and compile the class a second time. Note that the two instantiations  [2.x.56]  and  [2.x.57]  are completely independent classes; their only common feature is that they are both instantiated from the same general template, but they are not convertible into each other, for example, and share no code (both instantiations are compiled completely independently). 










[1.x.44]  [1.x.45] 




After this introduction, here is the constructor of the  [2.x.58]  class. It specifies the desired polynomial degree of the finite elements and associates the DoFHandler to the triangulation just as in the previous example program,  [2.x.59] : 

[1.x.46] 




[1.x.47]  [1.x.48] 




Grid creation is something inherently dimension dependent. However, as long as the domains are sufficiently similar in 2D or 3D, the library can abstract for you. In our case, we would like to again solve on the square  [2.x.60]  in 2D, or on the cube  [2.x.61]  in 3D; both can be termed  [2.x.62]  so we may use the same function in whatever dimension we are. Of course, the functions that create a hypercube in two and three dimensions are very much different, but that is something you need not care about. Let the library handle the difficult things. 

[1.x.49] 




[1.x.50]  [1.x.51] 




This function looks exactly like in the previous example, although it performs actions that in their details are quite different if  [2.x.63]  happens to be 3. The only significant difference from a user's perspective is the number of cells resulting, which is much higher in three than in two space dimensions! 

[1.x.52] 




[1.x.53]  [1.x.54] 




Unlike in the previous example, we would now like to use a non-constant right hand side function and non-zero boundary values. Both are tasks that are readily achieved with only a few new lines of code in the assemblage of the matrix and right hand side. 




More interesting, though, is the way we assemble matrix and right hand side vector dimension independently: there is simply no difference to the two-dimensional case. Since the important objects used in this function (quadrature formula, FEValues) depend on the dimension by way of a template parameter as well, they can take care of setting up properly everything for the dimension for which this function is compiled. By declaring all classes which might depend on the dimension using a template parameter, the library can make nearly all work for you and you don't have to care about most things. 

[1.x.55] 



We wanted to have a non-constant right hand side, so we use an object of the class declared above to generate the necessary data. Since this right hand side object is only used locally in the present function, we declare it here as a local variable: 

[1.x.56] 



Compared to the previous example, in order to evaluate the non-constant right hand side function we now also need the quadrature points on the cell we are presently on (previously, we only required values and gradients of the shape function from the FEValues object, as well as the quadrature weights,  [2.x.64]  ). We can tell the FEValues object to do for us by also giving it the #update_quadrature_points flag: 

[1.x.57] 



We then again define the same abbreviation as in the previous program. The value of this variable of course depends on the dimension which we are presently using, but the FiniteElement class does all the necessary work for you and you don't have to care about the dimension dependent parts: 

[1.x.58] 



Next, we again have to loop over all cells and assemble local contributions.  Note, that a cell is a quadrilateral in two space dimensions, but a hexahedron in 3D. In fact, the  [2.x.65]  data type is something different, depending on the dimension we are in, but to the outside world they look alike and you will probably never see a difference. In any case, the real type is hidden by using `auto`: 

[1.x.59] 



Now we have to assemble the local matrix and right hand side. This is done exactly like in the previous example, but now we revert the order of the loops (which we can safely do since they are independent of each other) and merge the loops for the local matrix and the local vector as far as possible to make things a bit faster.        


Assembling the right hand side presents the only significant difference to how we did things in  [2.x.66] : Instead of using a constant right hand side with value 1, we use the object representing the right hand side and evaluate it at the quadrature points: 

[1.x.60] 



As a final remark to these loops: when we assemble the local contributions into  [2.x.67] , we have to multiply the gradients of shape functions  [2.x.68]  and  [2.x.69]  at point number q_index and multiply it with the scalar weights JxW. This is what actually happens:  [2.x.70]  returns a  [2.x.71]  dimensional vector, represented by a  [2.x.72]  object, and the operator* that multiplies it with the result of  [2.x.73]  makes sure that the  [2.x.74]  components of the two vectors are properly contracted, and the result is a scalar floating point number that then is multiplied with the weights. Internally, this operator* makes sure that this happens correctly for all  [2.x.75]  components of the vectors, whether  [2.x.76]  be 2, 3, or any other space dimension; from a user's perspective, this is not something worth bothering with, however, making things a lot simpler if one wants to write code dimension independently. 




With the local systems assembled, the transfer into the global matrix and right hand side is done exactly as before, but here we have again merged some loops for efficiency: 

[1.x.61] 



As the final step in this function, we wanted to have non-homogeneous boundary values in this example, unlike the one before. This is a simple task, we only have to replace the  [2.x.77]  used there by an object of the class which describes the boundary values we would like to use (i.e. the  [2.x.78]  class declared above):    


The function  [2.x.79]  will only work on faces that have been marked with boundary indicator 0 (because that's what we say the function should work on with the second argument below). If there are faces with boundary id other than 0, then the function interpolate_boundary_values will do nothing on these faces. For the Laplace equation doing nothing is equivalent to assuming that on those parts of the boundary a zero Neumann boundary condition holds. 

[1.x.62] 




[1.x.63]  [1.x.64] 




Solving the linear system of equations is something that looks almost identical in most programs. In particular, it is dimension independent, so this function is copied verbatim from the previous example. 

[1.x.65] 



We have made one addition, though: since we suppress output from the linear solvers, we have to print the number of iterations by hand. 

[1.x.66] 




[1.x.67]  [1.x.68] 




This function also does what the respective one did in  [2.x.80] . No changes here for dimension independence either. 




Since the program will run both 2d and 3d versions of the Laplace solver, we use the dimension in the filename to generate distinct filenames for each run (in a better program, one would check whether  [2.x.81]  can have other values than 2 or 3, but we neglect this here for the sake of brevity). 

[1.x.69] 




[1.x.70]  [1.x.71] 




This is the function which has the top-level control over everything. Apart from one line of additional output, it is the same as for the previous example. 

[1.x.72] 




[1.x.73]  [1.x.74] 




And this is the main function. It also looks mostly like in  [2.x.82] , but if you look at the code below, note how we first create a variable of type  [2.x.83]  (forcing the compiler to compile the class template with  [2.x.84] ) and run a 2d simulation, and then we do the whole thing over in 3d. 




In practice, this is probably not what you would do very frequently (you probably either want to solve a 2d problem, or one in 3d, but not both at the same time). However, it demonstrates the mechanism by which we can simply change which dimension we want in a single place, and thereby force the compiler to recompile the dimension independent class templates for the dimension we request. The emphasis here lies on the fact that we only need to change a single place. This makes it rather trivial to debug the program in 2d where computations are fast, and then switch a single place to a 3 to run the much more computing intensive program in 3d for `real' computations. 




Each of the two blocks is enclosed in braces to make sure that the  [2.x.85]  variable goes out of scope (and releases the memory it holds) before we move on to allocate memory for the 3d case. Without the additional braces, the  [2.x.86]  variable would only be destroyed at the end of the function, i.e. after running the 3d problem, and would needlessly hog memory while the 3d run could actually use it. 

[1.x.75] 

[1.x.76][1.x.77] 




The output of the program looks as follows (the number of iterations may vary by one or two, depending on your computer, since this is often dependent on the round-off accuracy of floating point operations, which differs between processors): 

[1.x.78] 

It is obvious that in three spatial dimensions the number of cells and therefore also the number of degrees of freedom is much higher. What cannot be seen here, is that besides this higher number of rows and columns in the matrix, there are also significantly more entries per row of the matrix in three space dimensions. Together, this leads to a much higher numerical effort for solving the system of equation, which you can feel in the run time of the two solution steps when you actually run the program. 




The program produces two files:  [2.x.87]  and  [2.x.88] , which can be viewed using the programs VisIt or Paraview (in case you do not have these programs, you can easily change the output format in the program to something which you can view more easily). Visualizing solutions is a bit of an art, but it can also be fun, so you should play around with your favorite visualization tool to get familiar with its functionality. Here's what I have come up with for the 2d solution: 

 [2.x.89]  

( [2.x.90]  The picture shows the solution of the problem under consideration as a 3D plot. As can be seen, the solution is almost flat in the interior of the domain and has a higher curvature near the boundary. This, of course, is due to the fact that for Laplace's equation the curvature of the solution is equal to the right hand side and that was chosen as a quartic polynomial which is nearly zero in the interior and is only rising sharply when approaching the boundaries of the domain; the maximal values of the right hand side function are at the corners of the domain, where also the solution is moving most rapidly. It is also nice to see that the solution follows the desired quadratic boundary values along the boundaries of the domain. It can also be useful to verify a computed solution against an analytical solution. For an explanation of this technique, see  [2.x.91] . 

On the other hand, even though the picture does not show the mesh lines explicitly, you can see them as little kinks in the solution. This clearly indicates that the solution hasn't been computed to very high accuracy and that to get a better solution, we may have to compute on a finer mesh. 

In three spatial dimensions, visualization is a bit more difficult. The left picture shows the solution and the mesh it was computed on on the surface of the domain. This is nice, but it has the drawback that it completely hides what is happening on the inside. The picture on the right is an attempt at visualizing the interior as well, by showing surfaces where the solution has constant values (as indicated by the legend at the top left). Isosurface pictures look best if one makes the individual surfaces slightly transparent so that it is possible to see through them and see what's behind. 

 [2.x.92]  

 [2.x.93]  A final remark on visualization: the idea of visualization is to give insight, which is not the same as displaying information. In particular, it is easy to overload a picture with information, but while it shows more information it makes it also more difficult to glean insight. As an example, the program I used to generate these pictures, VisIt, by default puts tick marks on every axis, puts a big fat label "X Axis" on the  [2.x.94]  axis and similar for the other axes, shows the file name from which the data was taken in the top left and the name of the user doing so and the time and date on the bottom right. None of this is important here: the axes are equally easy to make out because the tripod at the bottom left is still visible, and we know from the program that the domain is  [2.x.95] , so there is no need for tick marks. As a consequence, I have switched off all the extraneous stuff in the picture: the art of visualization is to reduce the picture to those parts that are important to see what one wants to see, but no more. 




[1.x.79] [1.x.80][1.x.81] 




Essentially the possibilities for playing around with the program are the same as for the previous one, except that they will now also apply to the 3d case. For inspiration read up on [1.x.82]. [1.x.83] [1.x.84]  [2.x.96]  

 [2.x.97] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16] 

[1.x.17] [1.x.18][1.x.19] 


 [2.x.3]  

This example does not show revolutionary new things, but it shows many small improvements over the previous examples, and also many small things that can usually be found in finite element programs. Among them are:  [2.x.4]     [2.x.5]  Computations on successively refined grids. At least in the        mathematical sciences, it is common to compute solutions on        a hierarchy of grids, in order to get a feeling for the accuracy        of the solution; if you only have one solution on a single grid, you        usually can't guess the accuracy of the        solution. Furthermore, deal.II is designed to support adaptive        algorithms where iterative solution on successively refined        grids is at the heart of algorithms. Although adaptive grids        are not used in this example, the foundations for them is laid        here.    [2.x.6]  In practical applications, the domains are often subdivided        into triangulations by automatic mesh generators. In order to        use them, it is important to read coarse grids from a file. In        this example, we will read a coarse grid in UCD (unstructured        cell data) format. When this program was first written around        2000, UCD format was what the AVS Explorer used -- a program        reasonably widely used at the time but now no longer of        importance. (Nonetheless, the file format has survived and is        still understood by a number of programs.)    [2.x.7]  Finite element programs usually use extensive amounts of        computing time, so some optimizations are sometimes        necessary. We will show some of them.    [2.x.8]  On the other hand, finite element programs tend to be rather        complex, so debugging is an important aspect. We support safe        programming by using assertions that check the validity of        parameters and %internal states in a debug mode, but are removed        in optimized mode. ( [2.x.9]     [2.x.10]  Regarding the mathematical side, we show how to support a        variable coefficient in the elliptic operator and how to use        preconditioned iterative solvers for the linear systems of        equations.  [2.x.11]  

The equation to solve here is as follows: 

[1.x.20] 

If  [2.x.12]  was a constant coefficient, this would simply be the Poisson equation. However, if it is indeed spatially variable, it is a more complex equation (often referred to as the "extended Poisson equation"). Depending on what the variable  [2.x.13]  refers to it models a variety of situations with wide applicability: 

- If  [2.x.14]  is the electric potential, then  [2.x.15]  is the electric current   in a medium and the coefficient  [2.x.16]  is the conductivity of the medium at any   given point. (In this situation, the right hand side of the equation would   be the electric source density and would usually be zero or consist of   localized, Delta-like, functions.) 

- If  [2.x.17]  is the vertical deflection of a thin membrane, then  [2.x.18]  would be a   measure of the local stiffness. This is the interpretation that will allow   us to interpret the images shown in the results section below. 

Since the Laplace/Poisson equation appears in so many contexts, there are many more interpretations than just the two listed above. 

When assembling the linear system for this equation, we need the weak form which here reads as follows: 

[1.x.21] 

The implementation in the  [2.x.19]  function follows immediately from this. [1.x.22] [1.x.23] 


[1.x.24]  [1.x.25] 




Again, the first few include files are already known, so we won't comment on them: 

[1.x.26] 



This one is new. We want to read a triangulation from disk, and the class which does this is declared in the following file: 

[1.x.27] 



We will use a circular domain, and the object describing the boundary of it comes from this file: 

[1.x.28] 



This is C++ ... 

[1.x.29] 



Finally, this has been discussed in previous tutorial programs before: 

[1.x.30] 




[1.x.31]  [1.x.32] 




The main class is mostly as in the previous example. The most visible change is that the function  [2.x.20]  has been removed, since creating the grid is now done in the  [2.x.21]  function and the rest of its functionality is now in  [2.x.22] . Apart from this, everything is as before. 

[1.x.33] 




[1.x.34]  [1.x.35] 




In  [2.x.23] , we showed how to use non-constant boundary values and right hand side.  In this example, we want to use a variable coefficient in the elliptic operator instead. Since we have a function which just depends on the point in space we can do things a bit more simply and use a plain function instead of inheriting from Function. 




This is the implementation of the coefficient function for a single point. We let it return 20 if the distance to the origin is less than 0.5, and 1 otherwise. 

[1.x.36] 




[1.x.37]  [1.x.38] 





[1.x.39]  [1.x.40] 




This function is as before. 

[1.x.41] 




[1.x.42]  [1.x.43] 




This is the function  [2.x.24]  from the previous example, minus the generation of the grid. Everything else is unchanged: 

[1.x.44] 




[1.x.45]  [1.x.46] 




As in the previous examples, this function is not changed much with regard to its functionality, but there are still some optimizations which we will show. For this, it is important to note that if efficient solvers are used (such as the preconditioned CG method), assembling the matrix and right hand side can take a comparable time, and you should think about using one or two optimizations at some places. 




The first parts of the function are completely unchanged from before: 

[1.x.47] 



Next is the typical loop over all cells to compute local contributions and then to transfer them into the global matrix and vector. The only change in this part, compared to  [2.x.25] , is that we will use the  [2.x.26]  function defined above to compute the coefficient value at each quadrature point. 

[1.x.48] 



With the matrix so built, we use zero boundary values again: 

[1.x.49] 




[1.x.50]  [1.x.51] 




The solution process again looks mostly like in the previous examples. However, we will now use a preconditioned conjugate gradient algorithm. It is not very difficult to make this change. In fact, the only thing we have to alter is that we need an object which will act as a preconditioner. We will use SSOR (symmetric successive overrelaxation), with a relaxation factor of 1.2. For this purpose, the  [2.x.27]  class has a function which does one SSOR step, and we need to package the address of this function together with the matrix on which it should act (which is the matrix to be inverted) and the relaxation factor into one object. The  [2.x.28]  class does this for us. ( [2.x.29]  class takes a template argument denoting the matrix type it is supposed to work on. The default value is  [2.x.30] , which is exactly what we need here, so we simply stick with the default and do not specify anything in the angle brackets.) 




Note that for the present case, SSOR doesn't really perform much better than most other preconditioners (though better than no preconditioning at all). A brief comparison of different preconditioners is presented in the Results section of the next tutorial program,  [2.x.31] . 




With this, the rest of the function is trivial: instead of the  [2.x.32]  object we have created before, we now use the preconditioner we have declared, and the CG solver will do the rest for us: 

[1.x.52] 




[1.x.53]  [1.x.54] 




Writing output to a file is mostly the same as for the previous tutorial. The only difference is that we now need to construct a different filename for each refinement cycle. 




The function writes the output in VTU format, a variation of the VTK format that requires less disk space because it compresses the data. Of course, there are many other formats supported by the DataOut class if you desire to use a program for visualization that doesn't understand VTK or VTU. 

[1.x.55] 




[1.x.56]  [1.x.57] 




The second to last thing in this program is the definition of the  [2.x.33]  function. In contrast to the previous programs, we will compute on a sequence of meshes that after each iteration is globally refined. The function therefore consists of a loop over 6 cycles. In each cycle, we first print the cycle number, and then have to decide what to do with the mesh. If this is not the first cycle, we simply refine the existing mesh once globally. Before running through these cycles, however, we have to generate a mesh: 




In previous examples, we have already used some of the functions from the  [2.x.34]  class. Here we would like to read a grid from a file where the cells are stored and which may originate from someone else, or may be the product of a mesh generator tool. 




In order to read a grid from a file, we generate an object of data type GridIn and associate the triangulation to it (i.e. we tell it to fill our triangulation object when we ask it to read the file). Then we open the respective file and initialize the triangulation with the data in the file: 

[1.x.58] 



We would now like to read the file. However, the input file is only for a two-dimensional triangulation, while this function is a template for arbitrary dimension. Since this is only a demonstration program, we will not use different input files for the different dimensions, but rather quickly kill the whole program if we are not in 2D. Of course, since the main function below assumes that we are working in two dimensions we could skip this check, in this version of the program, without any ill effects.    


It turns out that more than 90 per cent of programming errors are invalid function parameters such as invalid array sizes, etc, so we use assertions heavily throughout deal.II to catch such mistakes. For this, the  [2.x.35]  macro is a good choice, since it makes sure that the condition which is given as first argument is valid, and if not throws an exception (its second argument) which will usually terminate the program giving information where the error occurred and what the reason was. (A longer discussion of what exactly the  [2.x.36]  macro does can be found in the  [2.x.37]  "exception documentation module".) This generally reduces the time to find programming errors dramatically and we have found assertions an invaluable means to program fast.    


On the other hand, all these checks (there are over 10,000 of them in the library at present) should not slow down the program too much if you want to do large computations. To this end, the  [2.x.38]  macro is only used in debug mode and expands to nothing if in optimized mode. Therefore, while you test your program on small problems and debug it, the assertions will tell you where the problems are. Once your program is stable, you can switch off debugging and the program will run your real computations without the assertions and at maximum speed. More precisely: turning off all the checks in the library (which prevent you from calling functions with wrong arguments, walking off of arrays, etc.) by compiling your program in optimized mode usually makes things run about four times faster. Even though optimized programs are more performant, we still recommend developing in debug mode since it allows the library to find lots of common programming errors automatically. For those who want to try: The way to switch from debug mode to optimized mode is to recompile your program with the command <code>make release</code>. The output of the  [2.x.39]  program should now indicate to you that the program is now compiled in optimized mode, and it will later also be linked to libraries that have been compiled for optimized mode. In order to switch back to debug mode, simply recompile with the command  [2.x.40] . 

[1.x.59] 



ExcInternalError is a globally defined exception, which may be thrown whenever something is terribly wrong. Usually, one would like to use more specific exceptions, and particular in this case one would of course try to do something else if  [2.x.41]  is not equal to two, e.g. create a grid using library functions. Aborting a program is usually not a good idea and assertions should really only be used for exceptional cases which should not occur, but might due to stupidity of the programmer, user, or someone else. The situation above is not a very clever use of Assert, but again: this is a tutorial and it might be worth to show what not to do, after all. 




So if we got past the assertion, we know that dim==2, and we can now actually read the grid. It is in UCD (unstructured cell data) format (though the convention is to use the suffix  [2.x.42]  for UCD files): 

[1.x.60] 



If you like to use another input format, you have to use one of the other  [2.x.43]  function. (See the documentation of the  [2.x.44]  class to find out what input formats are presently supported.) 




The grid in the file describes a circle. Therefore we have to use a manifold object which tells the triangulation where to put new points on the boundary when the grid is refined. Unlike  [2.x.45] , since GridIn does not know that the domain has a circular boundary (unlike  [2.x.46]  we have to explicitly attach a manifold to the boundary after creating the triangulation to get the correct result when we refine the mesh. 

[1.x.61] 



Now that we have a mesh for sure, we write some output and do all the things that we have already seen in the previous examples. 

[1.x.62] 




[1.x.63]  [1.x.64] 




The main function looks mostly like the one in the previous example, so we won't comment on it further: 

[1.x.65] 

[1.x.66][1.x.67] 




Here is the console output: 

[1.x.68] 






In each cycle, the number of cells quadruples and the number of CG iterations roughly doubles. Also, in each cycle, the program writes one output graphic file in VTU format. They are depicted in the following: 

 [2.x.47]  




Due to the variable coefficient (the curvature there is reduced by the same factor by which the coefficient is increased), the top region of the solution is flattened. The gradient of the solution is discontinuous along the interface, although this is not very clearly visible in the pictures above. We will look at this in more detail in the next example. 

The pictures also show that the solution computed by this program is actually pretty wrong on a very coarse mesh (its magnitude is wrong). That's because no numerical method guarantees that the solution on a coarse mesh is particularly accurate -- but we know that the solution [1.x.69] to the exact solution, and indeed you can see how the solutions from one mesh to the next seem to not change very much any more at the end. [1.x.70] [1.x.71]  [2.x.48]  

 [2.x.49] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28] 

[1.x.29] [1.x.30][1.x.31] 


 [2.x.3]  

This program is finally about one of the main features of deal.II: the use of adaptively (locally) refined meshes. The program is still based on  [2.x.4]  and  [2.x.5] , and, as you will see, it does not actually take very much code to enable adaptivity. Indeed, while we do a great deal of explaining, adaptive meshes can be added to an existing program with barely a dozen lines of additional code. The program shows what these lines are, as well as another important ingredient of adaptive mesh refinement (AMR): a criterion that can be used to determine whether it is necessary to refine a cell because the error is large on it, whether the cell can be coarsened because the error is particularly small on it, or whether we should just leave the cell as it is. We will discuss all of these issues in the following. 


[1.x.32][1.x.33] 


There are a number of ways how one can adaptively refine meshes. The basic structure of the overall algorithm is always the same and consists of a loop over the following steps: 

- Solve the PDE on the current mesh; 

- Estimate the error on each cell using some criterion that is indicative   of the error; 

- Mark those cells that have large errors for refinement, mark those that have   particularly small errors for coarsening, and leave the rest alone; 

- Refine and coarsen the cells so marked to obtain a new mesh; 

- Repeat the steps above on the new mesh until the overall error is   sufficiently small. 

For reasons that are probably lost to history (maybe that these functions used to be implemented in FORTRAN, a language that does not care about whether something is spelled in lower or UPPER case letters, with programmers often choosing upper case letters habitually), the loop above is often referenced in publications about mesh adaptivity as the SOLVE-ESTIMATE-MARK-REFINE loop (with this spelling). 

Beyond this structure, however, there are a variety of ways to achieve this. Fundamentally, they differ in how exactly one generates one mesh from the previous one. 

If one were to use triangles (which deal.II does not do), then there are two essential possibilities: 

- Longest-edge refinement: In this strategy, a triangle marked for refinement   is cut into two by introducing one new edge from the midpoint of the longest   edge to the opposite vertex. Of course, the midpoint from the longest edge   has to somehow be balanced by *also* refining the cell on the other side of   that edge (if there is one). If the edge in question is also the longest   edge of the neighboring cell, then we can just run a new edge through the   neighbor to the opposite vertex; otherwise a slightly more involved   construction is necessary that adds more new vertices on at least one   other edge of the neighboring cell, and then may propagate to the neighbors   of the neighbor until the algorithm terminates. This is hard to describe   in words, and because deal.II does not use triangles not worth the time here.   But if you're curious, you can always watch video lecture 15 at the link   shown at the top of this introduction. 

- Red-green refinement: An alternative is what is called "red-green refinement".   This strategy is even more difficult to describe (but also discussed in the   video lecture) and has the advantage that the refinement does not propagate   beyond the immediate neighbors of the cell that we want to refine. It is,   however, substantially more difficult to implement. 

There are other variations of these approaches, but the important point is that they always generate a mesh where the lines where two cells touch are entire edges of both adjacent cells. With a bit of work, this strategy is readily adapted to three-dimensional meshes made from tetrahedra. 

Neither of these methods works for quadrilaterals in 2d and hexahedra in 3d, or at least not easily. The reason is that the transition elements created out of the quadrilateral neighbors of a quadrilateral cell that is to be refined would be triangles, and we don't want this. Consequently, the approach to adaptivity chosen in deal.II is to use grids in which neighboring cells may differ in refinement level by one. This then results in nodes on the interfaces of cells which belong to one side, but are unbalanced on the other. The common term for these is &ldquo;hanging nodes&rdquo;, and these meshes then look like this in a very simple situation: 

 [2.x.6]  

A more complicated two-dimensional mesh would look like this (and is discussed in the "Results" section below): 

<img src="https://www.dealii.org/images/steps/developer/step_6_grid_5_ladutenko.svg"      alt="Fifth adaptively refined Ladutenko grid: the cells are clustered           along the inner circle."      width="300" height="300"> 

Finally, a three-dimensional mesh (from  [2.x.7] ) with such hanging nodes is shown here: 

<img src="https://www.dealii.org/images/steps/developer/ [2.x.8] .3d.mesh.png" alt=""      width="300" height="300"> 

The first and third mesh are of course based on a square and a cube, but as the second mesh shows, this is not necessary. The important point is simply that we can refine a mesh independently of its neighbors (subject to the constraint that a cell can be only refined once more than its neighbors), but that we end up with these &ldquo;hanging nodes&rdquo; if we do this. 


[1.x.34][1.x.35] 


Now that you have seen what these adaptively refined meshes look like, you should ask [1.x.36] we would want to do this. After all, we know from theory that if we refine the mesh globally, the error will go down to zero as 

[1.x.37] 

where  [2.x.9]  is some constant independent of  [2.x.10]  and  [2.x.11] ,  [2.x.12]  is the polynomial degree of the finite element in use, and  [2.x.13]  is the diameter of the largest cell. So if the [1.x.38] cell is important, then why would we want to make the mesh fine in some parts of the domain but not all? 

The answer lies in the observation that the formula above is not optimal. In fact, some more work shows that the following is a better estimate (which you should compare to the square of the estimate above): 

[1.x.39] 

(Because  [2.x.14] , this formula immediately implies the previous one if you just pull the mesh size out of the sum.) What this formula suggests is that it is not necessary to make the [1.x.40] cell small, but that the cells really only need to be small [1.x.41]! In other words: The mesh really only has to be fine where the solution has large variations, as indicated by the  [2.x.15] st derivative. This makes intuitive sense: if, for example, we use a linear element  [2.x.16] , then places where the solution is nearly linear (as indicated by  [2.x.17]  being small) will be well resolved even if the mesh is coarse. Only those places where the second derivative is large will be poorly resolved by large elements, and consequently that's where we should make the mesh small. 

Of course, this [1.x.42] is not very useful in practice since we don't know the exact solution  [2.x.18]  of the problem, and consequently, we cannot compute  [2.x.19] . But, and that is the approach commonly taken, we can compute numerical approximations of  [2.x.20]  based only on the discrete solution  [2.x.21]  that we have computed before. We will discuss this in slightly more detail below. This will then help us determine which cells have a large  [2.x.22] st derivative, and these are then candidates for refining the mesh. 


[1.x.43][1.x.44] 


The methods using triangular meshes mentioned above go to great lengths to make sure that each vertex is a vertex of all adjacent cells -- i.e., that there are no hanging nodes. This then automatically makes sure that we can define shape functions in such a way that they are globally continuous (if we use the common  [2.x.23]  Lagrange finite element methods we have been using so far in the tutorial programs, as represented by the FE_Q class). 

On the other hand, if we define shape functions on meshes with hanging nodes, we may end up with shape functions that are not continuous. To see this, think about the situation above where the top right cell is not refined, and consider for a moment the use of a bilinear finite element. In that case, the shape functions associated with the hanging nodes are defined in the obvious way on the two small cells adjacent to each of the hanging nodes. But how do we extend them to the big adjacent cells? Clearly, the function's extension to the big cell cannot be bilinear because then it needs to be linear along each edge of the large cell, and that means that it needs to be zero on the entire edge because it needs to be zero on the two vertices of the large cell on that edge. But it is not zero at the hanging node itself when seen from the small cells' side -- so it is not continuous. The following three figures show three of the shape functions along the edges in question that turn out to not be continuous when defined in the usual way simply based on the cells they are adjacent to: 

 [2.x.24]  


But we do want the finite element solution to be continuous so that we have a &ldquo;conforming finite element method&rdquo; where the discrete finite element space is a proper subset of the  [2.x.25]  function space in which we seek the solution of the Laplace equation. To guarantee that the global solution is continuous at these nodes as well, we have to state some additional constraints on the values of the solution at these nodes. The trick is to realize that while the shape functions shown above are discontinuous (and consequently an [1.x.45] linear combination of them is also discontinuous), that linear combinations in which the shape functions are added up as  [2.x.26]  can be continuous [1.x.46]. In other words, the coefficients  [2.x.27]  can not be chosen arbitrarily but have to satisfy certain constraints so that the function  [2.x.28]  is in fact continuous. What these constraints have to look is relatively easy to understand conceptually, but the implementation in software is complicated and takes several thousand lines of code. On the other hand, in user code, it is only about half a dozen lines you have to add when dealing with hanging nodes. 

In the program below, we will show how we can get these constraints from deal.II, and how to use them in the solution of the linear system of equations. Before going over the details of the program below, you may want to take a look at the  [2.x.29]  documentation module that explains how these constraints can be computed and what classes in deal.II work on them. 


[1.x.47][1.x.48] 


The practice of hanging node constraints is rather simpler than the theory we have outlined above. In reality, you will really only have to add about half a dozen lines of additional code to a program like  [2.x.30]  to make it work with adaptive meshes that have hanging nodes. The interesting part about this is that it is entirely independent of the equation you are solving: The algebraic nature of these constraints has nothing to do with the equation and only depends on the choice of finite element. As a consequence, the code to deal with these constraints is entirely contained in the deal.II library itself, and you do not need to worry about the details. 

The steps you need to make this work are essentially like this: 

- You have to create an AffineConstraints object, which (as the name   suggests) will store all constraints on the finite element space. In   the current context, these are the constraints due to our desire to   keep the solution space continuous even in the presence of hanging   nodes. (Below we will also briefly mention that we will also put   boundary values into this same object, but that is a separate matter.) 

- You have to fill this object using the function    [2.x.31]  to ensure continuity of   the elements of the finite element space. 

- You have to use this object when you copy the local contributions to   the matrix and right hand side into the global objects, by using    [2.x.32]  Up until   now, we have done this ourselves, but now with constraints, this   is where the magic happens and we apply the constraints to the   linear system. What this function does is make sure that the   degrees of freedom located at hanging nodes are not, in fact,   really free. Rather, they are factually eliminated from the   linear system by setting their rows and columns to zero and putting   something on the diagonal to ensure the matrix remains invertible.   The matrix resulting from this process remains symmetric and   positive definite for the Laplace equation we solve here, so we can   continue to use the Conjugate Gradient method for it. 

- You then solve the linear system as usual, but at the end of this   step, you need to make sure that the degrees of "freedom" located   on hanging nodes get their correct (constrained) value so that the   solution you then visualize or evaluate in other ways is in   fact continuous. This is done by calling    [2.x.33]  immediately after solving. 

These four steps are really all that is necessary -- it's that simple from a user perspective. The fact that, in the function calls mentioned above, you will run through several thousand lines of not-so-trivial code is entirely immaterial to this: In user code, there are really only four additional steps. 


[1.x.49][1.x.50] 


The next question, now that we know how to [1.x.51] with meshes that have these hanging nodes is how we [1.x.52] them. 

A simple way has already been shown in  [2.x.34] : If you [1.x.53] where it is necessary to refine the mesh, then you can create one by hand. But in reality, we don't know this: We don't know the solution of the PDE up front (because, if we did, we wouldn't have to use the finite element method), and consequently we do not know where it is necessary to add local mesh refinement to better resolve areas where the solution has strong variations. But the discussion above shows that maybe we can get away with using the discrete solution  [2.x.35]  on one mesh to estimate the derivatives  [2.x.36] , and then use this to determine which cells are too large and which already small enough. We can then generate a new mesh from the current one using local mesh refinement. If necessary, this step is then repeated until we are happy with our numerical solution -- or, more commonly, until we run out of computational resources or patience. 

So that's exactly what we will do. The locally refined grids are produced using an [1.x.54] which estimates the energy error for numerical solutions of the Laplace operator. Since it was developed by Kelly and co-workers, we often refer to it as the &ldquo;Kelly refinement indicator&rdquo; in the library, documentation, and mailing list. The class that implements it is called KellyErrorEstimator, and there is a great deal of information to be found in the documentation of that class that need not be repeated here. The summary, however, is that the class computes a vector with as many entries as there are  [2.x.37]  "active cells", and where each entry contains an estimate of the error on that cell. This estimate is then used to refine the cells of the mesh: those cells that have a large error will be marked for refinement, those that have a particularly small estimate will be marked for coarsening. We don't have to do this by hand: The functions in namespace GridRefinement will do all of this for us once we have obtained the vector of error estimates. 

It is worth noting that while the Kelly error estimator was developed for Laplace's equation, it has proven to be a suitable tool to generate locally refined meshes for a wide range of equations, not even restricted to elliptic only problems. Although it will create non-optimal meshes for other equations, it is often a good way to quickly produce meshes that are well adapted to the features of solutions, such as regions of great variation or discontinuities. 




[1.x.55][1.x.56] 


It turns out that one can see Dirichlet boundary conditions as just another constraint on the degrees of freedom. It's a particularly simple one, indeed: If  [2.x.38]  is a degree of freedom on the boundary, with position  [2.x.39] , then imposing the boundary condition  [2.x.40]  on  [2.x.41]  simply yields the constraint  [2.x.42] . 

The AffineConstraints class can handle such constraints as well, which makes it convenient to let the same object we use for hanging node constraints also deal with these Dirichlet boundary conditions. This way, we don't need to apply the boundary conditions after assembly (like we did in the earlier steps). All that is necessary is that we call the variant of  [2.x.43]  that returns its information in an AffineConstraints object, rather than the  [2.x.44]  we have used in previous tutorial programs. 


[1.x.57] [1.x.58] 




Since the concepts used for locally refined grids are so important, we do not show much other material in this example. The most important exception is that we show how to use biquadratic elements instead of the bilinear ones which we have used in all previous examples. In fact, the use of higher order elements is accomplished by only replacing three lines of the program, namely the initialization of the  [2.x.45]  member variable in the constructor of the main class of this program, and the use of an appropriate quadrature formula in two places. The rest of the program is unchanged. 

The only other new thing is a method to catch exceptions in the  [2.x.46]  function in order to output some information in case the program crashes for some reason. This is discussed below in more detail. [1.x.59] [1.x.60] 


[1.x.61]  [1.x.62] 




The first few files have already been covered in previous examples and will thus not be further commented on. 

[1.x.63] 



From the following include file we will import the declaration of H1-conforming finite element shape functions. This family of finite elements is called  [2.x.47] , and was used in all examples before already to define the usual bi- or tri-linear elements, but we will now use it for bi-quadratic elements: 

[1.x.64] 



We will not read the grid from a file as in the previous example, but generate it using a function of the library. However, we will want to write out the locally refined grids (just the grid, not the solution) in each step, so we need the following include file instead of  [2.x.48] : 

[1.x.65] 



When using locally refined grids, we will get so-called <code>hanging nodes</code>. However, the standard finite element methods assumes that the discrete solution spaces be continuous, so we need to make sure that the degrees of freedom on hanging nodes conform to some constraints such that the global solution is continuous. We are also going to store the boundary conditions in this object. The following file contains a class which is used to handle these constraints: 

[1.x.66] 



In order to refine our grids locally, we need a function from the library that decides which cells to flag for refinement or coarsening based on the error indicators we have computed. This function is defined here: 

[1.x.67] 



Finally, we need a simple way to actually compute the refinement indicators based on some error estimate. While in general, adaptivity is very problem-specific, the error indicator in the following file often yields quite nicely adapted grids for a wide class of problems. 

[1.x.68] 



Finally, this is as in previous programs: 

[1.x.69] 




[1.x.70]  [1.x.71] 




The main class is again almost unchanged. Two additions, however, are made: we have added the  [2.x.49]  function, which is used to adaptively refine the grid (instead of the global refinement in the previous examples), and a variable which will hold the constraints. 

[1.x.72] 



This is the new variable in the main class. We need an object which holds a list of constraints to hold the hanging nodes and the boundary conditions. 

[1.x.73] 




[1.x.74]  [1.x.75] 




The implementation of nonconstant coefficients is copied verbatim from  [2.x.50] : 

[1.x.76] 




[1.x.77]  [1.x.78] 





[1.x.79]  [1.x.80] 




The constructor of this class is mostly the same as before, but this time we want to use the quadratic element. To do so, we only have to replace the constructor argument (which was  [2.x.51]  in all previous examples) by the desired polynomial degree (here  [2.x.52] ): 

[1.x.81] 




[1.x.82]  [1.x.83] 




The next function sets up all the variables that describe the linear finite element problem, such as the DoFHandler, matrices, and vectors. The difference to what we did in  [2.x.53]  is only that we now also have to take care of hanging node constraints. These constraints are handled almost exclusively by the library, i.e. you only need to know that they exist and how to get them, but you do not have to know how they are formed or what exactly is done with them. 




At the beginning of the function, you find all the things that are the same as in  [2.x.54] : setting up the degrees of freedom (this time we have quadratic elements, but there is no difference from a user code perspective to the linear -- or any other degree, for that matter -- case), generating the sparsity pattern, and initializing the solution and right hand side vectors. Note that the sparsity pattern will have significantly more entries per row now, since there are now 9 degrees of freedom per cell (rather than only four), that can couple with each other. 

[1.x.84] 



We may now populate the AffineConstraints object with the hanging node constraints. Since we will call this function in a loop we first clear the current set of constraints from the last system and then compute new ones: 

[1.x.85] 



Now we are ready to interpolate the boundary values with indicator 0 (the whole boundary) and store the resulting constraints in our  [2.x.55]  object. Note that we do not to apply the boundary conditions after assembly, like we did in earlier steps: instead we put all constraints on our function space in the AffineConstraints object. We can add constraints to the AffineConstraints object in either order: if two constraints conflict then the constraint matrix either abort or throw an exception via the Assert macro. 

[1.x.86] 



After all constraints have been added, they need to be sorted and rearranged to perform some actions more efficiently. This postprocessing is done using the  [2.x.56]  function, after which no further constraints may be added any more: 

[1.x.87] 



Now we first build our compressed sparsity pattern like we did in the previous examples. Nevertheless, we do not copy it to the final sparsity pattern immediately.  Note that we call a variant of make_sparsity_pattern that takes the AffineConstraints object as the third argument. We are letting the routine know that we will never write into the locations given by  [2.x.57]  by setting the argument  [2.x.58]  to false (in other words, that we will never write into entries of the matrix that correspond to constrained degrees of freedom). If we were to condense the constraints after assembling, we would have to pass  [2.x.59]  instead because then we would first write into these locations only to later set them to zero again during condensation. 

[1.x.88] 



Now all non-zero entries of the matrix are known (i.e. those from regularly assembling the matrix and those that were introduced by eliminating constraints). We may copy our intermediate object to the sparsity pattern: 

[1.x.89] 



We may now, finally, initialize the sparse matrix: 

[1.x.90] 




[1.x.91]  [1.x.92] 




Next, we have to assemble the matrix. However, to copy the local matrix and vector on each cell into the global system, we are no longer using a hand-written loop. Instead, we use  [2.x.60]  that internally executes this loop while performing Gaussian elimination on rows and columns corresponding to constrained degrees on freedom. 




The rest of the code that forms the local contributions remains unchanged. It is worth noting, however, that under the hood several things are different than before. First, the variable  [2.x.61]  and return value of  [2.x.62]  now are 9 each, where they were 4 before. Introducing such variables as abbreviations is a good strategy to make code work with different elements without having to change too much code. Secondly, the  [2.x.63]  object of course needs to do other things as well, since the shape functions are now quadratic, rather than linear, in each coordinate variable. Again, however, this is something that is completely handled by the library. 

[1.x.93] 



Finally, transfer the contributions from  [2.x.64]  and  [2.x.65]  into the global objects. 

[1.x.94] 



Now we are done assembling the linear system. The constraint matrix took care of applying the boundary conditions and also eliminated hanging node constraints. The constrained nodes are still in the linear system (there is a nonzero entry, chosen in a way that the matrix is well conditioned, on the diagonal of the matrix and all other entries for this line are set to zero) but the computed values are invalid (i.e., the corresponding entries in  [2.x.66]  are currently meaningless). We compute the correct values for these nodes at the end of the  [2.x.67]  function. 

[1.x.95] 




[1.x.96]  [1.x.97] 




We continue with gradual improvements. The function that solves the linear system again uses the SSOR preconditioner, and is again unchanged except that we have to incorporate hanging node constraints. As mentioned above, the degrees of freedom from the AffineConstraints object corresponding to hanging node constraints and boundary values have been removed from the linear system by giving the rows and columns of the matrix a special treatment. This way, the values for these degrees of freedom have wrong, but well-defined values after solving the linear system. What we then have to do is to use the constraints to assign to them the values that they should have. This process, called  [2.x.68]  constraints, computes the values of constrained nodes from the values of the unconstrained ones, and requires only a single additional function call that you find at the end of this function: 







[1.x.98] 




[1.x.99]  [1.x.100] 




We use a sophisticated error estimation scheme to refine the mesh instead of global refinement. We will use the KellyErrorEstimator class which implements an error estimator for the Laplace equation; it can in principle handle variable coefficients, but we will not use these advanced features, but rather use its most simple form since we are not interested in quantitative results but only in a quick way to generate locally refined grids. 




Although the error estimator derived by Kelly et al. was originally developed for the Laplace equation, we have found that it is also well suited to quickly generate locally refined grids for a wide class of problems. This error estimator uses the solution gradient's jump at cell faces (which is a measure for the second derivatives) and scales it by the size of the cell. It is therefore a measure for the local smoothness of the solution at the place of each cell and it is thus understandable that it yields reasonable grids also for hyperbolic transport problems or the wave equation as well, although these grids are certainly suboptimal compared to approaches specially tailored to the problem. This error estimator may therefore be understood as a quick way to test an adaptive program. 




The way the estimator works is to take a  [2.x.69]  object describing the degrees of freedom and a vector of values for each degree of freedom as input and compute a single indicator value for each active cell of the triangulation (i.e. one value for each of the active cells). To do so, it needs two additional pieces of information: a face quadrature formula, i.e., a quadrature formula on  [2.x.70]  dimensional objects. We use a 3-point Gauss rule again, a choice that is consistent and appropriate with the bi-quadratic finite element shape functions in this program. (What constitutes a suitable quadrature rule here of course depends on knowledge of the way the error estimator evaluates the solution field. As said above, the jump of the gradient is integrated over each face, which would be a quadratic function on each face for the quadratic elements in use in this example. In fact, however, it is the square of the jump of the gradient, as explained in the documentation of that class, and that is a quartic function, for which a 3 point Gauss formula is sufficient since it integrates polynomials up to order 5 exactly.) 




Secondly, the function wants a list of boundary indicators for those boundaries where we have imposed Neumann values of the kind  [2.x.71] , along with a function  [2.x.72]  for each such boundary. This information is represented by a map from boundary indicators to function objects describing the Neumann boundary values. In the present example program, we do not use Neumann boundary values, so this map is empty, and in fact constructed using the default constructor of the map in the place where the function call expects the respective function argument. 




The output is a vector of values for all active cells. While it may make sense to compute the [1.x.101] of a solution degree of freedom very accurately, it is usually not necessary to compute the [1.x.102] corresponding to the solution on a cell particularly accurately. We therefore typically use a vector of floats instead of a vector of doubles to represent error indicators. 

[1.x.103] 



The above function returned one error indicator value for each cell in the  [2.x.73]  array. Refinement is now done as follows: refine those 30 per cent of the cells with the highest error values, and coarsen the 3 per cent of cells with the lowest values.    


One can easily verify that if the second number were zero, this would approximately result in a doubling of cells in each step in two space dimensions, since for each of the 30 per cent of cells, four new would be replaced, while the remaining 70 per cent of cells remain untouched. In practice, some more cells are usually produced since it is disallowed that a cell is refined twice while the neighbor cell is not refined; in that case, the neighbor cell would be refined as well.    


In many applications, the number of cells to be coarsened would be set to something larger than only three per cent. A non-zero value is useful especially if for some reason the initial (coarse) grid is already rather refined. In that case, it might be necessary to refine it in some regions, while coarsening in some other regions is useful. In our case here, the initial grid is very coarse, so coarsening is only necessary in a few regions where over-refinement may have taken place. Thus a small, non-zero value is appropriate here.    


The following function now takes these refinement indicators and flags some cells of the triangulation for refinement or coarsening using the method described above. It is from a class that implements several different algorithms to refine a triangulation based on cell-wise error indicators. 

[1.x.104] 



After the previous function has exited, some cells are flagged for refinement, and some other for coarsening. The refinement or coarsening itself is not performed by now, however, since there are cases where further modifications of these flags is useful. Here, we don't want to do any such thing, so we can tell the triangulation to perform the actions for which the cells are flagged: 

[1.x.105] 




[1.x.106]  [1.x.107] 




At the end of computations on each grid, and just before we continue the next cycle with mesh refinement, we want to output the results from this cycle. 




We have already seen in  [2.x.74]  how this can be achieved for the mesh itself. Here, we change a few things:  [2.x.75]   [2.x.76] We use two different formats: gnuplot and VTU. [2.x.77]   [2.x.78] We embed the cycle number in the output file name. [2.x.79]   [2.x.80] For gnuplot output, we set up a  [2.x.81]  object to provide a few extra visualization arguments so that edges appear curved. This is explained in further detail in  [2.x.82] . [2.x.83]   [2.x.84]  

[1.x.108] 




[1.x.109]  [1.x.110] 




The final function before  [2.x.85]  is again the main driver of the class,  [2.x.86] . It is similar to the one of  [2.x.87] , except that we generate a file in the program again instead of reading it from disk, in that we adaptively instead of globally refine the mesh, and that we output the solution on the final mesh in the present function. 




The first block in the main loop of the function deals with mesh generation. If this is the first cycle of the program, instead of reading the grid from a file on disk as in the previous example, we now again create it using a library function. The domain is again a circle with center at the origin and a radius of one (these are the two hidden arguments to the function, which have default values). 




You will notice by looking at the coarse grid that it is of inferior quality than the one which we read from the file in the previous example: the cells are less equally formed. However, using the library function this program works in any space dimension, which was not the case before. 




In case we find that this is not the first cycle, we want to refine the grid. Unlike the global refinement employed in the last example program, we now use the adaptive procedure described above. 




The rest of the loop looks as before: 

[1.x.111] 




[1.x.112]  [1.x.113] 




The main function is unaltered in its functionality from the previous example, but we have taken a step of additional caution. Sometimes, something goes wrong (such as insufficient disk space upon writing an output file, not enough memory when trying to allocate a vector or a matrix, or if we can't read from or write to a file for whatever reason), and in these cases the library will throw exceptions. Since these are run-time problems, not programming errors that can be fixed once and for all, this kind of exceptions is not switched off in optimized mode, in contrast to the  [2.x.88]  macro which we have used to test against programming errors. If uncaught, these exceptions propagate the call tree up to the  [2.x.89]  function, and if they are not caught there either, the program is aborted. In many cases, like if there is not enough memory or disk space, we can't do anything but we can at least print some text trying to explain the reason why the program failed. A way to do so is shown in the following. It is certainly useful to write any larger program in this way, and you can do so by more or less copying this function except for the  [2.x.90]  block that actually encodes the functionality particular to the present application. 

[1.x.114] 



The general idea behind the layout of this function is as follows: let's try to run the program as we did before... 

[1.x.115] 



...and if this should fail, try to gather as much information as possible. Specifically, if the exception that was thrown is an object of a class that is derived from the C++ standard class  [2.x.91]  member function to get a string which describes the reason why the exception was thrown.    


The deal.II exception classes are all derived from the standard class, and in particular, the  [2.x.92]  function will return approximately the same string as would be generated if the exception was thrown using the  [2.x.93]  macro. You have seen the output of such an exception in the previous example, and you then know that it contains the file and line number of where the exception occurred, and some other information. This is also what the following statements would print.    


Apart from this, there isn't much that we can do except exiting the program with an error code (this is what the  [2.x.94]  does): 

[1.x.116] 



If the exception that was thrown somewhere was not an object of a class derived from the standard  [2.x.95]  class, then we can't do anything at all. We then simply print an error message and exit. 

[1.x.117] 



If we got to this point, there was no exception which propagated up to the main function (there may have been exceptions, but they were caught somewhere in the program or the library). Therefore, the program performed as was expected and we can return without error. 

[1.x.118] 

[1.x.119][1.x.120] 




The output of the program looks as follows: 

[1.x.121] 






As intended, the number of cells roughly doubles in each cycle. The number of degrees is slightly more than four times the number of cells; one would expect a factor of exactly four in two spatial dimensions on an infinite grid (since the spacing between the degrees of freedom is half the cell width: one additional degree of freedom on each edge and one in the middle of each cell), but it is larger than that factor due to the finite size of the mesh and due to additional degrees of freedom which are introduced by hanging nodes and local refinement. 




The program outputs the solution and mesh in each cycle of the refinement loop. The solution looks as follows: 

 [2.x.96]  

It is interesting to follow how the program arrives at the final mesh: 

 [2.x.97]  


It is clearly visible that the region where the solution has a kink, i.e. the circle at radial distance 0.5 from the center, is refined most. Furthermore, the central region where the solution is very smooth and almost flat, is almost not refined at all, but this results from the fact that we did not take into account that the coefficient is large there. The region outside is refined rather arbitrarily, since the second derivative is constant there and refinement is therefore mostly based on the size of the cells and their deviation from the optimal square. 




[1.x.122] [1.x.123][1.x.124] 


[1.x.125][1.x.126] 




One thing that is always worth playing around with if one solves problems of appreciable size (much bigger than the one we have here) is to try different solvers or preconditioners. In the current case, the linear system is symmetric and positive definite, which makes the CG algorithm pretty much the canonical choice for solving. However, the SSOR preconditioner we use in the  [2.x.98]  function is up for grabs. 

In deal.II, it is relatively simple to change the preconditioner. For example, by changing the existing lines of code 

[1.x.127] 

into 

[1.x.128] 

we can try out different relaxation parameters for SSOR. By using 

[1.x.129] 

we can use Jacobi as a preconditioner. And by using 

[1.x.130] 

we can use a simple incomplete LU decomposition without any thresholding or strengthening of the diagonal (to use this preconditioner, you have to also add the header file  [2.x.99]  to the include list at the top of the file). 

Using these various different preconditioners, we can compare the number of CG iterations needed (available through the  [2.x.100]  call, see  [2.x.101] ) as well as CPU time needed (using the Timer class, discussed, for example, in  [2.x.102] ) and get the following results (left: iterations; right: CPU time): 

 [2.x.103]  

As we can see, all preconditioners behave pretty much the same on this simple problem, with the number of iterations growing like  [2.x.104]  and because each iteration requires around  [2.x.105]  operations the total CPU time grows like  [2.x.106]  (for the few smallest meshes, the CPU time is so small that it doesn't record). Note that even though it is the simplest method, Jacobi is the fastest for this problem. 

The situation changes slightly when the finite element is not a bi-quadratic one as set in the constructor of this program, but a bi-linear one. If one makes this change, the results are as follows: 

 [2.x.107]  

In other words, while the increase in iterations and CPU time is as before, Jacobi is now the method that requires the most iterations; it is still the fastest one, however, owing to the simplicity of the operations it has to perform. This is not to say that Jacobi is actually a good preconditioner -- for problems of appreciable size, it is definitely not, and other methods will be substantially better -- but really only that it is fast because its implementation is so simple that it can compensate for a larger number of iterations. 

The message to take away from this is not that simplicity in preconditioners is always best. While this may be true for the current problem, it definitely is not once we move to more complicated problems (elasticity or Stokes, for examples  [2.x.108]  or  [2.x.109] ). Secondly, all of these preconditioners still lead to an increase in the number of iterations as the number  [2.x.110]  of degrees of freedom grows, for example  [2.x.111] ; this, in turn, leads to a total growth in effort as  [2.x.112]  since each iteration takes  [2.x.113]  work. This behavior is undesirable: we would really like to solve linear systems with  [2.x.114]  unknowns in a total of  [2.x.115]  work; there is a class of preconditioners that can achieve this, namely geometric ( [2.x.116] ,  [2.x.117] ,  [2.x.118] ) or algebraic multigrid ( [2.x.119] ,  [2.x.120] , and several others) preconditioners. They are, however, significantly more complex than the preconditioners outlined above. 

Finally, the last message to take home is that when the data shown above was generated (in 2018), linear systems with 100,000 unknowns are easily solved on a desktop machine in about a second, making the solution of relatively simple 2d problems even to very high accuracy not that big a task as it used to be even in the past. At the time, the situation for 3d problems was entirely different, but even that has changed substantially in the intervening time -- though solving problems in 3d to high accuracy remains a challenge. 


[1.x.131][1.x.132] 


If you look at the meshes above, you will see even though the domain is the unit disk, and the jump in the coefficient lies along a circle, the cells that make up the mesh do not track this geometry well. The reason, already hinted at in  [2.x.121] , is that in the absence of other information, the Triangulation class only sees a bunch of coarse grid cells but has, of course, no real idea what kind of geometry they might represent when looked at together. For this reason, we need to tell the Triangulation what to do when a cell is refined: where should the new vertices at the edge midpoints and the cell midpoint be located so that the child cells better represent the desired geometry than the parent cell. 

To visualize what the triangulation actually knows about the geometry, it is not enough to just output the location of vertices and draw a straight line for each edge; instead, we have to output both interior and boundary lines as multiple segments so that they look curved. We can do this by making one change to the gnuplot part of  [2.x.122] : 

[1.x.133] 



In the code above, we already do this for faces that sit at the boundary: this happens automatically since we use  [2.x.123]  which attaches a SphericalManifold to the boundary of the domain. To make the mesh [1.x.134] also track a circular domain, we need to work a bit harder, though. First, recall that our coarse mesh consists of a central square cell and four cells around it. Now first consider what would happen if we also attached the SphericalManifold object not only to the four exterior faces but also the four cells at the perimeter as well as all of their faces. We can do this by adding the following snippet (testing that the center of a cell is larger than a small multiple, say one tenth, of the cell diameter away from center of the mesh only fails for the central square of the mesh): 

[1.x.135] 



After a few global refinement steps, this would lead to a mesh of the following kind: 


   [2.x.124]  

Creating good meshes, and in particular making them fit the geometry you want, is a complex topic in itself. You can find much more on this in  [2.x.125] ,  [2.x.126] , and  [2.x.127] , among other tutorial programs that cover the issue.  [2.x.128]  shows another, less manual way to achieve a mesh well fit to the problem here. Information on curved domains can also be found in the documentation module on  [2.x.129]  "Manifold descriptions". 

Why does it make sense to choose a mesh that tracks the internal interface? There are a number of reasons, but the most essential one comes down to what we actually integrate in our bilinear form. Conceptually, we want to integrate the term  [2.x.130]  as the contribution of cell  [2.x.131]  to the matrix entry  [2.x.132] . We can not compute it exactly and have to resort to quadrature. We know that quadrature is accurate if the integrand is smooth. That is because quadrature in essence computes a polynomial approximation to the integrand that coincides with the integrand in the quadrature points, and then computes the volume under this polynomial as an approximation to the volume under the original integrand. This polynomial interpolant is accurate if the integrand is smooth on a cell, but it is usually rather inaccurate if the integrand is discontinuous on a cell. 

Consequently, it is worthwhile to align cells in such a way that the interfaces across which the coefficient is discontinuous are aligned with cell interfaces. This way, the coefficient is constant on each cell, following which the integrand will be smooth, and its polynomial approximation and the quadrature approximation of the integral will both be accurate. Note that such an alignment is common in many practical cases, so deal.II provides a number of functions (such as  [2.x.133]  "material_id") to help manage such a scenario. Refer to  [2.x.134]  and  [2.x.135]  for examples of how material ids can be applied. 

Finally, let us consider the case of a coefficient that has a smooth and non-uniform distribution in space. We can repeat once again all of the above discussion on the representation of such a function with the quadrature. So, to simulate it accurately there are a few readily available options: you could reduce the cell size, increase the order of the polynomial used in the quadrature formula, select a more appropriate quadrature formula, or perform a combination of these steps. The key is that providing the best fit of the coefficient's spatial dependence with the quadrature polynomial will lead to a more accurate finite element solution of the PDE. 

As a final note: The discussion in the previous paragraphs shows, we here have a very concrete way of stating what we think of a good mesh -- it should be aligned with the jump in the coefficient. But one could also have asked this kind of question in a more general setting: Given some equation with a smooth solution and smooth coefficients, can we say what a good mesh would look like? This is a question for which the answer is easier to state in intuitive terms than mathematically: A good mesh has cells that all, by and large, look like squares (or cubes, in 3d). A bad mesh would contain cells that are very elongated in some directions or, more generally, for which there are cells that have both short and long edges. There are many ways in which one could assign a numerical quality index to each cell that measures whether the cell is "good" or "bad"; some of these are often chosen because they are cheap and easy to compute, whereas others are based on what enters into proofs of convergence. An example of the former would be the ratio of the longest to the shortest edge of a cell: In the ideal case, that ratio would be one; bad cells have values much larger than one. An example of the latter kind would consider the gradient (the "Jacobian") of the mapping from the reference cell  [2.x.136]  to the real cell  [2.x.137] ; this gradient is a matrix, and a quantity that enters into error estimates is the maximum over all points on the reference cell of the ratio of the largest to the smallest eigenvalue of this matrix. It is again not difficult to see that this ratio is constant if the cell  [2.x.138]  is an affine image of  [2.x.139] , and that it is one for squares and cubes. 

In practice, it might be interesting to visualize such quality measures. The function  [2.x.140]  provides one way to get this kind of information. Even better, visualization tools such as VisIt often allow you to visualize this sort of information for a variety of measures from within the visualization software itself; in the case of VisIt, just add a "pseudo-color" plot and select one of the mesh quality measures instead of the solution field. 


[1.x.138][1.x.139] 


From a mathematical perspective, solutions of the Laplace equation [1.x.140] on smoothly bounded, convex domains are known to be smooth themselves. The exact degree of smoothness, i.e., the function space in which the solution lives, depends on how smooth exactly the boundary of the domain is, and how smooth the right hand side is. Some regularity of the solution may be lost at the boundary, but one generally has that the solution is twice more differentiable in compact subsets of the domain than the right hand side. If, in particular, the right hand side satisfies  [2.x.141] , then  [2.x.142]  where  [2.x.143]  is any compact subset of  [2.x.144]  ( [2.x.145]  is an open domain, so a compact subset needs to keep a positive distance from  [2.x.146] ). 

The situation we chose for the current example is different, however: we look at an equation with a non-constant coefficient  [2.x.147] : [1.x.141] Here, if  [2.x.148]  is not smooth, then the solution will not be smooth either, regardless of  [2.x.149] . In particular, we expect that wherever  [2.x.150]  is discontinuous along a line (or along a plane in 3d), the solution will have a kink. This is easy to see: if for example  [2.x.151]  is continuous, then  [2.x.152]  needs to be continuous. This means that  [2.x.153]  must be continuously differentiable (not have a kink). Consequently, if  [2.x.154]  has a discontinuity, then  [2.x.155]  must have an opposite discontinuity so that the two exactly cancel and their product yields a function without a discontinuity. But for  [2.x.156]  to have a discontinuity,  [2.x.157]  must have a kink. This is of course exactly what is happening in the current example, and easy to observe in the pictures of the solution. 

In general, if the coefficient  [2.x.158]  is discontinuous along a line in 2d, or a plane in 3d, then the solution may have a kink, but the gradient of the solution will not go to infinity. That means, that the solution is at least still in the [1.x.142]  [2.x.159]  (i.e., roughly speaking, in the space of functions whose derivatives are bounded). On the other hand, we know that in the most extreme cases -- i.e., where the domain has reentrant corners, the right hand side only satisfies  [2.x.160] , or the coefficient  [2.x.161]  is only in  [2.x.162]  -- all we can expect is that  [2.x.163]  (i.e., the [1.x.143] of functions whose derivative is square integrable), a much larger space than  [2.x.164] . It is not very difficult to create cases where the solution is in a space  [2.x.165]  where we can get  [2.x.166]  to become as small as we want. Such cases are often used to test adaptive finite element methods because the mesh will have to resolve the singularity that causes the solution to not be in  [2.x.167]  any more. 

The typical example one uses for this is called the [1.x.144] (referring to  [2.x.168] ), which in the commonly used form has a coefficient  [2.x.169]  that has different values in the four quadrants of the plane (or eight different values in the octants of  [2.x.170] ). The exact degree of regularity (the  [2.x.171]  in the index of the Sobolev space above) depends on the values of  [2.x.172]  coming together at the origin, and by choosing the jumps large enough, the regularity of the solution can be made as close as desired to  [2.x.173] . 

To implement something like this, one could replace the coefficient function by the following (shown here only for the 2d case): 

[1.x.145] 

(Adding the  [2.x.174]  at the end ensures that either an exception is thrown or that the program aborts if we ever get to that point 

-- which of course we shouldn't, but this is a good way to insure yourself: we all make mistakes by sometimes not thinking of all cases, for example by checking for  [2.x.175]  to be less than and greater than zero, rather than greater-or-equal to zero, and thereby forgetting some cases that would otherwise lead to bugs that are awkward to find. The  [2.x.176]  at the end is only there to avoid compiler warnings that the function does not end in a  [2.x.177]  statement -- the compiler cannot see that the function would never actually get to that point because of the preceding  [2.x.178]  statement.) 

By playing with such cases where four or more sectors come together and on which the coefficient has different values, one can construct cases where the solution has singularities at the origin. One can also see how the meshes are refined in such cases. [1.x.146] [1.x.147]  [2.x.179]  

 [2.x.180] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27] 

[1.x.28] [1.x.29][1.x.30] 


In this program, we will mainly consider two aspects:  [2.x.3]     [2.x.4]  Verification of correctness of the program and generation of convergence   tables;    [2.x.5]  Non-homogeneous Neumann boundary conditions for the Helmholtz equation.  [2.x.6]  Besides these topics, again a variety of improvements and tricks will be shown. 


[1.x.31][1.x.32] 


There has probably never been a non-trivial finite element program that worked right from the start. It is therefore necessary to find ways to verify whether a computed solution is correct or not. Usually, this is done by choosing the set-up of a simulation in such a way that we know the exact continuous solution and evaluate the difference between continuous and computed discrete solution. If this difference converges to zero with the right order of convergence, this is already a good indication of correctness, although there may be other sources of error persisting which have only a small contribution to the total error or are of higher order. In the context of finite element simulations, this technique of picking the solution by choosing appropriate right hand sides and boundary conditions is often called the [1.x.33]. 

In this example, we will not go into the theories of systematic software verification which is a very complicated problem. Rather we will demonstrate the tools which deal.II can offer in this respect. This is basically centered around the functionality of a single function,  [2.x.7]  This function computes the difference between a given continuous function and a finite element field in various norms on each cell. Of course, like with any other integral, we can only evaluate these norms using quadrature formulas; the choice of the right quadrature formula is therefore crucial to the accurate evaluation of the error. This holds in particular for the  [2.x.8]  norm, where we evaluate the maximal deviation of numerical and exact solution only at the quadrature points; one should then not try to use a quadrature rule whose evaluation occurs only at points where [super-convergence](https://en.wikipedia.org/wiki/Superconvergence) might occur, such as the Gauss points of the lowest-order Gauss quadrature formula for which the integrals in the assembly of the matrix is correct (e.g., for linear elements, do not use the QGauss(2) quadrature formula). In fact, this is generally good advice also for the other norms: if your quadrature points are fortuitously chosen at locations where the error happens to be particularly small due to superconvergence, the computed error will look like it is much smaller than it really is and may even suggest a higher convergence order. Consequently, we will choose a different quadrature formula for the integration of these error norms than for the assembly of the linear system. 

The function  [2.x.9]  evaluates the desired norm on each cell  [2.x.10]  of the triangulation and returns a vector which holds these values for each cell. From the local values, we can then obtain the global error. For example, if the vector  [2.x.11]  with element  [2.x.12]  for all cells  [2.x.13]  contains the local  [2.x.14]  norms  [2.x.15] , then [1.x.34] is the global  [2.x.16]  error  [2.x.17] . 

In the program, we will show how to evaluate and use these quantities, and we will monitor their values under mesh refinement. Of course, we have to choose the problem at hand such that we can explicitly state the solution and its derivatives, but since we want to evaluate the correctness of the program, this is only reasonable. If we know that the program produces the correct solution for one (or, if one wants to be really sure: many) specifically chosen right hand sides, we can be rather confident that it will also compute the correct solution for problems where we don't know the exact values. 

In addition to simply computing these quantities, we will show how to generate nicely formatted tables from the data generated by this program that automatically computes convergence rates etc. In addition, we will compare different strategies for mesh refinement. 


[1.x.35][1.x.36] 


The second, totally unrelated, subject of this example program is the use of non-homogeneous boundary conditions. These are included into the variational form using boundary integrals which we have to evaluate numerically when assembling the right hand side vector. 

Before we go into programming, let's have a brief look at the mathematical formulation. The equation that we want to solve here is the Helmholtz equation "with the nice sign": [1.x.37] on the square  [2.x.18]  with  [2.x.19] , augmented by Dirichlet boundary conditions [1.x.38] on some part  [2.x.20]  of the boundary  [2.x.21] , and Neumann conditions [1.x.39] on the rest  [2.x.22] . In our particular testcase, we will use  [2.x.23] . (We say that this equation has the "nice sign" because the operator  [2.x.24]  with the identity  [2.x.25]  and  [2.x.26]  is a positive definite operator; the [1.x.40] is  [2.x.27]  and results from modeling time-harmonic processes. The operator is not positive definite if  [2.x.28]  is large, and this leads to all sorts of issues we need not discuss here. The operator may also not be invertible -- i.e., the equation does not have a unique solution -- if  [2.x.29]  happens to be one of the eigenvalues of  [2.x.30] .) 

Because we want to verify the convergence of our numerical solution  [2.x.31] , we want a setup so that we know the exact solution  [2.x.32] . This is where the Method of Manufactured Solutions comes in. To this end, let us choose a function [1.x.41] where the centers  [2.x.33]  of the exponentials are    [2.x.34] ,    [2.x.35] , and    [2.x.36] , and the half width is set to  [2.x.37] . The method of manufactured solution then says: choose 

[1.x.42] 

With this particular choice, we infer that of course the solution of the original problem happens to be  [2.x.38] . In other words, by choosing the right hand sides of the equation and the boundary conditions in a particular way, we have manufactured ourselves a problem to which we know the solution. This allows us then to compute the error of our numerical solution. In the code below, we represent  [2.x.39]  by the  [2.x.40]  class, and other classes will be used to denote  [2.x.41]  and  [2.x.42] . 

Using the above definitions, we can state the weak formulation of the equation, which reads: find  [2.x.43]  such that [1.x.43] for all test functions  [2.x.44] . The boundary term  [2.x.45]  has appeared by integration by parts and using  [2.x.46]  on  [2.x.47]  and  [2.x.48]  on  [2.x.49] . The cell matrices and vectors which we use to build the global matrices and right hand side vectors in the discrete formulation therefore look like this: [1.x.44] 

Since the generation of the domain integrals has been shown in previous examples several times, only the generation of the contour integral is of interest here. It basically works along the following lines: for domain integrals we have the  [2.x.50]  class that provides values and gradients of the shape values, as well as Jacobian determinants and other information and specified quadrature points in the cell; likewise, there is a class  [2.x.51]  that performs these tasks for integrations on faces of cells. One provides it with a quadrature formula for a manifold with dimension one less than the dimension of the domain is, and the cell and the number of its face on which we want to perform the integration. The class will then compute the values, gradients, normal vectors, weights, etc. at the quadrature points on this face, which we can then use in the same way as for the domain integrals. The details of how this is done are shown in the following program. 


[1.x.45][1.x.46] 


Besides the mathematical topics outlined above, we also want to use this program to illustrate one aspect of good programming practice, namely the use of namespaces. In programming the deal.II library, we have take great care not to use names for classes and global functions that are overly generic, say  [2.x.52]  etc. Furthermore, we have put everything into namespace  [2.x.53] . But when one writes application programs that aren't meant for others to use, one doesn't always pay this much attention. If you follow the programming style of  [2.x.54]  through  [2.x.55] , these functions then end up in the global namespace where, unfortunately, a lot of other stuff also lives (basically everything the C language provides, along with everything you get from the operating system through header files). To make things a bit worse, the designers of the C language were also not always careful in avoiding generic names; for example, the symbols <code>j1, jn</code> are defined in C header files (they denote Bessel functions). 

To avoid the problems that result if names of different functions or variables collide (often with confusing error messages), it is good practice to put everything you do into a [1.x.47]. Following this style, we will open a namespace  [2.x.56]  at the top of the program, import the deal.II namespace into it, put everything that's specific to this program (with the exception of  [2.x.57] , which must be in the global namespace) into it, and only close it at the bottom of the file. In other words, the structure of the program is of the kind 

[1.x.48] 

We will follow this scheme throughout the remainder of the deal.II tutorial. [1.x.49] [1.x.50] 


[1.x.51]  [1.x.52] 




These first include files have all been treated in previous examples, so we won't explain what is in them again. 

[1.x.53] 



In this example, we will not use the numeration scheme which is used per default by the DoFHandler class, but will renumber them using the Cuthill-McKee algorithm. As has already been explained in  [2.x.58] , the necessary functions are declared in the following file: 

[1.x.54] 



Then we will show a little trick how we can make sure that objects are not deleted while they are still in use. For this purpose, deal.II has the SmartPointer helper class, which is declared in this file: 

[1.x.55] 



Next, we will want to use the function  [2.x.59]  mentioned in the introduction, and we are going to use a ConvergenceTable that collects all important data during a run and prints it at the end as a table. These comes from the following two files: 

[1.x.56] 



And finally, we need to use the FEFaceValues class, which is declared in the same file as the FEValues class: 

[1.x.57] 



The last step before we go on with the actual implementation is to open a namespace  [2.x.60]  into which we will put everything, as discussed at the end of the introduction, and to import the members of namespace  [2.x.61]  into it: 

[1.x.58] 




[1.x.59]  [1.x.60] 




Before implementing the classes that actually solve something, we first declare and define some function classes that represent right hand side and solution classes. Since we want to compare the numerically obtained solution to the exact continuous one, we need a function object that represents the continuous solution. On the other hand, we need the right hand side function, and that one of course shares some characteristics with the solution. In order to reduce dependencies which arise if we have to change something in both classes at the same time, we move the common characteristics of both functions into a base class.    


The common characteristics for solution (as explained in the introduction, we choose a sum of three exponentials) and right hand side, are these: the number of exponentials, their centers, and their half width. We declare them in the following class. Since the number of exponentials is a compile-time constant we use a fixed-length  [2.x.62]  to store the center points: 

[1.x.61] 



The variables which denote the centers and the width of the exponentials have just been declared, now we still need to assign values to them. Here, we can show another small piece of template sorcery, namely how we can assign different values to these variables depending on the dimension. We will only use the 2d case in the program, but we show the 1d case for exposition of a useful technique.    


First we assign values to the centers for the 1d case, where we place the centers equidistantly at -1/3, 0, and 1/3. The <code>template &lt;&gt;</code> header for this definition indicates an explicit specialization. This means, that the variable belongs to a template, but that instead of providing the compiler with a template from which it can specialize a concrete variable by substituting  [2.x.63]  with some concrete value, we provide a specialization ourselves, in this case for  [2.x.64] . If the compiler then sees a reference to this variable in a place where the template argument equals one, it knows that it doesn't have to generate the variable from a template by substituting  [2.x.65] , but can immediately use the following definition: 

[1.x.62] 



Likewise, we can provide an explicit specialization for  [2.x.66] . We place the centers for the 2d case as follows: 

[1.x.63] 



There remains to assign a value to the half-width of the exponentials. We would like to use the same value for all dimensions. In this case, we simply provide the compiler with a template from which it can generate a concrete instantiation by substituting  [2.x.67]  with a concrete value: 

[1.x.64] 



After declaring and defining the characteristics of solution and right hand side, we can declare the classes representing these two. They both represent continuous functions, so they are derived from the Function&lt;dim&gt; base class, and they also inherit the characteristics defined in the SolutionBase class.    


The actual classes are declared in the following. Note that in order to compute the error of the numerical solution against the continuous one in the L2 and H1 (semi-)norms, we have to provide value and gradient of the exact solution. This is more than we have done in previous examples, where all we provided was the value at one or a list of points. Fortunately, the Function class also has virtual functions for the gradient, so we can simply overload the respective virtual member functions in the Function base class. Note that the gradient of a function in  [2.x.68]  space dimensions is a vector of size  [2.x.69] , i.e. a tensor of rank 1 and dimension  [2.x.70] . As for so many other things, the library provides a suitable class for this. One new thing about this class is that it explicitly uses the Tensor objects, which previously appeared as intermediate terms in  [2.x.71]  and  [2.x.72] . A tensor is a generalization of scalars (rank zero tensors), vectors (rank one tensors), and matrices (rank two tensors), as well as higher dimensional objects. The Tensor class requires two template arguments: the tensor rank and tensor dimension. For example, here we use tensors of rank one (vectors) with dimension  [2.x.73]  entries.) While this is a bit less flexible than using Vector, the compiler can generate faster code when the length of the vector is known at compile time. Additionally, specifying a Tensor of rank one and dimension  [2.x.74]  guarantees that the tensor will have the right shape (since it is built into the type of the object itself), so the compiler can catch most size-related mistakes for us.    


Like in  [2.x.75] , for compatibility with some compilers we explicitly declare the default constructor: 

[1.x.65] 



The actual definition of the values and gradients of the exact solution class is according to their mathematical definition and does not need much explanation.    


The only thing that is worth mentioning is that if we access elements of a base class that is template dependent (in this case the elements of SolutionBase&lt;dim&gt;), then the C++ language forces us to write  [2.x.76] , and similarly for other members of the base class. C++ does not require the  [2.x.77]  qualification if the base class is not template dependent. The reason why this is necessary is complicated; C++ books will explain under the phrase [1.x.66], and there is also a lengthy description in the deal.II FAQs. 

[1.x.67] 



Likewise, this is the computation of the gradient of the solution.  In order to accumulate the gradient from the contributions of the exponentials, we allocate an object  [2.x.78]  that denotes the mathematical quantity of a tensor of rank  [2.x.79]  and dimension  [2.x.80] . Its default constructor sets it to the vector containing only zeroes, so we need not explicitly care for its initialization.    


Note that we could as well have taken the type of the object to be Point&lt;dim&gt; instead of Tensor&lt;1,dim&gt;. Tensors of rank 1 and points are almost exchangeable, and have only very slightly different mathematical meanings. In fact, the Point&lt;dim&gt; class is derived from the Tensor&lt;1,dim&gt; class, which makes up for their mutual exchange ability. Their main difference is in what they logically mean: points are points in space, such as the location at which we want to evaluate a function (see the type of the first argument of this function for example). On the other hand, tensors of rank 1 share the same transformation properties, for example that they need to be rotated in a certain way when we change the coordinate system; however, they do not share the same connotation that points have and are only objects in a more abstract space than the one spanned by the coordinate directions. (In fact, gradients live in `reciprocal' space, since the dimension of their components is not that of a length, but of one over length). 

[1.x.68] 



For the gradient, note that its direction is along (x-x_i), so we add up multiples of this distance vector, where the factor is given by the exponentials. 

[1.x.69] 



Besides the function that represents the exact solution, we also need a function which we can use as right hand side when assembling the linear system of discretized equations. This is accomplished using the following class and the following definition of its function. Note that here we only need the value of the function, not its gradients or higher derivatives. 

[1.x.70] 



The value of the right hand side is given by the negative Laplacian of the solution plus the solution itself, since we wanted to solve Helmholtz's equation: 

[1.x.71] 



The first contribution is the Laplacian: 

[1.x.72] 



And the second is the solution itself: 

[1.x.73] 




[1.x.74]  [1.x.75] 




Then we need the class that does all the work. Except for its name, its interface is mostly the same as in previous examples.    


One of the differences is that we will use this class in several modes: for different finite elements, as well as for adaptive and global refinement. The decision whether global or adaptive refinement shall be used is communicated to the constructor of this class through an enumeration type declared at the top of the class. The constructor then takes a finite element object and the refinement mode as arguments.    


The rest of the member functions are as before except for the  [2.x.81]  function: After the solution has been computed, we perform some analysis on it, such as computing the error in various norms. To enable some output, it requires the number of the refinement cycle, and consequently gets it as an argument. 

[1.x.76] 



Now for the data elements of this class. Among the variables that we have already used in previous examples, only the finite element object differs: The finite elements which the objects of this class operate on are passed to the constructor of this class. It has to store a pointer to the finite element for the member functions to use. Now, for the present class there is no big deal in that, but since we want to show techniques rather than solutions in these programs, we will here point out a problem that often occurs -- and of course the right solution as well.      


Consider the following situation that occurs in all the example programs: we have a triangulation object, and we have a finite element object, and we also have an object of type DoFHandler that uses both of the first two. These three objects all have a lifetime that is rather long compared to most other objects: they are basically set at the beginning of the program or an outer loop, and they are destroyed at the very end. The question is: can we guarantee that the two objects which the DoFHandler uses, live at least as long as they are in use? This means that the DoFHandler must have some kind of knowledge on the destruction of the other objects.      


We will show here how the library managed to find out that there are still active references to an object and the object is still alive from the point of view of a using object. Basically, the method is along the following line: all objects that are subject to such potentially dangerous pointers are derived from a class called Subscriptor. For example, the Triangulation, DoFHandler, and a base class of the FiniteElement class are derived from Subscriptor. This latter class does not offer much functionality, but it has a built-in counter which we can subscribe to, thus the name of the class. Whenever we initialize a pointer to that object, we can increase its use counter, and when we move away our pointer or do not need it any more, we decrease the counter again. This way, we can always check how many objects still use that object. Additionally, the class requires to know about a pointer that it can use to tell the subscribing object about its invalidation.      


If an object of a class that is derived from the Subscriptor class is destroyed, it also has to call the destructor of the Subscriptor class. In this destructor, we tell all the subscribing objects about the invalidation of the object using the stored pointers. The same happens when the object appears on the right hand side of a move expression, i.e., it will no longer contain valid content after the operation. The subscribing class is expected to check the value stored in its corresponding pointer before trying to access the object subscribed to.      


This is exactly what the SmartPointer class is doing. It basically acts just like a pointer, i.e. it can be dereferenced, can be assigned to and from other pointers, and so on. On top of that it uses the mechanism described above to find out if the pointer this class is representing is dangling when we try to dereference it. In that case an exception is thrown.      


In the present example program, we want to protect the finite element object from the situation that for some reason the finite element pointed to is destroyed while still in use. We therefore use a SmartPointer to the finite element object; since the finite element object is actually never changed in our computations, we pass a const FiniteElement&lt;dim&gt; as template argument to the SmartPointer class. Note that the pointer so declared is assigned at construction time of the solve object, and destroyed upon destruction, so the lock on the destruction of the finite element object extends throughout the lifetime of this HelmholtzProblem object. 

[1.x.77] 



The second to last variable stores the refinement mode passed to the constructor. Since it is only set in the constructor, we can declare this variable constant, to avoid that someone sets it involuntarily (e.g. in an `if'-statement where == was written as = by chance). 

[1.x.78] 



For each refinement level some data (like the number of cells, or the L2 error of the numerical solution) will be generated and later printed. The TableHandler can be used to collect all this data and to output it at the end of the run as a table in a simple text or in LaTeX format. Here we don't only use the TableHandler but we use the derived class ConvergenceTable that additionally evaluates rates of convergence: 

[1.x.79] 




[1.x.80]  [1.x.81] 





[1.x.82]  [1.x.83] 




In the constructor of this class, we only set the variables passed as arguments, and associate the DoF handler object with the triangulation (which is empty at present, however). 

[1.x.84] 




[1.x.85]  [1.x.86] 




The following function sets up the degrees of freedom, sizes of matrices and vectors, etc. Most of its functionality has been showed in previous examples, the only difference being the renumbering step immediately after first distributing degrees of freedom.    


Renumbering the degrees of freedom is not overly difficult, as long as you use one of the algorithms included in the library. It requires only a single line of code. Some more information on this can be found in  [2.x.82] .    


Note, however, that when you renumber the degrees of freedom, you must do so immediately after distributing them, since such things as hanging nodes, the sparsity pattern etc. depend on the absolute numbers which are altered by renumbering.    


The reason why we introduce renumbering here is that it is a relatively cheap operation but often has a beneficial effect: While the CG iteration itself is independent of the actual ordering of degrees of freedom, we will use SSOR as a preconditioner. SSOR goes through all degrees of freedom and does some operations that depend on what happened before; the SSOR operation is therefore not independent of the numbering of degrees of freedom, and it is known that its performance improves by using renumbering techniques. A little experiment shows that indeed, for example, the number of CG iterations for the fifth refinement cycle of adaptive refinement with the Q1 program used here is 40 without, but 36 with renumbering. Similar savings can generally be observed for all the computations in this program. 

[1.x.87] 




[1.x.88]  [1.x.89] 




Assembling the system of equations for the problem at hand is mostly as for the example programs before. However, some things have changed anyway, so we comment on this function fairly extensively.    


At the top of the function you will find the usual assortment of variable declarations. Compared to previous programs, of importance is only that we expect to solve problems also with bi-quadratic elements and therefore have to use sufficiently accurate quadrature formula. In addition, we need to compute integrals over faces, i.e.  [2.x.83]  dimensional objects. The declaration of a face quadrature formula is then straightforward: 

[1.x.90] 



Then we need objects which can evaluate the values, gradients, etc of the shape functions at the quadrature points. While it seems that it should be feasible to do it with one object for both domain and face integrals, there is a subtle difference since the weights in the domain integrals include the measure of the cell in the domain, while the face integral quadrature requires the measure of the face in a lower-dimensional manifold. Internally these two classes are rooted in a common base class which does most of the work and offers the same interface to both domain and interface integrals.      


For the domain integrals in the bilinear form for Helmholtz's equation, we need to compute the values and gradients, as well as the weights at the quadrature points. Furthermore, we need the quadrature points on the real cell (rather than on the unit cell) to evaluate the right hand side function. The object we use to get at this information is the FEValues class discussed previously.      


For the face integrals, we only need the values of the shape functions, as well as the weights. We also need the normal vectors and quadrature points on the real cell since we want to determine the Neumann values from the exact solution object (see below). The class that gives us this information is called FEFaceValues: 

[1.x.91] 



Then we need some objects already known from previous examples: An object denoting the right hand side function, its values at the quadrature points on a cell, the cell matrix and right hand side, and the indices of the degrees of freedom on a cell.      


Note that the operations we will do with the right hand side object are only querying data, never changing the object. We can therefore declare it  [2.x.84] : 

[1.x.92] 



Finally we define an object denoting the exact solution function. We will use it to compute the Neumann values at the boundary from it. Usually, one would of course do so using a separate object, in particular since the exact solution is generally unknown while the Neumann values are prescribed. We will, however, be a little bit lazy and use what we already have in information. Real-life programs would to go other ways here, of course. 

[1.x.93] 



Now for the main loop over all cells. This is mostly unchanged from previous examples, so we only comment on the things that have changed. 

[1.x.94] 



The first thing that has changed is the bilinear form. It now contains the additional term from the Helmholtz equation: 

[1.x.95] 



Then there is that second term on the right hand side, the contour integral. First we have to find out whether the intersection of the faces of this cell with the boundary part Gamma2 is nonzero. To this end, we loop over all faces and check whether its boundary indicator equals  [2.x.85] , which is the value that we have assigned to that portions of the boundary composing Gamma2 in the  [2.x.86]  function further below. (The default value of boundary indicators is  [2.x.87] , so faces can only have an indicator equal to  [2.x.88]  if we have explicitly set it.) 

[1.x.96] 



If we came into here, then we have found an external face belonging to Gamma2. Next, we have to compute the values of the shape functions and the other quantities which we will need for the computation of the contour integral. This is done using the  [2.x.89]  function which we already know from the FEValue class: 

[1.x.97] 



And we can then perform the integration by using a loop over all quadrature points.                


On each quadrature point, we first compute the value of the normal derivative. We do so using the gradient of the exact solution and the normal vector to the face at the present quadrature point obtained from the  [2.x.90]  object. This is then used to compute the additional contribution of this face to the right hand side: 

[1.x.98] 



Now that we have the contributions of the present cell, we can transfer it to the global matrix and right hand side vector, as in the examples before: 

[1.x.99] 



Likewise, elimination and treatment of boundary values has been shown previously.      


We note, however that now the boundary indicator for which we interpolate boundary values (denoted by the second parameter to  [2.x.91] ) does not represent the whole boundary any more. Rather, it is that portion of the boundary which we have not assigned another indicator (see below). The degrees of freedom at the boundary that do not belong to Gamma1 are therefore excluded from the interpolation of boundary values, just as we want. 

[1.x.100] 




[1.x.101]  [1.x.102] 




Solving the system of equations is done in the same way as before: 

[1.x.103] 




[1.x.104]  [1.x.105] 




Now for the function doing grid refinement. Depending on the refinement mode passed to the constructor, we do global or adaptive refinement.    


Global refinement is simple, so there is not much to comment on.  In case of adaptive refinement, we use the same functions and classes as in the previous example program. Note that one could treat Neumann boundaries differently than Dirichlet boundaries, and one should in fact do so here since we have Neumann boundary conditions on part of the boundaries, but since we don't have a function here that describes the Neumann values (we only construct these values from the exact solution when assembling the matrix), we omit this detail even though doing this in a strictly correct way would not be hard to add.    


At the end of the switch, we have a default case that looks slightly strange: an  [2.x.92]  condition. Since the  [2.x.93]  macro raises an error whenever the condition is false, this means that whenever we hit this statement the program will be aborted. This in intentional: Right now we have only implemented two refinement strategies (global and adaptive), but someone might want to add a third strategy (for example adaptivity with a different refinement criterion) and add a third member to the enumeration that determines the refinement mode. If it weren't for the default case of the switch statement, this function would simply run to its end without doing anything. This is most likely not what was intended. One of the defensive programming techniques that you will find all over the deal.II library is therefore to always have default cases that abort, to make sure that values not considered when listing the cases in the switch statement are eventually caught, and forcing programmers to add code to handle them. We will use this same technique in other places further down as well. 

[1.x.106] 




[1.x.107]  [1.x.108] 




Finally we want to process the solution after it has been computed. For this, we integrate the error in various (semi-)norms, and we generate tables that will later be used to display the convergence against the continuous solution in a nice format. 

[1.x.109] 



Our first task is to compute error norms. In order to integrate the difference between computed numerical solution and the continuous solution (described by the Solution class defined at the top of this file), we first need a vector that will hold the norm of the error on each cell. Since accuracy with 16 digits is not so important for these quantities, we save some memory by using  [2.x.94]  instead of  [2.x.95]  values.      


The next step is to use a function from the library which computes the error in the L2 norm on each cell.  We have to pass it the DoF handler object, the vector holding the nodal values of the numerical solution, the continuous solution as a function object, the vector into which it shall place the norm of the error on each cell, a quadrature rule by which this norm shall be computed, and the type of norm to be used. Here, we use a Gauss formula with three points in each space direction, and compute the L2 norm.      


Finally, we want to get the global L2 norm. This can of course be obtained by summing the squares of the norms on each cell, and taking the square root of that value. This is equivalent to taking the l2 (lower case  [2.x.96] ) norm of the vector of norms on each cell: 

[1.x.110] 



By same procedure we get the H1 semi-norm. We re-use the  [2.x.97]  vector since it is no longer used after computing the  [2.x.98]  variable above. The global  [2.x.99]  semi-norm error is then computed by taking the sum of squares of the errors on each individual cell, and then the square root of it -- an operation that is conveniently performed by  [2.x.100]  

[1.x.111] 



Finally, we compute the maximum norm. Of course, we can't actually compute the true maximum of the error over *all* points in the domain, but only the maximum over a finite set of evaluation points that, for convenience, we will still call "quadrature points" and represent by an object of type Quadrature even though we do not actually perform any integration.      


There is then the question of what points precisely we want to evaluate at. It turns out that the result we get depends quite sensitively on the "quadrature" points being used. There is also the issue of superconvergence: Finite element solutions are, on some meshes and for polynomial degrees  [2.x.101] , particularly accurate at the node points as well as at Gauss-Lobatto points, much more accurate than at randomly chosen points. (See  [2.x.102]  and the discussion and references in Section 1.2 for more information on this.) In other words, if we are interested in finding the largest difference  [2.x.103] , then we ought to look at points  [2.x.104]  that are specifically not of this "special" kind of points and we should specifically not use `QGauss(fe->degree+1)` to define where we evaluate. Rather, we use a special quadrature rule that is obtained by iterating the trapezoidal rule by the degree of the finite element times two plus one in each space direction. Note that the constructor of the QIterated class takes a one-dimensional quadrature rule and a number that tells it how often it shall repeat this rule in each space direction.      


Using this special quadrature rule, we can then try to find the maximal error on each cell. Finally, we compute the global L infinity error from the L infinity errors on each cell with a call to  [2.x.105]  

[1.x.112] 



After all these errors have been computed, we finally write some output. In addition, we add the important data to the TableHandler by specifying the key of the column and the value.  Note that it is not necessary to define column keys beforehand -- it is sufficient to just add values, and columns will be introduced into the table in the order values are added the first time. 

[1.x.113] 




[1.x.114]  [1.x.115] 




As in previous example programs, the  [2.x.106]  function controls the flow of execution. The basic layout is as in previous examples: an outer loop over successively refined grids, and in this loop first problem setup, assembling the linear system, solution, and post-processing.    


The first task in the main loop is creation and refinement of grids. This is as in previous examples, with the only difference that we want to have part of the boundary marked as Neumann type, rather than Dirichlet.    


For this, we will use the following convention: Faces belonging to Gamma1 will have the boundary indicator  [2.x.107]  (which is the default, so we don't have to set it explicitly), and faces belonging to Gamma2 will use  [2.x.108]  as boundary indicator.  To set these values, we loop over all cells, then over all faces of a given cell, check whether it is part of the boundary that we want to denote by Gamma2, and if so set its boundary indicator to  [2.x.109] . For the present program, we consider the left and bottom boundaries as Gamma2. We determine whether a face is part of that boundary by asking whether the x or y coordinates (i.e. vector components 0 and 1) of the midpoint of a face equals -1, up to some small wiggle room that we have to give since it is instable to compare floating point numbers that are subject to round off in intermediate computations.    


It is worth noting that we have to loop over all cells here, not only the active ones. The reason is that upon refinement, newly created faces inherit the boundary indicator of their parent face. If we now only set the boundary indicator for active faces, coarsen some cells and refine them later on, they will again have the boundary indicator of the parent cell which we have not modified, instead of the one we intended. Consequently, we have to change the boundary indicators of faces of all cells on Gamma2, whether they are active or not. Alternatively, we could of course have done this job on the coarsest mesh (i.e. before the first refinement step) and refined the mesh only after that. 

[1.x.116] 



The next steps are already known from previous examples. This is mostly the basic set-up of every finite element program: 

[1.x.117] 



The last step in this chain of function calls is usually the evaluation of the computed solution for the quantities one is interested in. This is done in the following function. Since the function generates output that indicates the number of the present refinement step, we pass this number as an argument. 

[1.x.118] 




[1.x.119]  [1.x.120] 




After the last iteration we output the solution on the finest grid. This is done using the following sequence of statements which we have already discussed in previous examples. The first step is to generate a suitable filename (called  [2.x.110]  here, since we want to output data in VTK format; we add the prefix to distinguish the filename from that used for other output files further down below). Here, we augment the name by the mesh refinement algorithm, and as above we make sure that we abort the program if another refinement method is added and not handled by the following switch statement: 

[1.x.121] 



We augment the filename by a postfix denoting the finite element which we have used in the computation. To this end, the finite element base class stores the maximal polynomial degree of shape functions in each coordinate variable as a variable  [2.x.111] , and we use for the switch statement (note that the polynomial degree of bilinear shape functions is really 2, since they contain the term  [2.x.112] ; however, the polynomial degree in each coordinate variable is still only 1). We again use the same defensive programming technique to safeguard against the case that the polynomial degree has an unexpected value, using the  [2.x.113]  idiom in the default branch of the switch statement: 

[1.x.122] 



Once we have the base name for the output file, we add an extension appropriate for VTK output, open a file, and add the solution vector to the object that will do the actual output: 

[1.x.123] 



Now building the intermediate format as before is the next step. We introduce one more feature of deal.II here. The background is the following: in some of the runs of this function, we have used biquadratic finite elements. However, since almost all output formats only support bilinear data, the data is written only bilinear, and information is consequently lost.  Of course, we can't change the format in which graphic programs accept their inputs, but we can write the data differently such that we more closely resemble the information available in the quadratic approximation. We can, for example, write each cell as four sub-cells with bilinear data each, such that we have nine data points for each cell in the triangulation. The graphic programs will, of course, display this data still only bilinear, but at least we have given some more of the information we have.      


In order to allow writing more than one sub-cell per actual cell, the  [2.x.114]  function accepts a parameter (the default is  [2.x.115] , which is why you haven't seen this parameter in previous examples). This parameter denotes into how many sub-cells per space direction each cell shall be subdivided for output. For example, if you give  [2.x.116] , this leads to 4 cells in 2D and 8 cells in 3D. For quadratic elements, two sub-cells per space direction is obviously the right choice, so this is what we choose. In general, for elements of polynomial order  [2.x.117]  subdivisions, and the order of the elements is determined in the same way as above.      


With the intermediate format so generated, we can then actually write the graphical output: 

[1.x.124] 




[1.x.125]  [1.x.126] 




After graphical output, we would also like to generate tables from the error computations we have done in  [2.x.118] . There, we have filled a table object with the number of cells for each refinement step as well as the errors in different norms. 




For a nicer textual output of this data, one may want to set the precision with which the values will be written upon output. We use 3 digits for this, which is usually sufficient for error norms. By default, data is written in fixed point notation. However, for columns one would like to see in scientific notation another function call sets the  [2.x.119] , leading to floating point representation of numbers. 

[1.x.127] 



For the output of a table into a LaTeX file, the default captions of the columns are the keys given as argument to the  [2.x.120]  functions. To have TeX captions that differ from the default ones you can specify them by the following function calls. Note, that `\\' is reduced to `\' by the compiler such that the real TeX caption is, e.g., ` [2.x.121] -error'. 

[1.x.128] 



Finally, the default LaTeX format for each column of the table is `c' (centered). To specify a different (e.g. `right') one, the following function may be used: 

[1.x.129] 



After this, we can finally write the table to the standard output stream  [2.x.122]  (after one extra empty line, to make things look prettier). Note, that the output in text format is quite simple and that captions may not be printed directly above the specific columns. 

[1.x.130] 



The table can also be written into a LaTeX file.  The (nicely) formatted table can be viewed at after calling `latex filename' and e.g. `xdvi filename', where filename is the name of the file to which we will write output now. We construct the file name in the same way as before, but with a different prefix "error": 

[1.x.131] 




[1.x.132]  [1.x.133] 




In case of global refinement, it might be of interest to also output the convergence rates. This may be done by the functionality the ConvergenceTable offers over the regular TableHandler. However, we do it only for global refinement, since for adaptive refinement the determination of something like an order of convergence is somewhat more involved. While we are at it, we also show a few other things that can be done with tables. 

[1.x.134] 



The first thing is that one can group individual columns together to form so-called super columns. Essentially, the columns remain the same, but the ones that were grouped together will get a caption running across all columns in a group. For example, let's merge the "cycle" and "cells" columns into a super column named "n cells": 

[1.x.135] 



Next, it isn't necessary to always output all columns, or in the order in which they were originally added during the run. Selecting and re-ordering the columns works as follows (note that this includes super columns): 

[1.x.136] 



For everything that happened to the ConvergenceTable until this point, it would have been sufficient to use a simple TableHandler. Indeed, the ConvergenceTable is derived from the TableHandler but it offers the additional functionality of automatically evaluating convergence rates. For example, here is how we can let the table compute reduction and convergence rates (convergence rates are the binary logarithm of the reduction rate): 

[1.x.137] 



Each of these function calls produces an additional column that is merged with the original column (in our example the `L2' and the `H1' column) to a supercolumn. 




Finally, we want to write this convergence chart again, first to the screen and then, in LaTeX format, to disk. The filename is again constructed as above. 

[1.x.138] 



The final step before going to  [2.x.123]  is then to close the namespace  [2.x.124]  into which we have put everything we needed for this program: 

[1.x.139] 




[1.x.140]  [1.x.141] 




The main function is mostly as before. The only difference is that we solve three times, once for Q1 and adaptive refinement, once for Q1 elements and global refinement, and once for Q2 elements and global refinement. 




Since we instantiate several template classes below for two space dimensions, we make this more generic by declaring a constant at the beginning of the function denoting the number of space dimensions. If you want to run the program in 1d or 2d, you will then only have to change this one instance, rather than all uses below: 

[1.x.142] 



Now for the three calls to the main class. Each call is blocked into curly braces in order to destroy the respective objects (i.e. the finite element and the HelmholtzProblem object) at the end of the block and before we go to the next run. This avoids conflicts with variable names, and also makes sure that memory is released immediately after one of the three runs has finished, and not only at the end of the  [2.x.125]  block. 

[1.x.143] 

[1.x.144][1.x.145] 




The program generates two kinds of output. The first are the output files  [2.x.126] ,  [2.x.127] , and  [2.x.128] . We show the latter in a 3d view here: 


 [2.x.129]  





Secondly, the program writes tables not only to disk, but also to the screen while running. The output looks like the following (recall that columns labeled as " [2.x.130] " actually show the  [2.x.131]  [1.x.146]norm of the error, not the full  [2.x.132]  norm): 




[1.x.147] 




One can see the error reduction upon grid refinement, and for the cases where global refinement was performed, also the convergence rates can be seen. The linear and quadratic convergence rates of Q1 and Q2 elements in the  [2.x.133]  semi-norm can clearly be seen, as are the quadratic and cubic rates in the  [2.x.134]  norm. 





Finally, the program also generated LaTeX versions of the tables (not shown here) that is written into a file in a way so that it could be copy-pasted into a LaTeX document. 


[1.x.148][1.x.149] 


What we showed above is how to determine the size of the error  [2.x.135]  in a number of different norms. We did this primarily because we were interested in testing that our solutions *converge*. But from an engineering perspective, the question is often more practical: How fine do I have to make my mesh so that the error is "small enough"? In other words, if in the table above the  [2.x.136]  semi-norm has been reduced to `4.121e-03`, is this good enough for me to sign the blueprint and declare that our numerical simulation showed that the bridge is strong enough? 

In practice, we are rarely in this situation because I can not typically compare the numerical solution  [2.x.137]  against the exact solution  [2.x.138]  in situations that matter -- if I knew  [2.x.139] , I would not have to compute  [2.x.140] . But even if I could, the question to ask in general is then: `4.121e-03` *what*? The solution will have physical units, say kg-times-meter-squared, and I'm integrating a function with units square of the above over the domain, and then take the square root. So if the domain is two-dimensional, the units of  [2.x.141]  are kg-times-meter-cubed. The question is then: Is  [2.x.142]  kg-times-meter-cubed small? That depends on what you're trying to simulate: If you're an astronomer used to masses measured in solar masses and distances in light years, then yes, this is a fantastically small number. But if you're doing atomic physics, then no: That's not small, and your error is most certainly not sufficiently small; you need a finer mesh. 

In other words, when we look at these sorts of numbers, we generally need to compare against a "scale". One way to do that is to not look at the *absolute* error  [2.x.143]  in whatever norm, but at the *relative* error  [2.x.144] . If this ratio is  [2.x.145] , then you know that *on average*, the difference between  [2.x.146]  and  [2.x.147]  is 0.001 per cent -- probably small enough for engineering purposes. 

How do we compute  [2.x.148] ? We just need to do an integration loop over all cells, quadrature points on these cells, and then sum things up and take the square root at the end. But there is a simpler way often used: You can call 

[1.x.150] 

which computes  [2.x.149] . Alternatively, if you're particularly lazy and don't feel like creating the `zero_vector`, you could use that if the mesh is not too coarse, then  [2.x.150] , and we can compute  [2.x.151]  by calling 

[1.x.151] 

In both cases, one then only has to combine the vector of cellwise norms into one global norm as we already do in the program, by calling 

[1.x.152] 






[1.x.153][1.x.154] 


[1.x.155][1.x.156] 


Go ahead and run the program with higher order elements ( [2.x.152] ,  [2.x.153] , ...). You will notice that assertions in several parts of the code will trigger (for example in the generation of the filename for the data output). You might have to address these, but it should not be very hard to get the program to work! 

[1.x.157][1.x.158] 


Is Q1 or Q2 better? What about adaptive versus global refinement? A (somewhat unfair but typical) metric to compare them, is to look at the error as a function of the number of unknowns. 

To see this, create a plot in log-log style with the number of unknowns on the  [2.x.154]  axis and the  [2.x.155]  error on the  [2.x.156]  axis. You can add reference lines for  [2.x.157]  and  [2.x.158]  and check that global and adaptive refinement follow those. If one makes the (not completely unreasonable) assumption that with a good linear solver, the computational effort is proportional to the number of unknowns  [2.x.159] , then it is clear that an error reduction of  [2.x.160]  is substantially better than a reduction of the form  [2.x.161] : That is, that adaptive refinement gives us the desired error level with less computational work than if we used global refinement. This is not a particularly surprising conclusion, but it's worth checking these sorts of assumptions in practice. 

Of course, a fairer comparison would be to plot runtime (switch to release mode first!) instead of number of unknowns on the  [2.x.162]  axis. If you plotted run time against the number of unknowns by timing each refinement step (e.g., using the Timer class), you will notice that the linear solver is not perfect -- its run time grows faster than proportional to the linear system size -- and picking a better linear solver might be appropriate for this kind of comparison. [1.x.159] [1.x.160]  [2.x.163]  

 [2.x.164] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17] 

[1.x.18] [1.x.19][1.x.20] 




In real life, most partial differential equations are really systems of equations. Accordingly, the solutions are usually vector-valued. The deal.II library supports such problems (see the extensive documentation in the  [2.x.3]  module), and we will show that that is mostly rather simple. The only more complicated problems are in assembling matrix and right hand side, but these are easily understood as well. 

 [2.x.4]  

In this tutorial program we will want to solve the [1.x.21]. They are an extension to Laplace's equation with a vector-valued solution that describes the displacement in each space direction of a rigid body which is subject to a force. Of course, the force is also vector-valued, meaning that in each point it has a direction and an absolute value. 

One can write the elasticity equations in a number of ways. The one that shows the symmetry with the Laplace equation in the most obvious way is to write it as [1.x.22] where  [2.x.5]  is the vector-valued displacement at each point,  [2.x.6]  the force, and  [2.x.7]  is a rank-4 tensor (i.e., it has four indices) that encodes the stress-strain relationship -- in essence, it represents the [1.x.23] in Hookes law that relates the displacement to the forces.  [2.x.8]  will, in many cases, depend on  [2.x.9]  if the body whose deformation we want to simulate is composed of different materials. 

While the form of the equations above is correct, it is not the way they are usually derived. In truth, the gradient of the displacement  [2.x.10]  (a matrix) has no physical meaning whereas its symmetrized version, [1.x.24] does and is typically called the "strain". (Here and in the following,  [2.x.11] . We will also use the [1.x.25] that whenever the same index appears twice in an equation, summation over this index is implied; we will, however, not distinguish between upper and lower indices.) With this definition of the strain, the elasticity equations then read as [1.x.26] which you can think of as the more natural generalization of the Laplace equation to vector-valued problems. (The form shown first is equivalent to this form because the tensor  [2.x.12]  has certain symmetries, namely that  [2.x.13] , and consequently  [2.x.14] .) 

One can of course alternatively write these equations in component form: [1.x.27] 

In many cases, one knows that the material under consideration is isotropic, in which case by introduction of the two coefficients  [2.x.15]  and  [2.x.16]  the coefficient tensor reduces to [1.x.28] 

The elastic equations can then be rewritten in much simpler a form: [1.x.29] and the respective bilinear form is then [1.x.30] or also writing the first term a sum over components: [1.x.31] 

 [2.x.17]  As written, the equations above are generally considered to be the right description for the displacement of three-dimensional objects if the displacement is small and we can assume that [1.x.32] is valid. In that case, the indices  [2.x.18]  above all run over the set  [2.x.19]  (or, in the C++ source, over  [2.x.20] ). However, as is, the program runs in 2d, and while the equations above also make mathematical sense in that case, they would only describe a truly two-dimensional solid. In particular, they are not the appropriate description of an  [2.x.21]  cross-section of a body infinite in the  [2.x.22]  direction; this is in contrast to many other two-dimensional equations that can be obtained by assuming that the body has infinite extent in  [2.x.23] -direction and that the solution function does not depend on the  [2.x.24]  coordinate. On the other hand, there are equations for two-dimensional models of elasticity; see for example the Wikipedia article on [1.x.33], [1.x.34] and [1.x.35]. 

But let's get back to the original problem. How do we assemble the matrix for such an equation? A very long answer with a number of different alternatives is given in the documentation of the  [2.x.25]  module. Historically, the solution shown below was the only one available in the early years of the library. It turns out to also be the fastest. On the other hand, if a few per cent of compute time do not matter, there are simpler and probably more intuitive ways to assemble the linear system than the one discussed below but that weren't available until several years after this tutorial program was first written; if you are interested in them, take a look at the  [2.x.26]  module. 

Let us go back to the question of how to assemble the linear system. The first thing we need is some knowledge about how the shape functions work in the case of vector-valued finite elements. Basically, this comes down to the following: let  [2.x.27]  be the number of shape functions for the scalar finite element of which we build the vector element (for example, we will use bilinear functions for each component of the vector-valued finite element, so the scalar finite element is the  [2.x.28]  element which we have used in previous examples already, and  [2.x.29]  in two space dimensions). Further, let  [2.x.30]  be the number of shape functions for the vector element; in two space dimensions, we need  [2.x.31]  shape functions for each component of the vector, so  [2.x.32] . Then, the  [2.x.33] th shape function of the vector element has the form [1.x.36] where  [2.x.34]  is the  [2.x.35] th unit vector,  [2.x.36]  is the function that tells us which component of  [2.x.37]  is the one that is nonzero (for each vector shape function, only one component is nonzero, and all others are zero).  [2.x.38]  describes the space dependence of the shape function, which is taken to be the  [2.x.39] -th shape function of the scalar element. Of course, while  [2.x.40]  is in the range  [2.x.41] , the functions  [2.x.42]  and  [2.x.43]  have the ranges  [2.x.44]  (in 2D) and  [2.x.45] , respectively. 

For example (though this sequence of shape functions is not guaranteed, and you should not rely on it), the following layout could be used by the library: [1.x.37] 

where here [1.x.38] [1.x.39] 

In all but very rare cases, you will not need to know which shape function  [2.x.46]  of the scalar element belongs to a shape function  [2.x.47]  of the vector element. Let us therefore define [1.x.40] by which we can write the vector shape function as [1.x.41] You can now safely forget about the function  [2.x.48] , at least for the rest of this example program. 

Now using this vector shape functions, we can write the discrete finite element solution as [1.x.42] with scalar coefficients  [2.x.49] . If we define an analog function  [2.x.50]  as test function, we can write the discrete problem as follows: Find coefficients  [2.x.51]  such that [1.x.43] 

If we insert the definition of the bilinear form and the representation of  [2.x.52]  and  [2.x.53]  into this formula: [1.x.44] 

We note that here and in the following, the indices  [2.x.54]  run over spatial directions, i.e.  [2.x.55] , and that indices  [2.x.56]  run over degrees of freedom. 

The local stiffness matrix on cell  [2.x.57]  therefore has the following entries: [1.x.45] where  [2.x.58]  now are local degrees of freedom and therefore  [2.x.59] . In these formulas, we always take some component of the vector shape functions  [2.x.60] , which are of course given as follows (see their definition): [1.x.46] with the Kronecker symbol  [2.x.61] . Due to this, we can delete some of the sums over  [2.x.62]  and  [2.x.63] : [1.x.47] 



Likewise, the contribution of cell  [2.x.64]  to the right hand side vector is [1.x.48] 



This is the form in which we will implement the local stiffness matrix and right hand side vectors. 

As a final note: in the  [2.x.65]  example program, we will revisit the elastic problem laid out here, and will show how to solve it in %parallel on a cluster of computers. The resulting program will thus be able to solve this problem to significantly higher accuracy, and more efficiently if this is required. In addition, in  [2.x.66] ,  [2.x.67]  " [2.x.68] ", as well as a few other of the later tutorial programs, we will revisit some vector-valued problems and show a few techniques that may make it simpler to actually go through all the stuff shown above, with  [2.x.69]  etc. [1.x.49] [1.x.50] 


[1.x.51]  [1.x.52] 




As usual, the first few include files are already known, so we will not comment on them further. 

[1.x.53] 



In this example, we need vector-valued finite elements. The support for these can be found in the following include file: 

[1.x.54] 



We will compose the vector-valued finite elements from regular Q1 elements which can be found here, as usual: 

[1.x.55] 



This again is C++: 

[1.x.56] 



The last step is as in previous programs. In particular, just like in  [2.x.70] , we pack everything that's specific to this program into a namespace of its own. 

[1.x.57] 




[1.x.58]  [1.x.59] 




The main class is, except for its name, almost unchanged with respect to the  [2.x.71]  example.    


The only change is the use of a different class for the  [2.x.72]  variable: Instead of a concrete finite element class such as FE_Q, we now use a more generic one, FESystem. In fact, FESystem is not really a finite element itself in that it does not implement shape functions of its own. Rather, it is a class that can be used to stack several other elements together to form one vector-valued finite element. In our case, we will compose the vector-valued element of  [2.x.73]  objects, as shown below in the constructor of this class. 

[1.x.60] 




[1.x.61]  [1.x.62] 




Before going over to the implementation of the main class, we declare and define the function which describes the right hand side. This time, the right hand side is vector-valued, as is the solution, so we will describe the changes required for this in some more detail.    


To prevent cases where the return vector has not previously been set to the right size we test for this case and otherwise throw an exception at the beginning of the function. Note that enforcing that output arguments already have the correct size is a convention in deal.II, and enforced almost everywhere. The reason is that we would otherwise have to check at the beginning of the function and possibly change the size of the output vector. This is expensive, and would almost always be unnecessary (the first call to the function would set the vector to the right size, and subsequent calls would only have to do redundant checks). In addition, checking and possibly resizing the vector is an operation that can not be removed if we can't rely on the assumption that the vector already has the correct size; this is in contract to the Assert call that is completely removed if the program is compiled in optimized mode.    


Likewise, if by some accident someone tried to compile and run the program in only one space dimension (in which the elastic equations do not make much sense since they reduce to the ordinary Laplace equation), we terminate the program in the second assertion. The program will work just fine in 3d, however. 

[1.x.63] 



The rest of the function implements computing force values. We will use a constant (unit) force in x-direction located in two little circles (or spheres, in 3d) around points (0.5,0) and (-0.5,0), and y-force in an area around the origin; in 3d, the z-component of these centers is zero as well.      


For this, let us first define two objects that denote the centers of these areas. Note that upon construction of the Point objects, all components are set to zero. 

[1.x.64] 



If  [2.x.74]  is in a circle (sphere) of radius 0.2 around one of these points, then set the force in x-direction to one, otherwise to zero: 

[1.x.65] 



Likewise, if  [2.x.75]  is in the vicinity of the origin, then set the y-force to one, otherwise to zero: 

[1.x.66] 




[1.x.67]  [1.x.68] 





[1.x.69]  [1.x.70] 




Following is the constructor of the main class. As said before, we would like to construct a vector-valued finite element that is composed of several scalar finite elements (i.e., we want to build the vector-valued element so that each of its vector components consists of the shape functions of a scalar element). Of course, the number of scalar finite elements we would like to stack together equals the number of components the solution function has, which is  [2.x.76]  since we consider displacement in each space direction. The FESystem class can handle this: we pass it the finite element of which we would like to compose the system of, and how often it shall be repeated: 







[1.x.71] 



In fact, the FESystem class has several more constructors which can perform more complex operations than just stacking together several scalar finite elements of the same type into one; we will get to know these possibilities in later examples. 










[1.x.72]  [1.x.73] 




Setting up the system of equations is identical to the function used in the  [2.x.77]  example. The DoFHandler class and all other classes used here are fully aware that the finite element we want to use is vector-valued, and take care of the vector-valuedness of the finite element themselves. (In fact, they do not, but this does not need to bother you: since they only need to know how many degrees of freedom there are per vertex, line and cell, and they do not ask what they represent, i.e. whether the finite element under consideration is vector-valued or whether it is, for example, a scalar Hermite element with several degrees of freedom on each vertex). 

[1.x.74] 




[1.x.75]  [1.x.76] 




The big changes in this program are in the creation of matrix and right hand side, since they are problem-dependent. We will go through that process  [2.x.78] by-step, since it is a bit more complicated than in previous examples.    


The first parts of this function are the same as before, however: setting up a suitable quadrature formula, initializing an FEValues object for the (vector-valued) finite element we use as well as the quadrature object, and declaring a number of auxiliary arrays. In addition, we declare the ever same two abbreviations:  [2.x.79]  and  [2.x.80] . The number of degrees of freedom per cell we now obviously ask from the composed finite element rather than from the underlying scalar Q1 element. Here, it is  [2.x.81]  times the number of degrees of freedom per cell of the Q1 element, though this is not explicit knowledge we need to care about: 

[1.x.77] 



As was shown in previous examples as well, we need a place where to store the values of the coefficients at all the quadrature points on a cell. In the present situation, we have two coefficients, lambda and mu. 

[1.x.78] 



Well, we could as well have omitted the above two arrays since we will use constant coefficients for both lambda and mu, which can be declared like this. They both represent functions always returning the constant value 1.0. Although we could omit the respective factors in the assemblage of the matrix, we use them here for purpose of demonstration. 

[1.x.79] 



Like the two constant functions above, we will call the function right_hand_side just once per cell to make things simpler. 

[1.x.80] 



Now we can begin with the loop over all cells: 

[1.x.81] 



Next we get the values of the coefficients at the quadrature points. Likewise for the right hand side: 

[1.x.82] 



Then assemble the entries of the local stiffness matrix and right hand side vector. This follows almost one-to-one the pattern described in the introduction of this example.  One of the few comments in place is that we can compute the number  [2.x.82] , i.e. the index of the only nonzero vector component of shape function  [2.x.83]  using the  [2.x.84]  function call below.          


(By accessing the  [2.x.85]  variable of the return value of the  [2.x.86]  function, you might already have guessed that there is more in it. In fact, the function returns a  [2.x.87]  int, unsigned int@></code>, of which the first element is  [2.x.88]  and the second is the value  [2.x.89]  also noted in the introduction, i.e.  the index of this shape function within all the shape functions that are nonzero in this component, i.e.  [2.x.90]  in the diction of the introduction. This is not a number that we are usually interested in, however.)          


With this knowledge, we can assemble the local matrix contributions: 

[1.x.83] 



The first term is  [2.x.91] . Note that  [2.x.92]  returns the gradient of the only nonzero component of the i-th shape function at quadrature point q_point. The component  [2.x.93]  of the gradient, which is the derivative of this only nonzero vector component of the i-th shape function with respect to the comp(i)th coordinate is accessed by the appended brackets. 

[1.x.84] 



The second term is  [2.x.94] . We need not access a specific component of the gradient, since we only have to compute the scalar product of the two gradients, of which an overloaded version of <tt>operator*</tt> takes care, as in previous examples.                          


Note that by using the <tt>?:</tt> operator, we only do this if <tt>component_i</tt> equals <tt>component_j</tt>, otherwise a zero is added (which will be optimized away by the compiler). 

[1.x.85] 



Assembling the right hand side is also just as discussed in the introduction: 

[1.x.86] 



The transfer from local degrees of freedom into the global matrix and right hand side vector does not depend on the equation under consideration, and is thus the same as in all previous examples. 

[1.x.87] 




[1.x.88]  [1.x.89] 




The solver does not care about where the system of equations comes, as long as it stays positive definite and symmetric (which are the requirements for the use of the CG solver), which the system indeed is. Therefore, we need not change anything. 

[1.x.90] 




[1.x.91]  [1.x.92] 




The function that does the refinement of the grid is the same as in the  [2.x.95]  example. The quadrature formula is adapted to the linear elements again. Note that the error estimator by default adds up the estimated obtained from all components of the finite element solution, i.e., it uses the displacement in all directions with the same weight. If we would like the grid to be adapted to the x-displacement only, we could pass the function an additional parameter which tells it to do so and do not consider the displacements in all other directions for the error indicators. However, for the current problem, it seems appropriate to consider all displacement components with equal weight. 

[1.x.93] 




[1.x.94]  [1.x.95] 




The output happens mostly as has been shown in previous examples already. The only difference is that the solution function is vector valued. The DataOut class takes care of this automatically, but we have to give each component of the solution vector a different name.    


To do this, the  [2.x.96]  function wants a vector of strings. Since the number of components is the same as the number of dimensions we are working in, we use the  [2.x.97]  statement below.    


We note that some graphics programs have restriction on what characters are allowed in the names of variables. deal.II therefore supports only the minimal subset of these characters that is supported by all programs. Basically, these are letters, numbers, underscores, and some other characters, but in particular no whitespace and minus/hyphen. The library will throw an exception otherwise, at least if in debug mode.    


After listing the 1d, 2d, and 3d case, it is good style to let the program die if we run upon a case which we did not consider. Remember that the Assert macro generates an exception if the condition in the first parameter is not satisfied. Of course, the condition  [2.x.98]  can never be satisfied, so the program will always abort whenever it gets to the default statement: 

[1.x.96] 



After setting up the names for the different components of the solution vector, we can add the solution vector to the list of data vectors scheduled for output. Note that the following function takes a vector of strings as second argument, whereas the one which we have used in all previous examples accepted a string there. (In fact, the function we had used before would convert the single string into a vector with only one element and forwards that to the other function.) 

[1.x.97] 




[1.x.98]  [1.x.99] 




The  [2.x.99]  function does the same things as in  [2.x.100] , for example. This time, we use the square [-1,1]^d as domain, and we refine it globally four times before starting the first iteration.    


The reason for refining is a bit accidental: we use the QGauss quadrature formula with two points in each direction for integration of the right hand side; that means that there are four quadrature points on each cell (in 2D). If we only refine the initial grid once globally, then there will be only four quadrature points in each direction on the domain. However, the right hand side function was chosen to be rather localized and in that case, by pure chance, it happens that all quadrature points lie at points where the right hand side function is zero (in mathematical terms, the quadrature points happen to be at points outside the [1.x.100] of the right hand side function). The right hand side vector computed with quadrature will then contain only zeroes (even though it would of course be nonzero if we had computed the right hand side vector exactly using the integral) and the solution of the system of equations is the zero vector, i.e., a finite element function that is zero everywhere. In a sense, we should not be surprised that this is happening since we have chosen an initial grid that is totally unsuitable for the problem at hand.    


The unfortunate thing is that if the discrete solution is constant, then the error indicators computed by the KellyErrorEstimator class are zero for each cell as well, and the call to  [2.x.101]  will not flag any cells for refinement (why should it if the indicated error is zero for each cell?). The grid in the next iteration will therefore consist of four cells only as well, and the same problem occurs again.    


The conclusion needs to be: while of course we will not choose the initial grid to be well-suited for the accurate solution of the problem, we must at least choose it such that it has the chance to capture the important features of the solution. In this case, it needs to be able to see the right hand side. Thus, we refine globally four times. (Any larger number of global refinement steps would of course also work.) 

[1.x.101] 




[1.x.102]  [1.x.103] 




After closing the  [2.x.102]  namespace in the last line above, the following is the main function of the program and is again exactly like in  [2.x.103]  (apart from the changed class names, of course). 

[1.x.104] 

[1.x.105][1.x.106] 




There is not much to be said about the results of this program, other than that they look nice. All images were made using VisIt from the output files that the program wrote to disk. The first two pictures show the  [2.x.104] - and  [2.x.105] -displacements as scalar components: 

 [2.x.106]  


You can clearly see the sources of  [2.x.107] -displacement around  [2.x.108]  and  [2.x.109] , and of  [2.x.110] -displacement at the origin. 

What one frequently would like to do is to show the displacement as a vector field, i.e., vectors that for each point illustrate the direction and magnitude of displacement. Unfortunately, that's a bit more involved. To understand why this is so, remember that we have just defined our finite element as a collection of two  components (in  [2.x.111]  dimensions). Nowhere have we said that this is not just a pressure and a concentration (two scalar quantities) but that the two components actually are the parts of a vector-valued quantity, namely the displacement. Absent this knowledge, the DataOut class assumes that all individual variables we print are separate scalars, and VisIt and Paraview then faithfully assume that this is indeed what it is. In other words, once we have written the data as scalars, there is nothing in these programs that allows us to paste these two scalar fields back together as a vector field. Where we would have to attack this problem is at the root, namely in  [2.x.112] . We won't do so here but instead refer the reader to the  [2.x.113]  program where we show how to do this for a more general situation. That said, we couldn't help generating the data anyway that would show how this would look if implemented as discussed in  [2.x.114] . The vector field then looks like this (VisIt and Paraview randomly select a few hundred vertices from which to draw the vectors; drawing them from each individual vertex would make the picture unreadable): 

 [2.x.115]  


We note that one may have intuitively expected the solution to be symmetric about the  [2.x.116] - and  [2.x.117] -axes since the  [2.x.118] - and  [2.x.119] -forces are symmetric with respect to these axes. However, the force considered as a vector is not symmetric and consequently neither is the solution. [1.x.107] [1.x.108]  [2.x.120]  

 [2.x.121] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18] 

[1.x.19] [1.x.20][1.x.21] 




In this example, our aims are the following:  [2.x.3]     [2.x.4] solve the advection equation  [2.x.5] ;    [2.x.6] show how we can use multiple threads to get results quicker if we have a     multi-processor machine;    [2.x.7] develop a simple refinement criterion.  [2.x.8]  While the second aim is difficult to describe in general terms without reference to the code, we will discuss the other two aims in the following. The use of multiple threads will then be detailed at the relevant places within the program. We will, however, follow the general discussion of the WorkStream approach detailed in the  [2.x.9]  "Parallel computing with multiple processors accessing shared memory" documentation module. 


[1.x.22][1.x.23] 


In the present example program, we want to numerically approximate the solution of the advection equation [1.x.24] where  [2.x.10]  is a vector field that describes the advection direction and speed (which may be dependent on the space variables if  [2.x.11] ),  [2.x.12]  is a source function, and  [2.x.13]  is the solution. The physical process that this equation describes is that of a given flow field  [2.x.14] , with which another substance is transported, the density or concentration of which is given by  [2.x.15] . The equation does not contain diffusion of this second species within its carrier substance, but there are source terms. 

It is obvious that at the inflow, the above equation needs to be augmented by boundary conditions: [1.x.25] where  [2.x.16]  describes the inflow portion of the boundary and is formally defined by [1.x.26] and  [2.x.17]  being the outward normal to the domain at point  [2.x.18] . This definition is quite intuitive, since as  [2.x.19]  points outward, the scalar product with  [2.x.20]  can only be negative if the transport direction  [2.x.21]  points inward, i.e. at the inflow boundary. The mathematical theory states that we must not pose any boundary condition on the outflow part of the boundary. 

Unfortunately, the equation stated above cannot be solved in a stable way using the standard finite element method. The problem is that solutions to this equation possess insufficient regularity perpendicular to the transport direction: while they are smooth along the streamlines defined by the "wind field"  [2.x.22] , they may be discontinuous perpendicular to this direction. This is easy to understand: what the equation  [2.x.23]  means is in essence that the [1.x.27]. But the equation has no implications for the derivatives in the perpendicular direction, and consequently if  [2.x.24]  is discontinuous at a point on the inflow boundary, then this discontinuity will simply be transported along the streamline of the wind field that starts at this boundary point. These discontinuities lead to numerical instabilities that make a stable solution by a standard continuous finite element discretization impossible. 

A standard approach to address this difficulty is the  [2.x.25] "streamline-upwind Petrov-Galerkin" [2.x.26]  (SUPG) method, sometimes also called the streamline diffusion method. A good explanation of the method can be found in  [2.x.27]  . Formally, this method replaces the step in which we derive the the weak form of the differential equation from the strong form: Instead of multiplying the equation by a test function  [2.x.28]  and integrating over the domain, we instead multiply by  [2.x.29] , where  [2.x.30]  is a parameter that is chosen in the range of the (local) mesh width  [2.x.31] ; good results are usually obtained by setting  [2.x.32] . (Why this is called "streamline diffusion" will be explained below; for the moment, let us simply take for granted that this is how we derive a stable discrete formulation.) The value for  [2.x.33]  here is small enough that we do not introduce excessive diffusion, but large enough that the resulting problem is well-posed. 

Using the test functions as defined above, an initial weak form of the problem would ask for finding a function  [2.x.34]  so that for all test functions  [2.x.35]  we have [1.x.28] However, we would like to include inflow boundary conditions  [2.x.36]  weakly into this problem, and this can be done by requiring that in addition to the equation above we also have [1.x.29] for all test functions  [2.x.37]  that live on the boundary and that are from a suitable test space. It turns out that a suitable space of test functions happens to be  [2.x.38]  times the traces of the functions  [2.x.39]  in the test space we already use for the differential equation in the domain. Thus, we require that for all test functions  [2.x.40]  we have [1.x.30] Without attempting a justification (see again the literature on the finite element method in general, and the streamline diffusion method in particular), we can combine the equations for the differential equation and the boundary values in the following weak formulation of our stabilized problem: find a discrete function  [2.x.41]  such that for all discrete test functions  [2.x.42]  there holds [1.x.31] 


One would think that this leads to a system matrix to be inverted of the form [1.x.32] with basis functions  [2.x.43] .  However, this is a pitfall that happens to every numerical analyst at least once (including the author): we have here expanded the solution  [2.x.44] , but if we do so, we will have to solve the problem [1.x.33] where  [2.x.45]  is the vector of expansion coefficients, i.e., we have to solve the transpose problem of what we might have expected naively. 

This is a point we made in the introduction of  [2.x.46] . There, we argued that to avoid this very kind of problem, one should get in the habit of always multiplying with test functions [1.x.34] instead of from the right to obtain the correct matrix right away. In order to obtain the form of the linear system that we need, it is therefore best to rewrite the weak formulation to [1.x.35] and then to obtain [1.x.36] as system matrix. We will assemble this matrix in the program. 


[1.x.37][1.x.38] 


Looking at the bilinear form mentioned above, we see that the discrete solution has to satisfy an equation of which the left hand side in weak form has a domain term of the kind [1.x.39] or if we split this up, the form [1.x.40] If we wanted to see what strong form of the equation that would correspond to, we need to integrate the second term. This yields the following formulation, where for simplicity we'll ignore boundary terms for now: [1.x.41] Let us assume for a moment that the wind field  [2.x.47]  is divergence-free, i.e., that  [2.x.48] . Then applying the product rule to the derivative of the term in square brackets on the right and using the divergence-freeness will give us the following: [1.x.42] That means that the strong form of the equation would be of the sort [1.x.43] What is important to recognize now is that  [2.x.49]  is the  [2.x.50] derivative in direction  [2.x.51]  [2.x.52] . So, if we denote this by  [2.x.53]  (in the same way as we often write  [2.x.54]  for the derivative in normal direction at the boundary), then the strong form of the equation is [1.x.44] In other words, the unusual choice of test function is equivalent to the addition of term to the strong form that corresponds to a second order (i.e., diffusion) differential operator in the direction of the wind field  [2.x.55] , i.e., in "streamline direction". A fuller account would also have to explore the effect of the test function on boundary values and why it is necessary to also use the same test function for the right hand side, but the discussion above might make clear where the name "streamline diffusion" for the method originates from. 


[1.x.45][1.x.46] 


A "Galerkin method" is one where one obtains the weak formulation by multiplying the equation by a test function  [2.x.56]  (and then integrating over  [2.x.57] ) where the functions  [2.x.58]  are from the same space as the solution  [2.x.59]  (though possibly with different boundary values). But this is not strictly necessary: One could also imagine choosing the test functions from a different set of functions, as long as that different set has "as many dimensions" as the original set of functions so that we end up with as many independent equations as there are degrees of freedom (where all of this needs to be appropriately defined in the infinite-dimensional case). Methods that make use of this possibility (i.e., choose the set of test functions differently than the set of solutions) are called "Petrov-Galerkin" methods. In the current case, the test functions all have the form  [2.x.60]  where  [2.x.61]  is from the set of solutions. 


[1.x.47][1.x.48] 


[Upwind methods](https://en.wikipedia.org/wiki/Upwind_scheme) have a long history in the derivation of stabilized schemes for advection equations. Generally, the idea is that instead of looking at a function "here", we look at it a small distance further "upstream" or "upwind", i.e., where the information "here" originally came from. This might suggest not considering  [2.x.62] , but something like  [2.x.63] . Or, equivalently upon integration, we could evaluate  [2.x.64]  and instead consider  [2.x.65]  a bit downstream:  [2.x.66] . This would be cumbersome for a variety of reasons: First, we would have to define what  [2.x.67]  should be if  [2.x.68]  happens to be outside  [2.x.69] ; second, computing integrals numerically would be much more awkward since we no longer evaluate  [2.x.70]  and  [2.x.71]  at the same quadrature points. But since we assume that  [2.x.72]  is small, we can do a Taylor expansion: [1.x.49] This form for the test function should by now look familiar. 


[1.x.50][1.x.51] 


As the resulting matrix is no longer symmetric positive definite, we cannot use the usual Conjugate Gradient method (implemented in the SolverCG class) to solve the system. Instead, we use the GMRES (Generalized Minimum RESidual) method (implemented in SolverGMRES) that is suitable for problems of the kind we have here. 


[1.x.52][1.x.53] 


For the problem which we will solve in this tutorial program, we use the following domain and functions (in  [2.x.73]  space dimensions): [1.x.54] 

For  [2.x.74] , we extend  [2.x.75]  and  [2.x.76]  by simply duplicating the last of the components shown above one more time. 

With all of this, the following comments are in order:  [2.x.77]   [2.x.78]  The advection field  [2.x.79]  transports the solution roughly in diagonal direction from lower left to upper right, but with a wiggle structure superimposed.  [2.x.80]  The right hand side adds to the field generated by the inflow boundary conditions a blob in the lower left corner, which is then transported along.  [2.x.81]  The inflow boundary conditions impose a weighted sinusoidal structure that is transported along with the flow field. Since  [2.x.82]  on the boundary, the weighting term never gets very large.  [2.x.83]  


[1.x.55][1.x.56] 


In all previous examples with adaptive refinement, we have used an error estimator first developed by Kelly et al., which assigns to each cell  [2.x.84]  the following indicator: [1.x.57] where  [2.x.85]  denotes the jump of the normal derivatives across a face  [2.x.86]  of the cell  [2.x.87] . It can be shown that this error indicator uses a discrete analogue of the second derivatives, weighted by a power of the cell size that is adjusted to the linear elements assumed to be in use here: [1.x.58] which itself is related to the error size in the energy norm. 

The problem with this error indicator in the present case is that it assumes that the exact solution possesses second derivatives. This is already questionable for solutions to Laplace's problem in some cases, although there most problems allow solutions in  [2.x.88] . If solutions are only in  [2.x.89] , then the second derivatives would be singular in some parts (of lower dimension) of the domain and the error indicators would not reduce there under mesh refinement. Thus, the algorithm would continuously refine the cells around these parts, i.e. would refine into points or lines (in 2d). 

However, for the present case, solutions are usually not even in  [2.x.90]  (and this missing regularity is not the exceptional case as for Laplace's equation), so the error indicator described above is not really applicable. We will thus develop an indicator that is based on a discrete approximation of the gradient. Although the gradient often does not exist, this is the only criterion available to us, at least as long as we use continuous elements as in the present example. To start with, we note that given two cells  [2.x.91] ,  [2.x.92]  of which the centers are connected by the vector  [2.x.93] , we can approximate the directional derivative of a function  [2.x.94]  as follows: [1.x.59] where  [2.x.95]  and  [2.x.96]  denote  [2.x.97]  evaluated at the centers of the respective cells. We now multiply the above approximation by  [2.x.98]  and sum over all neighbors  [2.x.99]  of  [2.x.100] : [1.x.60] If the vectors  [2.x.101]  connecting  [2.x.102]  with its neighbors span the whole space (i.e. roughly:  [2.x.103]  has neighbors in all directions), then the term in parentheses in the left hand side expression forms a regular matrix, which we can invert to obtain an approximation of the gradient of  [2.x.104]  on  [2.x.105] : [1.x.61] We will denote the approximation on the right hand side by  [2.x.106] , and we will use the following quantity as refinement criterion: [1.x.62] which is inspired by the following (not rigorous) argument: [1.x.63] 

[1.x.64] [1.x.65] 

Just as in previous examples, we have to include several files of which the meaning has already been discussed: 

[1.x.66] 



The following two files provide classes and information for multithreaded programs. In the first one, the classes and functions are declared which we need to do assembly in parallel (i.e. the  [2.x.107]  namespace). The second file has a class MultithreadInfo which can be used to query the number of processors in your system, which is often useful when deciding how many threads to start in parallel. 

[1.x.67] 



The next new include file declares a base class  [2.x.108]  not unlike the  [2.x.109]  class, but with the difference that  [2.x.110]  returns a Tensor instead of a scalar. 

[1.x.68] 



This is C++, as we want to write some output to disk: 

[1.x.69] 



The last step is as in previous programs: 

[1.x.70] 




[1.x.71]  [1.x.72] 




Next we declare a class that describes the advection field. This, of course, is a vector field with as many components as there are space dimensions. One could now use a class derived from the  [2.x.111]  base class, as we have done for boundary values and coefficients in previous examples, but there is another possibility in the library, namely a base class that describes tensor valued functions. This is more convenient than overriding  [2.x.112]  with a method that knows about multiple function components: at the end of the day we need a Tensor, so we may as well just use a class that returns a Tensor. 

[1.x.73] 



In previous examples, we have used assertions that throw exceptions in several places. However, we have never seen how such exceptions are declared. This can be done as follows: 

[1.x.74] 



The syntax may look a little strange, but is reasonable. The format is basically as follows: use the name of one of the macros  [2.x.113]  denotes the number of additional parameters which the exception object shall take. In this case, as we want to throw the exception when the sizes of two vectors differ, we need two arguments, so we use  [2.x.114] . The first parameter then describes the name of the exception, while the following declare the data types of the parameters. The last argument is a sequence of output directives that will be piped into the  [2.x.115]  object, thus the strange format with the leading  [2.x.116]  operator and the like. Note that we can access the parameters which are passed to the exception upon construction (i.e. within the  [2.x.117]  call) by using the names  [2.x.118] , where  [2.x.119]  is the number of arguments as defined by the use of the respective macro  [2.x.120] .      


To learn how the preprocessor expands this macro into actual code, please refer to the documentation of the exception classes. In brief, this macro call declares and defines a class  [2.x.121]  inheriting from ExceptionBase which implements all necessary error output functions. 

[1.x.75] 



The following two functions implement the interface described above. The first simply implements the function as described in the introduction, while the second uses the same trick to avoid calling a virtual function as has already been introduced in the previous example program. Note the check for the right sizes of the arguments in the second function, which should always be present in such functions; it is our experience that many if not most programming errors result from incorrectly initialized arrays, incompatible parameters to functions and the like; using assertion as in this case can eliminate many of these problems. 

[1.x.76] 



Besides the advection field, we need two functions describing the source terms ( [2.x.122] ) and the boundary values. As described in the introduction, the source is a constant function in the vicinity of a source point, which we denote by the constant static variable  [2.x.123] . We set the values of this center using the same template tricks as we have shown in the  [2.x.124]  example program. The rest is simple and has been shown previously. 

[1.x.77] 



The only new thing here is that we check for the value of the  [2.x.125]  parameter. As this is a scalar function, it is obvious that it only makes sense if the desired component has the index zero, so we assert that this is indeed the case.  [2.x.126]  is a global predefined exception (probably the one most often used, we therefore made it global instead of local to some class), that takes three parameters: the index that is outside the allowed range, the first element of the valid range and the one past the last (i.e. again the half-open interval so often used in the C++ standard library): 

[1.x.78] 



Finally for the boundary values, which is just another class derived from the  [2.x.127]  base class: 

[1.x.79] 




[1.x.80]  [1.x.81] 




Here comes the main class of this program. It is very much like the main classes of previous examples, so we again only comment on the differences. 

[1.x.82] 



The next set of functions will be used to assemble the matrix. However, unlike in the previous examples, the  [2.x.128]  function will not do the work itself, but rather will delegate the actual assembly to helper functions  [2.x.129]  and  [2.x.130] . The rationale is that matrix assembly can be parallelized quite well, as the computation of the local contributions on each cell is entirely independent of other cells, and we only have to synchronize when we add the contribution of a cell to the global matrix.      


The strategy for parallelization we choose here is one of the possibilities mentioned in detail in the  [2.x.131]  module in the documentation. Specifically, we will use the WorkStream approach discussed there. Since there is so much documentation in this module, we will not repeat the rationale for the design choices here (for example, if you read through the module mentioned above, you will understand what the purpose of the  [2.x.132]  and  [2.x.133]  structures is). Rather, we will only discuss the specific implementation.      


If you read the page mentioned above, you will find that in order to parallelize assembly, we need two data structures -- one that corresponds to data that we need during local integration ("scratch data", i.e., things we only need as temporary storage), and one that carries information from the local integration to the function that then adds the local contributions to the corresponding elements of the global matrix. The former of these typically contains the FEValues and FEFaceValues objects, whereas the latter has the local matrix, local right hand side, and information about which degrees of freedom live on the cell for which we are assembling a local contribution. With this information, the following should be relatively self-explanatory: 

[1.x.83] 



FEValues and FEFaceValues are expensive objects to set up, so we include them in the scratch object so that as much data is reused between cells as possible. 

[1.x.84] 



We also store a few vectors that we will populate with values on each cell. Setting these objects up is, in the usual case, cheap; however, they require memory allocations, which can be expensive in multithreaded applications. Hence we keep them here so that computations on a cell do not require new allocations. 

[1.x.85] 



Finally, we need objects that describe the problem's data: 

[1.x.86] 



The following functions again are the same as they were in previous examples, as are the subsequent variables: 

[1.x.87] 




[1.x.88]  [1.x.89] 




Now, finally, here comes the class that will compute the difference approximation of the gradient on each cell and weighs that with a power of the mesh size, as described in the introduction. This class is a simple version of the  [2.x.134]  class in the library, that uses similar techniques to obtain finite difference approximations of the gradient of a finite element field, or of higher derivatives.    


The class has one public static function  [2.x.135]  that is called to compute a vector of error indicators, and a few private functions that do the actual work on all active cells. As in other parts of the library, we follow an informal convention to use vectors of floats for error indicators rather than the common vectors of doubles, as the additional accuracy is not necessary for estimated values.    


In addition to these two functions, the class declares two exceptions which are raised when a cell has no neighbors in each of the space directions (in which case the matrix described in the introduction would be singular and can't be inverted), while the other one is used in the more common case of invalid parameters to a function, namely a vector of wrong size.    


Two other comments: first, the class has no non-static member functions or variables, so this is not really a class, but rather serves the purpose of a  [2.x.136]  in C++. The reason that we chose a class over a namespace is that this way we can declare functions that are private. This can be done with namespaces as well, if one declares some functions in header files in the namespace and implements these and other functions in the implementation file. The functions not declared in the header file are still in the namespace but are not callable from outside. However, as we have only one file here, it is not possible to hide functions in the present case.    


The second comment is that the dimension template parameter is attached to the function rather than to the class itself. This way, you don't have to specify the template parameter yourself as in most other cases, but the compiler can figure its value out itself from the dimension of the DoFHandler object that one passes as first argument.    


Before jumping into the fray with the implementation, let us also comment on the parallelization strategy. We have already introduced the necessary framework for using the WorkStream concept in the declaration of the main class of this program above. We will use it again here. In the current context, this means that we have to define  [2.x.137]   [2.x.138] classes for scratch and copy objects, [2.x.139]   [2.x.140] a function that does the local computation on one cell, and [2.x.141]   [2.x.142] a function that copies the local result into a global object. [2.x.143]   [2.x.144]  Given this general framework, we will, however, deviate from it a bit. In particular, WorkStream was generally invented for cases where each local computation on a cell [1.x.90] to a global object -- for example, when assembling linear systems where we add local contributions into a global matrix and right hand side. WorkStream is designed to handle the potential conflict of multiple threads trying to do this addition at the same time, and consequently has to provide for some way to ensure that only one thread gets to do this at a time. Here, however, the situation is slightly different: we compute contributions from every cell individually, but then all we need to do is put them into an element of an output vector that is unique to each cell. Consequently, there is no risk that the write operations from two cells might conflict, and the elaborate machinery of WorkStream to avoid conflicting writes is not necessary. Consequently, what we will do is this: We still need a scratch object that holds, for example, the FEValues object. However, we only create a fake, empty copy data structure. Likewise, we do need the function that computes local contributions, but since it can already put the result into its final location, we do not need a copy-local-to-global function and will instead give the  [2.x.145]  function an empty function object -- the equivalent to a NULL function pointer. 

[1.x.91] 




[1.x.92]  [1.x.93] 








Now for the implementation of the main class. Constructor, destructor and the function  [2.x.146]  follow the same pattern that was used previously, so we need not comment on these three function: 

[1.x.94] 



In the following function, the matrix and right hand side are assembled. As stated in the documentation of the main class above, it does not do this itself, but rather delegates to the function following next, utilizing the WorkStream concept discussed in  [2.x.147]  .    


If you have looked through the  [2.x.148]  module, you will have seen that assembling in parallel does not take an incredible amount of extra code as long as you diligently describe what the scratch and copy data objects are, and if you define suitable functions for the local assembly and the copy operation from local contributions to global objects. This done, the following will do all the heavy lifting to get these operations done on multiple threads on as many cores as you have in your system: 

[1.x.95] 



As already mentioned above, we need to have scratch objects for the parallel computation of local contributions. These objects contain FEValues and FEFaceValues objects (as well as some arrays), and so we will need to have constructors and copy constructors that allow us to create them. For the cell terms we need the values and gradients of the shape functions, the quadrature points in order to determine the source density and the advection field at a given point, and the weights of the quadrature points times the determinant of the Jacobian at these points. In contrast, for the boundary integrals, we don't need the gradients, but rather the normal vectors to the cells. This determines which update flags we will have to pass to the constructors of the members of the class: 

[1.x.96] 



Now, this is the function that does the actual work. It is not very different from the  [2.x.149]  functions of previous example programs, so we will again only comment on the differences. The mathematical stuff closely follows what we have said in the introduction.    


There are a number of points worth mentioning here, though. The first one is that we have moved the FEValues and FEFaceValues objects into the ScratchData object. We have done so because the alternative would have been to simply create one every time we get into this function -- i.e., on every cell. It now turns out that the FEValues classes were written with the explicit goal of moving everything that remains the same from cell to cell into the construction of the object, and only do as little work as possible in  [2.x.150]  whenever we move to a new cell. What this means is that it would be very expensive to create a new object of this kind in this function as we would have to do it for every cell -- exactly the thing we wanted to avoid with the FEValues class. Instead, what we do is create it only once (or a small number of times) in the scratch objects and then re-use it as often as we can.    


This begs the question of whether there are other objects we create in this function whose creation is expensive compared to its use. Indeed, at the top of the function, we declare all sorts of objects. The  [2.x.151] ,  [2.x.152]  do not cost much to create, so there is no harm here. However, allocating memory in creating the  [2.x.153]  and similar variables below typically costs a significant amount of time, compared to just accessing the (temporary) values we store in them. Consequently, these would be candidates for moving into the  [2.x.154]  class. We will leave this as an exercise. 

[1.x.97] 



We define some abbreviations to avoid unnecessarily long lines: 

[1.x.98] 



We declare cell matrix and cell right hand side... 

[1.x.99] 



... an array to hold the global indices of the degrees of freedom of the cell on which we are presently working... 

[1.x.100] 



... then initialize the  [2.x.155]  object... 

[1.x.101] 



... obtain the values of right hand side and advection directions at the quadrature points... 

[1.x.102] 



... set the value of the streamline diffusion parameter as described in the introduction... 

[1.x.103] 



... and assemble the local contributions to the system matrix and right hand side as also discussed above: 

[1.x.104] 



Alias the AssemblyScratchData object to keep the lines from getting too long: 

[1.x.105] 



Besides the cell terms which we have built up now, the bilinear form of the present problem also contains terms on the boundary of the domain. Therefore, we have to check whether any of the faces of this cell are on the boundary of the domain, and if so assemble the contributions of this face as well. Of course, the bilinear form only contains contributions from the  [2.x.156]  part of the boundary, but to find out whether a certain part of a face of the present cell is part of the inflow boundary, we have to have information on the exact location of the quadrature points and on the direction of flow at this point; we obtain this information using the FEFaceValues object and only decide within the main loop whether a quadrature point is on the inflow boundary. 

[1.x.106] 



Ok, this face of the present cell is on the boundary of the domain. Just as for the usual FEValues object which we have used in previous examples and also above, we have to reinitialize the FEFaceValues object for the present face: 

[1.x.107] 



For the quadrature points at hand, we ask for the values of the inflow function and for the direction of flow: 

[1.x.108] 



Now loop over all quadrature points and see whether this face is on the inflow or outflow part of the boundary. The normal vector points out of the cell: since the face is at the boundary, the normal vector points out of the domain, so if the advection direction points into the domain, its scalar product with the normal vector must be negative (to see why this is true, consider the scalar product definition that uses a cosine): 

[1.x.109] 



If the face is part of the inflow boundary, then compute the contributions of this face to the global matrix and right hand side, using the values obtained from the FEFaceValues object and the formulae discussed in the introduction: 

[1.x.110] 



The final piece of information the copy routine needs is the global indices of the degrees of freedom on this cell, so we end by writing them to the local array: 

[1.x.111] 



The second function we needed to write was the one that copies the local contributions the previous function computed (and put into the AssemblyCopyData object) into the global matrix and right hand side vector objects. This is essentially what we always had as the last block of code when assembling something on every cell. The following should therefore be pretty obvious: 

[1.x.112] 



Here comes the linear solver routine. As the system is no longer symmetric positive definite as in all the previous examples, we cannot use the Conjugate Gradient method anymore. Rather, we use a solver that is more general and does not rely on any special properties of the matrix: the GMRES method. GMRES, like the conjugate gradient method, requires a decent preconditioner: we use a Jacobi preconditioner here, which works well enough for this problem. 

[1.x.113] 



The following function refines the grid according to the quantity described in the introduction. The respective computations are made in the class  [2.x.157] . 

[1.x.114] 



This function is similar to the one in step 6, but since we use a higher degree finite element we save the solution in a different way. Visualization programs like VisIt and Paraview typically only understand data that is associated with nodes: they cannot plot fifth-degree basis functions, which results in a very inaccurate picture of the solution we computed. To get around this we save multiple  [2.x.158] patches [2.x.159]  per cell: in 2D we save 64 bilinear `cells' to the VTU file for each cell, and in 3D we save 512. The end result is that the visualization program will use a piecewise linear interpolation of the cubic basis functions: this captures the solution detail and, with most screen resolutions, looks smooth. We save the grid in a separate step with no extra patches so that we have a visual representation of the cell faces.    


Version 9.1 of deal.II gained the ability to write higher degree polynomials (i.e., write piecewise bicubic visualization data for our piecewise bicubic solution) VTK and VTU output: however, not all recent versions of ParaView and VisIt (as of 2018) can read this format, so we use the older, more general (but less efficient) approach here. 

[1.x.115] 



VTU output can be expensive, both to compute and to write to disk. Here we ask ZLib, a compression library, to compress the data in a way that maximizes throughput. 

[1.x.116] 



... as is the main loop (setup -- solve -- refine), aside from the number of cycles and the initial grid: 

[1.x.117] 




[1.x.118]  [1.x.119] 




Now for the implementation of the  [2.x.160]  class. Let us start by defining constructors for the  [2.x.161]  class used by the  [2.x.162]  function: 

[1.x.120] 



We allocate a vector to hold iterators to all active neighbors of a cell. We reserve the maximal number of active neighbors in order to avoid later reallocations. Note how this maximal number of active neighbors is computed here. 

[1.x.121] 



Next comes the implementation of the  [2.x.163]  class. The first function does not much except for delegating work to the other function, but there is a bit of setup at the top.    


Before starting with the work, we check that the vector into which the results are written has the right size. Programming mistakes in which one forgets to size arguments correctly at the calling site are quite common. Because the resulting damage from not catching such errors is often subtle (e.g., corruption of data somewhere in memory, or non-reproducible results), it is well worth the effort to check for such things. 

[1.x.122] 



Here comes the function that estimates the local error by computing the finite difference approximation of the gradient. The function first computes the list of active neighbors of the present cell and then computes the quantities described in the introduction for each of the neighbors. The reason for this order is that it is not a one-liner to find a given neighbor with locally refined meshes. In principle, an optimized implementation would find neighbors and the quantities depending on them in one step, rather than first building a list of neighbors and in a second step their contributions but we will gladly leave this as an exercise. As discussed before, the worker function passed to  [2.x.164]  works on "scratch" objects that keep all temporary objects. This way, we do not need to create and initialize objects that are expensive to initialize within the function that does the work every time it is called for a given cell. Such an argument is passed as the second argument. The third argument would be a "copy-data" object (see  [2.x.165]  for more information) but we do not actually use any of these here. Since  [2.x.166]  insists on passing three arguments, we declare this function with three arguments, but simply ignore the last one.    


(This is unsatisfactory from an aesthetic perspective. It can be avoided by using an anonymous (lambda) function. If you allow, let us here show how. First, assume that we had declared this function to only take two arguments by omitting the unused last one. Now,  [2.x.167]  still wants to call this function with three arguments, so we need to find a way to "forget" the third argument in the call. Simply passing  [2.x.168]  the pointer to the function as we do above will not do this -- the compiler will complain that a function declared to have two arguments is called with three arguments. However, we can do this by passing the following as the third argument to  [2.x.169]   [2.x.170]  This is not much better than the solution implemented below: either the routine itself must take three arguments or it must be wrapped by something that takes three arguments. We don't use this since adding the unused argument at the beginning is simpler.    


Now for the details: 

[1.x.124] 



We need space for the tensor  [2.x.171] , which is the sum of outer products of the y-vectors. 

[1.x.125] 



First initialize the  [2.x.172]  object, as well as the  [2.x.173]  tensor: 

[1.x.126] 



Now, before we go on, we first compute a list of all active neighbors of the present cell. We do so by first looping over all faces and see whether the neighbor there is active, which would be the case if it is on the same level as the present cell or one level coarser (note that a neighbor can only be once coarser than the present cell, as we only allow a maximal difference of one refinement over a face in deal.II). Alternatively, the neighbor could be on the same level and be further refined; then we have to find which of its children are next to the present cell and select these (note that if a child of a neighbor of an active cell that is next to this active cell, needs necessarily be active itself, due to the one-refinement rule cited above).      


Things are slightly different in one space dimension, as there the one-refinement rule does not exist: neighboring active cells may differ in as many refinement levels as they like. In this case, the computation becomes a little more difficult, but we will explain this below.      


Before starting the loop over all neighbors of the present cell, we have to clear the array storing the iterators to the active neighbors, of course. 

[1.x.127] 



First define an abbreviation for the iterator to the face and the neighbor 

[1.x.128] 



Then check whether the neighbor is active. If it is, then it is on the same level or one level coarser (if we are not in 1D), and we are interested in it in any case. 

[1.x.129] 



If the neighbor is not active, then check its children. 

[1.x.130] 



To find the child of the neighbor which bounds to the present cell, successively go to its right child if we are left of the present cell (n==0), or go to the left child if we are on the right (n==1), until we find an active cell. 

[1.x.131] 



As this used some non-trivial geometrical intuition, we might want to check whether we did it right, i.e., check whether the neighbor of the cell we found is indeed the cell we are presently working on. Checks like this are often useful and have frequently uncovered errors both in algorithms like the line above (where it is simple to involuntarily exchange  [2.x.174]  or the like) and in the library (the assumptions underlying the algorithm above could either be wrong, wrongly documented, or are violated due to an error in the library). One could in principle remove such checks after the program works for some time, but it might be a good things to leave it in anyway to check for changes in the library or in the algorithm above.                    


Note that if this check fails, then this is certainly an error that is irrecoverable and probably qualifies as an internal error. We therefore use a predefined exception class to throw here. 

[1.x.132] 



If the check succeeded, we push the active neighbor we just found to the stack we keep: 

[1.x.133] 



If we are not in 1d, we collect all neighbor children `behind' the subfaces of the current face and move on: 

[1.x.134] 



OK, now that we have all the neighbors, lets start the computation on each of them. First we do some preliminaries: find out about the center of the present cell and the solution at this point. The latter is obtained as a vector of function values at the quadrature points, of which there are only one, of course. Likewise, the position of the center is the position of the first (and only) quadrature point in real space. 

[1.x.135] 



Now loop over all active neighbors and collect the data we need. 

[1.x.136] 



Then get the center of the neighbor cell and the value of the finite element function at that point. Note that for this information we have to reinitialize the  [2.x.175]  object for the neighbor cell. 

[1.x.137] 



Compute the vector  [2.x.176]  connecting the centers of the two cells. Note that as opposed to the introduction, we denote by  [2.x.177]  the normalized difference vector, as this is the quantity used everywhere in the computations. 

[1.x.138] 



Then add up the contribution of this cell to the Y matrix... 

[1.x.139] 



... and update the sum of difference quotients: 

[1.x.140] 



If now, after collecting all the information from the neighbors, we can determine an approximation of the gradient for the present cell, then we need to have passed over vectors  [2.x.178]  which span the whole space, otherwise we would not have all components of the gradient. This is indicated by the invertibility of the matrix.      


If the matrix is not invertible, then the present cell had an insufficient number of active neighbors. In contrast to all previous cases (where we raised exceptions) this is, however, not a programming error: it is a runtime error that can happen in optimized mode even if it ran well in debug mode, so it is reasonable to try to catch this error also in optimized mode. For this case, there is the  [2.x.179]  macro: it checks the condition like the  [2.x.180]  macro, but not only in debug mode; it then outputs an error message, but instead of aborting the program as in the case of the  [2.x.181]  macro, the exception is thrown using the  [2.x.182]  command of C++. This way, one has the possibility to catch this error and take reasonable counter actions. One such measure would be to refine the grid globally, as the case of insufficient directions can not occur if every cell of the initial grid has been refined at least once. 

[1.x.141] 



If, on the other hand, the matrix is invertible, then invert it, multiply the other quantity with it, and compute the estimated error using this quantity and the correct powers of the mesh width: 

[1.x.142] 



The last part of this function is the one where we write into the element of the output vector what we have just computed. The address of this vector has been stored in the scratch data object, and all we have to do is know how to get at the correct element inside this vector -- but we can ask the cell we're on the how-manyth active cell it is for this: 

[1.x.143] 




[1.x.144]  [1.x.145] 




The  [2.x.183]  function is similar to the previous examples. The primary difference is that we use MultithreadInfo to set the maximum number of threads (see the documentation module  [2.x.184]  "Parallel computing with multiple processors accessing shared memory" for more information). The number of threads used is the minimum of the environment variable DEAL_II_NUM_THREADS and the parameter of  [2.x.185] . If no value is given to  [2.x.186] , the default value from the Intel Threading Building Blocks (TBB) library is used. If the call to  [2.x.187]  is omitted, the number of threads will be chosen by TBB independently of DEAL_II_NUM_THREADS. 

[1.x.146] 

[1.x.147][1.x.148] 




The results of this program are not particularly spectacular. They consist of the console output, some grid files, and the solution on each of these grids. First for the console output: 

[1.x.149] 



Quite a number of cells are used on the finest level to resolve the features of the solution. Here are the fourth and tenth grids:  [2.x.188]  and the fourth and tenth solutions:  [2.x.189]  and both the grid and solution zoomed in:  [2.x.190]  

The solution is created by that part that is transported along the wiggly advection field from the left and lower boundaries to the top right, and the part that is created by the source in the lower left corner, and the results of which are also transported along. The grid shown above is well-adapted to resolve these features. The comparison between plots shows that, even though we are using a high-order approximation, we still need adaptive mesh refinement to fully resolve the wiggles. [1.x.150] [1.x.151]  [2.x.191]  

 [2.x.192] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5] 

[1.x.6] [1.x.7][1.x.8] 




This is a rather short example which only shows some aspects of using higher order mappings. By  [2.x.3] mapping [2.x.4]  we mean the transformation between the unit cell (i.e. the unit line, square, or cube) to the cells in real space. In all the previous examples, we have implicitly used linear or d-linear mappings; you will not have noticed this at all, since this is what happens if you do not do anything special. However, if your domain has curved boundaries, there are cases where the piecewise linear approximation of the boundary (i.e. by straight line segments) is not sufficient, and you want that your computational domain is an approximation to the real domain using curved boundaries as well. If the boundary approximation uses piecewise quadratic parabolas to approximate the true boundary, then we say that this is a quadratic or  [2.x.5]  approximation. If we use piecewise graphs of cubic polynomials, then this is a  [2.x.6]  approximation, and so on. 




For some differential equations, it is known that piecewise linear approximations of the boundary, i.e.  [2.x.7]  mappings, are not sufficient if the boundary of the exact domain is curved. Examples are the biharmonic equation using  [2.x.8]  elements, or the Euler equations of gas dynamics on domains with curved reflective boundaries. In these cases, it is necessary to compute the integrals using a higher order mapping. If we do not use such a higher order mapping, the order of approximation of the boundary dominates the order of convergence of the entire numerical scheme, irrespective of the order of convergence of the discretization in the interior of the domain. 




Rather than demonstrating the use of higher order mappings with one of these more complicated examples, we do only a brief computation: calculating the value of  [2.x.9]  by two different methods. 




The first method uses a triangulated approximation of the circle with unit radius and integrates a unit magnitude constant function ( [2.x.10] ) over it. Of course, if the domain were the exact unit circle, then the area would be  [2.x.11] , but since we only use an approximation by piecewise polynomial segments, the value of the area we integrate over is not exactly  [2.x.12] . However, it is known that as we refine the triangulation, a  [2.x.13]  mapping approximates the boundary with an order  [2.x.14] , where  [2.x.15]  is the mesh size. We will check the values of the computed area of the circle and their convergence towards  [2.x.16]  under mesh refinement for different mappings. We will also find a convergence behavior that is surprising at first, but has a good explanation. 




The second method works similarly, but this time does not use the area of the triangulated unit circle, but rather its perimeter.  [2.x.17]  is then approximated by half of the perimeter, as we choose the radius equal to one. 




 [2.x.18]  This tutorial shows in essence how to choose a particular mapping for integrals, by attaching a particular geometry to the triangulation (as had already been done in  [2.x.19] , for example) and then passing a mapping argument to the FEValues class that is used for all integrals in deal.II. The geometry we choose is a circle, for which deal.II already has a class (SphericalManifold) that can be used. If you want to define your own geometry, for example because it is complicated and cannot be described by the classes already available in deal.II, you will want to read through  [2.x.20] . [1.x.9] [1.x.10] 

The first of the following include files are probably well-known by now and need no further explanation. 

[1.x.11] 



This include file is new. Even if we are not solving a PDE in this tutorial, we want to use a dummy finite element with zero degrees of freedoms provided by the FE_Nothing class. 

[1.x.12] 



The following header file is also new: in it, we declare the MappingQ class which we will use for polynomial mappings of arbitrary order: 

[1.x.13] 



And this again is C++: 

[1.x.14] 



The last step is as in previous programs: 

[1.x.15] 



Now, as we want to compute the value of  [2.x.21] , we have to compare to something. These are the first few digits of  [2.x.22] , which we define beforehand for later use. Since we would like to compute the difference between two numbers which are quite accurate, with the accuracy of the computed approximation to  [2.x.23]  being in the range of the number of digits which a double variable can hold, we rather declare the reference value as a  [2.x.24]  and give it a number of extra digits: 

[1.x.16] 



Then, the first task will be to generate some output. Since this program is so small, we do not employ object oriented techniques in it and do not declare classes (although, of course, we use the object oriented features of the library). Rather, we just pack the functionality into separate functions. We make these functions templates on the number of space dimensions to conform to usual practice when using deal.II, although we will only use them for two space dimensions and throw an exception when attempted to use for any other spatial dimension.    


The first of these functions just generates a triangulation of a circle (hyperball) and outputs the  [2.x.25]  mapping of its cells for different values of  [2.x.26] . Then, we refine the grid once and do so again. 

[1.x.17] 



So first generate a coarse triangulation of the circle and associate a suitable boundary description to it. By default,  [2.x.27]  attaches a SphericalManifold to the boundary (and uses FlatManifold for the interior) so we simply call that function and move on: 

[1.x.18] 



Then alternate between generating output on the current mesh for  [2.x.28] ,  [2.x.29] , and  [2.x.30]  mappings, and (at the end of the loop body) refining the mesh once globally. 

[1.x.19] 



For this, first set up an object describing the mapping. This is done using the MappingQ class, which takes as argument to the constructor the polynomial degree which it shall use. 

[1.x.20] 



As a side note, for a piecewise linear mapping, you could give a value of  [2.x.31]  to the constructor of MappingQ, but there is also a class MappingQ1 that achieves the same effect. Historically, it did a lot of things in a simpler way than MappingQ but is today just a wrapper around the latter. It is, however, still the class that is used implicitly in many places of the library if you do not specify another mapping explicitly. 








In order to actually write out the present grid with this mapping, we set up an object which we will use for output. We will generate Gnuplot output, which consists of a set of lines describing the mapped triangulation. By default, only one line is drawn for each face of the triangulation, but since we want to explicitly see the effect of the mapping, we want to have the faces in more detail. This can be done by passing the output object a structure which contains some flags. In the present case, since Gnuplot can only draw straight lines, we output a number of additional points on the faces so that each face is drawn by 30 small lines instead of only one. This is sufficient to give us the impression of seeing a curved line, rather than a set of straight lines. 

[1.x.21] 



Finally, generate a filename and a file for output: 

[1.x.22] 



Then write out the triangulation to this file. The last argument of the function is a pointer to a mapping object. This argument has a default value, and if no value is given a simple MappingQ1 object is taken, which we briefly described above. This would then result in a piecewise linear approximation of the true boundary in the output. 

[1.x.23] 



At the end of the loop, refine the mesh globally. 

[1.x.24] 



Now we proceed with the main part of the code, the approximation of  [2.x.32] . The area of a circle is of course given by  [2.x.33] , so having a circle of radius 1, the area represents just the number that is searched for. The numerical computation of the area is performed by integrating the constant function of value 1 over the whole computational domain, i.e. by computing the areas  [2.x.34] , where the sum extends over all quadrature points on all active cells in the triangulation, with  [2.x.35]  being the weight of quadrature point  [2.x.36] . The integrals on each cell are approximated by numerical quadrature, hence the only additional ingredient we need is to set up a FEValues object that provides the corresponding `JxW` values of each cell. (Note that `JxW` is meant to abbreviate [1.x.25]; since in numerical quadrature the two factors always occur at the same places, we only offer the combined quantity, rather than two separate ones.) We note that here we won't use the FEValues object in its original purpose, i.e. for the computation of values of basis functions of a specific finite element at certain quadrature points. Rather, we use it only to gain the `JxW` at the quadrature points, irrespective of the (dummy) finite element we will give to the constructor of the FEValues object. The actual finite element given to the FEValues object is not used at all, so we could give any. 

[1.x.26] 



For the numerical quadrature on all cells we employ a quadrature rule of sufficiently high degree. We choose QGauss that is of order 8 (4 points), to be sure that the errors due to numerical quadrature are of higher order than the order (maximal 6) that will occur due to the order of the approximation of the boundary, i.e. the order of the mappings employed. Note that the integrand, the Jacobian determinant, is not a polynomial function (rather, it is a rational one), so we do not use Gauss quadrature in order to get the exact value of the integral as done often in finite element computations, but could as well have used any quadrature formula of like order instead. 

[1.x.27] 



Now start by looping over polynomial mapping degrees=1..4: 

[1.x.28] 



First generate the triangulation, the boundary and the mapping object as already seen. 

[1.x.29] 



We now create a finite element. Unlike the rest of the example programs, we do not actually need to do any computations with shape functions; we only need the `JxW` values from an FEValues object. Hence we use the special finite element class FE_Nothing which has exactly zero degrees of freedom per cell (as the name implies, the local basis on each cell is the empty set). A more typical usage of FE_Nothing is shown in  [2.x.37] . 

[1.x.30] 



Likewise, we need to create a DoFHandler object. We do not actually use it, but it will provide us with `active_cell_iterators` that are needed to reinitialize the FEValues object on each cell of the triangulation. 

[1.x.31] 



Now we set up the FEValues object, giving the Mapping, the dummy finite element and the quadrature object to the constructor, together with the update flags asking for the `JxW` values at the quadrature points only. This tells the FEValues object that it needs not compute other quantities upon calling the  [2.x.38]  function, thus saving computation time.          


The most important difference in the construction of the FEValues object compared to previous example programs is that we pass a mapping object as first argument, which is to be used in the computation of the mapping from unit to real cell. In previous examples, this argument was omitted, resulting in the implicit use of an object of type MappingQ1. 

[1.x.32] 



We employ an object of the ConvergenceTable class to store all important data like the approximated values for  [2.x.39]  and the error with respect to the true value of  [2.x.40] . We will also use functions provided by the ConvergenceTable class to compute convergence rates of the approximations to  [2.x.41] . 

[1.x.33] 



Now we loop over several refinement steps of the triangulation. 

[1.x.34] 



In this loop we first add the number of active cells of the current triangulation to the table. This function automatically creates a table column with superscription `cells`, in case this column was not created before. 

[1.x.35] 



Then we distribute the degrees of freedom for the dummy finite element. Strictly speaking we do not need this function call in our special case but we call it to make the DoFHandler happy -- otherwise it would throw an assertion in the  [2.x.42]  function below. 

[1.x.36] 



We define the variable area as `long double` like we did for the `pi` variable before. 

[1.x.37] 



Now we loop over all cells, reinitialize the FEValues object for each cell, and add up all the `JxW` values for this cell to `area`... 

[1.x.38] 



...and store the resulting area values and the errors in the table. We need a static cast to double as there is no add_value(string, long double) function implemented. Note that this also concerns the second call as the  [2.x.43]  function in the  [2.x.44]  namespace is overloaded on its argument types, so there exists a version taking and returning a  [2.x.45] , in contrast to the global namespace where only one such function is declared (which takes and returns a double). 

[1.x.39] 



We want to compute the convergence rates of the `error` column. Therefore we need to omit the other columns from the convergence rate evaluation before calling `evaluate_all_convergence_rates` 

[1.x.40] 



Finally we set the precision and scientific mode for output of some of the quantities... 

[1.x.41] 



...and write the whole table to  [2.x.46]  

[1.x.42] 



The following, second function also computes an approximation of  [2.x.47]  but this time via the perimeter  [2.x.48]  of the domain instead of the area. This function is only a variation of the previous function. So we will mainly give documentation for the differences. 

[1.x.43] 



We take the same order of quadrature but this time a `dim-1` dimensional quadrature as we will integrate over (boundary) lines rather than over cells. 

[1.x.44] 



We loop over all degrees, create the triangulation, the boundary, the mapping, the dummy finite element and the DoFHandler object as seen before. 

[1.x.45] 



Then we create a FEFaceValues object instead of a FEValues object as in the previous function. Again, we pass a mapping as first argument. 

[1.x.46] 



Now we run over all cells and over all faces of each cell. Only the contributions of the `JxW` values on boundary faces are added to the long double variable `perimeter`. 

[1.x.47] 



We reinit the FEFaceValues object with the cell iterator and the number of the face. 

[1.x.48] 



Then store the evaluated values in the table... 

[1.x.49] 



...and end this function as we did in the previous one: 

[1.x.50] 



The following main function just calls the above functions in the order of their appearance. Apart from this, it looks just like the main functions of previous tutorial programs. 

[1.x.51] 

[1.x.52][1.x.53] 




The program performs two tasks, the first being to generate a visualization of the mapped domain, the second to compute pi by the two methods described. Let us first take a look at the generated graphics. They are generated in Gnuplot format, and can be viewed with the commands 

[1.x.54] 

or using one of the other filenames. The second line makes sure that the aspect ratio of the generated output is actually 1:1, i.e. a circle is drawn as a circle on your screen, rather than as an ellipse. The third line switches off the key in the graphic, as that will only print information (the filename) which is not that important right now. Similarly, the fourth and fifth disable tick marks. The plot is then generated with a specific line width ("lw", here set to 4) and line type ("lt", here chosen by saying that the line should be drawn using the RGB color "black"). 

The following table shows the triangulated computational domain for  [2.x.49] ,  [2.x.50] , and  [2.x.51]  mappings, for the original coarse grid (left), and a once uniformly refined grid (right). 

 [2.x.52]  

These pictures show the obvious advantage of higher order mappings: they approximate the true boundary quite well also on rather coarse meshes. To demonstrate this a little further, here is part of the upper right quarter circle of the coarse meshes with  [2.x.53]  and  [2.x.54]  mappings, where the dashed red line marks the actual circle: 

 [2.x.55]  

Obviously the quadratic mapping approximates the boundary quite well, while for the cubic mapping the difference between approximated domain and true one is hardly visible already for the coarse grid. You can also see that the mapping only changes something at the outer boundaries of the triangulation. In the interior, all lines are still represented by linear functions, resulting in additional computations only on cells at the boundary. Higher order mappings are therefore usually not noticeably slower than lower order ones, because the additional computations are only performed on a small subset of all cells. 




The second purpose of the program was to compute the value of pi to good accuracy. This is the output of this part of the program: 

[1.x.55] 






One of the immediate observations from the output is that in all cases the values converge quickly to the true value of  [2.x.56] . Note that for the  [2.x.57]  mapping, we are already in the regime of roundoff errors and the convergence rate levels off, which is already quite a lot. However, also note that for the  [2.x.58]  mapping, even on the finest grid the accuracy is significantly worse than on the coarse grid for a  [2.x.59]  mapping! 




The last column of the output shows the convergence order, in powers of the mesh width  [2.x.60] . In the introduction, we had stated that the convergence order for a  [2.x.61]  mapping should be  [2.x.62] . However, in the example shown, the order is rather  [2.x.63] ! This at first surprising fact is explained by the properties of the  [2.x.64]  mapping. At order [1.x.56], it uses support points that are based on the [1.x.57]+1 point Gauss-Lobatto quadrature rule that selects the support points in such a way that the quadrature rule converges at order 2[1.x.58]. Even though these points are here only used for interpolation of a [1.x.59]th order polynomial, we get a superconvergence effect when numerically evaluating the integral, resulting in the observed high order of convergence. (This effect is also discussed in detail in the following publication: A. Bonito, A. Demlow, and J. Owen: "A priori error estimates for finite element approximations to eigenvalues and eigenfunctions of the Laplace-Beltrami operator", submitted, 2018.) [1.x.60] [1.x.61]  [2.x.65]  

 [2.x.66] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5] 

[1.x.6] [1.x.7][1.x.8] 


The problem we will be considering is the solution of Laplace's problem with Neumann boundary conditions only: [1.x.9] 

It is well known that if this problem is to have a solution, then the forces need to satisfy the compatibility condition [1.x.10] We will consider the special case that  [2.x.3]  is the circle of radius 1 around the origin, and  [2.x.4] ,  [2.x.5] . This choice satisfies the compatibility condition. 

The compatibility condition allows a solution of the above equation, but it nevertheless retains an ambiguity: since only derivatives of the solution appear in the equations, the solution is only determined up to a constant. For this reason, we have to pose another condition for the numerical solution, which fixes this constant. 

For this, there are various possibilities:  [2.x.6]   [2.x.7]  Fix one node of the discretization to zero or any other fixed value.   This amounts to an additional condition  [2.x.8] . Although this is   common practice, it is not necessarily a good idea, since we know that the   solutions of Laplace's equation are only in  [2.x.9] , which does not allow for   the definition of point values because it is not a subset of the continuous   functions. Therefore, even though fixing one node is allowed for   discretized functions, it is not for continuous functions, and one can   often see this in a resulting error spike at this point in the numerical   solution. 

 [2.x.10]  Fixing the mean value over the domain to zero or any other value. This   is allowed on the continuous level, since  [2.x.11]    by Sobolev's inequality, and thus also on the discrete level since we   there only consider subsets of  [2.x.12] . 

 [2.x.13]  Fixing the mean value over the boundary of the domain to zero or any   other value. This is also allowed on the continuous level, since    [2.x.14] , again by Sobolev's   inequality.  [2.x.15]  We will choose the last possibility, since we want to demonstrate another technique with it. 

While this describes the problem to be solved, we still have to figure out how to implement it. Basically, except for the additional mean value constraint, we have solved this problem several times, using Dirichlet boundary values, and we only need to drop the treatment of Dirichlet boundary nodes. The use of higher order mappings is also rather trivial and will be explained at the various places where we use it; in almost all conceivable cases, you will only consider the objects describing mappings as a black box which you need not worry about, because their only uses seem to be to be passed to places deep inside the library where functions know how to handle them (i.e. in the  [2.x.16]  classes and their descendants). 

The tricky point in this program is the use of the mean value constraint. Fortunately, there is a class in the library which knows how to handle such constraints, and we have used it quite often already, without mentioning its generality. Note that if we assume that the boundary nodes are spaced equally along the boundary, then the mean value constraint [1.x.11] can be written as [1.x.12] where the sum shall run over all degree of freedom indices which are located on the boundary of the computational domain. Let us denote by  [2.x.17]  that index on the boundary with the lowest number (or any other conveniently chosen index), then the constraint can also be represented by [1.x.13] This, luckily, is exactly the form of constraints for which the AffineConstraints class was designed. Note that we have used this class in several previous examples for the representation of hanging nodes constraints, which also have this form: there, the middle vertex shall have the mean of the values of the adjacent vertices. In general, the AffineConstraints class is designed to handle affine constraints of the form [1.x.14] where  [2.x.18]  denotes a matrix,  [2.x.19]  denotes a vector, and  [2.x.20]  the vector of nodal values. In this case, since  [2.x.21]  represents one homogeneous constraint,  [2.x.22]  is the zero vector. 

In this example, the mean value along the boundary allows just such a representation, with  [2.x.23]  being a matrix with just one row (i.e. there is only one constraint). In the implementation, we will create an AffineConstraints object, add one constraint (i.e. add another row to the matrix) referring to the first boundary node  [2.x.24] , and insert the weights with which all the other nodes contribute, which in this example happens to be just  [2.x.25] . 

Later, we will use this object to eliminate the first boundary node from the linear system of equations, reducing it to one which has a solution without the ambiguity of the constant shift value. One of the problems of the implementation will be that the explicit elimination of this node results in a number of additional elements in the matrix, of which we do not know in advance where they are located and how many additional entries will be in each of the rows of the matrix. We will show how we can use an intermediate object to work around this problem. 

But now on to the implementation of the program solving this problem... [1.x.15] [1.x.16] 

As usual, the program starts with a rather long list of include files which you are probably already used to by now: 

[1.x.17] 



Just this one is new: it declares a class DynamicSparsityPattern, which we will use and explain further down below. 

[1.x.18] 



We will make use of the  [2.x.26]  algorithm of the C++ standard library, so we have to include the following file for its declaration: 

[1.x.19] 



The last step is as in all previous programs: 

[1.x.20] 



Then we declare a class which represents the solution of a Laplace problem. As this example program is based on  [2.x.27] , the class looks rather the same, with the sole structural difference that the functions  [2.x.28]  itself, and is thus called  [2.x.29] , and that the output function was dropped since the solution function is so boring that it is not worth being viewed.    


The only other noteworthy change is that the constructor takes a value representing the polynomial degree of the mapping to be used later on, and that it has another member variable representing exactly this mapping. In general, this variable will occur in real applications at the same places where the finite element is declared or used. 

[1.x.21] 



Construct such an object, by initializing the variables. Here, we use linear finite elements (the argument to the  [2.x.30]  variable denotes the polynomial degree), and mappings of given order. Print to screen what we are about to do. 

[1.x.22] 



The first task is to set up the variables for this problem. This includes generating a valid  [2.x.31]  object, as well as the sparsity patterns for the matrix, and the object representing the constraints that the mean value of the degrees of freedom on the boundary be zero. 

[1.x.23] 



The first task is trivial: generate an enumeration of the degrees of freedom, and initialize solution and right hand side vector to their correct sizes: 

[1.x.24] 



The next task is to construct the object representing the constraint that the mean value of the degrees of freedom on the boundary shall be zero. For this, we first want a list of those nodes that are actually at the boundary. The  [2.x.32]  namespace has a function that returns an IndexSet object that contains the indices of all those degrees of freedom that are at the boundary.      


Once we have this index set, we wanted to know which is the first index corresponding to a degree of freedom on the boundary. We need this because we wanted to constrain one of the nodes on the boundary by the values of all other DoFs on the boundary. To get the index of this "first" degree of freedom is easy enough using the IndexSet class: 

[1.x.25] 



Then generate a constraints object with just this one constraint. First clear all previous content (which might reside there from the previous computation on a once coarser grid), then add this one line constraining the  [2.x.33]  to the sum of other boundary DoFs each with weight -1. Finally, close the constraints object, i.e. do some internal bookkeeping on it for faster processing of what is to come later: 

[1.x.26] 



Next task is to generate a sparsity pattern. This is indeed a tricky task here. Usually, we just call  [2.x.34]  and condense the result using the hanging node constraints. We have no hanging node constraints here (since we only refine globally in this example), but we have this global constraint on the boundary. This poses one severe problem in this context: the  [2.x.35]  class wants us to state beforehand the maximal number of entries per row, either for all rows or for each row separately. There are functions in the library which can tell you this number in case you just have hanging node constraints (namely  [2.x.36]  but how is this for the present case? The difficulty arises because the elimination of the constrained degree of freedom requires a number of additional entries in the matrix at places that are not so simple to determine. We would therefore have a problem had we to give a maximal number of entries per row here.      


Since this can be so difficult that no reasonable answer can be given that allows allocation of only a reasonable amount of memory, there is a class DynamicSparsityPattern, that can help us out here. It does not require that we know in advance how many entries rows could have, but allows just about any length. It is thus significantly more flexible in case you do not have good estimates of row lengths, however at the price that building up such a pattern is also significantly more expensive than building up a pattern for which you had information in advance. Nevertheless, as we have no other choice here, we'll just build such an object by initializing it with the dimensions of the matrix and calling another function  [2.x.37]  to get the sparsity pattern due to the differential operator, then condense it with the constraints object which adds those positions in the sparsity pattern that are required for the elimination of the constraint. 

[1.x.27] 



Finally, once we have the full pattern, we can initialize an object of type  [2.x.38]  from it and in turn initialize the matrix with it. Note that this is actually necessary, since the DynamicSparsityPattern is so inefficient compared to the  [2.x.39]  class due to the more flexible data structures it has to use, that we can impossibly base the sparse matrix class on it, but rather need an object of type  [2.x.40] , which we generate by copying from the intermediate object.      


As a further sidenote, you will notice that we do not explicitly have to  [2.x.41]  the sparsity pattern here. This, of course, is due to the fact that the  [2.x.42]  function generates a compressed object right from the start, to which you cannot add new entries anymore. The  [2.x.43]  call is therefore implicit in the  [2.x.44]  call. 

[1.x.28] 



The next function then assembles the linear system of equations, solves it, and evaluates the solution. This then makes three actions, and we will put them into eight true statements (excluding declaration of variables, and handling of temporary vectors). Thus, this function is something for the very lazy. Nevertheless, the functions called are rather powerful, and through them this function uses a good deal of the whole library. But let's look at each of the steps. 

[1.x.29] 



First, we have to assemble the matrix and the right hand side. In all previous examples, we have investigated various ways how to do this manually. However, since the Laplace matrix and simple right hand sides appear so frequently in applications, the library provides functions for actually doing this for you, i.e. they perform the loop over all cells, setting up the local matrices and vectors, and putting them together for the end result.      


The following are the two most commonly used ones: creation of the Laplace matrix and creation of a right hand side vector from body or boundary forces. They take the mapping object, the  [2.x.45]  object representing the degrees of freedom and the finite element in use, a quadrature formula to be used, and the output object. The function that creates a right hand side vector also has to take a function object describing the (continuous) right hand side function.      


Let us look at the way the matrix and body forces are integrated: 

[1.x.30] 



That's quite simple, right?      


Two remarks are in order, though: First, these functions are used in a lot of contexts. Maybe you want to create a Laplace or mass matrix for a vector values finite element; or you want to use the default Q1 mapping; or you want to assembled the matrix with a coefficient in the Laplace operator. For this reason, there are quite a large number of variants of these functions in the  [2.x.46]  and  [2.x.47]  namespaces. Whenever you need a slightly different version of these functions than the ones called above, it is certainly worthwhile to take a look at the documentation and to check whether something fits your needs.      


The second remark concerns the quadrature formula we use: we want to integrate over bilinear shape functions, so we know that we have to use at least an order two Gauss quadrature formula. On the other hand, we want the quadrature rule to have at least the order of the boundary approximation. Since the order of Gauss rule with  [2.x.48]  points is  [2.x.49] , and the order of the boundary approximation using polynomials of degree  [2.x.50]  is  [2.x.51] , we know that  [2.x.52] . Since r has to be an integer and (as mentioned above) has to be at least  [2.x.53] , this makes up for the formula above computing  [2.x.54] .      


Since the generation of the body force contributions to the right hand side vector was so simple, we do that all over again for the boundary forces as well: allocate a vector of the right size and call the right function. The boundary function has constant values, so we can generate an object from the library on the fly, and we use the same quadrature formula as above, but this time of lower dimension since we integrate over faces now instead of cells: 

[1.x.31] 



Then add the contributions from the boundary to those from the interior of the domain: 

[1.x.32] 



For assembling the right hand side, we had to use two different vector objects, and later add them together. The reason we had to do so is that the  [2.x.55]  and  [2.x.56]  functions first clear the output vector, rather than adding up their results to previous contents. This can reasonably be called a design flaw in the library made in its infancy, but unfortunately things are as they are for some time now and it is difficult to change such things that silently break existing code, so we have to live with that. 




Now, the linear system is set up, so we can eliminate the one degree of freedom which we constrained to the other DoFs on the boundary for the mean value constraint from matrix and right hand side vector, and solve the system. After that, distribute the constraints again, which in this case means setting the constrained degree of freedom to its proper value 

[1.x.33] 



Finally, evaluate what we got as solution. As stated in the introduction, we are interested in the H1 semi-norm of the solution. Here, as well, we have a function in the library that does this, although in a slightly non-obvious way: the  [2.x.57]  function integrates the norm of the difference between a finite element function and a continuous function. If we therefore want the norm of a finite element field, we just put the continuous function to zero. Note that this function, just as so many other ones in the library as well, has at least two versions, one which takes a mapping as argument (which we make us of here), and the one which we have used in previous examples which implicitly uses  [2.x.58] .  Also note that we take a quadrature formula of one degree higher, in order to avoid superconvergence effects where the solution happens to be especially close to the exact solution at certain points (we don't know whether this might be the case here, but there are cases known of this, and we just want to make sure): 

[1.x.34] 



Then, the function just called returns its results as a vector of values each of which denotes the norm on one cell. To get the global norm, we do the following: 

[1.x.35] 



Last task -- generate output: 

[1.x.36] 



The following function solving the linear system of equations is copied from  [2.x.59]  and is explained there in some detail: 

[1.x.37] 



Next, we write the solution as well as the material ids to a VTU file. This is similar to what was done in many other tutorial programs. The new ingredient presented in this tutorial program is that we want to ensure that the data written to the file used for visualization is actually a faithful representation of what is used internally by deal.II. That is because most of the visualization data formats only represent cells by their vertex coordinates, but have no way of representing the curved boundaries that are used in deal.II when using higher order mappings -- in other words, what you see in the visualization tool is not actually what you are computing on. (The same, incidentally, is true when using higher order shape functions: Most visualization tools only render bilinear/trilinear representations. This is discussed in detail in  [2.x.60]     


So we need to ensure that a high-order representation is written to the file. We need to consider two particular topics. Firstly, we tell the DataOut object via the  [2.x.61]  that we intend to interpret the subdivisions of the elements as a high-order Lagrange polynomial rather than a collection of bilinear patches. Recent visualization programs, like ParaView version 5.5 or newer, can then render a high-order solution (see a [1.x.38] for more details). Secondly, we need to make sure that the mapping is passed to the  [2.x.62]  method. Finally, the DataOut class only prints curved faces for [1.x.39] cells by default, so we need to ensure that also inner cells are printed in a curved representation via the mapping. 

[1.x.40] 



Finally the main function controlling the different steps to be performed. Its content is rather straightforward, generating a triangulation of a circle, associating a boundary to it, and then doing several cycles on subsequently finer grids. Note again that we have put mesh refinement into the loop header; this may be something for a test program, but for real applications you should consider that this implies that the mesh is refined after the loop is executed the last time since the increment clause (the last part of the three-parted loop header) is executed before the comparison part (the second one), which may be rather costly if the mesh is already quite refined. In that case, you should arrange code such that the mesh is not further refined after the last loop run (or you should do it at the beginning of each run except for the first one). 

[1.x.41] 



After all the data is generated, write a table of results to the screen: 

[1.x.42] 



Finally the main function. It's structure is the same as that used in several of the previous examples, so probably needs no more explanation. 

[1.x.43] 



This is the main loop, doing the computations with mappings of linear through cubic mappings. Note that since we need the object of type  [2.x.63]  only once, we do not even name it, but create an unnamed such object and call the  [2.x.64]  function of it, subsequent to which it is immediately destroyed again. 

[1.x.44] 

[1.x.45][1.x.46] 


This is what the program outputs: 

[1.x.47] 

As we expected, the convergence order for each of the different mappings is clearly quadratic in the mesh size. What  [2.x.65] is [2.x.66]  interesting, though, is that the error for a bilinear mapping (i.e. degree 1) is more than three times larger than that for the higher order mappings; it is therefore clearly advantageous in this case to use a higher order mapping, not because it improves the order of convergence but just to reduce the constant before the convergence order. On the other hand, using a cubic mapping only improves the result further by insignificant amounts, except on very coarse grids. 

We can also visualize the underlying meshes by using, for instance, ParaView. The image below shows initial meshes for different mapping degrees: 

 [2.x.67]  

Clearly, the effect is most pronounced when we go from the linear to quadratic mapping. This is also reflected in the error values given in the table above. The effect of going from quadratic to cubic degree is less dramatic, but still tangible owing to a more accurate description of the circular boundary. 

Next, let's look at the meshes after three global refinements 

 [2.x.68]  

Here, the differences are much less visible, especially for higher order mappings. Indeed, at this refinement level the error values reported in the table are essentially identical between mappings of degrees two and three. [1.x.48] [1.x.49]  [2.x.69]  

 [2.x.70] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15] 

 [2.x.3]  

[1.x.16] 

[1.x.17] [1.x.18][1.x.19] 


[1.x.20][1.x.21] 


This example is devoted to the  [2.x.4] discontinuous Galerkin method [2.x.5] , or in short, the DG method. It includes the following topics.  [2.x.6]     [2.x.7]  Discretization of the linear advection equation with the DG method.    [2.x.8]  Assembling of jump terms and other expressions on the interface between cells using FEInterfaceValues.    [2.x.9]  Assembling of the system matrix using the  [2.x.10]   [2.x.11]  

The particular concern of this program are the loops of DG methods. These turn out to be especially complex, primarily because for the face terms, we have to distinguish the cases of boundary, regular interior faces and interior faces with hanging nodes, respectively. The  [2.x.12]  handles the complexity on iterating over cells and faces and allows specifying "workers" for the different cell and face terms. The integration of face terms itself, including on adaptively refined faces, is done using the FEInterfaceValues class. 

[1.x.22][1.x.23] 


The model problem solved in this example is the linear advection equation [1.x.24] subject to the boundary conditions [1.x.25] on the inflow part  [2.x.13]  of the boundary  [2.x.14]  of the domain.  Here,  [2.x.15]  denotes a vector field,  [2.x.16]  the (scalar) solution function,  [2.x.17]  a boundary value function, [1.x.26] the inflow part of the boundary of the domain and  [2.x.18]  denotes the unit outward normal to the boundary  [2.x.19] . This equation is the conservative version of the advection equation already considered in  [2.x.20]  of this tutorial. 


On each cell  [2.x.21] , we multiply by a test function  [2.x.22]  from the left and integrate by parts to get: [1.x.27] When summing this expression over all cells  [2.x.23] , the boundary integral is done over all internal and external faces and as such there are three cases:  [2.x.24]   [2.x.25]  outer boundary on the inflow (we replace  [2.x.26]  by given  [2.x.27] ):    [2.x.28]   [2.x.29]  outer boundary on the outflow:    [2.x.30]   [2.x.31]  inner faces (integral from two sides turns into jump, we use the upwind velocity):    [2.x.32]   [2.x.33]  

Here, the jump is defined as  [2.x.34] , where the superscripts refer to the left ('+') and right ('-') values at the face. The upwind value  [2.x.35]  is defined to be  [2.x.36]  if  [2.x.37]  and  [2.x.38]  otherwise. 

As a result, the mesh-dependent weak form reads: [1.x.28] Here,  [2.x.39]  is the set of all active cells of the triangulation and  [2.x.40]  is the set of all active interior faces. This formulation is known as the upwind discontinuous Galerkin method. 

In order to implement this bilinear form, we need to compute the cell terms (first sum) using the usual way to achieve integration on a cell, the interface terms (second sum) using FEInterfaceValues, and the boundary terms (the other two terms). The summation of all those is done by  [2.x.41]  




[1.x.29][1.x.30] 


We solve the advection equation on  [2.x.42]  with  [2.x.43]  representing a circular counterclockwise flow field, and  [2.x.44]  on  [2.x.45]  and  [2.x.46]  on  [2.x.47] . 

We solve on a sequence of meshes by refining the mesh adaptively by estimating the norm of the gradient on each cell. After solving on each mesh, we output the solution in vtk format and compute the  [2.x.48]  norm of the solution. As the exact solution is either 0 or 1, we can measure the magnitude of the overshoot of the numerical solution with this. [1.x.31] [1.x.32] 

The first few files have already been covered in previous examples and will thus not be further commented on: 

[1.x.33] 



Here the discontinuous finite elements are defined. They are used in the same way as all other finite elements, though -- as you have seen in previous tutorial programs -- there isn't much user interaction with finite element classes at all: they are passed to  [2.x.49]  and  [2.x.50]  objects, and that is about it. 

[1.x.34] 



This header is needed for FEInterfaceValues to compute integrals on interfaces: 

[1.x.35] 



We are going to use the simplest possible solver, called Richardson iteration, that represents a simple defect correction. This, in combination with a block SSOR preconditioner (defined in precondition_block.h), that uses the special block matrix structure of system matrices arising from DG discretizations. 

[1.x.36] 



We are going to use gradients as refinement indicator. 

[1.x.37] 



Finally, the new include file for using the mesh_loop from the MeshWorker framework 

[1.x.38] 



Like in all programs, we finish this section by including the needed C++ headers and declaring we want to use objects in the dealii namespace without prefix. 

[1.x.39] 




[1.x.40]  [1.x.41]    


First, we define a class describing the inhomogeneous boundary data. Since only its values are used, we implement value_list(), but leave all other functions of Function undefined. 

[1.x.42] 



Given the flow direction, the inflow boundary of the unit square  [2.x.51]  are the right and the lower boundaries. We prescribe discontinuous boundary values 1 and 0 on the x-axis and value 0 on the right boundary. The values of this function on the outflow boundaries will not be used within the DG scheme. 

[1.x.43] 



Finally, a function that computes and returns the wind field  [2.x.52] . As explained in the introduction, we will use a rotational field around the origin in 2d. In 3d, we simply leave the  [2.x.53] -component unset (i.e., at zero), whereas the function can not be used in 1d in its current implementation: 

[1.x.44] 




[1.x.45]  [1.x.46]    


The following objects are the scratch and copy objects we use in the call to  [2.x.54]  The new object is the FEInterfaceValues object, that works similar to FEValues or FEFacesValues, except that it acts on an interface between two cells and allows us to assemble the interface terms in our weak form. 







[1.x.47] 




[1.x.48]  [1.x.49]    


After this preparations, we proceed with the main class of this program, called AdvectionProblem.    


This should all be pretty familiar to you. Interesting details will only come up in the implementation of the assemble function. 

[1.x.50] 



Furthermore we want to use DG elements. 

[1.x.51] 



The next four members represent the linear system to be solved.  [2.x.55]  are generated by  [2.x.56]  is computed in  [2.x.57]  is used to determine the location of nonzero elements in  [2.x.58] . 

[1.x.52] 



We start with the constructor. The 1 in the constructor call of  [2.x.59]  is the polynomial degree. 

[1.x.53] 



In the function that sets up the usual finite element data structures, we first need to distribute the DoFs. 

[1.x.54] 



We start by generating the sparsity pattern. To this end, we first fill an intermediate object of type DynamicSparsityPattern with the couplings appearing in the system. After building the pattern, this object is copied to  [2.x.60]  and can be discarded. 




To build the sparsity pattern for DG discretizations, we can call the function analogue to  [2.x.61]  which is called  [2.x.62]  

[1.x.55] 



Finally, we set up the structure of all components of the linear system. 

[1.x.56] 




[1.x.57]  [1.x.58] 




Here we see the major difference to assembling by hand. Instead of writing loops over cells and faces, the logic is contained in the call to  [2.x.63]  and we only need to specify what should happen on each cell, each boundary face, and each interior face. These three tasks are handled by the lambda functions inside the function below. 







[1.x.59] 



This is the function that will be executed for each cell. 

[1.x.60] 



We solve a homogeneous equation, thus no right hand side shows up in the cell term.  What's left is integrating the matrix entries. 

[1.x.61] 



This is the function called for boundary faces and consists of a normal integration using FEFaceValues. New is the logic to decide if the term goes into the system matrix (outflow) or the right-hand side (inflow). 

[1.x.62] 



This is the function called on interior faces. The arguments specify cells, face and subface indices (for adaptive refinement). We just pass them along to the reinit() function of FEInterfaceValues. 

[1.x.63] 



The following lambda function will handle copying the data from the cell and face assembly into the global matrix and right-hand side.      


While we would not need an AffineConstraints object, because there are no hanging node constraints in DG discretizations, we use an empty object here as this allows us to use its `copy_local_to_global` functionality. 

[1.x.64] 



Here, we finally handle the assembly. We pass in ScratchData and CopyData objects, the lambda functions from above, an specify that we want to assemble interior faces once. 

[1.x.65] 




[1.x.66]  [1.x.67]    


For this simple problem we use the simplest possible solver, called Richardson iteration, that represents a simple defect correction. This, in combination with a block SSOR preconditioner, that uses the special block matrix structure of system matrices arising from DG discretizations. The size of these blocks are the number of DoFs per cell. Here, we use a SSOR preconditioning as we have not renumbered the DoFs according to the flow field. If the DoFs are renumbered in the downstream direction of the flow, then a block Gauss-Seidel preconditioner (see the PreconditionBlockSOR class with relaxation=1) does a much better job. 

[1.x.68] 



Here we create the preconditioner, 

[1.x.69] 



then assign the matrix to it and set the right block size: 

[1.x.70] 



After these preparations we are ready to start the linear solver. 

[1.x.71] 



We refine the grid according to a very simple refinement criterion, namely an approximation to the gradient of the solution. As here we consider the DG(1) method (i.e. we use piecewise bilinear shape functions) we could simply compute the gradients on each cell. But we do not want to base our refinement indicator on the gradients on each cell only, but want to base them also on jumps of the discontinuous solution function over faces between neighboring cells. The simplest way of doing that is to compute approximative gradients by difference quotients including the cell under consideration and its neighbors. This is done by the  [2.x.64]  class that computes the approximate gradients in a way similar to the  [2.x.65]  described in  [2.x.66]  of this tutorial. In fact, the  [2.x.67]  class was developed following the  [2.x.68]  class of  [2.x.69] . Relating to the discussion in  [2.x.70] , here we consider  [2.x.71] . Furthermore we note that we do not consider approximate second derivatives because solutions to the linear advection equation are in general not in  [2.x.72]  but only in  [2.x.73]  (or, to be more precise: in  [2.x.74] , i.e., the space of functions whose derivatives in direction  [2.x.75]  are square integrable). 

[1.x.72] 



The  [2.x.76]  class computes the gradients to float precision. This is sufficient as they are approximate and serve as refinement indicators only. 

[1.x.73] 



Now the approximate gradients are computed 

[1.x.74] 



and they are cell-wise scaled by the factor  [2.x.77]  

[1.x.75] 



Finally they serve as refinement indicator. 

[1.x.76] 



The output of this program consists of a vtk file of the adaptively refined grids and the numerical solutions. Finally, we also compute the L-infinity norm of the solution using  [2.x.78]  

[1.x.77] 



The following  [2.x.79]  function is similar to previous examples. 

[1.x.78] 



The following  [2.x.80]  function is similar to previous examples as well, and need not be commented on. 

[1.x.79] 

[1.x.80][1.x.81] 




The output of this program consist of the console output and solutions in vtk format: 

[1.x.82] 



We show the solutions on the initial mesh, the mesh after two and after five adaptive refinement steps. 

 [2.x.81]   [2.x.82]   [2.x.83]  

And finally we show a plot of a 3d computation. 

 [2.x.84]  


[1.x.83] [1.x.84][1.x.85] 


In this program we have used discontinuous elements. It is a legitimate question to ask why not simply use the normal, continuous ones. Of course, to everyone with a background in numerical methods, the answer is obvious: the continuous Galerkin (cG) method is not stable for the transport equation, unless one specifically adds stabilization terms. The DG method, however, [1.x.86] stable. Illustrating this with the current program is not very difficult; in fact, only the following minor modifications are necessary: 

- Change the element to FE_Q instead of FE_DGQ. 

- Add handling of hanging node constraints in exactly the same way as  [2.x.85] . 

- We need a different solver; the direct solver in  [2.x.86]  is a convenient   choice. An experienced deal.II user will be able to do this in less than 10 minutes. 

While the 2d solution has been shown above, containing a number of small spikes at the interface that are, however, stable in height under mesh refinement, results look much different when using a continuous element: 

 [2.x.87]  

In refinement iteration 5, the image can't be plotted in a reasonable way any more as a 3d plot. We thus show a color plot with a range of  [2.x.88]  (the solution values of the exact solution lie in  [2.x.89] , of course). In any case, it is clear that the continuous Galerkin solution exhibits oscillatory behavior that gets worse and worse as the mesh is refined more and more. 

There are a number of strategies to stabilize the cG method, if one wants to use continuous elements for some reason. Discussing these methods is beyond the scope of this tutorial program; an interested reader could, for example, take a look at  [2.x.90] . 




[1.x.87] [1.x.88][1.x.89] 


Given that the exact solution is known in this case, one interesting avenue for further extensions would be to confirm the order of convergence for this program. In the current case, the solution is non-smooth, and so we can not expect to get a particularly high order of convergence, even if we used higher order elements. But even if the solution [1.x.90] smooth, the equation is not elliptic and so it is not immediately clear that we should obtain a convergence order that equals that of the optimal interpolation estimates (i.e. for example that we would get  [2.x.91]  convergence in the  [2.x.92]  norm by using quadratic elements). 

In fact, for hyperbolic equations, theoretical predictions often indicate that the best one can hope for is an order one half below the interpolation estimate. For example, for the streamline diffusion method (an alternative method to the DG method used here to stabilize the solution of the transport equation), one can prove that for elements of degree  [2.x.93] , the order of convergence is  [2.x.94]  on arbitrary meshes. While the observed order is frequently  [2.x.95]  on uniformly refined meshes, one can construct so-called Peterson meshes on which the worse theoretical bound is actually attained. This should be relatively simple to verify, for example using the  [2.x.96]  function. 

A different direction is to observe that the solution of transport problems often has discontinuities and that therefore a mesh in which we [1.x.91] every cell in every coordinate direction may not be optimal. Rather, a better strategy would be to only cut cells in the direction parallel to the discontinuity. This is called [1.x.92] and is the subject of  [2.x.97] . [1.x.93] [1.x.94]  [2.x.98]  

 [2.x.99] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19] 

[1.x.20] [1.x.21][1.x.22] 


[1.x.23][1.x.24] 




In this example program, we will not so much be concerned with describing new ways how to use deal.II and its facilities, but rather with presenting methods of writing modular and extensible finite element programs. The main reason for this is the size and complexity of modern research software: applications implementing modern error estimation concepts and adaptive solution methods tend to become rather large. For example, when this program was written in 2002, the three largest applications by the main authors of deal.II, are at the time of writing of this example program:  [2.x.3]   [2.x.4]  a program for solving conservation hyperbolic equations by the      Discontinuous Galerkin Finite Element method: 33,775 lines of      code;  [2.x.5]  a parameter estimation program: 28,980 lines of code;  [2.x.6]  a wave equation solver: 21,020 lines of code.  [2.x.7]  

(The library proper - without example programs and test suite - has slightly more than 150,000 lines of code as of spring 2002. It is of course several times larger now.) The sizes of these applications are at the edge of what one person, even an experienced programmer, can manage. 




The numbers above make one thing rather clear: monolithic programs that are not broken up into smaller, mostly independent pieces have no way of surviving, since even the author will quickly lose the overview of the various dependencies between different parts of a program. Only data encapsulation, for example using object oriented programming methods, and modularization by defining small but fixed interfaces can help structure data flow and mutual interdependencies. It is also an absolute prerequisite if more than one person is developing a program, since otherwise confusion will quickly prevail as one developer would need to know if another changed something about the internals of a different module if they were not cleanly separated. 




In previous examples, you have seen how the library itself is broken up into several complexes each building atop the underlying ones, but relatively independent of the other ones:  [2.x.8]   [2.x.9] the triangulation class complex, with associated iterator classes;  [2.x.10] the finite element classes;  [2.x.11] the DoFHandler class complex, with associated iterators, built on     the triangulation and finite element classes;  [2.x.12] the classes implementing mappings between unit and real cells;  [2.x.13] the FEValues class complex, built atop the finite elements and     mappings.  [2.x.14]  Besides these, and a large number of smaller classes, there are of course the following "tool" modules:  [2.x.15]   [2.x.16] output in various graphical formats;  [2.x.17] linear algebra classes.  [2.x.18]  These complexes can also be found as a flow chart on the front page of the deal.II manual website. 




The goal of this program is now to give an example of how a relatively simple finite element program could be structured such that we end up with a set of modules that are as independent of each other as possible. This allows to change the program at one end, without having to worry that it might break at the other, as long as we do not touch the interface through which the two ends communicate. The interface in C++, of course, is the declaration of abstract base classes. 




Here, we will implement (again) a Laplace solver, although with a number of differences compared to previous example programs:  [2.x.19]   [2.x.20] The classes that implement the process of numerically solving the     equation are no more responsible for driving the process of     "solving-estimating error-refining-solving again", but we delegate     this to external functions. This allows first to use it as a     building block in a larger context, where the solution of a     Laplace equation might only be one part (for example, in a     nonlinear problem, where Laplace equations might have to be solved     in each nonlinear step). It would also allow to build a framework     around this class that would allow using solvers for other     equations (but with the same external interface) instead, in case     some techniques shall be evaluated for different types of partial     differential equations.  [2.x.21] It splits the process of evaluating the computed solution to a     separate set of classes. The reason is that one is usually not     interested in the solution of a PDE per se, but rather in certain     aspects of it. For example, one might wish to compute the traction     at a certain boundary in elastic computations, or in the signal of     a seismic wave at a receiver position at a given     location. Sometimes, one might have an interest in several of     these aspects. Since the evaluation of a solution is something     that does not usually affect the process of solution, we split it     off into a separate module, to allow for the development of such     evaluation filters independently of the development of the solver     classes.  [2.x.22] Separate the classes that implement mesh refinement from the     classes that compute the solution.  [2.x.23] Separate the description of the test case with which we will     present the program, from the rest of the program.  [2.x.24] Parallelize the assembly of linear systems using the WorkStream     facilities. This follows the extensive description that can be     found in the  [2.x.25]  "Parallel computing with multiple processors accessing shared memory"     documentation module. The implementation essentially follows what     has already been described in  [2.x.26] .  [2.x.27]  




The things the program does are not new. In fact, this is more like a melange of previous programs, cannibalizing various parts and functions from earlier examples. It is the way they are arranged in this program that should be the focus of the reader, i.e. the software design techniques used in the program to achieve the goal of implementing the desired mathematical method. However, we must stress that software design is in part also a subjective matter: different persons have different programming backgrounds and have different opinions about the "right" style of programming; this program therefore expresses only what the author considers useful practice, and is not necessarily a style that you have to adopt in order to write successful numerical software if you feel uncomfortable with the chosen ways. It should serve as a case study, however, inspiring the reader with ideas to the desired end. 




Once you have worked through the program, you will remark that it is already somewhat complex in its structure. Nevertheless, it only has about 850 lines of code, without comments. In real applications, there would of course be comments and class documentation, which would bring that to maybe 1200 lines. Yet, compared to the applications listed above, this is still small, as they are 20 to 25 times as large. For programs as large, a proper design right from the start is thus indispensable. Otherwise, it will have to be redesigned at one point in its life, once it becomes too large to be manageable. 




Despite of this, all three programs listed above have undergone major revisions, or even rewrites. The wave program, for example, was once entirely teared to parts when it was still significantly smaller, just to assemble it again in a more modular form. By that time, it had become impossible to add functionality without affecting older parts of the code (the main problem with the code was the data flow: in time dependent application, the major concern is when to store data to disk and when to reload it again; if this is not done in an organized fashion, then you end up with data released too early, loaded too late, or not released at all). Although the present example program thus draws from several years of experience, it is certainly not without flaws in its design, and in particular might not be suited for an application where the objective is different. It should serve as an inspiration for writing your own application in a modular way, to avoid the pitfalls of too closely coupled codes. 




[1.x.25][1.x.26] 




What the program actually does is not even the main point of this program, the structure of the program is more important. However, in a few words, a description would be: solve the Laplace equation for a given right hand side such that the solution is the function  [2.x.28] . The goal of the computation is to get the value of the solution at the point  [2.x.29] , and to compare the accuracy with which we resolve this value for two refinement criteria, namely global refinement and refinement by the error indicator by Kelly et al. which we have already used in previous examples. 




The results will, as usual, be discussed in the respective section of this document. In doing so, we will find a slightly irritating observation about the relative performance of the two refinement criteria. In a later example program, building atop this one, we will devise a different method that should hopefully perform better than the techniques discussed here. 




So much now for all the theoretical and anecdotal background. The best way of learning about a program is to look at it, so here it is: [1.x.27] [1.x.28] 

As in all programs, we start with a list of include files from the library, and as usual they are in the standard order which is  [2.x.30]  --  [2.x.31]  --  [2.x.32]  (as each of these categories roughly builds upon previous ones), then C++ standard headers: 

[1.x.29] 



Now for the C++ standard headers: 

[1.x.30] 



The last step is as in all previous programs: 

[1.x.31] 




[1.x.32]  [1.x.33] 




As for the program itself, we first define classes that evaluate the solutions of a Laplace equation. In fact, they can evaluate every kind of solution, as long as it is described by a  [2.x.33]  object, and a solution vector. We define them here first, even before the classes that actually generate the solution to be evaluated, since we need to declare an abstract base class that the solver classes can refer to.    


From an abstract point of view, we declare a pure base class that provides an evaluation operator() which will do the evaluation of the solution (whatever derived classes might consider an  [2.x.34] ). Since this is the only real function of this base class (except for some bookkeeping machinery), one usually terms such a class that only has an  [2.x.35]  a  [2.x.36]  in C++ terminology, since it is used just like a function object.    


Objects of this functor type will then later be passed to the solver object, which applies it to the solution just computed. The evaluation objects may then extract any quantity they like from the solution. The advantage of putting these evaluation functions into a separate hierarchy of classes is that by design they cannot use the internals of the solver object and are therefore independent of changes to the way the solver works. Furthermore, it is trivial to write another evaluation class without modifying the solver class, which speeds up programming (not being able to use internals of another class also means that you do not have to worry about them -- programming evaluators is usually a rather quickly done task), as well as compilation (if solver and evaluation classes are put into different files: the solver only needs to see the declaration of the abstract base class, and therefore does not need to be recompiled upon addition of a new evaluation class, or modification of an old one).  On a related note, you can reuse the evaluation classes for other projects, solving different equations.    


In order to improve separation of code into different modules, we put the evaluation classes into a namespace of their own. This makes it easier to actually solve different equations in the same program, by assembling it from existing building blocks. The reason for this is that classes for similar purposes tend to have the same name, although they were developed in different contexts. In order to be able to use them together in one program, it is necessary that they are placed in different namespaces. This we do here: 

[1.x.34] 



Now for the abstract base class of evaluation classes: its main purpose is to declare a pure virtual function  [2.x.37]  taking a  [2.x.38]  object, and the solution vector. In order to be able to use pointers to this base class only, it also has to declare a virtual destructor, which however does nothing. Besides this, it only provides for a little bit of bookkeeping: since we usually want to evaluate solutions on subsequent refinement levels, we store the number of the present refinement cycle, and provide a function to change this number. 

[1.x.35] 




[1.x.36]  [1.x.37] 




The next thing is to implement actual evaluation classes. As noted in the introduction, we'd like to extract a point value from the solution, so the first class does this in its  [2.x.39] . The actual point is given to this class through the constructor, as well as a table object into which it will put its findings.      


Finding out the value of a finite element field at an arbitrary point is rather difficult, if we cannot rely on knowing the actual finite element used, since then we cannot, for example, interpolate between nodes. For simplicity, we therefore assume here that the point at which we want to evaluate the field is actually a node. If, in the process of evaluating the solution, we find that we did not encounter this point upon looping over all vertices, we then have to throw an exception in order to signal to the calling functions that something has gone wrong, rather than silently ignore this error.      


In the  [2.x.40]  example program, we have already seen how such an exception class can be declared, using the  [2.x.41]  macros. We use this mechanism here again.      


From this, the actual declaration of this class should be evident. Note that of course even if we do not list a destructor explicitly, an implicit destructor is generated from the compiler, and it is virtual just as the one of the base class. 

[1.x.38] 



As for the definition, the constructor is trivial, just taking data and storing it in object-local ones: 

[1.x.39] 



Now for the function that is mainly of interest in this class, the computation of the point value: 

[1.x.40] 



First allocate a variable that will hold the point value. Initialize it with a value that is clearly bogus, so that if we fail to set it to a reasonable value, we will note at once. This may not be necessary in a function as small as this one, since we can easily see all possible paths of execution here, but it proved to be helpful for more complex cases, and so we employ this strategy here as well. 

[1.x.41] 



Then loop over all cells and all their vertices, and check whether a vertex matches the evaluation point. If this is the case, then extract the point value, set a flag that we have found the point of interest, and exit the loop. 

[1.x.42] 



In order to extract the point value from the global solution vector, pick that component that belongs to the vertex of interest, and, in case the solution is vector-valued, take the first component of it: 

[1.x.43] 



Note that by this we have made an assumption that is not valid always and should be documented in the class declaration if this were code for a real application rather than a tutorial program: we assume that the finite element used for the solution we try to evaluate actually has degrees of freedom associated with vertices. This, for example, does not hold for discontinuous elements, were the support points for the shape functions happen to be located at the vertices, but are not associated with the vertices but rather with the cell interior, since association with vertices would imply continuity there. It would also not hold for edge oriented elements, and the like.                  


Ideally, we would check this at the beginning of the function, for example by a statement like <code>Assert (dof_handler.get_fe().dofs_per_vertex @> 0, ExcNotImplemented())</code>, which should make it quite clear what is going wrong when the exception is triggered. In this case, we omit it (which is indeed bad style), but knowing that that does not hurt here, since the statement  [2.x.42]  would fail if we asked it to give us the DoF index of a vertex if there were none.                  


We stress again that this restriction on the allowed finite elements should be stated in the class documentation. 




Since we found the right point, we now set the respective flag and exit the innermost loop. The outer loop will also be terminated due to the set flag. 

[1.x.44] 



Finally, we'd like to make sure that we have indeed found the evaluation point, since if that were not so we could not give a reasonable value of the solution there and the rest of the computations were useless anyway. So make sure through the  [2.x.43]  macro already used in the  [2.x.44]  program that we have indeed found this point. If this is not so, the macro throws an exception of the type that is given to it as second argument, but compared to a straightforward  [2.x.45]  statement, it fills the exception object with a set of additional information, for example the source file and line number where the exception was generated, and the condition that failed. If you have a  [2.x.46]  clause in your main function (as this program has), you will catch all exceptions that are not caught somewhere in between and thus already handled, and this additional information will help you find out what happened and where it went wrong. 

[1.x.45] 



Note that we have used the  [2.x.47]  macro in other example programs as well. It differed from the  [2.x.48]  macro used here in that it simply aborts the program, rather than throwing an exception, and that it did so only in debug mode. It was the right macro to use to check about the size of vectors passed as arguments to functions, and the like.        


However, here the situation is different: whether we find the evaluation point or not may change from refinement to refinement (for example, if the four cells around point are coarsened away, then the point may vanish after refinement and coarsening). This is something that cannot be predicted from a few number of runs of the program in debug mode, but should be checked always, also in production runs. Thus the use of the  [2.x.49]  macro here. 




Now, if we are sure that we have found the evaluation point, we can add the results into the table of results: 

[1.x.46] 




[1.x.47]  [1.x.48] 




A different, maybe slightly odd kind of  [2.x.50]  of a solution is to output it to a file in a graphical format. Since in the evaluation functions we are given a  [2.x.51]  object and the solution vector, we have all we need to do this, so we can do it in an evaluation class. The reason for actually doing so instead of putting it into the class that computed the solution is that this way we have more flexibility: if we choose to only output certain aspects of it, or not output it at all. In any case, we do not need to modify the solver class, we just have to modify one of the modules out of which we build this program. This form of encapsulation, as above, helps us to keep each part of the program rather simple as the interfaces are kept simple, and no access to hidden data is possible.      


Since this class which generates the output is derived from the common  [2.x.52]  base class, its main interface is the  [2.x.53]  function. Furthermore, it has a constructor taking a string that will be used as the base part of the file name to which output will be sent (we will augment it by a number indicating the number of the refinement cycle -- the base class has this information at hand --, and a suffix), and the constructor also takes a value that indicates which format is requested, i.e. for which graphics program we shall generate output (from this we will then also generate the suffix of the filename to which we write).      


Regarding the output format, the DataOutBase namespace provides an enumeration field  [2.x.54]  which lists names for all supported output formats. At the time of writing of this program, the supported graphics formats are represented by the enum values  [2.x.55] ,  [2.x.56] ,  [2.x.57] ,  [2.x.58] , etc, but this list will certainly grow over time. Now, within various functions of that base class, you can use values of this type to get information about these graphics formats (for example the default suffix used for files of each format), and you can call a generic  [2.x.59]  function, which then branches to the  [2.x.60] , etc functions which we have used in previous examples already, based on the value of a second argument given to it denoting the required output format. This mechanism makes it simple to write an extensible program that can decide which output format to use at runtime, and it also makes it rather simple to write the program in a way such that it takes advantage of newly implemented output formats, without the need to change the application program.      


Of these two fields, the base name and the output format descriptor, the constructor takes values and stores them for later use by the actual evaluation function. 

[1.x.49] 



Following the description above, the function generating the actual output is now relatively straightforward. The only particularly interesting feature over previous example programs is the use of the  [2.x.61]  function, returning the usual suffix for files of a given format (e.g. ".eps" for encapsulated postscript files, ".gnuplot" for Gnuplot files), and of the generic  [2.x.62]  function with a second argument, which internally branches to the actual output functions for the different graphics formats, based on the value of the format descriptor passed as second argument.      


Also note that we have to prefix  [2.x.63]  to access a member variable of the template dependent base class. The reason here, and further down in the program is the same as the one described in the  [2.x.64]  example program (look for  [2.x.65]  there). 

[1.x.50] 




[1.x.51]  [1.x.52] 




In practical applications, one would add here a list of other possible evaluation classes, representing quantities that one may be interested in. For this example, that much shall be sufficient, so we close the namespace. 

[1.x.53] 




[1.x.54]  [1.x.55] 




After defining what we want to know of the solution, we should now care how to get at it. We will pack everything we need into a namespace of its own, for much the same reasons as for the evaluations above.    


Since we have discussed Laplace solvers already in considerable detail in previous examples, there is not much new stuff following. Rather, we have to a great extent cannibalized previous examples and put them, in slightly different form, into this example program. We will therefore mostly be concerned with discussing the differences to previous examples.    


Basically, as already said in the introduction, the lack of new stuff in this example is deliberate, as it is more to demonstrate software design practices, rather than mathematics. The emphasis in explanations below will therefore be more on the actual implementation. 

[1.x.56] 




[1.x.57]  [1.x.58] 




In defining a Laplace solver, we start out by declaring an abstract base class, that has no functionality itself except for taking and storing a pointer to the triangulation to be used later.      


This base class is very general, and could as well be used for any other stationary problem. It provides declarations of functions that shall, in derived classes, solve a problem, postprocess the solution with a list of evaluation objects, and refine the grid, respectively. None of these functions actually does something itself in the base class.      


Due to the lack of actual functionality, the programming style of declaring very abstract base classes is similar to the style used in Smalltalk or Java programs, where all classes are derived from entirely abstract classes  [2.x.66] , even number representations. The author admits that he does not particularly like the use of such a style in C++, as it puts style over reason. Furthermore, it promotes the use of virtual functions for everything (for example, in Java, all functions are virtual per se), which, however, has proven to be rather inefficient in many applications where functions are often only accessing data, not doing computations, and therefore quickly return; the overhead of virtual functions can then be significant. The opinion of the author is to have abstract base classes wherever at least some part of the code of actual implementations can be shared and thus separated into the base class.      


Besides all these theoretical questions, we here have a good reason, which will become clearer to the reader below. Basically, we want to be able to have a family of different Laplace solvers that differ so much that no larger common subset of functionality could be found. We therefore just declare such an abstract base class, taking a pointer to a triangulation in the constructor and storing it henceforth. Since this triangulation will be used throughout all computations, we have to make sure that the triangulation is valid until it is last used. We do this by keeping a  [2.x.67]  to this triangulation, as explained in  [2.x.68] .      


Note that while the pointer itself is declared constant (i.e. throughout the lifetime of this object, the pointer points to the same object), it is not declared as a pointer to a constant triangulation. In fact, by this we allow that derived classes refine or coarsen the triangulation within the  [2.x.69]  function.      


Finally, we have a function  [2.x.70]  is only a tool for the driver functions to decide whether we want to go on with mesh refinement or not. It returns the number of degrees of freedom the present simulation has. 

[1.x.59] 



The implementation of the only two non-abstract functions is then rather boring: 

[1.x.60] 




[1.x.61]  [1.x.62] 




Following now the main class that implements assembling the matrix of the linear system, solving it, and calling the postprocessor objects on the solution. It implements the  [2.x.71]  and  [2.x.72]  functions declared in the base class. It does not, however, implement the  [2.x.73]  method, as mesh refinement will be implemented in a number of derived classes.      


It also declares a new abstract virtual function,  [2.x.74] , that needs to be overloaded in subclasses. The reason is that we will implement two different classes that will implement different methods to assemble the right hand side vector. This function might also be interesting in cases where the right hand side depends not simply on a continuous function, but on something else as well, for example the solution of another discretized problem, etc. The latter happens frequently in non-linear problems.      


As we mentioned previously, the actual content of this class is not new, but a mixture of various techniques already used in previous examples. We will therefore not discuss them in detail, but refer the reader to these programs.      


Basically, in a few words, the constructor of this class takes pointers to a triangulation, a finite element, and a function object representing the boundary values. These are either passed down to the base class's constructor, or are stored and used to generate a  [2.x.75]  object later. Since finite elements and quadrature formula should match, it is also passed a quadrature object.      


The  [2.x.76]  sets up the data structures for the actual solution, calls the functions to assemble the linear system, and solves it.      


The  [2.x.77]  function finally takes an evaluation object and applies it to the computed solution.      


The  [2.x.78]  function finally implements the pure virtual function of the base class. 

[1.x.63] 



In the protected section of this class, we first have a number of member variables, of which the use should be clear from the previous examples: 

[1.x.64] 



Then we declare an abstract function that will be used to assemble the right hand side. As explained above, there are various cases for which this action differs strongly in what is necessary, so we defer this to derived classes: 

[1.x.65] 



Next, in the private section, we have a small class which represents an entire linear system, i.e. a matrix, a right hand side, and a solution vector, as well as the constraints that are applied to it, such as those due to hanging nodes. Its constructor initializes the various subobjects, and there is a function that implements a conjugate gradient method as solver. 

[1.x.66] 



Finally, there is a set of functions which will be used to assemble the actual system matrix. The main function of this group,  [2.x.79]  computes the matrix in parallel on multicore systems, using the following two helper functions. The mechanism for doing so is the same as in the  [2.x.80]  example program and follows the WorkStream concept outlined in  [2.x.81]  . The main function also calls the virtual function assembling the right hand side. 

[1.x.67] 



Now here comes the constructor of the class. It does not do much except store pointers to the objects given, and generate  [2.x.82]  object initialized with the given pointer to a triangulation. This causes the DoF handler to store that pointer, but does not already generate a finite element numbering (we only ask for that in the  [2.x.83]  function). 

[1.x.68] 



The destructor is simple, it only clears the information stored in the DoF handler object to release the memory. 

[1.x.69] 



The next function is the one which delegates the main work in solving the problem: it sets up the DoF handler object with the finite element given to the constructor of this object, the creates an object that denotes the linear system (i.e. the matrix, the right hand side vector, and the solution vector), calls the function to assemble it, and finally solves it: 

[1.x.70] 



As stated above, the  [2.x.84]  function takes an evaluation object, and applies it to the computed solution. This function may be called multiply, once for each evaluation of the solution which the user required. 

[1.x.71] 



The  [2.x.85]  function should be self-explanatory: 

[1.x.72] 



The following function assembles matrix and right hand side of the linear system to be solved in each step. We will do things in parallel at a couple of levels. First, note that we need to assemble both the matrix and the right hand side. These are independent operations, and we should do this in parallel. To this end, we use the concept of "tasks" that is discussed in the  [2.x.86]  documentation module. In essence, what we want to say "here is something that needs to be worked on, go do it whenever a CPU core is available", then do something else, and when we need the result of the first operation wait for its completion. At the second level, we want to assemble the matrix using the exact same strategy we have already used in  [2.x.87] , namely the WorkStream concept.      


While we could consider either assembling the right hand side or assembling the matrix as the thing to do in the background while doing the other, we will opt for the former approach simply because the call to  [2.x.88]  is so much simpler to write than the call to  [2.x.89]  with its many arguments. In any case, the code then looks like this to assemble the entire linear system: 

[1.x.73] 



The syntax above requires some explanation. There are multiple version of  [2.x.90]  that expect different arguments. In  [2.x.91] , we used one version that took a pair of iterators, a pair of pointers to member functions with very specific argument lists, a pointer or reference to the object on which these member functions have to work, and a scratch and copy data object. This is a bit restrictive since the member functions called this way have to have an argument list that exactly matches what  [2.x.92]  expects: the local assembly function needs to take an iterator, a scratch object and a copy object; and the copy-local-to-global function needs to take exactly a copy object. But, what if we want something that's slightly more general? For example, in the current program, the copy-local-to-global function needs to know which linear system object to write the local contributions into, i.e., it also has to take a  [2.x.93]  argument. That won't work with the approach using member function pointers.        


Fortunately, C++ offers a way out. These are called function objects. In essence, what  [2.x.94]  wants to do is not call a member function. It wants to call some function that takes an iterator, a scratch object and a copy object in the first case, and a copy object in the second case. Whether these are member functions, global functions, or something else, is really not of much concern to WorkStream. Consequently, there is a second version of the function that just takes function objects -- objects that have an  [2.x.95]  and that consequently can be called like functions, whatever they really represent. The typical way to generate such function objects is using a [1.x.74] that wraps the function call including the individual arguments with fixed values. All the arguments that are part of the outer function signature are specified as regular function arguments in the lambda function. The fixed values are passed into the lambda function using the capture list (`[...]`). It is possible to use a capture default or to list all the variables that are to be bound to the lambda explicitly. For the sake of clarity we decided to omit the capture default here, but that capture list could equally well be `[&]`, meaning that all used variables are copied into the lambda by reference.        


At this point, we have assembled the matrix and condensed it. The right hand side may or may not have been completely assembled, but we would like to condense the right hand side vector next. We can only do this if the assembly of this vector has finished, so we have to wait for the task to finish; in computer science, waiting for a task is typically called "joining" the task, explaining the name of the function we call below.        


Since that task may or may not have finished, and since we may have to wait for it to finish, we may as well try to pack other things that need to be done anyway into this gap. Consequently, we first interpolate boundary values before we wait for the right hand side. Of course, another possibility would have been to also interpolate the boundary values on a separate task since doing so is independent of the other things we have done in this function so far. Feel free to find the correct syntax to also create a task for this interpolation and start it at the top of this function, along with the assembly of the right hand side. (You will find that this is slightly more complicated since there are multiple versions of  [2.x.96]  and so simply taking the address  [2.x.97]  produces a set of overloaded functions that can't be passed to  [2.x.98]  right away -- you have to select which element of this overload set you want by casting the address expression to a function pointer type that is specific to the version of the function that you want to call on the task.) 

[1.x.75] 



Now that we have the complete linear system, we can also treat boundary values, which need to be eliminated from both the matrix and the right hand side: 

[1.x.76] 



The second half of this set of functions deals with the local assembly on each cell and copying local contributions into the global matrix object. This works in exactly the same way as described in  [2.x.99] : 

[1.x.77] 



Now for the functions that implement actions in the linear system class. First, the constructor initializes all data elements to their correct sizes, and sets up a number of additional data structures, such as constraints due to hanging nodes. Since setting up the hanging nodes and finding out about the nonzero elements of the matrix is independent, we do that in parallel (if the library was configured to use concurrency, at least; otherwise, the actions are performed sequentially). Note that we start only one thread, and do the second action in the main thread. Since only one thread is generated, we don't use the  [2.x.100]  class here, but rather use the one created thread object directly to wait for this particular thread's exit.      


Note that taking up the address of the  [2.x.101]  function is a little tricky, since there are actually three of them, one for each supported space dimension. Taking addresses of overloaded functions is somewhat complicated in C++, since the address-of operator  [2.x.102]  in that case returns more like a set of values (the addresses of all functions with that name), and selecting the right one is then the next step. If the context dictates which one to take (for example by assigning to a function pointer of known type), then the compiler can do that by itself, but if this set of pointers shall be given as the argument to a function that takes a template, the compiler could choose all without having a preference for one. We therefore have to make it clear to the compiler which one we would like to have; for this, we could use a cast, but for more clarity, we assign it to a temporary  [2.x.103]  (short for <code>pointer to make_hanging_node_constraints</code>) with the right type, and using this pointer instead. 

[1.x.78] 



Start a side task then continue on the main thread 

[1.x.79] 



Wait for the side task to be done before going further 

[1.x.80] 



Finally initialize the matrix and right hand side vector 

[1.x.81] 



The second function of this class simply solves the linear system by a preconditioned conjugate gradient method. This has been extensively discussed before, so we don't dwell into it any more. 

[1.x.82] 




[1.x.83]  [1.x.84] 




In the previous section, a base class for Laplace solvers was implemented, that lacked the functionality to assemble the right hand side vector, however, for reasons that were explained there. Now we implement a corresponding class that can do this for the case that the right hand side of a problem is given as a function object.      


The actions of the class are rather what you have seen already in previous examples already, so a brief explanation should suffice: the constructor takes the same data as does that of the underlying class (to which it passes all information) except for one function object that denotes the right hand side of the problem. A pointer to this object is stored (again as a  [2.x.104] , in order to make sure that the function object is not deleted as long as it is still used by this class).      


The only functional part of this class is the  [2.x.105]  method that does what its name suggests. 

[1.x.85] 



The constructor of this class basically does what it is announced to do above... 

[1.x.86] 



... as does the  [2.x.106]  function. Since this is explained in several of the previous example programs, we leave it at that. 

[1.x.87] 




[1.x.88]  [1.x.89] 




By now, all functions of the abstract base class except for the  [2.x.107]  function have been implemented. We will now have two classes that implement this function for the  [2.x.108]  class, one doing global refinement, one a form of local refinement.      


The first, doing global refinement, is rather simple: its main function just calls  [2.x.109] , which does all the work.      


Note that since the  [2.x.110]  base class of the  [2.x.111]  class is virtual, we have to declare a constructor that initializes the immediate base class as well as the abstract virtual one.      


Apart from this technical complication, the class is probably simple enough to be left without further comments. 

[1.x.90] 




[1.x.91]  [1.x.92] 




The second class implementing refinement strategies uses the Kelly refinement indicator used in various example programs before. Since this indicator is already implemented in a class of its own inside the deal.II library, there is not much t do here except cal the function computing the indicator, then using it to select a number of cells for refinement and coarsening, and refinement the mesh accordingly.      


Again, this should now be sufficiently standard to allow the omission of further comments. 

[1.x.93] 




[1.x.94]  [1.x.95] 




As this is one more academic example, we'd like to compare exact and computed solution against each other. For this, we need to declare function classes representing the exact solution (for comparison and for the Dirichlet boundary values), as well as a class that denotes the right hand side of the equation (this is simply the Laplace operator applied to the exact solution we'd like to recover).    


For this example, let us choose as exact solution the function  [2.x.112] . In more than two dimensions, simply repeat the sine-factor with  [2.x.113]  and so on. Given this, the following two classes are probably straightforward from the previous examples. 

[1.x.96] 




[1.x.97]  [1.x.98] 




What is now missing are only the functions that actually select the various options, and run the simulation on successively finer grids to monitor the progress as the mesh is refined.    


This we do in the following function: it takes a solver object, and a list of postprocessing (evaluation) objects, and runs them with intermittent mesh refinement: 

[1.x.99] 



We will give an indicator of the step we are presently computing, in order to keep the user informed that something is still happening, and that the program is not in an endless loop. This is the head of this status line: 

[1.x.100] 



Then start a loop which only terminates once the number of degrees of freedom is larger than 20,000 (you may of course change this limit, if you need more -- or less -- accuracy from your program). 

[1.x.101] 



Then give the  [2.x.114]  indication for this iteration. Note that the  [2.x.115]  is needed to have the text actually appear on the screen, rather than only in some buffer that is only flushed the next time we issue an end-line. 

[1.x.102] 



Now solve the problem on the present grid, and run the evaluators on it. The long type name of iterators into the list is a little annoying, but could be shortened by an alias, if so desired. 

[1.x.103] 



Now check whether more iterations are required, or whether the loop shall be ended: 

[1.x.104] 



Finally end the line in which we displayed status reports: 

[1.x.105] 



The final function is one which takes the name of a solver (presently "kelly" and "global" are allowed), creates a solver object out of it using a coarse grid (in this case the ubiquitous unit square) and a finite element object (here the likewise ubiquitous bilinear one), and uses that solver to ask for the solution of the problem on a sequence of successively refined grids.    


The function also sets up two of evaluation functions, one evaluating the solution at the point (0.5,0.5), the other writing out the solution to a file. 

[1.x.106] 



First minor task: tell the user what is going to happen. Thus write a header line, and a line with all '-' characters of the same length as the first one right below. 

[1.x.107] 



Then set up triangulation, finite element, etc. 

[1.x.108] 



Create a solver object of the kind indicated by the argument to this function. If the name is not recognized, throw an exception! The respective solver object is stored in a  [2.x.116]  to avoid having to delete the pointer after use. 

[1.x.109] 



Next create a table object in which the values of the numerical solution at the point (0.5,0.5) will be stored, and create a respective evaluation object: 

[1.x.110] 



Also generate an evaluator which writes out the solution: 

[1.x.111] 



Take these two evaluation objects and put them in a list... 

[1.x.112] 



... which we can then pass on to the function that actually runs the simulation on successively refined grids: 

[1.x.113] 



When this all is done, write out the results of the point evaluations: 

[1.x.114] 



And one blank line after all results: 

[1.x.115] 



There is not much to say about the main function. It follows the same pattern as in all previous examples, with attempts to catch thrown exceptions, and displaying as much information as possible if we should get some. The rest is self-explanatory. 

[1.x.116] 

[1.x.117][1.x.118] 





The results of this program are not that interesting - after all its purpose was not to demonstrate some new mathematical idea, and also not how to program with deal.II, but rather to use the material which we have developed in the previous examples to form something which demonstrates a way to build modern finite element software in a modular and extensible way. 




Nevertheless, we of course show the results of the program. Of foremost interest is the point value computation, for which we had implemented the corresponding evaluation class. The results (i.e. the output) of the program looks as follows: 

[1.x.119] 




What surprises here is that the exact value is 1.59491554..., and that it is apparently surprisingly complicated to compute the solution even to only one per cent accuracy, although the solution is smooth (in fact infinitely often differentiable). This smoothness is shown in the graphical output generated by the program, here coarse grid and the first 9 refinement steps of the Kelly refinement indicator: 


 [2.x.117]  


While we're already at watching pictures, this is the eighth grid, as viewed from top: 


 [2.x.118]  


However, we are not yet finished with evaluation the point value computation. In fact, plotting the error  [2.x.119]  for the two refinement criteria yields the following picture: 


 [2.x.120]  





What  [2.x.121] is [2.x.122]  disturbing about this picture is that not only is the adaptive mesh refinement not better than global refinement as one would usually expect, it is even significantly worse since its convergence is irregular, preventing all extrapolation techniques when using the values of subsequent meshes! On the other hand, global refinement provides a perfect  [2.x.123]  or  [2.x.124]  convergence history and provides every opportunity to even improve on the point values by extrapolation. Global mesh refinement must therefore be considered superior in this example! This is even more surprising as the evaluation point is not somewhere in the left part where the mesh is coarse, but rather to the right and the adaptive refinement should refine the mesh around the evaluation point as well. 




We thus close the discussion of this example program with a question: 

<p align="center">   <strong> [2.x.125] What is wrong with adaptivity if it is not better than   global refinement? [2.x.126] </strong> 





 [2.x.127] Exercise at the end of this example: [2.x.128]  There is a simple reason for the bad and irregular behavior of the adapted mesh solutions. It is simple to find out by looking at the mesh around the evaluation point in each of the steps - the data for this is in the output files of the program. An exercise would therefore be to modify the mesh refinement routine such that the problem (once you remark it) is avoided. The second exercise is to check whether the results are then better than global refinement, and if so if even a better order of convergence (in terms of the number of degrees of freedom) is achieved, or only by a better constant. 




( [2.x.129] Very brief answers for the impatient: [2.x.130]  at steps with larger errors, the mesh is not regular at the point of evaluation, i.e. some of the adjacent cells have hanging nodes; this destroys some superapproximation effects of which the globally refined mesh can profit. Answer 2: this quick hack 

[1.x.120] 

in the refinement function of the Kelly refinement class right before executing refinement would improve the results (exercise: what does the code do?), making them consistently better than global refinement. Behavior is still irregular, though, so no results about an order of convergence are possible.) [1.x.121] [1.x.122]  [2.x.131]  

 [2.x.132] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43] 

[1.x.44] [1.x.45][1.x.46] 


[1.x.47][1.x.48] 


The Heidelberg group of Professor Rolf Rannacher, to which the three initial authors of the deal.II library belonged during their PhD time and partly also afterwards, has been involved with adaptivity and error estimation for finite element discretizations since the mid-1990ies. The main achievement is the development of error estimates for arbitrary functionals of the solution, and of optimal mesh refinement for its computation. 

We will not discuss the derivation of these concepts in too great detail, but will implement the main ideas in the present example program. For a thorough introduction into the general idea, we refer to the seminal work of Becker and Rannacher  [2.x.3] ,  [2.x.4] , and the overview article of the same authors in Acta Numerica  [2.x.5] ; the first introduces the concept of error estimation and adaptivity for general functional output for the Laplace equation, while the second gives many examples of applications of these concepts to a large number of other, more complicated equations. For applications to individual types of equations, see also the publications by Becker  [2.x.6] ,  [2.x.7] , Kanschat  [2.x.8] ,  [2.x.9] , Suttmeier  [2.x.10] ,  [2.x.11] ,  [2.x.12] ,  [2.x.13] , Bangerth  [2.x.14] ,  [2.x.15] ,  [2.x.16] ,  [2.x.17] , and Hartmann  [2.x.18] ,  [2.x.19] ,  [2.x.20] . All of these works, from the original introduction by Becker and Rannacher to individual contributions to particular equations, have later been summarized in a book by Bangerth and Rannacher that covers all of these topics, see  [2.x.21] . 


The basic idea is the following: in applications, one is not usually interested in the solution per se, but rather in certain aspects of it. For example, in simulations of flow problems, one may want to know the lift or drag of a body immersed in the fluid; it is this quantity that we want to know to best accuracy, and whether the rest of the solution of the describing equations is well resolved is not of primary interest. Likewise, in elasticity one might want to know about values of the stress at certain points to guess whether maximal load values of joints are safe, for example. Or, in radiative transfer problems, mean flux intensities are of interest. 

In all the cases just listed, it is the evaluation of a functional  [2.x.22]  of the solution which we are interested in, rather than the values of  [2.x.23]  everywhere. Since the exact solution  [2.x.24]  is not available, but only its numerical approximation  [2.x.25] , it is sensible to ask whether the computed value  [2.x.26]  is within certain limits of the exact value  [2.x.27] , i.e. we want to bound the error with respect to this functional,  [2.x.28] . 

For simplicity of exposition, we henceforth assume that both the quantity of interest  [2.x.29] , as well as the equation are linear, and we will in particular show the derivation for the Laplace equation with homogeneous Dirichlet boundary conditions, although the concept is much more general. For this general case, we refer to the references listed above.  The goal is to obtain bounds on the error,  [2.x.30] . For this, let us denote by  [2.x.31]  the solution of a dual problem, defined as follows: [1.x.49] where  [2.x.32]  is the bilinear form associated with the differential equation, and the test functions are chosen from the corresponding solution space. Then, taking as special test function  [2.x.33]  the error, we have that [1.x.50] and we can, by Galerkin orthogonality, rewrite this as [1.x.51] where  [2.x.34]  can be chosen from the discrete test space in whatever way we find convenient. 

Concretely, for Laplace's equation, the error identity reads [1.x.52] Because we want to use this formula not only to compute error, but also to refine the mesh, we need to rewrite the expression above as a sum over cells where each cell's contribution can then be used as an error indicator for this cell. Thus, we split the scalar products into terms for each cell, and integrate by parts on each of them: [1.x.53] 

Next we use that  [2.x.35] , and that for solutions of the Laplace equation, the solution is smooth enough that  [2.x.36]  is continuous almost everywhere -- so the terms involving  [2.x.37]  on one cell cancels with that on its neighbor, where the normal vector has the opposite sign. (The same is not true for  [2.x.38] , though.) At the boundary of the domain, where there is no neighbor cell with which this term could cancel, the weight  [2.x.39]  can be chosen as zero, and the whole term disappears. 

Thus, we have [1.x.54] 

In a final step, note that when taking the normal derivative of  [2.x.40] , we mean the value of this quantity as taken from this side of the cell (for the usual Lagrange elements, derivatives are not continuous across edges). We then rewrite the above formula by exchanging half of the edge integral of cell  [2.x.41]  with the neighbor cell  [2.x.42] , to obtain [1.x.55] 

Using that for the normal vectors on adjacent cells we have  [2.x.43] , we define the jump of the normal derivative by [1.x.56] and get the final form after setting the discrete function  [2.x.44] , which is by now still arbitrary, to the point interpolation of the dual solution,  [2.x.45] : [1.x.57] 



With this, we have obtained an exact representation of the error of the finite element discretization with respect to arbitrary (linear) functionals  [2.x.46] . Its structure is a weighted form of a residual estimator, as both  [2.x.47]  and  [2.x.48]  are cell and edge residuals that vanish on the exact solution, and  [2.x.49]  are weights indicating how important the residuals on a certain cell is for the evaluation of the given functional. Furthermore, it is a cell-wise quantity, so we can use it as a mesh refinement criterion. The question, is: how to evaluate it? After all, the evaluation requires knowledge of the dual solution  [2.x.50] , which carries the information about the quantity we want to know to best accuracy. 

In some, very special cases, this dual solution is known. For example, if the functional  [2.x.51]  is the point evaluation,  [2.x.52] , then the dual solution has to satisfy [1.x.58] with the Dirac delta function on the right hand side, and the dual solution is the Green's function with respect to the point  [2.x.53] . For simple geometries, this function is analytically known, and we could insert it into the error representation formula. 

However, we do not want to restrict ourselves to such special cases. Rather, we will compute the dual solution numerically, and approximate  [2.x.54]  by some numerically obtained  [2.x.55] . We note that it is not sufficient to compute this approximation  [2.x.56]  using the same method as used for the primal solution  [2.x.57] , since then  [2.x.58] , and the overall error estimate would be zero. Rather, the approximation  [2.x.59]  has to be from a larger space than the primal finite element space. There are various ways to obtain such an approximation (see the cited literature), and we will choose to compute it with a higher order finite element space. While this is certainly not the most efficient way, it is simple since we already have all we need to do that in place, and it also allows for simple experimenting. For more efficient methods, again refer to the given literature, in particular  [2.x.60] ,  [2.x.61] . 

With this, we end the discussion of the mathematical side of this program and turn to the actual implementation. 




 [2.x.62]  There are two steps above that do not seem necessary if all you care about is computing the error: namely, (i) the subtraction of  [2.x.63]  from  [2.x.64] , and (ii) splitting the integral into a sum of cells and integrating by parts on each. Indeed, neither of these two steps change  [2.x.65]  at all, as we only ever consider identities above until the substitution of  [2.x.66]  by  [2.x.67] . In other words, if you care only about [1.x.59]  [2.x.68] , then these steps are not necessary. On the other hand, if you want to use the error estimate also as a refinement criterion for each cell of the mesh, then it is necessary to (i) break the estimate into a sum of cells, and (ii) massage the formulas in such a way that each cell's contributions have something to do with the local error. (While the contortions above do not change the value of the [1.x.60]  [2.x.69] , they change the values we compute for each cell  [2.x.70] .) To this end, we want to write everything in the form "residual times dual weight" where a "residual" is something that goes to zero as the approximation becomes  [2.x.71]  better and better. For example, the quantity  [2.x.72]  is not a residual, since it simply converges to the (normal component of) the gradient of the exact solution. On the other hand,  [2.x.73]  is a residual because it converges to  [2.x.74] . All of the steps we have taken above in developing the final form of  [2.x.75]  have indeed had the goal of bringing the final formula into a form where each term converges to zero as the discrete solution  [2.x.76]  converges to  [2.x.77] . This then allows considering each cell's contribution as an "error indicator" that also converges to zero -- as it should as the mesh is refined. 




[1.x.61][1.x.62] 


The  [2.x.78]  example program builds heavily on the techniques already used in the  [2.x.79]  program. Its implementation of the dual weighted residual error estimator explained above is done by deriving a second class, properly called  [2.x.80]  base class, and having a class ( [2.x.81] ) that joins the two again and controls the solution of the primal and dual problem, and then uses both to compute the error indicator for mesh refinement. 

The program continues the modular concept of the previous example, by implementing the dual functional, describing quantity of interest, by an abstract base class, and providing two different functionals which implement this interface. Adding a different quantity of interest is thus simple. 

One of the more fundamental differences is the handling of data. A common case is that you develop a program that solves a certain equation, and test it with different right hand sides, different domains, different coefficients and boundary values, etc. Usually, these have to match, so that exact solutions are known, or that their combination makes sense at all. 

We demonstrate a way how this can be achieved in a simple, yet very flexible way. We will put everything that belongs to a certain setup into one class, and provide a little C++ mortar around it, so that entire setups (domains, coefficients, right hand sides, etc.) can be exchanged by only changing something in  [2.x.82] one [2.x.83]  place. 

Going this way a little further, we have also centralized all the other parameters that describe how the program is to work in one place, such as the order of the finite element, the maximal number of degrees of freedom, the evaluation objects that shall be executed on the computed solutions, and so on. This allows for simpler configuration of the program, and we will show in a later program how to use a library class that can handle setting these parameters by reading an input file. The general aim is to reduce the places within a program where one may have to look when wanting to change some parameter, as it has turned out in practice that one forgets where they are as programs grow. Furthermore, putting all options describing what the program does in a certain run into a file (that can be stored with the results) helps repeatability of results more than if the various flags were set somewhere in the program, where their exact values are forgotten after the next change to this place. 

Unfortunately, the program has become rather long. While this admittedly reduces its usefulness as an example program, we think that it is a very good starting point for development of a program for other kinds of problems, involving different equations than the Laplace equation treated here. Furthermore, it shows everything that we can show you about our way of a posteriori error estimation, and its structure should make it simple for you to adjust this method to other problems, other functionals, other geometries, coefficients, etc. 

The author believes that the present program is his masterpiece among the example programs, regarding the mathematical complexity, as well as the simplicity to add extensions. If you use this program as a basis for your own programs, we would kindly like to ask you to state this fact and the name of the author of the example program, Wolfgang Bangerth, in publications that arise from that, of your program consists in a considerable part of the example program. [1.x.63] [1.x.64] 

Start out with well known things... 

[1.x.65] 



The last step is as in all previous programs: 

[1.x.66] 




[1.x.67]  [1.x.68] 




As mentioned in the introduction, significant parts of the program have simply been taken over from the  [2.x.84]  example program. We therefore only comment on those things that are new.    


First, the framework for evaluation of solutions is unchanged, i.e. the base class is the same, and the class to evaluate the solution at a grid point is unchanged: 

[1.x.69] 




[1.x.70]  [1.x.71] 

[1.x.72] 




[1.x.73]  [1.x.74] 

[1.x.75] 




[1.x.76]  [1.x.77] 




Besides the class implementing the evaluation of the solution at one point, we here provide one which evaluates the gradient at a grid point. Since in general the gradient of a finite element function is not continuous at a vertex, we have to be a little bit more careful here. What we do is to loop over all cells, even if we have found the point already on one cell, and use the mean value of the gradient at the vertex taken from all adjacent cells.      


Given the interface of the  [2.x.85]  class, the declaration of this class provides little surprise, and neither does the constructor: 

[1.x.78] 



The more interesting things happen inside the function doing the actual evaluation: 

[1.x.79] 



This time initialize the return value with something useful, since we will have to add up a number of contributions and take the mean value afterwards... 

[1.x.80] 



...then have some objects of which the meaning will become clear below... 

[1.x.81] 



...and next loop over all cells and their vertices, and count how often the vertex has been found: 

[1.x.82] 



Things are now no more as simple, since we can't get the gradient of the finite element field as before, where we simply had to pick one degree of freedom at a vertex.                


Rather, we have to evaluate the finite element field on this cell, and at a certain point. As you know, evaluating finite element fields at certain points is done through the  [2.x.86]  class, so we use that. The question is: the  [2.x.87]  object needs to be a given a quadrature formula and can then compute the values of finite element quantities at the quadrature points. Here, we don't want to do quadrature, we simply want to specify some points!                


Nevertheless, the same way is chosen: use a special quadrature rule with points at the vertices, since these are what we are interested in. The appropriate rule is the trapezoidal rule, so that is the reason why we used that one above.                


Thus: initialize the  [2.x.88]  object on this cell, 

[1.x.83] 



and extract the gradients of the solution vector at the vertices: 

[1.x.84] 



Now we have the gradients at all vertices, so pick out that one which belongs to the evaluation point (note that the order of vertices is not necessarily the same as that of the quadrature points): 

[1.x.85] 



Check that the evaluation point was indeed found, 

[1.x.86] 



and if so take the x-derivative of the gradient there as the value which we are interested in, and increase the counter indicating how often we have added to that variable: 

[1.x.87] 



Finally break out of the innermost loop iterating over the vertices of the present cell, since if we have found the evaluation point at one vertex it cannot be at a following vertex as well: 

[1.x.88] 



Now we have looped over all cells and vertices, so check whether the point was found: 

[1.x.89] 



We have simply summed up the contributions of all adjacent cells, so we still have to compute the mean value. Once this is done, report the status: 

[1.x.90] 




[1.x.91]  [1.x.92] 




Since this program has a more difficult structure (it computed a dual solution in addition to a primal one), writing out the solution is no more done by an evaluation object since we want to write both solutions at once into one file, and that requires some more information than available to the evaluation classes.      


However, we also want to look at the grids generated. This again can be done with one such class. Its structure is analog to the  [2.x.89]  class of the previous example program, so we do not discuss it here in more detail. Furthermore, everything that is used here has already been used in previous example programs. 

[1.x.93] 




[1.x.94]  [1.x.95] 




Next are the actual solver classes. Again, we discuss only the differences to the previous program. 

[1.x.96] 




[1.x.97]  [1.x.98] 




This class is almost unchanged, with the exception that it declares two more functions:  [2.x.90]  will be used to generate output files from the actual solutions computed by derived classes, and the  [2.x.91]  function by which the testing framework sets the number of the refinement cycle to a local variable in this class; this number is later used to generate filenames for the solution output. 

[1.x.99] 




[1.x.100]  [1.x.101] 




Likewise, the  [2.x.92]  class is entirely unchanged and will thus not be discussed. 

[1.x.102] 



The remainder of the class is essentially a copy of  [2.x.93]  as well, including the data structures and functions necessary to compute the linear system in parallel using the WorkStream framework: 

[1.x.103] 



The following few functions and constructors are verbatim copies taken from  [2.x.94] : 

[1.x.104] 



Now for the functions that implement actions in the linear system class. First, the constructor initializes all data elements to their correct sizes, and sets up a number of additional data structures, such as constraints due to hanging nodes. Since setting up the hanging nodes and finding out about the nonzero elements of the matrix is independent, we do that in parallel (if the library was configured to use concurrency, at least; otherwise, the actions are performed sequentially). Note that we start only one thread, and do the second action in the main thread. Since only one thread is generated, we don't use the  [2.x.95]  class here, but rather use the one created thread object directly to wait for this particular thread's exit. The approach is generally the same as the one we have used in  [2.x.96]  above.      


Note that taking the address of the  [2.x.97]  function is a little tricky, since there are actually three functions of this name, one for each supported space dimension. Taking addresses of overloaded functions is somewhat complicated in C++, since the address-of operator  [2.x.98]  in that case returns a set of values (the addresses of all functions with that name), and selecting the right one is then the next step. If the context dictates which one to take (for example by assigning to a function pointer of known type), then the compiler can do that by itself, but if this set of pointers shall be given as the argument to a function that takes a template, the compiler could choose all without having a preference for one. We therefore have to make it clear to the compiler which one we would like to have; for this, we could use a cast, but for more clarity, we assign it to a temporary  [2.x.99]  (short for <code>pointer to make_hanging_node_constraints</code>) with the right type, and using this pointer instead. 

[1.x.105] 



Start a side task then continue on the main thread 

[1.x.106] 



Wait for the side task to be done before going further 

[1.x.107] 




[1.x.108]  [1.x.109] 




The  [2.x.100]  class is also mostly unchanged except for implementing the  [2.x.101]  function. We keep the  [2.x.102]  classes in this program, and they can then rely on the default implementation of this function which simply outputs the primal solution. The class implementing dual weighted error estimators will overload this function itself, to also output the dual solution. 

[1.x.110] 




[1.x.111]  [1.x.112] 




For the following two classes, the same applies as for most of the above: the class is taken from the previous example as-is: 

[1.x.113] 




[1.x.114]  [1.x.115] 




This class is a variant of the previous one, in that it allows to weight the refinement indicators we get from the library's Kelly indicator by some function. We include this class since the goal of this example program is to demonstrate automatic refinement criteria even for complex output quantities such as point values or stresses. If we did not solve a dual problem and compute the weights thereof, we would probably be tempted to give a hand-crafted weighting to the indicators to account for the fact that we are going to evaluate these quantities. This class accepts such a weighting function as argument to its constructor: 

[1.x.116] 



Now, here comes the main function, including the weighting: 

[1.x.117] 



First compute some residual based error indicators for all cells by a method already implemented in the library. What exactly we compute here is described in more detail in the documentation of that class. 

[1.x.118] 



Next weigh each entry in the vector of indicators by the value of the function given to the constructor, evaluated at the cell center. We need to write the result into the vector entry that corresponds to the current cell, which we can obtain by asking the cell what its index among all active cells is using  [2.x.103]  (In reality, this index is zero for the first cell we handle in the loop, one for the second cell, etc., and we could as well just keep track of this index using an integer counter; but using  [2.x.104]  makes this more explicit.) 

[1.x.119] 




[1.x.120]  [1.x.121]    


In this example program, we work with the same data sets as in the previous one, but as it may so happen that someone wants to run the program with different boundary values and right hand side functions, or on a different grid, we show a simple technique to do exactly that. For more clarity, we furthermore pack everything that has to do with equation data into a namespace of its own.    


The underlying assumption is that this is a research program, and that there we often have a number of test cases that consist of a domain, a right hand side, boundary values, possibly a specified coefficient, and a number of other parameters. They often vary all at the same time when shifting from one example to another. To make handling such sets of problem description parameters simple is the goal of the following.    


Basically, the idea is this: let us have a structure for each set of data, in which we pack everything that describes a test case: here, these are two subclasses, one called  [2.x.105]  for the boundary values of the exact solution, and one called  [2.x.106] , and then a way to generate the coarse grid. Since the solution of the previous example program looked like curved ridges, we use this name here for the enclosing class. Note that the names of the two inner classes have to be the same for all enclosing test case classes, and also that we have attached the dimension template argument to the enclosing class rather than to the inner ones, to make further processing simpler.  (From a language viewpoint, a namespace would be better to encapsulate these inner classes, rather than a structure. However, namespaces cannot be given as template arguments, so we use a structure to allow a second object to select from within its given argument. The enclosing structure, of course, has no member variables apart from the classes it declares, and a static function to generate the coarse mesh; it will in general never be instantiated.)    


The idea is then the following (this is the right time to also take a brief look at the code below): we can generate objects for boundary values and right hand side by simply giving the name of the outer class as a template argument to a class which we call here  [2.x.107] , and it then creates objects for the inner classes. In this case, to get all that characterizes the curved ridge solution, we would simply generate an instance of  [2.x.108] , and everything we need to know about the solution would be static member variables and functions of that object.    


This approach might seem like overkill in this case, but will become very handy once a certain set up is not only characterized by Dirichlet boundary values and a right hand side function, but in addition by material properties, Neumann values, different boundary descriptors, etc. In that case, the  [2.x.109]  class might consist of a dozen or more objects, and each descriptor class (like the  [2.x.110]  class below) would have to provide them. Then, you will be happy to be able to change from one set of data to another by only changing the template argument to the  [2.x.111]  class at one place, rather than at many.    


With this framework for different test cases, we are almost finished, but one thing remains: by now we can select statically, by changing one template argument, which data set to choose. In order to be able to do that dynamically, i.e. at run time, we need a base class. This we provide in the obvious way, see below, with virtual abstract functions. It forces us to introduce a second template parameter  [2.x.112]  which we need for the base class (which could be avoided using some template magic, but we omit that), but that's all.    


Adding new testcases is now simple, you don't have to touch the framework classes, only a structure like the  [2.x.113]  one is needed. 

[1.x.122] 




[1.x.123]  [1.x.124] 




Based on the above description, the  [2.x.114]  class then looks as follows. To allow using the  [2.x.115]  class with this class, we derived from the  [2.x.116]  class. 

[1.x.125] 



And now for the derived class that takes the template argument as explained above.      


Here we pack the data elements into private variables, and allow access to them through the methods of the base class. 

[1.x.126] 



We have to provide definitions for the static member variables of the above class: 

[1.x.127] 



And definitions of the member functions: 

[1.x.128] 




[1.x.129]  [1.x.130] 




The class that is used to describe the boundary values and right hand side of the  [2.x.117]  problem already used in the  [2.x.118]  example program is then like so: 

[1.x.131] 




[1.x.132]  [1.x.133] 




This example program was written while giving practical courses for a lecture on adaptive finite element methods and duality based error estimates. For these courses, we had one exercise, which required to solve the Laplace equation with constant right hand side on a square domain with a square hole in the center, and zero boundary values. Since the implementation of the properties of this problem is so particularly simple here, lets do it. As the number of the exercise was 2.3, we take the liberty to retain this name for the class as well. 

[1.x.134] 



We need a class to denote the boundary values of the problem. In this case, this is simple: it's the zero function, so don't even declare a class, just an alias: 

[1.x.135] 



Second, a class that denotes the right hand side. Since they are constant, just subclass the corresponding class of the library and be done: 

[1.x.136] 



Finally a function to generate the coarse grid. This is somewhat more complicated here, see immediately below. 

[1.x.137] 



As stated above, the grid for this example is the square [-1,1]^2 with the square [-1/2,1/2]^2 as hole in it. We create the coarse grid as 4 times 4 cells with the middle four ones missing. To understand how exactly the mesh is going to look, it may be simplest to just look at the "Results" section of this tutorial program first. In general, if you'd like to understand more about creating meshes either from scratch by hand, as we do here, or using other techniques, you should take a look at  [2.x.119] .      


Of course, the example has an extension to 3d, but since this function cannot be written in a dimension independent way we choose not to implement this here, but rather only specialize the template for dim=2. If you compile the program for 3d, you'll get a message from the linker that this function is not implemented for 3d, and needs to be provided.      


For the creation of this geometry, the library has no predefined method. In this case, the geometry is still simple enough to do the creation by hand, rather than using a mesh generator. 

[1.x.138] 



We first define the space dimension, to allow those parts of the function that are actually dimension independent to use this variable. That makes it simpler if you later take this as a starting point to implement a 3d version of this mesh. The next step is then to have a list of vertices. Here, they are 24 (5 times 5, with the middle one omitted). It is probably best to draw a sketch here. 

[1.x.139] 



Next, we have to define the cells and the vertices they contain. 

[1.x.140] 



Again, we generate a C++ vector type from this, but this time by looping over the cells (yes, this is boring). Additionally, we set the material indicator to zero for all the cells: 

[1.x.141] 



Finally pass all this information to the library to generate a triangulation. The last parameter may be used to pass information about non-zero boundary indicators at certain faces of the triangulation to the library, but we don't want that here, so we give an empty object: 

[1.x.142] 



And since we want that the evaluation point (3/4,3/4) in this example is a grid point, we refine once globally: 

[1.x.143] 




[1.x.144]  [1.x.145]    


As you have now read through this framework, you may be wondering why we have not chosen to implement the classes implementing a certain setup (like the  [2.x.120]  class) directly as classes derived from  [2.x.121] . Indeed, we could have done very well so. The only reason is that then we would have to have member variables for the solution and right hand side classes in the  [2.x.122]  class, as well as member functions overloading the abstract functions of the base class giving access to these member variables. The  [2.x.123]  class has the sole reason to relieve us from the need to reiterate these member variables and functions that would be necessary in all such classes. In some way, the template mechanism here only provides a way to have default implementations for a number of functions that depend on external quantities and can thus not be provided using normal virtual functions, at least not without the help of templates.    


However, there might be good reasons to actually implement classes derived from  [2.x.124] , for example if the solution or right hand side classes require constructors that take arguments, which the  [2.x.125]  class cannot provide. In that case, subclassing is a worthwhile strategy. Other possibilities for special cases are to derive from  [2.x.126]  where  [2.x.127]  denotes a class, or even to explicitly specialize  [2.x.128] . The latter allows to transparently use the way the  [2.x.129]  class is used for other set-ups, but with special actions taken for special arguments.    


A final observation favoring the approach taken here is the following: we have found numerous times that when starting a project, the number of parameters (usually boundary values, right hand side, coarse grid, just as here) was small, and the number of test cases was small as well. One then starts out by handcoding them into a number of  [2.x.130]  statements. Over time, projects grow, and so does the number of test cases. The number of  [2.x.131]  statements grows with that, and their length as well, and one starts to find ways to consider impossible examples where domains, boundary values, and right hand sides do not fit together any more, and starts losing the overview over the whole structure. Encapsulating everything belonging to a certain test case into a structure of its own has proven worthwhile for this, as it keeps everything that belongs to one test case in one place. Furthermore, it allows to put these things all in one or more files that are only devoted to test cases and their data, without having to bring their actual implementation into contact with the rest of the program. 










[1.x.146]  [1.x.147] 




As with the other components of the program, we put everything we need to describe dual functionals into a namespace of its own, and define an abstract base class that provides the interface the class solving the dual problem needs for its work.    


We will then implement two such classes, for the evaluation of a point value and of the derivative of the solution at that point. For these functionals we already have the corresponding evaluation objects, so they are complementary. 

[1.x.148] 




[1.x.149]  [1.x.150] 




First start with the base class for dual functionals. Since for linear problems the characteristics of the dual problem play a role only in the right hand side, we only need to provide for a function that assembles the right hand side for a given discretization: 

[1.x.151] 




[1.x.152]  [1.x.153] 




As a first application, we consider the functional corresponding to the evaluation of the solution's value at a given point which again we assume to be a vertex. Apart from the constructor that takes and stores the evaluation point, this class consists only of the function that implements assembling the right hand side. 

[1.x.154] 



As for doing the main purpose of the class, assembling the right hand side, let us first consider what is necessary: The right hand side of the dual problem is a vector of values J(phi_i), where J is the error functional, and phi_i is the i-th shape function. Here, J is the evaluation at the point x0, i.e. J(phi_i)=phi_i(x0).      


Now, we have assumed that the evaluation point is a vertex. Thus, for the usual finite elements we might be using in this program, we can take for granted that at such a point exactly one shape function is nonzero, and in particular has the value one. Thus, we set the right hand side vector to all-zeros, then seek for the shape function associated with that point and set the corresponding value of the right hand side vector to one: 

[1.x.155] 



So, first set everything to zeros... 

[1.x.156] 



...then loop over cells and find the evaluation point among the vertices (or very close to a vertex, which may happen due to floating point round-off): 

[1.x.157] 



Ok, found, so set corresponding entry, and leave function since we are finished: 

[1.x.158] 



Finally, a sanity check: if we somehow got here, then we must have missed the evaluation point, so raise an exception unconditionally: 

[1.x.159] 




[1.x.160]  [1.x.161] 




As second application, we again consider the evaluation of the x-derivative of the solution at one point. Again, the declaration of the class, and the implementation of its constructor is not too interesting: 

[1.x.162] 



What is interesting is the implementation of this functional: here, J(phi_i)=d/dx phi_i(x0).      


We could, as in the implementation of the respective evaluation object take the average of the gradients of each shape function phi_i at this evaluation point. However, we take a slightly different approach: we simply take the average over all cells that surround this point. The question which cells  [2.x.132]  the evaluation point is made dependent on the mesh width by including those cells for which the distance of the cell's midpoint to the evaluation point is less than the cell's diameter.      


Taking the average of the gradient over the area/volume of these cells leads to a dual solution which is very close to the one which would result from the point evaluation of the gradient. It is simple to justify theoretically that this does not change the method significantly. 

[1.x.163] 



Again, first set all entries to zero: 

[1.x.164] 



Initialize a  [2.x.133]  object with a quadrature formula, have abbreviations for the number of quadrature points and shape functions... 

[1.x.165] 



...and have two objects that are used to store the global indices of the degrees of freedom on a cell, and the values of the gradients of the shape functions at the quadrature points: 

[1.x.166] 



Finally have a variable in which we will sum up the area/volume of the cells over which we integrate, by integrating the unit functions on these cells: 

[1.x.167] 



Then start the loop over all cells, and select those cells which are close enough to the evaluation point: 

[1.x.168] 



If we have found such a cell, then initialize the  [2.x.134]  object and integrate the x-component of the gradient of each shape function, as well as the unit function for the total area/volume. 

[1.x.169] 



If we have the local contributions, distribute them to the global vector: 

[1.x.170] 



After we have looped over all cells, check whether we have found any at all, by making sure that their volume is non-zero. If not, then the results will be botched, as the right hand side should then still be zero, so throw an exception: 

[1.x.171] 



Finally, we have by now only integrated the gradients of the shape functions, not taking their mean value. We fix this by dividing by the measure of the volume over which we have integrated: 

[1.x.172] 




[1.x.173]  [1.x.174] 

[1.x.175] 




[1.x.176]  [1.x.177] 




In the same way as the  [2.x.135]  class above, we now implement a  [2.x.136] . It has all the same features, the only difference is that it does not take a function object denoting a right hand side object, but now takes a  [2.x.137]  object that will assemble the right hand side vector of the dual problem. The rest of the class is rather trivial.      


Since both primal and dual solver will use the same triangulation, but different discretizations, it now becomes clear why we have made the  [2.x.138]  class a virtual one: since the final class will be derived from both  [2.x.139]  as well as  [2.x.140]  instances, would we not have marked the inheritance as virtual. Since in many applications the base class would store much more information than just the triangulation which needs to be shared between primal and dual solvers, we do not usually want to use two such base classes. 

[1.x.178] 




[1.x.179]  [1.x.180] 




Here finally comes the main class of this program, the one that implements the dual weighted residual error estimator. It joins the primal and dual solver classes to use them for the computation of primal and dual solutions, and implements the error representation formula for use as error estimate and mesh refinement.      


The first few of the functions of this class are mostly overriders of the respective functions of the base class: 

[1.x.181] 



In the private section, we have two functions that are used to call the  [2.x.141]  functions of the primal and dual base classes. These two functions will be called in parallel by the  [2.x.142]  function of this class. 

[1.x.182] 



Then declare abbreviations for active cell iterators, to avoid that we have to write this lengthy name over and over again: 







[1.x.183] 



Next, declare a data type that we will us to store the contribution of faces to the error estimator. The idea is that we can compute the face terms from each of the two cells to this face, as they are the same when viewed from both sides. What we will do is to compute them only once, based on some rules explained below which of the two adjacent cells will be in charge to do so. We then store the contribution of each face in a map mapping faces to their values, and only collect the contributions for each cell by looping over the cells a second time and grabbing the values from the map.        


The data type of this map is declared here: 

[1.x.184] 



In the computation of the error estimates on cells and faces, we need a number of helper objects, such as  [2.x.143]  and  [2.x.144]  functions, but also temporary objects storing the values and gradients of primal and dual solutions, for example. These fields are needed in the three functions that do the integration on cells, and regular and irregular faces, respectively.        


There are three reasonable ways to provide these fields: first, as local variables in the function that needs them; second, as member variables of this class; third, as arguments passed to that function.        


These three alternatives all have drawbacks: the third that their number is not negligible and would make calling these functions a lengthy enterprise. The second has the drawback that it disallows parallelization, since the threads that will compute the error estimate have to have their own copies of these variables each, so member variables of the enclosing class will not work. The first approach, although straightforward, has a subtle but important drawback: we will call these functions over and over again, many thousands of times maybe; it now turns out that allocating vectors and other objects that need memory from the heap is an expensive business in terms of run-time, since memory allocation is expensive when several threads are involved. It is thus significantly better to allocate the memory only once, and recycle the objects as often as possible.        


What to do? Our answer is to use a variant of the third strategy. In fact, this is exactly what the WorkStream concept is supposed to do (we have already introduced it above, but see also  [2.x.145] ). To avoid that we have to give these functions a dozen or so arguments, we pack all these variables into two structures, one which is used for the computations on cells, the other doing them on the faces. Both are then joined into the WeightedResidualScratchData class that will serve as the "scratch data" class of the WorkStream concept: 

[1.x.185] 



 [2.x.146]  generally wants both a scratch object and a copy object. Here, for reasons similar to what we had in  [2.x.147]  when discussing the computation of an approximation of the gradient, we don't actually need a "copy data" structure. Since WorkStream insists on having one of these, we just declare an empty structure that does nothing other than being there. 

[1.x.186] 



Regarding the evaluation of the error estimator, we have one driver function that uses  [2.x.148]  to call the second function on every cell: 

[1.x.187] 



Then we have functions that do the actual integration of the error representation formula. They will treat the terms on the cell interiors, on those faces that have no hanging nodes, and on those faces with hanging nodes, respectively: 

[1.x.188] 



In the implementation of this class, we first have the constructors of the  [2.x.149]  member classes, and the  [2.x.150]  constructor. They only initialize fields to their correct lengths, so we do not have to discuss them in too much detail: 

[1.x.189] 



The next five functions are boring, as they simply relay their work to the base classes. The first calls the primal and dual solvers in parallel, while postprocessing the solution and retrieving the number of degrees of freedom is done by the primal class. 

[1.x.190] 



Now, it is becoming more interesting: the  [2.x.151]  function asks the error estimator to compute the cell-wise error indicators, then uses their absolute values for mesh refinement. 

[1.x.191] 



First call the function that computes the cell-wise and global error: 

[1.x.192] 



Then note that marking cells for refinement or coarsening only works if all indicators are positive, to allow their comparison. Thus, drop the signs on all these indicators: 

[1.x.193] 



Finally, we can select between different strategies for refinement. The default here is to refine those cells with the largest error indicators that make up for a total of 80 per cent of the error, while we coarsen those with the smallest indicators that make up for the bottom 2 per cent of the error. 

[1.x.194] 



Since we want to output both the primal and the dual solution, we overload the  [2.x.152]  function. The only interesting feature of this function is that the primal and dual solutions are defined on different finite element spaces, which is not the format the  [2.x.153]  class expects. Thus, we have to transfer them to a common finite element space. Since we want the solutions only to see them qualitatively, we contend ourselves with interpolating the dual solution to the (smaller) primal space. For the interpolation, there is a library function, that takes a AffineConstraints object including the hanging node constraints. The rest is standard. 

[1.x.195] 



Add the data vectors for which we want output. Add them both, the  [2.x.154]  functions can handle as many data vectors as you wish to write to output: 

[1.x.196] 




[1.x.197]  [1.x.198] 





[1.x.199]  [1.x.200]      


As for the actual computation of error estimates, let's start with the function that drives all this, i.e. calls those functions that actually do the work, and finally collects the results. 

[1.x.201] 



The first task in computing the error is to set up vectors that denote the primal solution, and the weights (z-z_h)=(z-I_hz), both in the finite element space for which we have computed the dual solution. For this, we have to interpolate the primal solution to the dual finite element space, and to subtract the interpolation of the computed dual solution to the primal finite element space. Fortunately, the library provides functions for the interpolation into larger or smaller finite element spaces, so this is mostly obvious.        


First, let's do that for the primal solution: it is cell-wise interpolated into the finite element space in which we have solved the dual problem: But, again as in the  [2.x.155]  function we first need to create an AffineConstraints object including the hanging node constraints, but this time of the dual finite element space. 

[1.x.202] 



Then for computing the interpolation of the numerically approximated dual solution z into the finite element space of the primal solution and subtracting it from z: use the  [2.x.156]  function, that gives (z-I_hz) in the element space of the dual solution. 

[1.x.203] 



Note that this could probably have been more efficient since those constraints have been used previously when assembling matrix and right hand side for the primal problem and writing out the dual solution. We leave the optimization of the program in this respect as an exercise. 




Having computed the dual weights we now proceed with computing the cell and face residuals of the primal solution. First we set up a map between face iterators and their jump term contributions of faces to the error estimator. The reason is that we compute the jump terms only once, from one side of the face, and want to collect them only afterwards when looping over all cells a second time.        


We initialize this map already with a value of -1e20 for all faces, since this value will stand out in the results if something should go wrong and we fail to compute the value for a face for some reason. Secondly, this initialization already makes the  [2.x.157]  object allocate all objects it may possibly need. This is important since we will write into this structure from parallel threads, and doing so would not be thread-safe if the map needed to allocate memory and thereby reshape its data structures. In other words, the initial initialization relieves us from the necessity to synchronize the threads through a mutex each time they write to (and modify the structure of) this map. 

[1.x.204] 



Then hand it all off to  [2.x.158]  to compute the estimators for all cells in parallel: 

[1.x.205] 



Once the error contributions are computed, sum them up. For this, note that the cell terms are already set, and that only the edge terms need to be collected. Thus, loop over all cells and their faces, make sure that the contributions of each of the faces are there, and add them up. Only take minus one half of the jump term, since the other half will be taken by the neighboring cell. 

[1.x.206] 




[1.x.207]  [1.x.208] 




Next we have the function that is called to estimate the error on a single cell. The function may be called multiple times if the library was configured to use multithreading. Here it goes: 

[1.x.209] 



Because of WorkStream, estimate_on_one_cell requires a CopyData object even if it is no used. The next line silences a warning about this unused variable. 

[1.x.210] 



First task on each cell is to compute the cell residual contributions of this cell, and put them into the  [2.x.159]  variable: 

[1.x.211] 



After computing the cell terms, turn to the face terms. For this, loop over all faces of the present cell, and see whether something needs to be computed on it: 

[1.x.212] 



First, if this face is part of the boundary, then there is nothing to do. However, to make things easier when summing up the contributions of the faces of cells, we enter this face into the list of faces with a zero contribution to the error. 

[1.x.213] 



Next, note that since we want to compute the jump terms on each face only once although we access it twice (if it is not at the boundary), we have to define some rules who is responsible for computing on a face:            


First, if the neighboring cell is on the same level as this one, i.e. neither further refined not coarser, then the one with the lower index within this level does the work. In other words: if the other one has a lower index, then skip work on this face: 

[1.x.214] 



Likewise, we always work from the coarser cell if this and its neighbor differ in refinement. Thus, if the neighboring cell is less refined than the present one, then do nothing since we integrate over the subfaces when we visit the coarse cell. 

[1.x.215] 



Now we know that we are in charge here, so actually compute the face jump terms. If the face is a regular one, i.e.  the other side's cell is neither coarser not finer than this cell, then call one function, and if the cell on the other side is further refined, then use another function. Note that the case that the cell on the other side is coarser cannot happen since we have decided above that we handle this case when we pass over that other cell. 

[1.x.216] 




[1.x.217]  [1.x.218] 




As for the actual computation of the error contributions, first turn to the cell terms: 

[1.x.219] 



The tasks to be done are what appears natural from looking at the error estimation formula: first get the right hand side and Laplacian of the numerical solution at the quadrature points for the cell residual, 

[1.x.220] 



...then get the dual weights... 

[1.x.221] 



...and finally build the sum over all quadrature points and store it with the present cell: 

[1.x.222] 




[1.x.223]  [1.x.224] 




On the other hand, computation of the edge terms for the error estimate is not so simple. First, we have to distinguish between faces with and without hanging nodes. Because it is the simple case, we first consider the case without hanging nodes on a face (let's call this the `regular' case): 

[1.x.225] 



The first step is to get the values of the gradients at the quadrature points of the finite element field on the present cell. For this, initialize the  [2.x.160]  object corresponding to this side of the face, and extract the gradients using that object. 

[1.x.226] 



The second step is then to extract the gradients of the finite element solution at the quadrature points on the other side of the face, i.e. from the neighboring cell.        


For this, do a sanity check before: make sure that the neighbor actually exists (yes, we should not have come here if the neighbor did not exist, but in complicated software there are bugs, so better check this), and if this is not the case throw an error. 

[1.x.227] 



If we have that, then we need to find out with which face of the neighboring cell we have to work, i.e. the  [2.x.161]  the neighbor the present cell is of the cell behind the present face. For this, there is a function, and we put the result into a variable with the name  [2.x.162] : 

[1.x.228] 



Then define an abbreviation for the neighbor cell, initialize the  [2.x.163]  object on that cell, and extract the gradients on that cell: 

[1.x.229] 



Now that we have the gradients on this and the neighboring cell, compute the jump residual by multiplying the jump in the gradient with the normal vector: 

[1.x.230] 



Next get the dual weights for this face: 

[1.x.231] 



Finally, we have to compute the sum over jump residuals, dual weights, and quadrature weights, to get the result for this face: 

[1.x.232] 



Double check that the element already exists and that it was not already written to... 

[1.x.233] 



...then store computed value at assigned location. Note that the stored value does not contain the factor 1/2 that appears in the error representation. The reason is that the term actually does not have this factor if we loop over all faces in the triangulation, but only appears if we write it as a sum over all cells and all faces of each cell; we thus visit the same face twice. We take account of this by using this factor -1/2 later, when we sum up the contributions for each cell individually. 

[1.x.234] 




[1.x.235]  [1.x.236] 




We are still missing the case of faces with hanging nodes. This is what is covered in this function: 

[1.x.237] 



First again two abbreviations, and some consistency checks whether the function is called only on faces for which it is supposed to be called: 

[1.x.238] 



Then find out which neighbor the present cell is of the adjacent cell. Note that we will operate on the children of this adjacent cell, but that their orientation is the same as that of their mother, i.e. the neighbor direction is the same. 

[1.x.239] 



Then simply do everything we did in the previous function for one face for all the sub-faces now: 

[1.x.240] 



Start with some checks again: get an iterator pointing to the cell behind the present subface and check whether its face is a subface of the one we are considering. If that were not the case, then there would be either a bug in the  [2.x.164]  function called above, or -- worse 

-- some function in the library did not keep to some underlying assumptions about cells, their children, and their faces. In any case, even though this assertion should not be triggered, it does not harm to be cautious, and in optimized mode computations the assertion will be removed anyway. 

[1.x.241] 



Now start the work by again getting the gradient of the solution first at this side of the interface, 

[1.x.242] 



then at the other side, 

[1.x.243] 



and finally building the jump residuals. Since we take the normal vector from the other cell this time, revert the sign of the first term compared to the other function: 

[1.x.244] 



Then get dual weights: 

[1.x.245] 



At last, sum up the contribution of this sub-face, and set it in the global map: 

[1.x.246] 



Once the contributions of all sub-faces are computed, loop over all sub-faces to collect and store them with the mother face for simple use when later collecting the error terms of cells. Again make safety checks that the entries for the sub-faces have been computed and do not carry an invalid value. 

[1.x.247] 



Finally store the value with the parent face. 

[1.x.248] 




[1.x.249]  [1.x.250] 




In the previous example program, we have had two functions that were used to drive the process of solving on subsequently finer grids. We extend this here to allow for a number of parameters to be passed to these functions, and put all of that into framework class.    


You will have noted that this program is built up of a number of small parts (evaluation functions, solver classes implementing various refinement methods, different dual functionals, different problem and data descriptions), which makes the program relatively simple to extend, but also allows to solve a large number of different problems by replacing one part by another. We reflect this flexibility by declaring a structure in the following framework class that holds a number of parameters that may be set to test various combinations of the parts of this program, and which can be used to test it at various problems and discretizations in a simple way. 

[1.x.251] 



First, we declare two abbreviations for simple use of the respective data types: 

[1.x.252] 



Then we have the structure which declares all the parameters that may be set. In the default constructor of the structure, these values are all set to default values, for simple use. 

[1.x.253] 



First allow for the degrees of the piecewise polynomials by which the primal and dual problems will be discretized. They default to (bi-, tri-)linear ansatz functions for the primal, and (bi-, tri-)quadratic ones for the dual problem. If a refinement criterion is chosen that does not need the solution of a dual problem, the value of the dual finite element degree is of course ignored. 

[1.x.254] 



Then have an object that describes the problem type, i.e. right hand side, domain, boundary values, etc. The pointer needed here defaults to the Null pointer, i.e. you will have to set it in actual instances of this object to make it useful. 

[1.x.255] 



Since we allow to use different refinement criteria (global refinement, refinement by the Kelly error indicator, possibly with a weight, and using the dual estimator), define a number of enumeration values, and subsequently a variable of that type. It will default to  [2.x.165] . 

[1.x.256] 



Next, an object that describes the dual functional. It is only needed if the dual weighted residual refinement is chosen, and also defaults to a Null pointer. 

[1.x.257] 



Then a list of evaluation objects. Its default value is empty, i.e. no evaluation objects. 

[1.x.258] 



Next to last, a function that is used as a weight to the  [2.x.166]  class. The default value of this pointer is zero, but you have to set it to some other value if you want to use the  [2.x.167]  refinement criterion. 

[1.x.259] 



Finally, we have a variable that denotes the maximum number of degrees of freedom we allow for the (primal) discretization. If it is exceeded, we stop the process of solving and intermittent mesh refinement. Its default value is 20,000. 

[1.x.260] 



Finally the default constructor of this class: 

[1.x.261] 



The driver framework class only has one method which calls solver and mesh refinement intermittently, and does some other small tasks in between. Since it does not need data besides the parameters given to it, we make it static: 

[1.x.262] 



As for the implementation, first the constructor of the parameter object, setting all values to their defaults: 

[1.x.263] 



Then the function which drives the whole process: 

[1.x.264] 



First create a triangulation from the given data object, 

[1.x.265] 



then a set of finite elements and appropriate quadrature formula: 

[1.x.266] 



Next, select one of the classes implementing different refinement criteria. 

[1.x.267] 



Now that all objects are in place, run the main loop. The stopping criterion is implemented at the bottom of the loop.      


In the loop, first set the new cycle number, then solve the problem, output its solution(s), apply the evaluation objects to it, then decide whether we want to refine the mesh further and solve again on this mesh, or jump out of the loop. 

[1.x.268] 



Clean up the screen after the loop has run: 

[1.x.269] 




[1.x.270]  [1.x.271] 




Here finally comes the main function. It drives the whole process by specifying a set of parameters to be used for the simulation (polynomial degrees, evaluation and dual functionals, etc), and passes them packed into a structure to the frame work class above. 

[1.x.272] 



Describe the problem we want to solve here by passing a descriptor object to the function doing the rest of the work: 

[1.x.273] 



First set the refinement criterion we wish to use: 

[1.x.274] 



Here, we could as well have used  [2.x.168]  or  [2.x.169] . Note that the information given about dual finite elements, dual functional, etc is only important for the given choice of refinement criterion, and is ignored otherwise. 




Then set the polynomial degrees of primal and dual problem. We choose here bi-linear and bi-quadratic ones: 

[1.x.275] 



Then set the description of the test case, i.e. domain, boundary values, and right hand side. These are prepackaged in classes. We take here the description of  [2.x.170] , but you can also use  [2.x.171] : 

[1.x.276] 



Next set first a dual functional, then a list of evaluation objects. We choose as default the evaluation of the value at an evaluation point, represented by the classes  [2.x.172]  in the namespaces of evaluation and dual functional classes. You can also set the  [2.x.173]  classes for the x-derivative instead of the value at the evaluation point.        


Note that dual functional and evaluation objects should match. However, you can give as many evaluation functionals as you want, so you can have both point value and derivative evaluated after each step.  One such additional evaluation is to output the grid in each step. 

[1.x.277] 



Set the maximal number of degrees of freedom after which we want the program to stop refining the mesh further: 

[1.x.278] 



Finally pass the descriptor object to a function that runs the entire solution with it: 

[1.x.279] 



Catch exceptions to give information about things that failed: 

[1.x.280] 

[1.x.281][1.x.282] 


[1.x.283][1.x.284] 




This program offers a lot of possibilities to play around. We can thus only show a small part of all possible results that can be obtained with the help of this program. However, you are encouraged to just try it out, by changing the settings in the main program. Here, we start by simply letting it run, unmodified: 

[1.x.285] 




First let's look what the program actually computed. On the seventh grid, primal and dual numerical solutions look like this (using a color scheme intended to evoke the snow-capped mountains of Colorado that the original author of this program now calls home):  [2.x.174]  Apparently, the region at the bottom left is so unimportant for the point value evaluation at the top right that the grid is left entirely unrefined there, even though the solution has singularities at the inner corner of that cell! Due to the symmetry in right hand side and domain, the solution should actually look like at the top right in all four corners, but the mesh refinement criterion involving the dual solution chose to refine them differently -- because we said that we really only care about a single function value somewhere at the top right. 




Here are some of the meshes that are produced in refinement cycles 0, 2, 4 (top row), and 5, 7, and 8 (bottom row): 

 [2.x.175]  

Note the subtle interplay between resolving the corner singularities, and resolving around the point of evaluation. It will be rather difficult to generate such a mesh by hand, as this would involve to judge quantitatively how much which of the four corner singularities should be resolved, and to set the weight compared to the vicinity of the evaluation point. 




The program prints the point value and the estimated error in this quantity. From extrapolating it, we can guess that the exact value is somewhere close to 0.0334473, plus or minus 0.0000001 (note that we get almost 6 valid digits from only 22,000 (primal) degrees of freedom. This number cannot be obtained from the value of the functional alone, but I have used the assumption that the error estimator is mostly exact, and extrapolated the computed value plus the estimated error, to get an approximation of the true value. Computing with more degrees of freedom shows that this assumption is indeed valid. 




From the computed results, we can generate two graphs: one that shows the convergence of the error  [2.x.176]  (taking the extrapolated value as correct) in the point value, and the value that we get by adding up computed value  [2.x.177]  and estimated error eta (if the error estimator  [2.x.178]  were exact, then the value  [2.x.179]  would equal the exact point value, and the error in this quantity would always be zero; however, since the error estimator is only a - good - approximation to the true error, we can by this only reduce the size of the error). In this graph, we also indicate the complexity  [2.x.180]  to show that mesh refinement acts optimal in this case. The second chart compares true and estimated error, and shows that the two are actually very close to each other, even for such a complicated quantity as the point value: 


 [2.x.181]  


[1.x.286][1.x.287] 




Since we have accepted quite some effort when using the mesh refinement driven by the dual weighted error estimator (for solving the dual problem, and for evaluating the error representation), it is worth while asking whether that effort was successful. To this end, we first compare the achieved error levels for different mesh refinement criteria. To generate this data, simply change the value of the mesh refinement criterion variable in the main program. The results are thus (for the weight in the Kelly indicator, we have chosen the function  [2.x.182] , where  [2.x.183]  is the distance to the evaluation point; it can be shown that this is the optimal weight if we neglect the effects of boundaries): 

 [2.x.184]  




Checking these numbers, we see that for global refinement, the error is proportional to  [2.x.185] , and for the dual estimator  [2.x.186] . Generally speaking, we see that the dual weighted error estimator is better than the other refinement indicators, at least when compared with those that have a similarly regular behavior. The Kelly indicator produces smaller errors, but jumps about the picture rather irregularly, with the error also changing signs sometimes. Therefore, its behavior does not allow to extrapolate the results to larger values of N. Furthermore, if we trust the error estimates of the dual weighted error estimator, the results can be improved by adding the estimated error to the computed values. In terms of reliability, the weighted estimator is thus better than the Kelly indicator, although the latter sometimes produces smaller errors. 




[1.x.288][1.x.289] 




Besides evaluating the values of the solution at a certain point, the program also offers the possibility to evaluate the x-derivatives at a certain point, and also to tailor mesh refinement for this. To let the program compute these quantities, simply replace the two occurrences of  [2.x.187]  in the main function by  [2.x.188] , and let the program run: 

[1.x.290] 






The solution looks roughly the same as before (the exact solution of course  [2.x.189] is [2.x.190]  the same, only the grid changed a little), but the dual solution is now different. A close-up around the point of evaluation shows this:  [2.x.191]  This time, the grids in refinement cycles 0, 5, 6, 7, 8, and 9 look like this: 

 [2.x.192]  

Note the asymmetry of the grids compared with those we obtained for the point evaluation. This is due to the fact that the domain and the primal solution may be symmetric about the diagonal, but the  [2.x.193] -derivative is not, and the latter enters the refinement criterion. 




Then, it is interesting to compare actually computed values of the quantity of interest (i.e. the x-derivative of the solution at one point) with a reference value of -0.0528223... plus or minus 0.0000005. We get this reference value by computing on finer grid after some more mesh refinements, with approximately 130,000 cells. Recall that if the error is  [2.x.194]  in the optimal case, then taking a mesh with ten times more cells gives us one additional digit in the result. 




In the left part of the following chart, you again see the convergence of the error towards this extrapolated value, while on the right you see a comparison of true and estimated error: 

 [2.x.195]  

After an initial phase where the true error changes its sign, the estimated error matches it quite well, again. Also note the dramatic improvement in the error when using the estimated error to correct the computed value of  [2.x.196] . 




[1.x.291][1.x.292] 




If instead of the  [2.x.197]  data set, we choose  [2.x.198]  in the main function, and choose  [2.x.199]  as the evaluation point, then we can redo the computations of the previous example program, to compare whether the results obtained with the help of the dual weighted error estimator are better than those we had previously. 




First, the meshes after 9 adaptive refinement cycles obtained with the point evaluation and derivative evaluation refinement criteria, respectively, look like this: 

 [2.x.200]  

The features of the solution can still be seen in the mesh, but since the solution is smooth, the singularities of the dual solution entirely dominate the mesh refinement criterion, and lead to strongly concentrated meshes. The solution after the seventh refinement step looks like the following: 

 [2.x.201]  

Obviously, the solution is worse at some places, but the mesh refinement process should have taken care that these places are not important for computing the point value. 





The next point is to compare the new (duality based) mesh refinement criterion with the old ones. These are the results: 

 [2.x.202]  




The results are, well, somewhat mixed. First, the Kelly indicator disqualifies itself by its unsteady behavior, changing the sign of the error several times, and with increasing errors under mesh refinement. The dual weighted error estimator has a monotone decrease in the error, and is better than the weighted Kelly and global refinement, but the margin is not as large as expected. This is, here, due to the fact the global refinement can exploit the regular structure of the meshes around the point of evaluation, which leads to a better order of convergence for the point error. However, if we had a mesh that is not locally rectangular, for example because we had to approximate curved boundaries, or if the coefficients were not constant, then this advantage of globally refinement meshes would vanish, while the good performance of the duality based estimator would remain. 





[1.x.293][1.x.294] 




The results here are not too clearly indicating the superiority of the dual weighted error estimation approach for mesh refinement over other mesh refinement criteria, such as the Kelly indicator. This is due to the relative simplicity of the shown applications. If you are not convinced yet that this approach is indeed superior, you are invited to browse through the literature indicated in the introduction, where plenty of examples are provided where the dual weighted approach can reduce the necessary numerical work by orders of magnitude, making this the only way to compute certain quantities to reasonable accuracies at all. 




Besides the objections you may raise against its use as a mesh refinement criterion, consider that accurate knowledge of the error in the quantity one might want to compute is of great use, since we can stop computations when we are satisfied with the accuracy. Using more traditional approaches, it is very difficult to get accurate estimates for arbitrary quantities, except for, maybe, the error in the energy norm, and we will then have no guarantee that the result we computed satisfies any requirements on its accuracy. Also, as was shown for the evaluation of point values and derivatives, the error estimate can be used to extrapolate the results, yielding much higher accuracy in the quantity we want to know. 




Leaving these mathematical considerations, we tried to write the program in a modular way, such that implementing another test case, or another evaluation and dual functional is simple. You are encouraged to take the program as a basis for your own experiments, and to play a little. [1.x.295] [1.x.296]  [2.x.203]  

 [2.x.204] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29] 

 [2.x.3]  

[1.x.30]  [2.x.4]  


[1.x.31] [1.x.32][1.x.33] 


[1.x.34][1.x.35] 


This program deals with an example of a non-linear elliptic partial differential equation, the minimal surface equation. You can imagine the solution of this equation to describe the surface spanned by a soap film that is enclosed by a closed wire loop. We imagine the wire to not just be a planar loop, but in fact curved. The surface tension of the soap film will then reduce the surface to have minimal surface. The solution of the minimal surface equation describes this shape with the wire's vertical displacement as a boundary condition. For simplicity, we will here assume that the surface can be written as a graph  [2.x.5]  although it is clear that it is not very hard to construct cases where the wire is bent in such a way that the surface can only locally be constructed as a graph but not globally. 

Because the equation is non-linear, we can't solve it directly. Rather, we have to use Newton's method to compute the solution iteratively. 

 [2.x.6]  ( [2.x.7]  




[1.x.36][1.x.37] 


In a classical sense, the problem is given in the following form: 


  [1.x.38] 



 [2.x.8]  is the domain we get by projecting the wire's positions into  [2.x.9]  space. In this example, we choose  [2.x.10]  as the unit disk. 

As described above, we solve this equation using Newton's method in which we compute the  [2.x.11] th approximate solution from the  [2.x.12] th [2.x.13]  one, and use a damping parameter  [2.x.14]  to get better global convergence behavior:   [1.x.39] 

with   [1.x.40] and  [2.x.15]  the derivative of F in direction of  [2.x.16] : [1.x.41] 

Going through the motions to find out what  [2.x.17]  is, we find that we have to solve a linear elliptic PDE in every Newton step, with  [2.x.18]  as the solution of: 

  [1.x.42] 

In order to solve the minimal surface equation, we have to solve this equation repeatedly, once per Newton step. To solve this, we have to take a look at the boundary condition of this problem. Assuming that  [2.x.19]  already has the right boundary values, the Newton update  [2.x.20]  should have zero boundary conditions, in order to have the right boundary condition after adding both.  In the first Newton step, we are starting with the solution  [2.x.21] , the Newton update still has to deliver the right boundary condition to the solution  [2.x.22] . 


Summing up, we have to solve the PDE above with the boundary condition  [2.x.23]  in the first step and with  [2.x.24]  in all the following steps. 


[1.x.43][1.x.44] 


Starting with the strong formulation above, we get the weak formulation by multiplying both sides of the PDE with a test function  [2.x.25]  and integrating by parts on both sides:   [1.x.45] Here the solution  [2.x.26]  is a function in  [2.x.27] , subject to the boundary conditions discussed above. Reducing this space to a finite dimensional space with basis  [2.x.28] , we can write the solution: 

[1.x.46] 

Using the basis functions as test functions and defining  [2.x.29] , we can rewrite the weak formulation: 

[1.x.47] 

where the solution  [2.x.30]  is given by the coefficients  [2.x.31] . This linear system of equations can be rewritten as: 

[1.x.48] 

where the entries of the matrix  [2.x.32]  are given by: 

[1.x.49] 

and the right hand side  [2.x.33]  is given by: 

[1.x.50] 


[1.x.51][1.x.52] 


The matrix that corresponds to the Newton step above can be reformulated to show its structure a bit better. Rewriting it slightly, we get that it has the form [1.x.53] where the matrix  [2.x.34]  (of size  [2.x.35]  in  [2.x.36]  space dimensions) is given by the following expression: [1.x.54] From this expression, it is obvious that  [2.x.37]  is symmetric, and so  [2.x.38]  is symmetric as well. On the other hand,  [2.x.39]  is also positive definite, which confers the same property onto  [2.x.40] . This can be seen by noting that the vector  [2.x.41]  is an eigenvector of  [2.x.42]  with eigenvalue  [2.x.43]  while all vectors  [2.x.44]  that are perpendicular to  [2.x.45]  and each other are eigenvectors with eigenvalue  [2.x.46] . Since all eigenvalues are positive,  [2.x.47]  is positive definite and so is  [2.x.48] . We can thus use the CG method for solving the Newton steps. (The fact that the matrix  [2.x.49]  is symmetric and positive definite should not come as a surprise. It results from taking the derivative of an operator that results from taking the derivative of an energy functional: the minimal surface equation simply minimizes some non-quadratic energy. Consequently, the Newton matrix, as the matrix of second derivatives of a scalar energy, must be symmetric since the derivative with regard to the  [2.x.50] th and  [2.x.51] th degree of freedom should clearly commute. Likewise, if the energy functional is convex, then the matrix of second derivatives must be positive definite, and the direct calculation above simply reaffirms this.) 

It is worth noting, however, that the positive definiteness degenerates for problems where  [2.x.52]  becomes large. In other words, if we simply multiply all boundary values by 2, then to first order  [2.x.53]  and  [2.x.54]  will also be multiplied by two, but as a consequence the smallest eigenvalue of  [2.x.55]  will become smaller and the matrix will become more ill-conditioned. (More specifically, for  [2.x.56]  we have that  [2.x.57]  whereas  [2.x.58] ; thus, the condition number of  [2.x.59] , which is a multiplicative factor in the condition number of  [2.x.60]  grows like  [2.x.61] .) It is simple to verify with the current program that indeed multiplying the boundary values used in the current program by larger and larger values results in a problem that will ultimately no longer be solvable using the simple preconditioned CG method we use here. 


[1.x.55][1.x.56] 


As stated above, Newton's method works by computing a direction  [2.x.62]  and then performing the update  [2.x.63]  with a step length  [2.x.64] . It is a common observation that for strongly nonlinear models, Newton's method does not converge if we always choose  [2.x.65]  unless one starts with an initial guess  [2.x.66]  that is sufficiently close to the solution  [2.x.67]  of the nonlinear problem. In practice, we don't always have such an initial guess, and consequently taking full Newton steps (i.e., using  [2.x.68] ) does frequently not work. 

A common strategy therefore is to use a smaller step length for the first few steps while the iterate  [2.x.69]  is still far away from the solution  [2.x.70]  and as we get closer use larger values for  [2.x.71]  until we can finally start to use full steps  [2.x.72]  as we are close enough to the solution. The question is of course how to choose  [2.x.73] . There are basically two widely used approaches: line search and trust region methods. 

In this program, we simply always choose the step length equal to 0.1. This makes sure that for the testcase at hand we do get convergence although it is clear that by not eventually reverting to full step lengths we forego the rapid, quadratic convergence that makes Newton's method so appealing. Obviously, this is a point one eventually has to address if the program was made into one that is meant to solve more realistic problems. We will comment on this issue some more in the [1.x.57]. 


[1.x.58][1.x.59] 


Overall, the program we have here is not unlike  [2.x.74]  in many regards. The layout of the main class is essentially the same. On the other hand, the driving algorithm in the  [2.x.75]  function is different and works as follows:  [2.x.76]   [2.x.77]    Start with the function  [2.x.78]  and modify it in such a way   that the values of  [2.x.79]  along the boundary equal the correct   boundary values  [2.x.80]  (this happens in    [2.x.81] ). Set    [2.x.82] .  [2.x.83]  

 [2.x.84]    Compute the Newton update by solving the system  [2.x.85]    with boundary condition  [2.x.86]  on  [2.x.87] .  [2.x.88]  

 [2.x.89]    Compute a step length  [2.x.90] . In this program, we always set    [2.x.91] . To make things easier to extend later on, this   happens in a function of its own, namely in    [2.x.92] .  [2.x.93]  

 [2.x.94]    The new approximation of the solution is given by    [2.x.95] .  [2.x.96]  

 [2.x.97]    If  [2.x.98]  is a multiple of 5 then refine the mesh, transfer the   solution  [2.x.99]  to the new mesh and set the values of  [2.x.100]    in such a way that along the boundary we have    [2.x.101]  (again in    [2.x.102] ). Note that   this isn't automatically   guaranteed even though by construction we had that before mesh   refinement  [2.x.103]  because mesh refinement   adds new nodes to the mesh where we have to interpolate the old   solution to the new nodes upon bringing the solution from the old to   the new mesh. The values we choose by interpolation may be close to   the exact boundary conditions but are, in general, nonetheless not   the correct values.  [2.x.104]  

 [2.x.105]    Set  [2.x.106]  and go to step 2.  [2.x.107]   [2.x.108]  

The testcase we solve is chosen as follows: We seek to find the solution of minimal surface over the unit disk  [2.x.109]  where the surface attains the values  [2.x.110]  along the boundary. [1.x.60] [1.x.61] 


[1.x.62]  [1.x.63] 




The first few files have already been covered in previous examples and will thus not be further commented on. 

[1.x.64] 



We will use adaptive mesh refinement between Newton iterations. To do so, we need to be able to work with a solution on the new mesh, although it was computed on the old one. The SolutionTransfer class transfers the solution from the old to the new mesh: 







[1.x.65] 



We then open a namespace for this program and import everything from the dealii namespace into it, as in previous programs: 

[1.x.66] 




[1.x.67]  [1.x.68] 




The class template is basically the same as in  [2.x.111] .  Three additions are made: 

- There are two solution vectors, one for the Newton update  [2.x.112] , and one for the current iterate  [2.x.113] . 

- The  [2.x.114]  function takes an argument that denotes whether this is the first time it is called or not. The difference is that the first time around we need to distribute the degrees of freedom and set the solution vector for  [2.x.115]  to the correct size. The following times, the function is called after we have already done these steps as part of refining the mesh in  [2.x.116] . 

- We then also need new functions:  [2.x.117]  takes care of setting the boundary values on the solution vector correctly, as discussed at the end of the introduction.  [2.x.118]  is a function that computes the norm of the nonlinear (discrete) residual. We use this function to monitor convergence of the Newton iteration. The function takes a step length  [2.x.119]  as argument to compute the residual of  [2.x.120] . This is something one typically needs for step length control, although we will not use this feature here. Finally,  [2.x.121]  computes the step length  [2.x.122]  in each Newton iteration. As discussed in the introduction, we here use a fixed step length and leave implementing a better strategy as an exercise. 







[1.x.69] 




[1.x.70]  [1.x.71] 




The boundary condition is implemented just like in  [2.x.123] .  It is chosen as  [2.x.124] : 







[1.x.72] 




[1.x.73]  [1.x.74] 





[1.x.75]  [1.x.76] 




The constructor and destructor of the class are the same as in the first few tutorials. 







[1.x.77] 




[1.x.78]  [1.x.79] 




As always in the setup-system function, we setup the variables of the finite element method. There are same differences to  [2.x.125] , because there we start solving the PDE from scratch in every refinement cycle whereas here we need to take the solution from the previous mesh onto the current mesh. Consequently, we can't just reset solution vectors. The argument passed to this function thus indicates whether we can distributed degrees of freedom (plus compute constraints) and set the solution vector to zero or whether this has happened elsewhere already (specifically, in  [2.x.126] ). 







[1.x.80] 



The remaining parts of the function are the same as in  [2.x.127] . 







[1.x.81] 




[1.x.82]  [1.x.83] 




This function does the same as in the previous tutorials except that now, of course, the matrix and right hand side functions depend on the previous iteration's solution. As discussed in the introduction, we need to use zero boundary values for the Newton updates; we compute them at the end of this function.    


The top of the function contains the usual boilerplate code, setting up the objects that allow us to evaluate shape functions at quadrature points and temporary storage locations for the local matrices and vectors, as well as for the gradients of the previous solution at the quadrature points. We then start the loop over all cells: 

[1.x.84] 



For the assembly of the linear system, we have to obtain the values of the previous solution's gradients at the quadrature points. There is a standard way of doing this: the  [2.x.128]  function takes a vector that represents a finite element field defined on a DoFHandler, and evaluates the gradients of this field at the quadrature points of the cell with which the FEValues object has last been reinitialized. The values of the gradients at all quadrature points are then written into the second argument: 

[1.x.85] 



With this, we can then do the integration loop over all quadrature points and shape functions.  Having just computed the gradients of the old solution in the quadrature points, we are able to compute the coefficients  [2.x.129]  in these points.  The assembly of the system itself then looks similar to what we always do with the exception of the nonlinear terms, as does copying the results from the local objects into the global ones: 

[1.x.86] 



Finally, we remove hanging nodes from the system and apply zero boundary values to the linear system that defines the Newton updates  [2.x.130] : 

[1.x.87] 




[1.x.88]  [1.x.89] 




The solve function is the same as always. At the end of the solution process we update the current solution by setting  [2.x.131] . 

[1.x.90] 




[1.x.91]  [1.x.92] 




The first part of this function is the same as in  [2.x.132] ... However, after refining the mesh we have to transfer the old solution to the new one which we do with the help of the SolutionTransfer class. The process is slightly convoluted, so let us describe it in detail: 

[1.x.93] 



Then we need an additional step: if, for example, you flag a cell that is once more refined than its neighbor, and that neighbor is not flagged for refinement, we would end up with a jump of two refinement levels across a cell interface.  To avoid these situations, the library will silently also have to refine the neighbor cell once. It does so by calling the  [2.x.133]  function before actually doing the refinement and coarsening.  This function flags a set of additional cells for refinement or coarsening, to enforce rules like the one-hanging-node rule.  The cells that are flagged for refinement and coarsening after calling this function are exactly the ones that will actually be refined or coarsened. Usually, you don't have to do this by hand  [2.x.134]  does this for you). However, we need to initialize the SolutionTransfer class and it needs to know the final set of cells that will be coarsened or refined in order to store the data from the old mesh and transfer to the new one. Thus, we call the function by hand: 

[1.x.94] 



With this out of the way, we initialize a SolutionTransfer object with the present DoFHandler and attach the solution vector to it, followed by doing the actual refinement and distribution of degrees of freedom on the new mesh 

[1.x.95] 



Finally, we retrieve the old solution interpolated to the new mesh. Since the SolutionTransfer function does not actually store the values of the old solution, but rather indices, we need to preserve the old solution vector until we have gotten the new interpolated values. Thus, we have the new values written into a temporary vector, and only afterwards write them into the solution vector object. Once we have this solution we have to make sure that the  [2.x.135]  we now have actually has the correct boundary values. As explained at the end of the introduction, this is not automatically the case even if the solution before refinement had the correct boundary values, and so we have to explicitly make sure that it now has: 

[1.x.96] 



On the new mesh, there are different hanging nodes, which we have to compute again. To ensure there are no hanging nodes of the old mesh in the object, it's first cleared.  To be on the safe side, we then also make sure that the current solution's vector entries satisfy the hanging node constraints (see the discussion in the documentation of the SolutionTransfer class for why this is necessary): 

[1.x.97] 



We end the function by updating all the remaining data structures, indicating to  [2.x.136]  that this is not the first go-around and that it needs to preserve the content of the solution vector: 

[1.x.98] 




[1.x.99]  [1.x.100] 




The next function ensures that the solution vector's entries respect the boundary values for our problem.  Having refined the mesh (or just started computations), there might be new nodal points on the boundary. These have values that are simply interpolated from the previous mesh (or are just zero), instead of the correct boundary values. This is fixed up by setting all boundary nodes explicit to the right value: 

[1.x.101] 




[1.x.102]  [1.x.103] 




In order to monitor convergence, we need a way to compute the norm of the (discrete) residual, i.e., the norm of the vector  [2.x.137]  with  [2.x.138]  as discussed in the introduction. It turns out that (although we don't use this feature in the current version of the program) one needs to compute the residual  [2.x.139]  when determining optimal step lengths, and so this is what we implement here: the function takes the step length  [2.x.140]  as an argument. The original functionality is of course obtained by passing a zero as argument.    


In the function below, we first set up a vector for the residual, and then a vector for the evaluation point  [2.x.141] . This is followed by the same boilerplate code we use for all integration operations: 

[1.x.104] 



The actual computation is much as in  [2.x.142] . We first evaluate the gradients of  [2.x.143]  at the quadrature points, then compute the coefficient  [2.x.144] , and then plug it all into the formula for the residual: 

[1.x.105] 



At the end of this function we also have to deal with the hanging node constraints and with the issue of boundary values. With regard to the latter, we have to set to zero the elements of the residual vector for all entries that correspond to degrees of freedom that sit at the boundary. The reason is that because the value of the solution there is fixed, they are of course no "real" degrees of freedom and so, strictly speaking, we shouldn't have assembled entries in the residual vector for them. However, as we always do, we want to do exactly the same thing on every cell and so we didn't not want to deal with the question of whether a particular degree of freedom sits at the boundary in the integration above. Rather, we will simply set to zero these entries after the fact. To this end, we need to determine which degrees of freedom do in fact belong to the boundary and then loop over all of those and set the residual entry to zero. This happens in the following lines which we have already seen used in  [2.x.145] , using the appropriate function from namespace DoFTools: 

[1.x.106] 



At the end of the function, we return the norm of the residual: 

[1.x.107] 




[1.x.108]  [1.x.109] 




As discussed in the introduction, Newton's method frequently does not converge if we always take full steps, i.e., compute  [2.x.146] . Rather, one needs a damping parameter (step length)  [2.x.147]  and set  [2.x.148] . This function is the one called to compute  [2.x.149] .    


Here, we simply always return 0.1. This is of course a sub-optimal choice: ideally, what one wants is that the step size goes to one as we get closer to the solution, so that we get to enjoy the rapid quadratic convergence of Newton's method. We will discuss better strategies below in the results section. 

[1.x.110] 




[1.x.111]  [1.x.112] 




This last function to be called from `run()` outputs the current solution (and the Newton update) in graphical form as a VTU file. It is entirely the same as what has been used in previous tutorials. 

[1.x.113] 




[1.x.114]  [1.x.115] 




In the run function, we build the first grid and then have the top-level logic for the Newton iteration.    


As described in the introduction, the domain is the unit disk around the origin, created in the same way as shown in  [2.x.150] . The mesh is globally refined twice followed later on by several adaptive cycles.    


Before starting the Newton loop, we also need to do a bit of setup work: We need to create the basic data structures and ensure that the first Newton iterate already has the correct boundary values, as discussed in the introduction. 

[1.x.116] 



The Newton iteration starts next. We iterate until the (norm of the) residual computed at the end of the previous iteration is less than  [2.x.151] , as checked at the end of the `do { ... } while` loop that starts here. Because we don't have a reasonable value to initialize the variable, we just use the largest value that can be represented as a `double`. 

[1.x.117] 



On every mesh we do exactly five Newton steps. We print the initial residual here and then start the iterations on this mesh.          


In every Newton step the system matrix and the right hand side have to be computed first, after which we store the norm of the right hand side as the residual to check against when deciding whether to stop the iterations. We then solve the linear system (the function also updates  [2.x.152] ) and output the norm of the residual at the end of this Newton step.          


After the end of this loop, we then also output the solution on the current mesh in graphical form and increment the counter for the mesh refinement cycle. 

[1.x.118] 




[1.x.119]  [1.x.120] 




Finally the main function. This follows the scheme of all other main functions: 

[1.x.121] 

[1.x.122][1.x.123] 




The output of the program looks as follows: 

[1.x.124] 



Obviously, the scheme converges, if not very fast. We will come back to strategies for accelerating the method below. 

One can visualize the solution after each set of five Newton iterations, i.e., on each of the meshes on which we approximate the solution. This yields the following set of images: 

 [2.x.153]  

It is clearly visible, that the solution minimizes the surface after each refinement. The solution converges to a picture one would imagine a soap bubble to be that is located inside a wire loop that is bent like the boundary. Also it is visible, how the boundary is smoothed out after each refinement. On the coarse mesh, the boundary doesn't look like a sine, whereas it does the finer the mesh gets. 

The mesh is mostly refined near the boundary, where the solution increases or decreases strongly, whereas it is coarsened on the inside of the domain, where nothing interesting happens, because there isn't much change in the solution. The ninth solution and mesh are shown here: 

 [2.x.154]  




[1.x.125] [1.x.126][1.x.127] 


The program shows the basic structure of a solver for a nonlinear, stationary problem. However, it does not converge particularly fast, for good reasons: 

- The program always takes a step size of 0.1. This precludes the rapid,   quadratic convergence for which Newton's method is typically chosen. 

- It does not connect the nonlinear iteration with the mesh refinement   iteration. 

Obviously, a better program would have to address these two points. We will discuss them in the following. 


[1.x.128][1.x.129] 


Newton's method has two well known properties: 

- It may not converge from arbitrarily chosen starting points. Rather, a   starting point has to be close enough to the solution to guarantee   convergence. However, we can enlarge the area from which Newton's method   converges by damping the iteration using a [1.x.130] 0< [2.x.155] . 

- It exhibits rapid convergence of quadratic order if (i) the step length is   chosen as  [2.x.156] , and (ii) it does in fact converge with this choice   of step length. 

A consequence of these two observations is that a successful strategy is to choose  [2.x.157]  for the initial iterations until the iterate has come close enough to allow for convergence with full step length, at which point we want to switch to  [2.x.158] . The question is how to choose  [2.x.159]  in an automatic fashion that satisfies these criteria. 

We do not want to review the literature on this topic here, but only briefly mention that there are two fundamental approaches to the problem: backtracking line search and trust region methods. The former is more widely used for partial differential equations and essentially does the following: 

- Compute a search direction 

- See if the resulting residual of  [2.x.160]  with    [2.x.161]  is "substantially smaller" than that of  [2.x.162]  alone. 

- If so, then take  [2.x.163] . 

- If not, try whether the residual is "substantially smaller" with    [2.x.164] . 

- If so, then take  [2.x.165] . 

- If not, try whether the residual is "substantially smaller" with    [2.x.166] . 

- Etc. One can of course choose other factors  [2.x.167]  than the  [2.x.168]  chosen above, for  [2.x.169] . It is obvious where the term "backtracking" comes from: we try a long step, but if that doesn't work we try a shorter step, and ever shorter step, etc. The function  [2.x.170]  is written the way it is to support exactly this kind of use case. 

Whether we accept a particular step length  [2.x.171]  depends on how we define "substantially smaller". There are a number of ways to do so, but without going into detail let us just mention that the most common ones are to use the Wolfe and Armijo-Goldstein conditions. For these, one can show the following: 

- There is always a step length  [2.x.172]  for which the conditions are   satisfied, i.e., the iteration never gets stuck as long as the problem is   convex. 

- If we are close enough to the solution, then the conditions allow for    [2.x.173] , thereby enabling quadratic convergence. 

We will not dwell on this here any further but leave the implementation of such algorithms as an exercise. We note, however, that when implemented correctly then it is a common observation that most reasonably nonlinear problems can be solved in anywhere between 5 and 15 Newton iterations to engineering accuracy &mdash; substantially fewer than we need with the current version of the program. 

More details on globalization methods including backtracking can be found, for example, in  [2.x.174]  and  [2.x.175] . 

A separate point, very much worthwhile making, however, is that in practice the implementation of efficient nonlinear solvers is about as complicated as the implementation of efficient finite element methods. One should not attempt to reinvent the wheel by implementing all of the necessary steps oneself. Rather, just like building finite element solvers on libraries such as deal.II, one should be building nonlinear solvers on libraries such as [SUNDIALS](https://computing.llnl.gov/projects/sundials). In fact, deal.II has interfaces to SUNDIALS and in particular to its nonlinear solver sub-package KINSOL through the  [2.x.176]  class. It would not be very difficult to base the current problem on that interface. 




[1.x.131][1.x.132] 


We currently do exactly 5 iterations on each mesh. But is this optimal? One could ask the following questions: 

- Maybe it is worthwhile doing more iterations on the initial meshes since   there, computations are cheap. 

- On the other hand, we do not want to do too many iterations on every mesh:   yes, we could drive the residual to zero on every mesh, but that would only   mean that the nonlinear iteration error is far smaller than the   discretization error. 

- Should we use solve the linear systems in each Newton step with higher or   lower accuracy? 

Ultimately, what this boils down to is that we somehow need to couple the discretization error on the current mesh with the nonlinear residual we want to achieve with the Newton iterations on a given mesh, and to the linear iteration we want to achieve with the CG method within each Newton iterations. 

How to do this is, again, not entirely trivial, and we again leave it as a future exercise. [1.x.133] [1.x.134]  [2.x.177]  

 [2.x.178] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19] 

 [2.x.3]  

[1.x.20] 

[1.x.21] [1.x.22][1.x.23] 




This example shows the basic usage of the multilevel functions in deal.II. It solves almost the same problem as used in  [2.x.4] , but demonstrating the things one has to provide when using multigrid as a preconditioner. In particular, this requires that we define a hierarchy of levels, provide transfer operators from one level to the next and back, and provide representations of the Laplace operator on each level. 

In order to allow sufficient flexibility in conjunction with systems of differential equations and block preconditioners, quite a few different objects have to be created before starting the multilevel method, although most of what needs to be done is provided by deal.II itself. These are 

  - the object handling transfer between grids; we use the MGTransferPrebuilt     class for this that does almost all of the work inside the library, 

  - the solver on the coarsest level; here, we use MGCoarseGridHouseholder, 

  - the smoother on all other levels, which in our case will be the      [2.x.5]  class using SOR as the underlying method, 

  - and  [2.x.6]  a class having a special level multiplication, i.e. we     basically store one matrix per grid level and allow multiplication with it. 

Most of these objects will only be needed inside the function that actually solves the linear system. There, these objects are combined in an object of type Multigrid, containing the implementation of the V-cycle, which is in turn used by the preconditioner PreconditionMG, ready for plug-in into a linear solver of the LAC library. 

The multigrid method implemented here for adaptively refined meshes follows the outline in the  [2.x.7]  "Multigrid paper", which describes the underlying implementation in deal.II and also introduces a lot of the nomenclature. First, we have to distinguish between level meshes, namely cells that have the same refinement distance from the coarse mesh, and the leaf mesh consisting of active cells of the hierarchy (in older work we refer to this as the global mesh, but this term is overused). Most importantly, the leaf mesh is not identical with the level mesh on the finest level. The following image shows what we consider to be a "level mesh": 

 [2.x.8]  

The fine level in this mesh consists only of the degrees of freedom that are defined on the refined cells, but does not extend to that part of the domain that is not refined. While this guarantees that the overall effort grows as  [2.x.9]  as necessary for optimal multigrid complexity, it leads to problems when defining where to smooth and what boundary conditions to pose for the operators defined on individual levels if the level boundary is not an external boundary. These questions are discussed in detail in the article cited above. 

[1.x.24][1.x.25] 


The problem we solve here is similar to  [2.x.10] , with two main differences: first, the multigrid preconditioner, obviously. We also change the discontinuity of the coefficients such that the local assembler does not look more complicated than necessary. [1.x.26] [1.x.27] 


[1.x.28]  [1.x.29] 




Again, the first few include files are already known, so we won't comment on them: 

[1.x.30] 



These, now, are the include necessary for the multilevel methods. The first one declares how to handle Dirichlet boundary conditions on each of the levels of the multigrid method. For the actual description of the degrees of freedom, we do not need any new include file because DoFHandler already has all necessary methods implemented. We will only need to distribute the DoFs for the levels further down. 




The rest of the include files deals with the mechanics of multigrid as a linear operator (solver or preconditioner). 

[1.x.31] 



We will be using  [2.x.11]  to loop over the cells, so include it here: 

[1.x.32] 



This is C++: 

[1.x.33] 




[1.x.34]  [1.x.35]    


We use  [2.x.12]  to assemble our matrices. For this, we need a ScratchData object to store temporary data on each cell (this is just the FEValues object) and a CopyData object that will contain the output of each cell assembly. For more details about the usage of scratch and copy objects, see the WorkStream namespace. 

[1.x.36] 




[1.x.37]  [1.x.38] 




This main class is similar to the same class in  [2.x.13] . As far as member functions is concerned, the only additions are: 

- The  [2.x.14]  function that assembles the matrices that correspond to the discrete operators on intermediate levels. 

- The  [2.x.15]  function that assembles our PDE on a single cell. 

[1.x.39] 



The following members are the essential data structures for the multigrid method. The first four represent the sparsity patterns and the matrices on individual levels of the multilevel hierarchy, very much like the objects for the global mesh above.      


Then we have two new matrices only needed for multigrid methods with local smoothing on adaptive meshes. They convey data between the interior part of the refined region and the refinement edge, as outlined in detail in the  [2.x.16]  "multigrid paper".      


The last object stores information about the boundary indices on each level and information about indices lying on a refinement edge between two different refinement levels. It thus serves a similar purpose as AffineConstraints, but on each level. 

[1.x.40] 




[1.x.41]  [1.x.42] 




Just one short remark about the constructor of the Triangulation: by convention, all adaptively refined triangulations in deal.II never change by more than one level across a face between cells. For our multigrid algorithms, however, we need a slightly stricter guarantee, namely that the mesh also does not change by more than refinement level across vertices that might connect two cells. In other words, we must prevent the following situation:    


 [2.x.17]     


This is achieved by passing the  [2.x.18]  flag to the constructor of the triangulation class. 

[1.x.43] 




[1.x.44]  [1.x.45] 




In addition to just distributing the degrees of freedom in the DoFHandler, we do the same on each level. Then, we follow the same procedure as before to set up the system on the leaf mesh. 

[1.x.46] 



The multigrid constraints have to be initialized. They need to know where Dirichlet boundary conditions are prescribed. 

[1.x.47] 



Now for the things that concern the multigrid data structures. First, we resize the multilevel objects to hold matrices and sparsity patterns for every level. The coarse level is zero (this is mandatory right now but may change in a future revision). Note that these functions take a complete, inclusive range here (not a starting index and size), so the finest level is  [2.x.19] . We first have to resize the container holding the SparseMatrix classes, since they have to release their SparsityPattern before the can be destroyed upon resizing. 

[1.x.48] 



Now, we have to provide a matrix on each level. To this end, we first use the  [2.x.20]  function to generate a preliminary compressed sparsity pattern on each level (see the  [2.x.21]  module for more information on this topic) and then copy it over to the one we really want. The next step is to initialize the interface matrices with the fitting sparsity pattern.      


It may be worth pointing out that the interface matrices only have entries for degrees of freedom that sit at or next to the interface between coarser and finer levels of the mesh. They are therefore even sparser than the matrices on the individual levels of our multigrid hierarchy. Therefore, we use a function specifically build for this purpose to generate it. 

[1.x.49] 




[1.x.50]  [1.x.51] 




The cell_worker function is used to assemble the matrix and right-hand side on the given cell. This function is used for the active cells to generate the system_matrix and on each level to build the level matrices.    


Note that we also assemble a right-hand side when called from assemble_multigrid() even though it is not used. 

[1.x.52] 




[1.x.53]  [1.x.54] 




The following function assembles the linear system on the active cells of the mesh. For this, we pass two lambda functions to the mesh_loop() function. The cell_worker function redirects to the class member function of the same name, while the copier is specific to this function and copies local matrix and vector to the corresponding global ones using the constraints. 

[1.x.55] 




[1.x.56]  [1.x.57] 




The next function is the one that builds the matrices that define the multigrid method on each level of the mesh. The integration core is the same as above, but the loop below will go over all existing cells instead of just the active ones, and the results must be entered into the correct level matrices. Fortunately, MeshWorker hides most of that from us, and thus the difference between this function and the previous lies only in the setup of the assembler and the different iterators in the loop.    


We generate an AffineConstraints object for each level containing the boundary and interface dofs as constrained entries. The corresponding object is then used to generate the level matrices. 

[1.x.58] 



Interface entries are ignored by the boundary_constraints object above when filling the mg_matrices[cd.level]. Instead, we copy these entries into the interface matrix of the current level manually: 

[1.x.59] 




[1.x.60]  [1.x.61] 




This is the other function that is significantly different in support of the multigrid solver (or, in fact, the preconditioner for which we use the multigrid method).    


Let us start out by setting up two of the components of multilevel methods: transfer operators between levels, and a solver on the coarsest level. In finite element methods, the transfer operators are derived from the finite element function spaces involved and can often be computed in a generic way independent of the problem under consideration. In that case, we can use the MGTransferPrebuilt class that, given the constraints of the final linear system and the MGConstrainedDoFs object that knows about the boundary conditions on the each level and the degrees of freedom on interfaces between different refinement level can build the matrices for those transfer operations from a DoFHandler object with level degrees of freedom.    


The second part of the following lines deals with the coarse grid solver. Since our coarse grid is very coarse indeed, we decide for a direct solver (a Householder decomposition of the coarsest level matrix), even if its implementation is not particularly sophisticated. If our coarse mesh had many more cells than the five we have here, something better suited would obviously be necessary here. 

[1.x.62] 



The next component of a multilevel solver or preconditioner is that we need a smoother on each level. A common choice for this is to use the application of a relaxation method (such as the SOR, Jacobi or Richardson method) or a small number of iterations of a solver method (such as CG or GMRES). The  [2.x.22]  and MGSmootherPrecondition classes provide support for these two kinds of smoothers. Here, we opt for the application of a single SOR iteration. To this end, we define an appropriate alias and then setup a smoother object.      


The last step is to initialize the smoother object with our level matrices and to set some smoothing parameters. The  [2.x.23]  function can optionally take additional arguments that will be passed to the smoother object on each level. In the current case for the SOR smoother, this could, for example, include a relaxation parameter. However, we here leave these at their default values. The call to  [2.x.24]  indicates that we will use two pre- and two post-smoothing steps on each level; to use a variable number of smoother steps on different levels, more options can be set in the constructor call to the  [2.x.25]  object.      


The last step results from the fact that we use the SOR method as a smoother - which is not symmetric - but we use the conjugate gradient iteration (which requires a symmetric preconditioner) below, we need to let the multilevel preconditioner make sure that we get a symmetric operator even for nonsymmetric smoothers: 

[1.x.63] 



The next preparatory step is that we must wrap our level and interface matrices in an object having the required multiplication functions. We will create two objects for the interface objects going from coarse to fine and the other way around; the multigrid algorithm will later use the transpose operator for the latter operation, allowing us to initialize both up and down versions of the operator with the matrices we already built: 

[1.x.64] 



Now, we are ready to set up the V-cycle operator and the multilevel preconditioner. 

[1.x.65] 



With all this together, we can finally get about solving the linear system in the usual way: 

[1.x.66] 




[1.x.67]  [1.x.68] 




The following two functions postprocess a solution once it is computed. In particular, the first one refines the mesh at the beginning of each cycle while the second one outputs results at the end of each such cycle. The functions are almost unchanged from those in  [2.x.26] . 

[1.x.69] 




[1.x.70]  [1.x.71] 




Like several of the functions above, this is almost exactly a copy of the corresponding function in  [2.x.27] . The only difference is the call to  [2.x.28]  that takes care of forming the matrices on every level that we need in the multigrid method. 

[1.x.72] 




[1.x.73]  [1.x.74] 




This is again the same function as in  [2.x.29] : 

[1.x.75] 

[1.x.76][1.x.77] 


On the finest mesh, the solution looks like this: 

 [2.x.30]  

More importantly, we would like to see if the multigrid method really improved the solver performance. Therefore, here is the textual output: 

<pre> Cycle 0    Number of active cells:       80    Number of degrees of freedom: 89 (by level: 8, 25, 89)    Number of CG iterations: 8 

Cycle 1    Number of active cells:       158    Number of degrees of freedom: 183 (by level: 8, 25, 89, 138)    Number of CG iterations: 9 

Cycle 2    Number of active cells:       302    Number of degrees of freedom: 352 (by level: 8, 25, 89, 223, 160)    Number of CG iterations: 10 

Cycle 3    Number of active cells:       578    Number of degrees of freedom: 649 (by level: 8, 25, 89, 231, 494, 66)    Number of CG iterations: 10 

Cycle 4    Number of active cells:       1100    Number of degrees of freedom: 1218 (by level: 8, 25, 89, 274, 764, 417, 126)    Number of CG iterations: 10 

Cycle 5    Number of active cells:       2096    Number of degrees of freedom: 2317 (by level: 8, 25, 89, 304, 779, 1214, 817)    Number of CG iterations: 11 

Cycle 6    Number of active cells:       3986    Number of degrees of freedom: 4366 (by level: 8, 25, 89, 337, 836, 2270, 897, 1617)    Number of CG iterations: 10 

Cycle 7    Number of active cells:       7574    Number of degrees of freedom: 8350 (by level: 8, 25, 89, 337, 1086, 2835, 2268, 1789, 3217)    Number of CG iterations: 11 </pre> 

That's almost perfect multigrid performance: the linear residual gets reduced by 12 orders of magnitude in 10 iteration steps, and the results are almost independent of the mesh size. That's obviously in part due to the simple nature of the problem solved, but it shows the power of multigrid methods. 


[1.x.78][1.x.79] 




We encourage you to generate timings for the solve() call and compare to  [2.x.31] . You will see that the multigrid method has quite an overhead on coarse meshes, but that it always beats other methods on fine meshes because of its optimal complexity. 

A close inspection of this program's performance shows that it is mostly dominated by matrix-vector operations.  [2.x.32]  shows one way how this can be avoided by working with matrix-free methods. 

Another avenue would be to use algebraic multigrid methods. The geometric multigrid method used here can at times be a bit awkward to implement because it needs all those additional data structures, and it becomes even more difficult if the program is to run in %parallel on machines coupled through MPI, for example. In that case, it would be simpler if one could use a black-box preconditioner that uses some sort of multigrid hierarchy for good performance but can figure out level matrices and similar things by itself. Algebraic multigrid methods do exactly this, and we will use them in  [2.x.33]  for the solution of a Stokes problem and in  [2.x.34]  and  [2.x.35]  for a parallel variation. That said, a parallel version of this example program with MPI can be found in  [2.x.36] . 

Finally, one may want to think how to use geometric multigrid for other kinds of problems, specifically  [2.x.37]  "vector valued problems". This is the topic of  [2.x.38]  where we use the techniques shown here for the Stokes equation. [1.x.80] [1.x.81]  [2.x.39]  

 [2.x.40] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21] 

[1.x.22] [1.x.23][1.x.24] 


[1.x.25][1.x.26] 


This program does not introduce any new mathematical ideas; in fact, all it does is to do the exact same computations that  [2.x.3]  already does, but it does so in a different manner: instead of using deal.II's own linear algebra classes, we build everything on top of classes deal.II provides that wrap around the linear algebra implementation of the [1.x.27] library. And since PETSc allows to distribute matrices and vectors across several computers within an MPI network, the resulting code will even be able to solve the problem in %parallel. If you don't know what PETSc is, then this would be a good time to take a quick glimpse at their homepage. 

As a prerequisite of this program, you need to have PETSc installed, and if you want to run in %parallel on a cluster, you also need [1.x.28] to partition meshes. The installation of deal.II together with these two additional libraries is described in the [1.x.29] file. 

Now, for the details: as mentioned, the program does not compute anything new, so the use of finite element classes, etc., is exactly the same as before. The difference to previous programs is that we have replaced almost all uses of classes  [2.x.4]  by their near-equivalents  [2.x.5]  and  [2.x.6]  that store data in a way so that every processor in the MPI network only stores a part of the matrix or vector. More specifically, each processor will only store those rows of the matrix that correspond to a degree of freedom it "owns". For vectors, they either store only elements that correspond to degrees of freedom the processor owns (this is what is necessary for the right hand side), or also some additional elements that make sure that every processor has access the solution components that live on the cells the processor owns (so-called  [2.x.7]  "locally active DoFs") or also on neighboring cells (so-called  [2.x.8]  "locally relevant DoFs"). 

The interface the classes from the PETScWrapper namespace provide is very similar to that of the deal.II linear algebra classes, but instead of implementing this functionality themselves, they simply pass on to their corresponding PETSc functions. The wrappers are therefore only used to give PETSc a more modern, object oriented interface, and to make the use of PETSc and deal.II objects as interchangeable as possible. The main point of using PETSc is that it can run in %parallel. We will make use of this by partitioning the domain into as many blocks ("subdomains") as there are processes in the MPI network. At the same time, PETSc also provides dummy MPI stubs, so you can run this program on a single machine if PETSc was configured without MPI. 


[1.x.30][1.x.31] 


Developing software to run in %parallel via MPI requires a bit of a change in mindset because one typically has to split up all data structures so that every processor only stores a piece of the entire problem. As a consequence, you can't typically access all components of a solution vector on each processor -- each processor may simply not have enough memory to hold the entire solution vector. Because data is split up or "distributed" across processors, we call the programming model used by MPI "distributed memory computing" (as opposed to "shared memory computing", which would mean that multiple processors can all access all data within one memory space, for example whenever multiple cores in a single machine work on a common task). Some of the fundamentals of distributed memory computing are discussed in the  [2.x.9]  "Parallel computing with multiple processors using distributed memory" documentation module, which is itself a sub-module of the  [2.x.10]  "Parallel computing" module. 

In general, to be truly able to scale to large numbers of processors, one needs to split between the available processors [1.x.32] data structure whose size scales with the size of the overall problem. (For a definition of what it means for a program to "scale", see  [2.x.11]  "this glossary entry".) This includes, for example, the triangulation, the matrix, and all global vectors (solution, right hand side). If one doesn't split all of these objects, one of those will be replicated on all processors and will eventually simply become too large if the problem size (and the number of available processors) becomes large. (On the other hand, it is completely fine to keep objects with a size that is independent of the overall problem size on every processor. For example, each copy of the executable will create its own finite element object, or the local matrix we use in the assembly.) 

In the current program (as well as in the related  [2.x.12] ), we will not go quite this far but present a gentler introduction to using MPI. More specifically, the only data structures we will parallelize are matrices and vectors. We do, however, not split up the Triangulation and DoFHandler classes: each process still has a complete copy of these objects, and all processes have exact copies of what the other processes have. We will then simply have to mark, in each copy of the triangulation on each of the processors, which processor owns which cells. This process is called "partitioning" a mesh into  [2.x.13]  "subdomains". 

For larger problems, having to store the [1.x.33] mesh on every processor will clearly yield a bottleneck. Splitting up the mesh is slightly, though not much more complicated (from a user perspective, though it is [1.x.34] more complicated under the hood) to achieve and we will show how to do this in  [2.x.14]  and some other programs. There are numerous occasions where, in the course of discussing how a function of this program works, we will comment on the fact that it will not scale to large problems and why not. All of these issues will be addressed in  [2.x.15]  and in particular  [2.x.16] , which scales to very large numbers of processes. 

Philosophically, the way MPI operates is as follows. You typically run a program via 

[1.x.35] 

which means to run it on (say) 32 processors. (If you are on a cluster system, you typically need to [1.x.36] the program to run whenever 32 processors become available; this will be described in the documentation of your cluster. But under the hood, whenever those processors become available, the same call as above will generally be executed.) What this does is that the MPI system will start 32 [1.x.37] of the  [2.x.17]  executable. (The MPI term for each of these running executables is that you have 32  [2.x.18]  "MPI processes".) This may happen on different machines that can't even read from each others' memory spaces, or it may happen on the same machine, but the end result is the same: each of these 32 copies will run with some memory allocated to it by the operating system, and it will not directly be able to read the memory of the other 31 copies. In order to collaborate in a common task, these 32 copies then have to [1.x.38] with each other. MPI, short for [1.x.39], makes this possible by allowing programs to [1.x.40]. You can think of this as the mail service: you can put a letter to a specific address into the mail and it will be delivered. But that's the extent to which you can control things. If you want the receiver to do something with the content of the letter, for example return to you data you want from over there, then two things need to happen: (i) the receiver needs to actually go check whether there is anything in their mailbox, and (ii) if there is, react appropriately, for example by sending data back. If you wait for this return message but the original receiver was distracted and not paying attention, then you're out of luck: you'll simply have to wait until your requested over there will be worked on. In some cases, bugs will lead the original receiver to never check your mail, and in that case you will wait forever -- this is called a [1.x.41]. ( [2.x.19]  

In practice, one does not usually program at the level of sending and receiving individual messages, but uses higher level operations. For example, in the program we will use function calls that take a number from each processor, add them all up, and return the sum to all processors. Internally, this is implemented using individual messages, but to the user this is transparent. We call such operations [1.x.42] because [1.x.43] processors participate in them. Collectives allow us to write programs where not every copy of the executable is doing something completely different (this would be incredibly difficult to program) but where in essence all copies are doing the same thing (though on different data) for themselves, running through the same blocks of code; then they communicate data through collectives; and then go back to doing something for themselves again running through the same blocks of data. This is the key piece to being able to write programs, and it is the key component to making sure that programs can run on any number of processors, since we do not have to write different code for each of the participating processors. 

(This is not to say that programs are never written in ways where different processors run through different blocks of code in their copy of the executable. Programs internally also often communicate in other ways than through collectives. But in practice, %parallel finite element codes almost always follow the scheme where every copy of the program runs through the same blocks of code at the same time, interspersed by phases where all processors communicate with each other.) 

In reality, even the level of calling MPI collective functions is too low. Rather, the program below will not contain any direct calls to MPI at all, but only deal.II functions that hide this communication from users of the deal.II. This has the advantage that you don't have to learn the details of MPI and its rather intricate function calls. That said, you do have to understand the general philosophy behind MPI as outlined above. 


[1.x.44][1.x.45] 


The techniques this program then demonstrates are: 

- How to use the PETSc wrapper classes; this will already be visible in the   declaration of the principal class of this program,  [2.x.20] . 

- How to partition the mesh into subdomains; this happens in the    [2.x.21]  function. 

- How to parallelize operations for jobs running on an MPI network; here, this   is something one has to pay attention to in a number of places, most   notably in the   [2.x.22]  function. 

- How to deal with vectors that store only a subset of vector entries   and for which we have to ensure that they store what we need on the   current processors. See for example the    [2.x.23]    functions. 

- How to deal with status output from programs that run on multiple   processors at the same time. This is done via the  [2.x.24]    variable in the program, initialized in the constructor. 

Since all this can only be demonstrated using actual code, let us go straight to the code without much further ado. [1.x.46] [1.x.47] 


[1.x.48]  [1.x.49] 




First the usual assortment of header files we have already used in previous example programs: 

[1.x.50] 



And here come the things that we need particularly for this example program and that weren't in  [2.x.25] . First, we replace the standard output  [2.x.26]  which is used in parallel computations for generating output only on one of the MPI processes. 

[1.x.51] 



We are going to query the number of processes and the number of the present process by calling the respective functions in the  [2.x.27]  namespace. 

[1.x.52] 



Then, we are going to replace all linear algebra components that involve the (global) linear system by classes that wrap interfaces similar to our own linear algebra classes around what PETSc offers (PETSc is a library written in C, and deal.II comes with wrapper classes that provide the PETSc functionality with an interface that is similar to the interface we already had for our own linear algebra classes). In particular, we need vectors and matrices that are distributed across several  [2.x.28]  "processes" in MPI programs (and simply map to sequential, local vectors and matrices if there is only a single process, i.e., if you are running on only one machine, and without MPI support): 

[1.x.53] 



Then we also need interfaces for solvers and preconditioners that PETSc provides: 

[1.x.54] 



And in addition, we need some algorithms for partitioning our meshes so that they can be efficiently distributed across an MPI network. The partitioning algorithm is implemented in the  [2.x.29]  namespace, and we need an additional include file for a function in  [2.x.30]  that allows to sort the indices associated with degrees of freedom so that they are numbered according to the subdomain they are associated with: 

[1.x.55] 



And this is simply C++ again: 

[1.x.56] 



The last step is as in all previous programs: 

[1.x.57] 




[1.x.58]  [1.x.59] 




The first real part of the program is the declaration of the main class.  As mentioned in the introduction, almost all of this has been copied verbatim from  [2.x.31] , so we only comment on the few differences between the two tutorials.  There is one (cosmetic) change in that we let  [2.x.32]  return a value, namely the number of iterations it took to converge, so that we can output this to the screen at the appropriate place. 

[1.x.60] 



The first change is that we have to declare a variable that indicates the  [2.x.33]  "MPI communicator" over which we are supposed to distribute our computations. 

[1.x.61] 



Then we have two variables that tell us where in the parallel world we are. The first of the following variables,  [2.x.34] , tells us how many MPI processes there exist in total, while the second one,  [2.x.35] , indicates which is the number of the present process within this space of processes (in MPI language, this corresponds to the  [2.x.36]  "rank" of the process). The latter will have a unique value for each process between zero and (less than)  [2.x.37] . If this program is run on a single machine without MPI support, then their values are  [2.x.38] , respectively. 

[1.x.62] 



Next up is a stream-like variable  [2.x.39] . It is, in essence, just something we use for convenience: in a parallel program, if each process outputs status information, then there quickly is a lot of clutter. Rather, we would want to only have one  [2.x.40]  "process" output everything once, for example the one with  [2.x.41]  "rank" zero. At the same time, it seems silly to prefix [1.x.63] place where we create output with an  [2.x.42]  condition.      


To make this simpler, the ConditionalOStream class does exactly this under the hood: it acts as if it were a stream, but only forwards to a real, underlying stream if a flag is set. By setting this condition to  [2.x.43]  (where  [2.x.44]  corresponds to the rank of an MPI process), we make sure that output is only generated from the first process and that we don't get the same lines of output over and over again, once per process. Thus, we can use  [2.x.45]  everywhere and in every process, but on all but one process nothing will ever happen to the information that is piped into the object via  [2.x.46] . 

[1.x.64] 



The remainder of the list of member variables is fundamentally the same as in  [2.x.47] . However, we change the declarations of matrix and vector types to use parallel PETSc objects instead. Note that we do not use a separate sparsity pattern, since PETSc manages this internally as part of its matrix data structures. 

[1.x.65] 




[1.x.66]  [1.x.67] 




The following is taken from  [2.x.48]  without change: 

[1.x.68] 




[1.x.69]  [1.x.70] 





[1.x.71]  [1.x.72] 




The first step in the actual implementation is the constructor of the main class. Apart from initializing the same member variables that we already had in  [2.x.49] , we here initialize the MPI communicator variable we shall use with the global MPI communicator linking all processes together (in more complex applications, one could here use a communicator object that only links a subset of all processes), and call the  [2.x.50]  helper functions to determine the number of processes and where the present one fits into this picture. In addition, we make sure that output is only generated by the (globally) first process. We do so by passing the stream we want to output to  [2.x.51]  and a true/false flag as arguments where the latter is determined by testing whether the process currently executing the constructor call is the first in the MPI universe. 

[1.x.73] 




[1.x.74]  [1.x.75] 




Next, the function in which we set up the various variables for the global linear system to be solved needs to be implemented.    


However, before we proceed with this, there is one thing to do for a parallel program: we need to determine which MPI process is responsible for each of the cells. Splitting cells among processes, commonly called "partitioning the mesh", is done by assigning a  [2.x.52]  "subdomain id" to each cell. We do so by calling into the METIS library that does this in a very efficient way, trying to minimize the number of nodes on the interfaces between subdomains. Rather than trying to call METIS directly, we do this by calling the  [2.x.53]  function that does this at a much higher level of programming.    




 [2.x.54]  As mentioned in the introduction, we could avoid this manual partitioning step if we used the  [2.x.55]  class for the triangulation object instead (as we do in  [2.x.56] ). That class does, in essence, everything a regular triangulation does, but it then also automatically partitions the mesh after every mesh creation or refinement operation.    


Following partitioning, we need to enumerate all degrees of freedom as usual.  However, we would like to enumerate the degrees of freedom in a way so that all degrees of freedom associated with cells in subdomain zero (which resides on process zero) come before all DoFs associated with cells on subdomain one, before those on cells on process two, and so on. We need this since we have to split the global vectors for right hand side and solution, as well as the matrix into contiguous chunks of rows that live on each of the processors, and we will want to do this in a way that requires minimal communication. This particular enumeration can be obtained by re-ordering degrees of freedom indices using  [2.x.57]     


The final step of this initial setup is that we get ourselves an IndexSet that indicates the subset of the global number of unknowns this process is responsible for. (Note that a degree of freedom is not necessarily owned by the process that owns a cell just because the degree of freedom lives on this cell: some degrees of freedom live on interfaces between subdomains, and are consequently only owned by one of the processes adjacent to this interface.)    


Before we move on, let us recall a fact already discussed in the introduction: The triangulation we use here is replicated across all processes, and each process has a complete copy of the entire triangulation, with all cells. Partitioning only provides a way to identify which cells out of all each process "owns", but it knows everything about all of them. Likewise, the DoFHandler object knows everything about every cell, in particular the degrees of freedom that live on each cell, whether it is one that the current process owns or not. This can not scale to large problems because eventually just storing the entire mesh, and everything that is associated with it, on every process will become infeasible if the problem is large enough. On the other hand, if we split the triangulation into parts so that every process stores only those cells it "owns" but nothing else (or, at least a sufficiently small fraction of everything else), then we can solve large problems if only we throw a large enough number of MPI processes at them. This is what we are going to in  [2.x.58] , for example, using the  [2.x.59]  class.  On the other hand, most of the rest of what we demonstrate in the current program will actually continue to work whether we have the entire triangulation available, or only a piece of it. 

[1.x.76] 



We need to initialize the objects denoting hanging node constraints for the present grid. As with the triangulation and DoFHandler objects, we will simply store [1.x.77] constraints on each process; again, this will not scale, but we show in  [2.x.60]  how one can work around this by only storing on each MPI process the constraints for degrees of freedom that actually matter on this particular process. 

[1.x.78] 



Now we create the sparsity pattern for the system matrix. Note that we again compute and store all entries and not only the ones relevant to this process (see  [2.x.61]  or  [2.x.62]  for a more efficient way to handle this). 

[1.x.79] 



Now we determine the set of locally owned DoFs and use that to initialize parallel vectors and matrix. Since the matrix and vectors need to work in parallel, we have to pass them an MPI communication object, as well as information about the partitioning contained in the IndexSet  [2.x.63]   The IndexSet contains information about the global size (the [1.x.80] number of degrees of freedom) and also what subset of rows is to be stored locally.  Note that the system matrix needs that partitioning information for the rows and columns. For square matrices, as it is the case here, the columns should be partitioned in the same way as the rows, but in the case of rectangular matrices one has to partition the columns in the same way as vectors are partitioned with which the matrix is multiplied, while rows have to partitioned in the same way as destination vectors of matrix-vector multiplications: 

[1.x.81] 




[1.x.82]  [1.x.83] 




We now assemble the matrix and right hand side of the problem. There are some things worth mentioning before we go into detail. First, we will be assembling the system in parallel, i.e., each process will be responsible for assembling on cells that belong to this particular process. Note that the degrees of freedom are split in a way such that all DoFs in the interior of cells and between cells belonging to the same subdomain belong to the process that  [2.x.64]  the cell. However, even then we sometimes need to assemble on a cell with a neighbor that belongs to a different process, and in these cases when we add up the local contributions into the global matrix or right hand side vector, we have to transfer these entries to the process that owns these elements. Fortunately, we don't have to do this by hand: PETSc does all this for us by caching these elements locally, and sending them to the other processes as necessary when we call the  [2.x.65]  functions on the matrix and vector at the end of this function.    


The second point is that once we have handed over matrix and vector contributions to PETSc, it is a) hard, and b) very inefficient to get them back for modifications. This is not only the fault of PETSc, it is also a consequence of the distributed nature of this program: if an entry resides on another processor, then it is necessarily expensive to get it. The consequence of this is that we should not try to first assemble the matrix and right hand side as if there were no hanging node constraints and boundary values, and then eliminate these in a second step (using, for example,  [2.x.66]  Rather, we should try to eliminate hanging node constraints before handing these entries over to PETSc. This is easy: instead of copying elements by hand into the global matrix (as we do in  [2.x.67] ), we use the  [2.x.68]  functions to take care of hanging nodes at the same time. We also already did this in  [2.x.69] . The second step, elimination of boundary nodes, could also be done this way by putting the boundary values into the same AffineConstraints object as hanging nodes (see the way it is done in  [2.x.70] , for example); however, it is not strictly necessary to do this here because eliminating boundary values can be done with only the data stored on each process itself, and consequently we use the approach used before in  [2.x.71] , i.e., via  [2.x.72]     


All of this said, here is the actual implementation starting with the general setup of helper variables.  (Note that we still use the deal.II full matrix and vector types for the local systems as these are small and need not be shared across processes.) 

[1.x.84] 



The next thing is the loop over all elements. Note that we do not have to do [1.x.85] the work on every process: our job here is only to assemble the system on cells that actually belong to this MPI process, all other cells will be taken care of by other processes. This is what the if-clause immediately after the for-loop takes care of: it queries the subdomain identifier of each cell, which is a number associated with each cell that tells us about the owner process. In more generality, the subdomain id is used to split a domain into several parts (we do this above, at the beginning of  [2.x.73] ), and which allows to identify which subdomain a cell is living on. In this application, we have each process handle exactly one subdomain, so we identify the terms  [2.x.74] .      


Apart from this, assembling the local system is relatively uneventful if you have understood how this is done in  [2.x.75] . As mentioned above, distributing local contributions into the global matrix and right hand sides also takes care of hanging node constraints in the same way as is done in  [2.x.76] . 

[1.x.86] 



The next step is to "compress" the vector and the system matrix. This means that each process sends the additions that were made to those entries of the matrix and vector that the process did not own itself to the process that owns them. After receiving these additions from other processes, each process then adds them to the values it already has. These additions are combining the integral contributions of shape functions living on several cells just as in a serial computation, with the difference that the cells are assigned to different processes. 

[1.x.87] 



The global matrix and right hand side vectors have now been formed. We still have to apply boundary values, in the same way as we did, for example, in  [2.x.77] ,  [2.x.78] , and a number of other programs.      


The last argument to the call to  [2.x.79]  below allows for some optimizations. It controls whether we should also delete entries (i.e., set them to zero) in the matrix columns corresponding to boundary nodes, or to keep them (and passing  [2.x.80]  means: yes, do eliminate the columns). If we do eliminate columns, then the resulting matrix will be symmetric again if it was before; if we don't, then it won't. The solution of the resulting system should be the same, though. The only reason why we may want to make the system symmetric again is that we would like to use the CG method, which only works with symmetric matrices. The reason why we may [1.x.88] want to make the matrix symmetric is because this would require us to write into column entries that actually reside on other processes, i.e., it involves communicating data. This is always expensive.      


Experience tells us that CG also works (and works almost as well) if we don't remove the columns associated with boundary nodes, which can be explained by the special structure of this particular non-symmetry. To avoid the expense of communication, we therefore do not eliminate the entries in the affected columns. 

[1.x.89] 




[1.x.90]  [1.x.91] 




Having assembled the linear system, we next need to solve it. PETSc offers a variety of sequential and parallel solvers, for which we have written wrappers that have almost the same interface as is used for the deal.II solvers used in all previous example programs. The following code should therefore look rather familiar.    


At the top of the function, we set up a convergence monitor, and assign it the accuracy to which we would like to solve the linear system. Next, we create an actual solver object using PETSc's CG solver which also works with parallel (distributed) vectors and matrices. And finally a preconditioner; we choose to use a block Jacobi preconditioner which works by computing an incomplete LU decomposition on each diagonal block of the matrix.  (In other words, each MPI process computes an ILU from the rows it stores by throwing away columns that correspond to row indices not stored locally; this yields a square matrix block from which we can compute an ILU. That means that if you run the program with only one process, then you will use an ILU(0) as a preconditioner, while if it is run on many processes, then we will have a number of blocks on the diagonal and the preconditioner is the ILU(0) of each of these blocks. In the extreme case of one degree of freedom per processor, this preconditioner is then simply a Jacobi preconditioner since the diagonal matrix blocks consist of only a single entry. Such a preconditioner is relatively easy to compute because it does not require any kind of communication between processors, but it is in general not very efficient for large numbers of processors.)    


Following this kind of setup, we then solve the linear system: 

[1.x.92] 



The next step is to distribute hanging node constraints. This is a little tricky, since to fill in the value of a constrained node you need access to the values of the nodes to which it is constrained (for example, for a Q1 element in 2d, we need access to the two nodes on the big side of a hanging node face, to compute the value of the constrained node in the middle).      


The problem is that we have built our vectors (in  [2.x.81] ) in such a way that every process is responsible for storing only those elements of the solution vector that correspond to the degrees of freedom this process "owns". There are, however, cases where in order to compute the value of the vector entry for a constrained degree of freedom on one process, we need to access vector entries that are stored on other processes.  PETSc (and, for that matter, the MPI model on which it is built) does not allow to simply query vector entries stored on other processes, so what we do here is to get a copy of the "distributed" vector where we store all elements locally. This is simple, since the deal.II wrappers have a conversion constructor for the deal.II Vector class. (This conversion of course requires communication, but in essence every process only needs to send its data to every other process once in bulk, rather than having to respond to queries for individual elements): 

[1.x.93] 



Of course, as in previous discussions, it is clear that such a step cannot scale very far if you wanted to solve large problems on large numbers of processes, because every process now stores [1.x.94] of the solution vector. (We will show how to do this better in  [2.x.82] .)  On the other hand, distributing hanging node constraints is simple on this local copy, using the usual function  [2.x.83]  In particular, we can compute the values of [1.x.95] constrained degrees of freedom, whether the current process owns them or not: 

[1.x.96] 



Then transfer everything back into the global vector. The following operation copies those elements of the localized solution that we store locally in the distributed solution, and does not touch the other ones. Since we do the same operation on all processors, we end up with a distributed vector (i.e., a vector that on every process only stores the vector entries corresponding to degrees of freedom that are owned by this process) that has all the constrained nodes fixed.      


We end the function by returning the number of iterations it took to converge, to allow for some output. 

[1.x.97] 




[1.x.98]  [1.x.99] 




Using some kind of refinement indicator, the mesh can be refined. The problem is basically the same as with distributing hanging node constraints: in order to compute the error indicator (even if we were just interested in the indicator on the cells the current process owns), we need access to more elements of the solution vector than just those the current processor stores. To make this happen, we do essentially what we did in  [2.x.84]  already, namely get a [1.x.100] copy of the solution vector onto every process, and use that to compute. This is in itself expensive as explained above and it is particular unnecessary since we had just created and then destroyed such a vector in  [2.x.85] , but efficiency is not the point of this program and so let us opt for a design in which every function is as self-contained as possible.    


Once we have such a "localized" vector that contains [1.x.101] elements of the solution vector, we can compute the indicators for the cells that belong to the present process. In fact, we could of course compute [1.x.102] refinement indicators since our Triangulation and DoFHandler objects store information about all cells, and since we have a complete copy of the solution vector. But in the interest in showing how to operate in %parallel, let us demonstrate how one would operate if one were to only compute [1.x.103] error indicators and then exchange the remaining ones with the other processes. (Ultimately, each process needs a complete set of refinement indicators because every process needs to refine their mesh, and needs to refine it in exactly the same way as all of the other processes.)    


So, to do all of this, we need to: 

- First, get a local copy of the distributed solution vector. 

- Second, create a vector to store the refinement indicators. 

- Third, let the KellyErrorEstimator compute refinement indicators for all cells belonging to the present subdomain/process. The last argument of the call indicates which subdomain we are interested in. The three arguments before it are various other default arguments that one usually does not need (and does not state values for, but rather uses the defaults), but which we have to state here explicitly since we want to modify the value of a following argument (i.e., the one indicating the subdomain). 

[1.x.104] 



Now all processes have computed error indicators for their own cells and stored them in the respective elements of the  [2.x.86]  vector. The elements of this vector for cells not owned by the present process are zero. However, since all processes have a copy of the entire triangulation and need to keep these copies in sync, they need the values of refinement indicators for all cells of the triangulation. Thus, we need to distribute our results. We do this by creating a distributed vector where each process has its share and sets the elements it has computed. Consequently, when you view this vector as one that lives across all processes, then every element of this vector has been set once. We can then assign this parallel vector to a local, non-parallel vector on each process, making [1.x.105] error indicators available on every process.      


So in the first step, we need to set up a parallel vector. For simplicity, every process will own a chunk with as many elements as this process owns cells, so that the first chunk of elements is stored with process zero, the next chunk with process one, and so on. It is important to remark, however, that these elements are not necessarily the ones we will write to. This is a consequence of the order in which cells are arranged, i.e., the order in which the elements of the vector correspond to cells is not ordered according to the subdomain these cells belong to. In other words, if on this process we compute indicators for cells of a certain subdomain, we may write the results to more or less random elements of the distributed vector; in particular, they may not necessarily lie within the chunk of vector we own on the present process. They will subsequently have to be copied into another process' memory space, an operation that PETSc does for us when we call the  [2.x.87]  function. This inefficiency could be avoided with some more code, but we refrain from it since it is not a major factor in the program's total runtime.      


So here is how we do it: count how many cells belong to this process, set up a distributed vector with that many elements to be stored locally, copy over the elements we computed locally, and finally compress the result. In fact, we really only copy the elements that are nonzero, so we may miss a few that we computed to zero, but this won't hurt since the original values of the vector are zero anyway. 

[1.x.106] 



So now we have this distributed vector that contains the refinement indicators for all cells. To use it, we need to obtain a local copy and then use it to mark cells for refinement or coarsening, and actually do the refinement and coarsening. It is important to recognize that [1.x.107] process does this to its own copy of the triangulation, and does it in exactly the same way. 

[1.x.108] 




[1.x.109]  [1.x.110] 




The final function of significant interest is the one that creates graphical output. This works the same way as in  [2.x.88] , with two small differences. Before discussing these, let us state the general philosophy this function will work: we intend for all of the data to be generated on a single process, and subsequently written to a file. This is, as many other parts of this program already discussed, not something that will scale. Previously, we had argued that we will get into trouble with triangulations, DoFHandlers, and copies of the solution vector where every process has to store all of the data, and that there will come to be a point where each process simply doesn't have enough memory to store that much data. Here, the situation is different: it's not only the memory, but also the run time that's a problem. If one process is responsible for processing [1.x.111] of the data while all of the other processes do nothing, then this one function will eventually come to dominate the overall run time of the program.  In particular, the time this function takes is going to be proportional to the overall size of the problem (counted in the number of cells, or the number of degrees of freedom), independent of the number of processes we throw at it.    


Such situations need to be avoided, and we will show in  [2.x.89]  and  [2.x.90]  how to address this issue. For the current problem, the solution is to have each process generate output data only for its own local cells, and write them to separate files, one file per process. This is how  [2.x.91]  operates. Alternatively, one could simply leave everything in a set of independent files and let the visualization software read all of them (possibly also using multiple processors) and create a single visualization out of all of them; this is the path  [2.x.92] ,  [2.x.93] , and all other parallel programs developed later on take.    


More specifically for the current function, all processes call this function, but not all of them need to do the work associated with generating output. In fact, they shouldn't, since we would try to write to the same file multiple times at once. So we let only the first process do this, and all the other ones idle around during this time (or start their work for the next iteration, or simply yield their CPUs to other jobs that happen to run at the same time). The second thing is that we not only output the solution vector, but also a vector that indicates which subdomain each cell belongs to. This will make for some nice pictures of partitioned domains.    


To implement this, process zero needs a complete set of solution components in a local vector. Just as with the previous function, the efficient way to do this would be to re-use the vector already created in the  [2.x.94]  function, but to keep things more self-contained, we simply re-create one here from the distributed solution vector.    


An important thing to realize is that we do this localization operation on all processes, not only the one that actually needs the data. This can't be avoided, however, with the simplified communication model of MPI we use for vectors in this tutorial program: MPI does not have a way to query data on another process, both sides have to initiate a communication at the same time. So even though most of the processes do not need the localized solution, we have to place the statement converting the distributed into a localized vector so that all processes execute it.    


(Part of this work could in fact be avoided. What we do is send the local parts of all processes to all other processes. What we would really need to do is to initiate an operation on all processes where each process simply sends its local chunk of data to process zero, since this is the only one that actually needs it, i.e., we need something like a gather operation. PETSc can do this, but for simplicity's sake we don't attempt to make use of this here. We don't, since what we do is not very expensive in the grand scheme of things: it is one vector communication among all processes, which has to be compared to the number of communications we have to do when solving the linear system, setting up the block-ILU for the preconditioner, and other operations.) 

[1.x.112] 



This being done, process zero goes ahead with setting up the output file as in  [2.x.95] , and attaching the (localized) solution vector to the output object. 

[1.x.113] 



The only other thing we do here is that we also output one value per cell indicating which subdomain (i.e., MPI process) it belongs to. This requires some conversion work, since the data the library provides us with is not the one the output class expects, but this is not difficult. First, set up a vector of integers, one per cell, that is then filled by the subdomain id of each cell.          


The elements of this vector are then converted to a floating point vector in a second step, and this vector is added to the DataOut object, which then goes off creating output in VTK format: 

[1.x.114] 




[1.x.115]  [1.x.116] 




Lastly, here is the driver function. It is almost completely unchanged from  [2.x.96] , with the exception that we replace  [2.x.97]  stream. Apart from this, the only other cosmetic change is that we output how many degrees of freedom there are per process, and how many iterations it took for the linear solver to converge: 

[1.x.117] 




[1.x.118]  [1.x.119] 




The  [2.x.98]  works the same way as most of the main functions in the other example programs, i.e., it delegates work to the  [2.x.99]  function of a managing object, and only wraps everything into some code to catch exceptions: 

[1.x.120] 



Here is the only real difference: MPI and PETSc both require that we initialize these libraries at the beginning of the program, and un-initialize them at the end. The class MPI_InitFinalize takes care of all of that. The trailing argument `1` means that we do want to run each MPI process with a single thread, a prerequisite with the PETSc parallel linear algebra. 

[1.x.121] 

[1.x.122][1.x.123] 




If the program above is compiled and run on a single processor machine, it should generate results that are very similar to those that we already got with  [2.x.100] . However, it becomes more interesting if we run it on a multicore machine or a cluster of computers. The most basic way to run MPI programs is using a command line like 

[1.x.124] 

to run the  [2.x.101]  executable with 32 processors. 

(If you work on a cluster, then there is typically a step in between where you need to set up a job script and submit the script to a scheduler. The scheduler will execute the script whenever it can allocate 32 unused processors for your job. How to write such job scripts differs from cluster to cluster, and you should find the documentation of your cluster to see how to do this. On my system, I have to use the command  [2.x.102]  with a whole host of options to run a job in parallel.) 

Whether directly or through a scheduler, if you run this program on 8 processors, you should get output like the following: 

[1.x.125] 

(This run uses a few more refinement cycles than the code available in the examples/ directory. The run also used a version of METIS from 2004 that generated different partitionings; consequently, the numbers you get today are slightly different.) 

As can be seen, we can easily get to almost four million unknowns. In fact, the code's runtime with 8 processes was less than 7 minutes up to (and including) cycle 14, and 14 minutes including the second to last step. (These are numbers relevant to when the code was initially written, in 2004.) I lost the timing information for the last step, though, but you get the idea. All this is after release mode has been enabled by running  [2.x.103] , and with the generation of graphical output switched off for the reasons stated in the program comments above. ( [2.x.104]  The biggest 2d computations I did had roughly 7.1 million unknowns, and were done on 32 processes. It took about 40 minutes. Not surprisingly, the limiting factor for how far one can go is how much memory one has, since every process has to hold the entire mesh and DoFHandler objects, although matrices and vectors are split up. For the 7.1M computation, the memory consumption was about 600 bytes per unknown, which is not bad, but one has to consider that this is for every unknown, whether we store the matrix and vector entries locally or not. 




Here is some output generated in the 12th cycle of the program, i.e. with roughly 300,000 unknowns: 

 [2.x.105]  

As one would hope for, the x- (left) and y-displacements (right) shown here closely match what we already saw in  [2.x.106] . As shown there and in  [2.x.107] , we could as well have produced a vector plot of the displacement field, rather than plotting it as two separate scalar fields. What may be more interesting, though, is to look at the mesh and partition at this step: 

 [2.x.108]  

Again, the mesh (left) shows the same refinement pattern as seen previously. The right panel shows the partitioning of the domain across the 8 processes, each indicated by a different color. The picture shows that the subdomains are smaller where mesh cells are small, a fact that needs to be expected given that the partitioning algorithm tries to equilibrate the number of cells in each subdomain; this equilibration is also easily identified in the output shown above, where the number of degrees per subdomain is roughly the same. 




It is worth noting that if we ran the same program with a different number of processes, that we would likely get slightly different output: a different mesh, different number of unknowns and iterations to convergence. The reason for this is that while the matrix and right hand side are the same independent of the number of processes used, the preconditioner is not: it performs an ILU(0) on the chunk of the matrix of  [2.x.109] each processor separately [2.x.110] . Thus, it's effectiveness as a preconditioner diminishes as the number of processes increases, which makes the number of iterations increase. Since a different preconditioner leads to slight changes in the computed solution, this will then lead to slightly different mesh cells tagged for refinement, and larger differences in subsequent steps. The solution will always look very similar, though. 




Finally, here are some results for a 3d simulation. You can repeat these by changing 

[1.x.126] 

to 

[1.x.127] 

in the main function. If you then run the program in parallel, you get something similar to this (this is for a job with 16 processes): 

[1.x.128] 






The last step, going up to 1.5 million unknowns, takes about 55 minutes with 16 processes on 8 dual-processor machines (of the kind available in 2003). The graphical output generated by this job is rather large (cycle 5 already prints around 82 MB of data), so we contend ourselves with showing output from cycle 4: 

 [2.x.111]  




The left picture shows the partitioning of the cube into 16 processes, whereas the right one shows the x-displacement along two cutplanes through the cube. 




[1.x.129] [1.x.130][1.x.131] 


The program keeps a complete copy of the Triangulation and DoFHandler objects on every processor. It also creates complete copies of the solution vector, and it creates output on only one processor. All of this is obviously the bottleneck as far as parallelization is concerned. 

Internally, within deal.II, parallelizing the data structures used in hierarchic and unstructured triangulations is a hard problem, and it took us a few more years to make this happen. The  [2.x.112]  tutorial program and the  [2.x.113]  documentation module talk about how to do these steps and what it takes from an application perspective. An obvious extension of the current program would be to use this functionality to completely distribute computations to many more processors than used here. [1.x.132] [1.x.133]  [2.x.114]  

 [2.x.115] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37] 

[1.x.38] [1.x.39][1.x.40] 




This tutorial program is another one in the series on the elasticity problem that we have already started with  [2.x.3]  and  [2.x.4] . It extends it into two different directions: first, it solves the quasistatic but time dependent elasticity problem for large deformations with a Lagrangian mesh movement approach. Secondly, it shows some more techniques for solving such problems using %parallel processing with PETSc's linear algebra. In addition to this, we show how to work around one of the two major bottlenecks of  [2.x.5] , namely that we generated graphical output from only one process, and that this scaled very badly with larger numbers of processes and on large problems. (The other bottleneck, namely that every processor has to hold the entire mesh and DoFHandler, is addressed in  [2.x.6] .) Finally, a good number of assorted improvements and techniques are demonstrated that have not been shown yet in previous programs. 

As before in  [2.x.7] , the program runs just as fine on a single sequential machine as long as you have PETSc installed. Information on how to tell deal.II about a PETSc installation on your system can be found in the deal.II README file, which is linked to from the [1.x.41] in your installation of deal.II, or on [1.x.42]. 


[1.x.43][1.x.44] 


[1.x.45][1.x.46] 


In general, time-dependent small elastic deformations are described by the elastic wave equation [1.x.47] where  [2.x.8]  is the deformation of the body,  [2.x.9]  and  [2.x.10]  the density and attenuation coefficient, and  [2.x.11]  external forces. In addition, initial conditions [1.x.48] and Dirichlet (displacement) or Neumann (traction) boundary conditions need to be specified for a unique solution: [1.x.49] 

In above formulation,  [2.x.12]  is the symmetric gradient of the displacement, also called the  [2.x.13] strain [2.x.14] .  [2.x.15]  is a tensor of rank 4, called the  [2.x.16] stress-strain   tensor [2.x.17]  (the inverse of the [1.x.50]) that contains knowledge of the elastic strength of the material; its symmetry properties make sure that it maps symmetric tensors of rank 2 (&ldquo;matrices&rdquo; of dimension  [2.x.18] , where  [2.x.19]  is the spatial dimensionality) onto symmetric tensors of the same rank. We will comment on the roles of the strain and stress tensors more below. For the moment it suffices to say that we interpret the term  [2.x.20]  as the vector with components  [2.x.21] , where summation over indices  [2.x.22]  is implied. 

The quasistatic limit of this equation is motivated as follows: each small perturbation of the body, for example by changes in boundary condition or the forcing function, will result in a corresponding change in the configuration of the body. In general, this will be in the form of waves radiating away from the location of the disturbance. Due to the presence of the damping term, these waves will be attenuated on a time scale of, say,  [2.x.23] . Now, assume that all changes in external forcing happen on times scales that are much larger than  [2.x.24] . In that case, the dynamic nature of the change is unimportant: we can consider the body to always be in static equilibrium, i.e. we can assume that at all times the body satisfies [1.x.51] 

Note that the differential equation does not contain any time derivatives any more -- all time dependence is introduced through boundary conditions and a possibly time-varying force function  [2.x.25] . The changes in configuration can therefore be considered as being stationary instantaneously. An alternative view of this is that  [2.x.26]  is not really a time variable, but only a time-like parameter that governs the evolution of the problem. 

While these equations are sufficient to describe small deformations, computing large deformations is a little more complicated and, in general, leads to nonlinear equations such as those treated in  [2.x.27] . In the following, let us consider some of the tools one would employ when simulating problems in which the deformation becomes [1.x.52]. 

 [2.x.28]  The model we will consider below is not founded on anything that would be mathematically sound: we will consider a model in which we produce a small deformation, deform the physical coordinates of the body by this deformation, and then consider the next loading step again as a linear problem. This isn't consistent, since the assumption of linearity implies that deformations are infinitesimal and so moving around the vertices of our mesh by a finite amount before solving the next linear problem is an inconsistent approach. We should therefore note that it is not surprising that the equations discussed below can't be found in the literature: [1.x.53] On the other hand, the implementation techniques we consider are very much what one would need to use when implementing a [1.x.54] model, as we will see in  [2.x.29] . 


To come back to defining our "artificial" model, let us first introduce a tensorial stress variable  [2.x.30] , and write the differential equations in terms of the stress: [1.x.55] 

Note that these equations are posed on a domain  [2.x.31]  that changes with time, with the boundary moving according to the displacements  [2.x.32]  of the points on the boundary. To complete this system, we have to specify the incremental relationship between the stress and the strain, as follows: [1.x.56] [1.x.57] where a dot indicates a time derivative. Both the stress  [2.x.33]  and the strain  [2.x.34]  are symmetric tensors of rank 2. 


[1.x.58][1.x.59] 


Numerically, this system is solved as follows: first, we discretize the time component using a backward Euler scheme. This leads to a discrete equilibrium of force at time step  [2.x.35] : [1.x.60] where [1.x.61] and  [2.x.36]  the incremental displacement for time step  [2.x.37] . In addition, we have to specify initial data  [2.x.38] . This way, if we want to solve for the displacement increment, we have to solve the following system: 

[1.x.62] 

The weak form of this set of equations, which as usual is the basis for the finite element formulation, reads as follows: find  [2.x.39]  such that [1.x.63] 

[1.x.64] 

Using that  [2.x.40] , these equations can be simplified to 

[1.x.65] 



We note that, for simplicity, in the program we will always assume that there are no boundary forces, i.e.  [2.x.41] , and that the deformation of the body is driven by body forces  [2.x.42]  and prescribed boundary displacements  [2.x.43]  alone. It is also worth noting that when integrating by parts, we would get terms of the form  [2.x.44] , but that we replace them with the term involving the symmetric gradient  [2.x.45]  instead of  [2.x.46] . Due to the symmetry of  [2.x.47] , the two terms are mathematically equivalent, but the symmetric version avoids the potential for round-off errors making the resulting matrix slightly non-symmetric. 

The system at time step  [2.x.48] , to be solved on the old domain  [2.x.49] , has exactly the form of a stationary elastic problem, and is therefore similar to what we have already implemented in previous example programs. We will therefore not comment on the space discretization beyond saying that we again use lowest order continuous finite elements. 

There are differences, however:  [2.x.50]     [2.x.51]  We have to move (update) the mesh after each time step, in order to be   able to solve the next time step on a new domain; 

   [2.x.52]  We need to know  [2.x.53]  to compute the next incremental   displacement, i.e. we need to compute it at the end of the time step   to make sure it is available for the next time step. Essentially,   the stress variable is our window to the history of deformation of   the body.  [2.x.54]  These two operations are done in the functions  [2.x.55]  and  [2.x.56]  in the program. While moving the mesh is only a technicality, updating the stress is a little more complicated and will be discussed in the next section. 


[1.x.66][1.x.67] 


As indicated above, we need to have the stress variable  [2.x.57]  available when computing time step  [2.x.58] , and we can compute it using [1.x.68] [1.x.69] There are, despite the apparent simplicity of this equation, two questions that we need to discuss. The first concerns the way we store  [2.x.59] : even if we compute the incremental updates  [2.x.60]  using lowest-order finite elements, then its symmetric gradient  [2.x.61]  is in general still a function that is not easy to describe. In particular, it is not a piecewise constant function, and on general meshes (with cells that are not rectangles %parallel to the coordinate axes) or with non-constant stress-strain tensors  [2.x.62]  it is not even a bi- or trilinear function. Thus, it is a priori not clear how to store  [2.x.63]  in a computer program. 

To decide this, we have to see where it is used. The only place where we require the stress is in the term  [2.x.64] . In practice, we of course replace this term by numerical quadrature: [1.x.70] where  [2.x.65]  are the quadrature weights and  [2.x.66]  the quadrature points on cell  [2.x.67] . This should make clear that what we really need is not the stress  [2.x.68]  in itself, but only the values of the stress in the quadrature points on all cells. This, however, is a simpler task: we only have to provide a data structure that is able to hold one symmetric tensor of rank 2 for each quadrature point on all cells (or, since we compute in parallel, all quadrature points of all cells that the present MPI process &ldquo;owns&rdquo;). At the end of each time step we then only have to evaluate  [2.x.69] , multiply it by the stress-strain tensor  [2.x.70] , and use the result to update the stress  [2.x.71]  at quadrature point  [2.x.72] . 

The second complication is not visible in our notation as chosen above. It is due to the fact that we compute  [2.x.73]  on the domain  [2.x.74] , and then use this displacement increment to both update the stress as well as move the mesh nodes around to get to  [2.x.75]  on which the next increment is computed. What we have to make sure, in this context, is that moving the mesh does not only involve moving around the nodes, but also making corresponding changes to the stress variable: the updated stress is a variable that is defined with respect to the coordinate system of the material in the old domain, and has to be transferred to the new domain. The reason for this can be understood as follows: locally, the incremental deformation  [2.x.76]  can be decomposed into three parts, a linear translation (the constant part of the displacement increment field in the neighborhood of a point), a dilational component (that part of the gradient of the displacement field that has a nonzero divergence), and a rotation. A linear translation of the material does not affect the stresses that are frozen into it -- the stress values are simply translated along. The dilational or compressional change produces a corresponding stress update. However, the rotational component does not necessarily induce a nonzero stress update (think, in 2d, for example of the situation where  [2.x.77] , with which  [2.x.78] ). Nevertheless, if the material was prestressed in a certain direction, then this direction will be rotated along with the material.  To this end, we have to define a rotation matrix  [2.x.79]  that describes, in each point the rotation due to the displacement increments. It is not hard to see that the actual dependence of  [2.x.80]  on  [2.x.81]  can only be through the curl of the displacement, rather than the displacement itself or its full gradient (as mentioned above, the constant components of the increment describe translations, its divergence the dilational modes, and the curl the rotational modes). Since the exact form of  [2.x.82]  is cumbersome, we only state it in the program code, and note that the correct updating formula for the stress variable is then [1.x.71] [1.x.72] 

Both stress update and rotation are implemented in the function  [2.x.83]  of the example program. 


[1.x.73][1.x.74] 


In  [2.x.84] , the main bottleneck for %parallel computations as far as run time is concerned was that only the first processor generated output for the entire domain. Since generating graphical output is expensive, this did not scale well when larger numbers of processors were involved. We will address this here. (For a definition of what it means for a program to "scale", see  [2.x.85]  "this glossary entry".) 

Basically, what we need to do is let every process generate graphical output for that subset of cells that it owns, write them into separate files and have a way to display all files for a certain timestep at the same time. This way the code produces one  [2.x.86]  file per process per time step. The two common VTK file viewers ParaView and VisIt both support opening more than one  [2.x.87]  file at once. To simplify the process of picking the correct files and allow moving around in time, both support record files that reference all files for a given timestep. Sadly, the record files have a different format between VisIt and Paraview, so we write out both formats. 

The code will generate the files  [2.x.88] , where  [2.x.89]  is the timestep number (starting from 1) and  [2.x.90]  is the process rank (starting from 0). These files contain the locally owned cells for the timestep and processor. The files  [2.x.91]  is the visit record for timestep  [2.x.92]  is the same for ParaView. (More recent versions of VisIt can actually read  [2.x.93]  files as well, but it doesn't hurt to output both kinds of record files.) Finally, the file  [2.x.94]  is a special record only supported by ParaView that references all time steps. So in ParaView, only solution.pvd needs to be opened, while one needs to select the group of all .visit files in VisIt for the same effect. 


[1.x.75][1.x.76] 


In  [2.x.95] , we used a regular triangulation that was simply replicated on every processor, and a corresponding DoFHandler. Both had no idea that they were used in a %parallel context -- they just existed in their entirety on every processor, and we argued that this was eventually going to be a major memory bottleneck. 

We do not address this issue here (we will do so in  [2.x.96] ) but make the situation slightly more automated. In  [2.x.97] , we created the triangulation and then manually "partitioned" it, i.e., we assigned  [2.x.98]  "subdomain ids" to every cell that indicated which  [2.x.99]  "MPI process" "owned" the cell. Here, we use a class  [2.x.100]  that at least does this part automatically: whenever you create or refine such a triangulation, it automatically partitions itself among all involved processes (which it knows about because you have to tell it about the  [2.x.101]  "MPI communicator" that connects these processes upon construction of the triangulation). Otherwise, the  [2.x.102]  looks, for all practical purposes, like a regular Triangulation object. 

The convenience of using this class does not only result from being able to avoid the manual call to  [2.x.103]  Rather, the DoFHandler class now also knows that you want to use it in a parallel context, and by default automatically enumerates degrees of freedom in such a way that all DoFs owned by process zero come before all DoFs owned by process 1, etc. In other words, you can also avoid the call to  [2.x.104]  

There are other benefits. For example, because the triangulation knows that it lives in a %parallel universe, it also knows that it "owns" certain cells (namely, those whose subdomain id equals its MPI rank; previously, the triangulation only stored these subdomain ids, but had no way to make sense of them). Consequently, in the assembly function, you can test whether a cell is "locally owned" (i.e., owned by the current process, see  [2.x.105] ) when you loop over all cells using the syntax 

[1.x.77] 

This knowledge extends to the DoFHandler object built on such triangulations, which can then identify which degrees of freedom are locally owned (see  [2.x.106] ) via calls such as  [2.x.107]  and  [2.x.108]  Finally, the DataOut class also knows how to deal with such triangulations and will simply skip generating graphical output on cells not locally owned. 

Of course, as has been noted numerous times in the discussion in  [2.x.109] , keeping the entire triangulation on every process will not scale: large problems may simply not fit into each process's memory any more, even if we have sufficiently many processes around to solve them in a reasonable time. In such cases, the  [2.x.110]  is no longer a reasonable basis for computations and we will show in  [2.x.111]  how the  [2.x.112]  class can be used to work around this, namely by letting each process store only a [1.x.78] of the triangulation. 


[1.x.79][1.x.80] 


The overall structure of the program can be inferred from the  [2.x.113]  function that first calls  [2.x.114]  for the first time step, and then  [2.x.115]  on all subsequent time steps. The difference between these functions is only that in the first time step we start on a coarse mesh, solve on it, refine the mesh adaptively, and then start again with a clean state on that new mesh. This procedure gives us a better starting mesh, although we should of course keep adapting the mesh as iterations proceed -- this isn't done in this program, but commented on below. 

The common part of the two functions treating time steps is the following sequence of operations on the present mesh:  [2.x.116]   [2.x.117]   [2.x.118] ]:   This first function is also the most interesting one. It assembles the   linear system corresponding to the discretized version of equation   [1.x.81]. This leads to a system matrix  [2.x.119]  built up of local contributions on each cell  [2.x.120]  with entries   [1.x.82]   In practice,  [2.x.121]  is computed using numerical quadrature according to the   formula   [1.x.83]   with quadrature points  [2.x.122]  and weights  [2.x.123] . We have built these   contributions before, in  [2.x.124]  and  [2.x.125] , but in both of these cases we   have done so rather clumsily by using knowledge of how the rank-4 tensor  [2.x.126]    is composed, and considering individual elements of the strain tensors    [2.x.127] . This is not really   convenient, in particular if we want to consider more complicated elasticity   models than the isotropic case for which  [2.x.128]  had the convenient form    [2.x.129] . While we in fact do not use a more complicated   form than this in the present program, we nevertheless want to write it in a   way that would easily allow for this. It is then natural to introduce   classes that represent symmetric tensors of rank 2 (for the strains and   stresses) and 4 (for the stress-strain tensor  [2.x.130] ). Fortunately, deal.II   provides these: the  [2.x.131]  class template   provides a full-fledged implementation of such tensors of rank  [2.x.132]    (which needs to be an even number) and dimension  [2.x.133] . 

  What we then need is two things: a way to create the stress-strain rank-4   tensor  [2.x.134]  as well as to create a symmetric tensor of rank 2 (the strain   tensor) from the gradients of a shape function  [2.x.135]  at a quadrature   point  [2.x.136]  on a given cell. At the top of the implementation of this   example program, you will find such functions. The first one,    [2.x.137] , takes two arguments corresponding to   the Lam&eacute; constants  [2.x.138]  and  [2.x.139]  and returns the stress-strain tensor   for the isotropic case corresponding to these constants (in the program, we   will choose constants corresponding to steel); it would be simple to replace   this function by one that computes this tensor for the anisotropic case, or   taking into account crystal symmetries, for example. The second one,    [2.x.140]  and indices    [2.x.141]  and  [2.x.142]  and returns the symmetric gradient, i.e. the strain,   corresponding to shape function  [2.x.143] , evaluated on the cell   on which the  [2.x.144]  object was last reinitialized. 

  Given this, the innermost loop of  [2.x.145]  computes the   local contributions to the matrix in the following elegant way (the variable    [2.x.146] , corresponding to the tensor  [2.x.147] , has   previously been initialized with the result of the first function above):   [1.x.84] 

  It is worth noting the expressive power of this piece of code, and to   compare it with the complications we had to go through in previous examples   for the elasticity problem. (To be fair, the SymmetricTensor class   template did not exist when these previous examples were written.) For   simplicity,  [2.x.148]  provides for the (double summation) product   between symmetric tensors of even rank here. 

  Assembling the local contributions   [1.x.85] 

  to the right hand side of [1.x.86] is equally   straightforward (note that we do not consider any boundary tractions  [2.x.149]  here). Remember that we only had to store the old stress in the   quadrature points of cells. In the program, we will provide a variable    [2.x.150]  that allows to access the stress    [2.x.151]  in each quadrature point. With this the code for the right   hand side looks as this, again rather elegant:   [1.x.87] 

  Note that in the multiplication  [2.x.152] , we have made use of the fact that for the chosen finite element, only   one vector component (namely  [2.x.153] ) of  [2.x.154]  is   nonzero, and that we therefore also have to consider only one component of    [2.x.155] . 

  This essentially concludes the new material we present in this function. It   later has to deal with boundary conditions as well as hanging node   constraints, but this parallels what we had to do previously in other   programs already. 

 [2.x.156]   [2.x.157] ]:   Unlike the previous one, this function is not really interesting, since it   does what similar functions have done in all previous tutorial programs --   solving the linear system using the CG method, using an incomplete LU   decomposition as a preconditioner (in the %parallel case, it uses an ILU of   each processor's block separately). It is virtually unchanged   from  [2.x.158] . 

 [2.x.159]   [2.x.160]  [via    [2.x.161] ]: Based on the displacement field  [2.x.162]  computed before, we update the stress values in all quadrature points   according to [1.x.88] and [1.x.89],   including the rotation of the coordinate system. 

 [2.x.163]   [2.x.164] : Given the solution computed before, in this   function we deform the mesh by moving each vertex by the displacement vector   field evaluated at this particular vertex. 

 [2.x.165]   [2.x.166] : This function simply outputs the solution   based on what we have said above, i.e. every processor computes output only   for its own portion of the domain. In addition to the solution, we also compute the norm of   the stress averaged over all the quadrature points on each cell.  [2.x.167]  

With this general structure of the code, we only have to define what case we want to solve. For the present program, we have chosen to simulate the quasistatic deformation of a vertical cylinder for which the bottom boundary is fixed and the top boundary is pushed down at a prescribed vertical velocity. However, the horizontal velocity of the top boundary is left unspecified -- one can imagine this situation as a well-greased plate pushing from the top onto the cylinder, the points on the top boundary of the cylinder being allowed to slide horizontally along the surface of the plate, but forced to move downward by the plate. The inner and outer boundaries of the cylinder are free and not subject to any prescribed deflection or traction. In addition, gravity acts on the body. 

The program text will reveal more about how to implement this situation, and the results section will show what displacement pattern comes out of this simulation. [1.x.90] [1.x.91] 

First the usual list of header files that have already been used in previous example programs: 

[1.x.92] 



And here the only three new things among the header files: an include file in which symmetric tensors of rank 2 and 4 are implemented, as introduced in the introduction: 

[1.x.93] 



And lastly a header that contains some functions that will help us compute rotaton matrices of the local coordinate systems at specific points in the domain. 

[1.x.94] 



This is then simply C++ again: 

[1.x.95] 



The last step is as in all previous programs: 

[1.x.96] 




[1.x.97]  [1.x.98] 




As was mentioned in the introduction, we have to store the old stress in quadrature point so that we can compute the residual forces at this point during the next time step. This alone would not warrant a structure with only one member, but in more complicated applications, we would have to store more information in quadrature points as well, such as the history variables of plasticity, etc. In essence, we have to store everything that affects the present state of the material here, which in plasticity is determined by the deformation history variables.    


We will not give this class any meaningful functionality beyond being able to store data, i.e. there are no constructors, destructors, or other member functions. In such cases of `dumb' classes, we usually opt to declare them as  [2.x.168] , to indicate that they are closer to C-style structures than C++-style classes. 

[1.x.99] 




[1.x.100]  [1.x.101] 




Next, we define the linear relationship between the stress and the strain in elasticity. It is given by a tensor of rank 4 that is usually written in the form  [2.x.169] . This tensor maps symmetric tensor of rank 2 to symmetric tensors of rank 2. A function implementing its creation for given values of the Lam&eacute; constants  [2.x.170]  and  [2.x.171]  is straightforward: 

[1.x.102] 



With this function, we will define a static member variable of the main class below that will be used throughout the program as the stress-strain tensor. Note that in more elaborate programs, this will probably be a member variable of some class instead, or a function that returns the stress-strain relationship depending on other input. For example in damage theory models, the Lam&eacute; constants are considered a function of the prior stress/strain history of a point. Conversely, in plasticity the form of the stress-strain tensor is modified if the material has reached the yield stress in a certain point, and possibly also depending on its prior history.    


In the present program, however, we assume that the material is completely elastic and linear, and a constant stress-strain tensor is sufficient for our present purposes. 














[1.x.103]  [1.x.104] 




Before the rest of the program, here are a few functions that we need as tools. These are small functions that are called in inner loops, so we mark them as  [2.x.172] .    


The first one computes the symmetric strain tensor for shape function  [2.x.173]  by forming the symmetric gradient of this shape function. We need that when we want to form the matrix, for example.    


We should note that in previous examples where we have treated vector-valued problems, we have always asked the finite element object in which of the vector component the shape function is actually non-zero, and thereby avoided to compute any terms that we could prove were zero anyway. For this, we used the  [2.x.174]  function that returns in which component a shape function was zero, and also that the  [2.x.175]  and  [2.x.176]  functions only returned the value and gradient of the single non-zero component of a shape function if this is a vector-valued element.    


This was an optimization, and if it isn't terribly time critical, we can get away with a simpler technique: just ask the  [2.x.177]  for the value or gradient of a given component of a given shape function at a given quadrature point. This is what the  [2.x.178]  call does: return the full gradient of the  [2.x.179] th component of shape function  [2.x.180]  at quadrature point  [2.x.181] . If a certain component of a certain shape function is always zero, then this will simply always return zero.    


As mentioned, using  [2.x.182]  instead of the combination of  [2.x.183]  and  [2.x.184]  may be less efficient, but its implementation is optimized for such cases and shouldn't be a big slowdown. We demonstrate the technique here since it is so much simpler and straightforward. 

[1.x.105] 



Declare a temporary that will hold the return value: 

[1.x.106] 



First, fill diagonal terms which are simply the derivatives in direction  [2.x.185]  component of the vector-valued shape function: 

[1.x.107] 



Then fill the rest of the strain tensor. Note that since the tensor is symmetric, we only have to compute one half (here: the upper right corner) of the off-diagonal elements, and the implementation of the  [2.x.186]  class makes sure that at least to the outside the symmetric entries are also filled (in practice, the class of course stores only one copy). Here, we have picked the upper right half of the tensor, but the lower left one would have been just as good: 

[1.x.108] 



The second function does something very similar (and therefore is given the same name): compute the symmetric strain tensor from the gradient of a vector-valued field. If you already have a solution field, the  [2.x.187]  function allows you to extract the gradients of each component of your solution field at a quadrature point. It returns this as a vector of rank-1 tensors: one rank-1 tensor (gradient) per vector component of the solution. From this we have to reconstruct the (symmetric) strain tensor by transforming the data storage format and symmetrization. We do this in the same way as above, i.e. we avoid a few computations by filling first the diagonal and then only one half of the symmetric tensor (the  [2.x.188]  class makes sure that it is sufficient to write only one of the two symmetric components).    


Before we do this, though, we make sure that the input has the kind of structure we expect: that is that there are  [2.x.189]  vector components, i.e. one displacement component for each coordinate direction. We test this with the  [2.x.190]  macro that will simply abort our program if the condition is not met. 

[1.x.109] 



Finally, below we will need a function that computes the rotation matrix induced by a displacement at a given point. In fact, of course, the displacement at a single point only has a direction and a magnitude, it is the change in direction and magnitude that induces rotations. In effect, the rotation matrix can be computed from the gradients of a displacement, or, more specifically, from the curl.    


The formulas by which the rotation matrices are determined are a little awkward, especially in 3d. For 2d, there is a simpler way, so we implement this function twice, once for 2d and once for 3d, so that we can compile and use the program in both space dimensions if so desired -- after all, deal.II is all about dimension independent programming and reuse of algorithm thoroughly tested with cheap computations in 2d, for the more expensive computations in 3d. Here is one case, where we have to implement different algorithms for 2d and 3d, but then can write the rest of the program in a way that is independent of the space dimension.    


So, without further ado to the 2d implementation: 

[1.x.110] 



First, compute the curl of the velocity field from the gradients. Note that we are in 2d, so the rotation is a scalar: 

[1.x.111] 



From this, compute the angle of rotation: 

[1.x.112] 



And from this, build the antisymmetric rotation matrix. We want this rotation matrix to represent the rotation of the local coordinate system with respect to the global Cartesian basis, to we construct it with a negative angle. The rotation matrix therefore represents the rotation required to move from the local to the global coordinate system. 

[1.x.113] 



The 3d case is a little more contrived: 

[1.x.114] 



Again first compute the curl of the velocity field. This time, it is a real vector: 

[1.x.115] 



From this vector, using its magnitude, compute the tangent of the angle of rotation, and from it the actual angle of rotation with respect to the Cartesian basis: 

[1.x.116] 



Now, here's one problem: if the angle of rotation is too small, that means that there is no rotation going on (for example a translational motion). In that case, the rotation matrix is the identity matrix.      


The reason why we stress that is that in this case we have that  [2.x.191] . Further down, we need to divide by that number in the computation of the axis of rotation, and we would get into trouble when dividing doing so. Therefore, let's shortcut this and simply return the identity matrix if the angle of rotation is really small: 

[1.x.117] 



Otherwise compute the real rotation matrix. For this, again we rely on a predefined function to compute the rotation matrix of the local coordinate system. 

[1.x.118] 




[1.x.119]  [1.x.120] 




This is the main class of the program. Since the namespace already indicates what problem we are solving, let's call it by what it does: it directs the flow of the program, i.e. it is the toplevel driver.    


The member variables of this class are essentially as before, i.e. it has to have a triangulation, a DoF handler and associated objects such as constraints, variables that describe the linear system, etc. There are a good number of more member functions now, which we will explain below.    


The external interface of the class, however, is unchanged: it has a public constructor and destructor, and it has a  [2.x.192]  function that initiated all the work. 

[1.x.121] 



The private interface is more extensive than in  [2.x.193] . First, we obviously need functions that create the initial mesh, set up the variables that describe the linear system on the present mesh (i.e. matrices and vectors), and then functions that actually assemble the system, direct what has to be solved in each time step, a function that solves the linear system that arises in each timestep (and returns the number of iterations it took), and finally output the solution vector on the correct mesh: 

[1.x.122] 



All, except for the first two, of these functions are called in each timestep. Since the first time step is a little special, we have separate functions that describe what has to happen in a timestep: one for the first, and one for all following timesteps: 

[1.x.123] 



Then we need a whole bunch of functions that do various things. The first one refines the initial grid: we start on the coarse grid with a pristine state, solve the problem, then look at it and refine the mesh accordingly, and start the same process over again, again with a pristine state. Thus, refining the initial mesh is somewhat simpler than refining a grid between two successive time steps, since it does not involve transferring data from the old to the new triangulation, in particular the history data that is stored in each quadrature point. 

[1.x.124] 



At the end of each time step, we want to move the mesh vertices around according to the incremental displacement computed in this time step. This is the function in which this is done: 

[1.x.125] 



Next are two functions that handle the history variables stored in each quadrature point. The first one is called before the first timestep to set up a pristine state for the history variables. It only works on those quadrature points on cells that belong to the present processor: 

[1.x.126] 



The second one updates the history variables at the end of each timestep: 

[1.x.127] 



This is the new shared Triangulation: 

[1.x.128] 



One difference of this program is that we declare the quadrature formula in the class declaration. The reason is that in all the other programs, it didn't do much harm if we had used different quadrature formulas when computing the matrix and the right hand side, for example. However, in the present case it does: we store information in the quadrature points, so we have to make sure all parts of the program agree on where they are and how many there are on each cell. Thus, let us first declare the quadrature formula that will be used throughout... 

[1.x.129] 



... and then also have a vector of history objects, one per quadrature point on those cells for which we are responsible (i.e. we don't store history data for quadrature points on cells that are owned by other processors). Note that, instead of storing and managing this data ourself, we could use the CellDataStorage class like is done in  [2.x.194] . However, for the purpose of demonstration, in this case we manage the storage manually. 

[1.x.130] 



The way this object is accessed is through a  [2.x.195]  that each cell, face, or edge holds: it is a  [2.x.196]  pointer that can be used by application programs to associate arbitrary data to cells, faces, or edges. What the program actually does with this data is within its own responsibility, the library just allocates some space for these pointers, and application programs can set and read the pointers for each of these objects. 








Further: we need the objects of linear systems to be solved, i.e. matrix, right hand side vector, and the solution vector. Since we anticipate solving big problems, we use the same types as in  [2.x.197] , i.e. distributed %parallel matrices and vectors built on top of the PETSc library. Conveniently, they can also be used when running on only a single machine, in which case this machine happens to be the only one in our %parallel universe.      


However, as a difference to  [2.x.198] , we do not store the solution vector -- which here is the incremental displacements computed in each time step -- in a distributed fashion. I.e., of course it must be a distributed vector when computing it, but immediately after that we make sure each processor has a complete copy. The reason is that we had already seen in  [2.x.199]  that many functions needed a complete copy. While it is not hard to get it, this requires communication on the network, and is thus slow. In addition, these were repeatedly the same operations, which is certainly undesirable unless the gains of not always having to store the entire vector outweighs it. When writing this program, it turned out that we need a complete copy of the solution in so many places that it did not seem worthwhile to only get it when necessary. Instead, we opted to obtain the complete copy once and for all, and instead get rid of the distributed copy immediately. Thus, note that the declaration of  [2.x.200]  does not denote a distribute vector as would be indicated by the middle namespace  [2.x.201] : 

[1.x.131] 



The next block of variables is then related to the time dependent nature of the problem: they denote the length of the time interval which we want to simulate, the present time and number of time step, and length of present timestep: 

[1.x.132] 



Then a few variables that have to do with %parallel processing: first, a variable denoting the MPI communicator we use, and then two numbers telling us how many participating processors there are, and where in this world we are. Finally, a stream object that makes sure only one processor is actually generating output to the console. This is all the same as in  [2.x.202] : 

[1.x.133] 



We are storing the locally owned and the locally relevant indices: 

[1.x.134] 



Finally, we have a static variable that denotes the linear relationship between the stress and strain. Since it is a constant object that does not depend on any input (at least not in this program), we make it a static variable and will initialize it in the same place where we define the constructor of this class: 

[1.x.135] 




[1.x.136]  [1.x.137] 




Before we go on to the main functionality of this program, we have to define what forces will act on the body whose deformation we want to study. These may either be body forces or boundary forces. Body forces are generally mediated by one of the four basic physical types of forces: gravity, strong and weak interaction, and electromagnetism. Unless one wants to consider subatomic objects (for which quasistatic deformation is irrelevant and an inappropriate description anyway), only gravity and electromagnetic forces need to be considered. Let us, for simplicity assume that our body has a certain mass density, but is either non-magnetic and not electrically conducting or that there are no significant electromagnetic fields around. In that case, the body forces are simply  [2.x.203]  is the material density and  [2.x.204]  is a vector in negative z-direction with magnitude 9.81 m/s^2.  Both the density and  [2.x.205]  are defined in the function, and we take as the density 7700 kg/m^3, a value commonly assumed for steel.    


To be a little more general and to be able to do computations in 2d as well, we realize that the body force is always a function returning a  [2.x.206]  dimensional vector. We assume that gravity acts along the negative direction of the last, i.e.  [2.x.207] th coordinate. The rest of the implementation of this function should be mostly self-explanatory given similar definitions in previous example programs. Note that the body force is independent of the location; to avoid compiler warnings about unused function arguments, we therefore comment out the name of the first argument of the  [2.x.208]  function: 

[1.x.138] 




[1.x.139]  [1.x.140] 




In addition to body forces, movement can be induced by boundary forces and forced boundary displacement. The latter case is equivalent to forces being chosen in such a way that they induce certain displacement.    


For quasistatic displacement, typical boundary forces would be pressure on a body, or tangential friction against another body. We chose a somewhat simpler case here: we prescribe a certain movement of (parts of) the boundary, or at least of certain components of the displacement vector. We describe this by another vector-valued function that, for a given point on the boundary, returns the prescribed displacement.    


Since we have a time-dependent problem, the displacement increment of the boundary equals the displacement accumulated during the length of the timestep. The class therefore has to know both the present time and the length of the present time step, and can then approximate the incremental displacement as the present velocity times the present timestep.    


For the purposes of this program, we choose a simple form of boundary displacement: we displace the top boundary with constant velocity downwards. The rest of the boundary is either going to be fixed (and is then described using an object of type  [2.x.209] ) or free (Neumann-type, in which case nothing special has to be done).  The implementation of the class describing the constant downward motion should then be obvious using the knowledge we gained through all the previous example programs: 

[1.x.141] 




[1.x.142]  [1.x.143] 




Now for the implementation of the main class. First, we initialize the stress-strain tensor, which we have declared as a static const variable. We chose Lam&eacute; constants that are appropriate for steel: 

[1.x.144] 




[1.x.145]  [1.x.146] 




The next step is the definition of constructors and destructors. There are no surprises here: we choose linear and continuous finite elements for each of the  [2.x.210]  vector components of the solution, and a Gaussian quadrature formula with 2 points in each coordinate direction. The destructor should be obvious: 

[1.x.147] 



The last of the public functions is the one that directs all the work,  [2.x.211] . It initializes the variables that describe where in time we presently are, then runs the first time step, then loops over all the other time steps. Note that for simplicity we use a fixed time step, whereas a more sophisticated program would of course have to choose it in some more reasonable way adaptively: 

[1.x.148] 




[1.x.149]  [1.x.150] 




The next function in the order in which they were declared above is the one that creates the coarse grid from which we start. For this example program, we want to compute the deformation of a cylinder under axial compression. The first step therefore is to generate a mesh for a cylinder of length 3 and with inner and outer radii of 0.8 and 1, respectively. Fortunately, there is a library function for such a mesh.    


In a second step, we have to associated boundary conditions with the upper and lower faces of the cylinder. We choose a boundary indicator of 0 for the boundary faces that are characterized by their midpoints having z-coordinates of either 0 (bottom face), an indicator of 1 for z=3 (top face); finally, we use boundary indicator 2 for all faces on the inside of the cylinder shell, and 3 for the outside. 

[1.x.151] 



Once all this is done, we can refine the mesh once globally: 

[1.x.152] 



As the final step, we need to set up a clean state of the data that we store in the quadrature points on all cells that are treated on the present processor. 

[1.x.153] 




[1.x.154]  [1.x.155] 




The next function is the one that sets up the data structures for a given mesh. This is done in most the same way as in  [2.x.212] : distribute the degrees of freedom, then sort these degrees of freedom in such a way that each processor gets a contiguous chunk of them. Note that subdivisions into chunks for each processor is handled in the functions that create or refine grids, unlike in the previous example program (the point where this happens is mostly a matter of taste; here, we chose to do it when grids are created since in the  [2.x.213]  and  [2.x.214]  functions we want to output the number of cells on each processor at a point where we haven't called the present function yet). 

[1.x.156] 



The next step is to set up constraints due to hanging nodes. This has been handled many times before: 

[1.x.157] 



And then we have to set up the matrix. Here we deviate from  [2.x.215] , in which we simply used PETSc's ability to just know about the size of the matrix and later allocate those nonzero elements that are being written to. While this works just fine from a correctness viewpoint, it is not at all efficient: if we don't give PETSc a clue as to which elements are written to, it is (at least at the time of this writing) unbearably slow when we set the elements in the matrix for the first time (i.e. in the first timestep). Later on, when the elements have been allocated, everything is much faster. In experiments we made, the first timestep can be accelerated by almost two orders of magnitude if we instruct PETSc which elements will be used and which are not.      


To do so, we first generate the sparsity pattern of the matrix we are going to work with, and make sure that the condensation of hanging node constraints add the necessary additional entries in the sparsity pattern: 

[1.x.158] 



Note that we have used the  [2.x.216]  class here that was already introduced in  [2.x.217] , rather than the  [2.x.218]  class that we have used in all other cases. The reason for this is that for the latter class to work we have to give an initial upper bound for the number of entries in each row, a task that is traditionally done by  [2.x.219] . However, this function suffers from a serious problem: it has to compute an upper bound to the number of nonzero entries in each row, and this is a rather complicated task, in particular in 3d. In effect, while it is quite accurate in 2d, it often comes up with much too large a number in 3d, and in that case the  [2.x.220]  allocates much too much memory at first, often several 100 MBs. This is later corrected when  [2.x.221]  is called and we realize that we don't need all that much memory, but at time it is already too late: for large problems, the temporary allocation of too much memory can lead to out-of-memory situations.      


In order to avoid this, we resort to the  [2.x.222]  class that is slower but does not require any up-front estimate on the number of nonzero entries per row. It therefore only ever allocates as much memory as it needs at any given time, and we can build it even for large 3d problems.      


It is also worth noting that due to the specifics of  [2.x.223]  the sparsity pattern we construct is global, i.e. comprises all degrees of freedom whether they will be owned by the processor we are on or another one (in case this program is run in %parallel via MPI). This of course is not optimal -- it limits the size of the problems we can solve, since storing the entire sparsity pattern (even if only for a short time) on each processor does not scale well. However, there are several more places in the program in which we do this, for example we always keep the global triangulation and DoF handler objects around, even if we only work on part of them. At present, deal.II does not have the necessary facilities to completely distribute these objects (a task that, indeed, is very hard to achieve with adaptive meshes, since well-balanced subdivisions of a domain tend to become unbalanced as the mesh is adaptively refined).      


With this data structure, we can then go to the PETSc sparse matrix and tell it to preallocate all the entries we will later want to write to: 

[1.x.159] 



After this point, no further explicit knowledge of the sparsity pattern is required any more and we can let the  [2.x.224]  variable go out of scope without any problem. 




The last task in this function is then only to reset the right hand side vector as well as the solution vector to its correct size; remember that the solution vector is a local one, unlike the right hand side that is a distributed %parallel one and therefore needs to know the MPI communicator over which it is supposed to transmit messages: 

[1.x.160] 




[1.x.161]  [1.x.162] 




Again, assembling the system matrix and right hand side follows the same structure as in many example programs before. In particular, it is mostly equivalent to  [2.x.225] , except for the different right hand side that now only has to take into account internal stresses. In addition, assembling the matrix is made significantly more transparent by using the  [2.x.226]  class: note the elegance of forming the scalar products of symmetric tensors of rank 2 and 4. The implementation is also more general since it is independent of the fact that we may or may not be using an isotropic elasticity tensor.    


The first part of the assembly routine is as always: 

[1.x.163] 



As in  [2.x.227] , we only need to loop over all cells that belong to the present processor: 

[1.x.164] 



Then loop over all indices i,j and quadrature points and assemble the system matrix contributions from this cell.  Note how we extract the symmetric gradients (strains) of the shape functions at a given quadrature point from the  [2.x.228]  object, and the elegance with which we form the triple contraction  [2.x.229] ; the latter needs to be compared to the clumsy computations needed in  [2.x.230] , both in the introduction as well as in the respective place in the program: 

[1.x.165] 



Then also assemble the local right hand side contributions. For this, we need to access the prior stress value in this quadrature point. To get it, we use the user pointer of this cell that points into the global array to the quadrature point data corresponding to the first quadrature point of the present cell, and then add an offset corresponding to the index of the quadrature point we presently consider: 

[1.x.166] 



In addition, we need the values of the external body forces at the quadrature points on this cell: 

[1.x.167] 



Then we can loop over all degrees of freedom on this cell and compute local contributions to the right hand side: 

[1.x.168] 



Now that we have the local contributions to the linear system, we need to transfer it into the global objects. This is done exactly as in  [2.x.231] : 

[1.x.169] 



Now compress the vector and the system matrix: 

[1.x.170] 



The last step is to again fix up boundary values, just as we already did in previous programs. A slight complication is that the  [2.x.232]  function wants to have a solution vector compatible with the matrix and right hand side (i.e. here a distributed %parallel vector, rather than the sequential vector we use in this program) in order to preset the entries of the solution vector with the correct boundary values. We provide such a compatible vector in the form of a temporary vector which we then copy into the sequential one. 




We make up for this complication by showing how boundary values can be used flexibly: following the way we create the triangulation, there are three distinct boundary indicators used to describe the domain, corresponding to the bottom and top faces, as well as the inner/outer surfaces. We would like to impose boundary conditions of the following type: The inner and outer cylinder surfaces are free of external forces, a fact that corresponds to natural (Neumann-type) boundary conditions for which we don't have to do anything. At the bottom, we want no movement at all, corresponding to the cylinder being clamped or cemented in at this part of the boundary. At the top, however, we want a prescribed vertical downward motion compressing the cylinder; in addition, we only want to restrict the vertical movement, but not the horizontal ones -- one can think of this situation as a well-greased plate sitting on top of the cylinder pushing it downwards: the atoms of the cylinder are forced to move downward, but they are free to slide horizontally along the plate. 




The way to describe this is as follows: for boundary indicator zero (bottom face) we use a dim-dimensional zero function representing no motion in any coordinate direction. For the boundary with indicator 1 (top surface), we use the  [2.x.233]  class, but we specify an additional argument to the  [2.x.234]  function denoting which vector components it should apply to; this is a vector of bools for each vector component and because we only want to restrict vertical motion, it has only its last component set: 

[1.x.171] 




[1.x.172]  [1.x.173] 




The next function is the one that controls what all has to happen within a timestep. The order of things should be relatively self-explanatory from the function names: 

[1.x.174] 




[1.x.175]  [1.x.176] 




Solving the linear system again works mostly as before. The only difference is that we want to only keep a complete local copy of the solution vector instead of the distributed one that we get as output from PETSc's solver routines. To this end, we declare a local temporary variable for the distributed vector and initialize it with the contents of the local variable (remember that the  [2.x.235]  function called in  [2.x.236]  preset the values of boundary nodes in this vector), solve with it, and at the end of the function copy it again into the complete local vector that we declared as a member variable. Hanging node constraints are then distributed only on the local copy, i.e. independently of each other on each of the processors: 

[1.x.177] 




[1.x.178]  [1.x.179] 




This function generates the graphical output in .vtu format as explained in the introduction. Each process will only work on the cells it owns, and then write the result into a file of its own. Additionally, processor 0 will write the record files the reference all the .vtu files.    


The crucial part of this function is to give the  [2.x.237]  class a way to only work on the cells that the present process owns. 







[1.x.180] 



Then, just as in  [2.x.238] , define the names of solution variables (which here are the displacement increments) and queue the solution vector for output. Note in the following switch how we make sure that if the space dimension should be unhandled that we throw an exception saying that we haven't implemented this case yet (another case of defensive programming): 

[1.x.181] 



The next thing is that we wanted to output something like the average norm of the stresses that we have stored in each cell. This may seem complicated, since on the present processor we only store the stresses in quadrature points on those cells that actually belong to the present process. In other words, it seems as if we can't compute the average stresses for all cells. However, remember that our class derived from  [2.x.239]  only iterates over those cells that actually do belong to the present processor, i.e. we don't have to compute anything for all the other cells as this information would not be touched. The following little loop does this. We enclose the entire block into a pair of braces to make sure that the iterator variables do not remain accidentally visible beyond the end of the block in which they are used: 

[1.x.182] 



Loop over all the cells... 

[1.x.183] 



On these cells, add up the stresses over all quadrature points... 

[1.x.184] 



...then write the norm of the average to their destination: 

[1.x.185] 



And on the cells that we are not interested in, set the respective value in the vector to a bogus value (norms must be positive, and a large negative value should catch your eye) in order to make sure that if we were somehow wrong about our assumption that these elements would not appear in the output file, that we would find out by looking at the graphical output: 

[1.x.186] 



Finally attach this vector as well to be treated for output: 

[1.x.187] 



As a last piece of data, let us also add the partitioning of the domain into subdomains associated with the processors if this is a parallel job. This works in the exact same way as in the  [2.x.240]  program: 

[1.x.188] 



Finally, with all this data, we can instruct deal.II to munge the information and produce some intermediate data structures that contain all these solution and other data vectors: 

[1.x.189] 



Let us call a function that opens the necessary output files and writes the data we have generated into them. The function automatically constructs the file names from the given directory name (the first argument) and file name base (second argument). It augments the resulting string by pieces that result from the time step number and a "piece number" that corresponds to a part of the overall domain that can consist of one or more subdomains.      


The function also writes a record files (with suffix `.pvd`) for Paraview that describes how all of these output files combine into the data for this single time step: 

[1.x.190] 



The record files must be written only once and not by each processor, so we do this on processor 0: 

[1.x.191] 



Finally, we write the paraview record, that references all .pvtu files and their respective time. Note that the variable times_and_names is declared static, so it will retain the entries from the previous timesteps. 

[1.x.192] 




[1.x.193]  [1.x.194] 




This and the next function handle the overall structure of the first and following timesteps, respectively. The first timestep is slightly more involved because we want to compute it multiple times on successively refined meshes, each time starting from a clean state. At the end of these computations, in which we compute the incremental displacements each time, we use the last results obtained for the incremental displacements to compute the resulting stress updates and move the mesh accordingly. On this new mesh, we then output the solution and any additional data we consider important.    


All this is interspersed by generating output to the console to update the person watching the screen on what is going on. As in  [2.x.241] , the use of  [2.x.242]  makes sure that only one of the parallel processes is actually writing to the console, without having to explicitly code an if-statement in each place where we generate output: 

[1.x.195] 




[1.x.196]  [1.x.197] 




Subsequent timesteps are simpler, and probably do not require any more documentation given the explanations for the previous function above: 

[1.x.198] 




[1.x.199]  [1.x.200] 




The following function is called when solving the first time step on successively refined meshes. After each iteration, it computes a refinement criterion, refines the mesh, and sets up the history variables in each quadrature point again to a clean state. 

[1.x.201] 



First, let each process compute error indicators for the cells it owns: 

[1.x.202] 



Then set up a global vector into which we merge the local indicators from each of the %parallel processes: 

[1.x.203] 



Once we have that, copy it back into local copies on all processors and refine the mesh accordingly: 

[1.x.204] 



Finally, set up quadrature point data again on the new mesh, and only on those cells that we have determined to be ours: 

[1.x.205] 




[1.x.206]  [1.x.207] 




At the end of each time step, we move the nodes of the mesh according to the incremental displacements computed in this time step. To do this, we keep a vector of flags that indicate for each vertex whether we have already moved it around, and then loop over all cells and move those vertices of the cell that have not been moved yet. It is worth noting that it does not matter from which of the cells adjacent to a vertex we move this vertex: since we compute the displacement using a continuous finite element, the displacement field is continuous as well and we can compute the displacement of a given vertex from each of the adjacent cells. We only have to make sure that we move each node exactly once, which is why we keep the vector of flags.    


There are two noteworthy things in this function. First, how we get the displacement field at a given vertex using the  [2.x.243]  function that returns the index of the  [2.x.244]  of the given cell. In the present case, displacement in the k-th coordinate direction corresponds to the k-th component of the finite element. Using a function like this bears a certain risk, because it uses knowledge of the order of elements that we have taken together for this program in the  [2.x.245]  element. If we decided to add an additional variable, for example a pressure variable for stabilization, and happened to insert it as the first variable of the element, then the computation below will start to produce nonsensical results. In addition, this computation rests on other assumptions: first, that the element we use has, indeed, degrees of freedom that are associated with vertices. This is indeed the case for the present Q1 element, as would be for all Qp elements of polynomial order  [2.x.246] . However, it would not hold for discontinuous elements, or elements for mixed formulations. Secondly, it also rests on the assumption that the displacement at a vertex is determined solely by the value of the degree of freedom associated with this vertex; in other words, all shape functions corresponding to other degrees of freedom are zero at this particular vertex. Again, this is the case for the present element, but is not so for all elements that are presently available in deal.II. Despite its risks, we choose to use this way in order to present a way to query individual degrees of freedom associated with vertices.    


In this context, it is instructive to point out what a more general way would be. For general finite elements, the way to go would be to take a quadrature formula with the quadrature points in the vertices of a cell. The  [2.x.247]  formula for the trapezoidal rule does exactly this. With this quadrature formula, we would then initialize an  [2.x.248]  object in each cell, and use the  [2.x.249]  function to obtain the values of the solution function in the quadrature points, i.e. the vertices of the cell. These are the only values that we really need, i.e. we are not at all interested in the weights (or the  [2.x.250]  values) associated with this particular quadrature formula, and this can be specified as the last argument in the constructor to  [2.x.251] . The only point of minor inconvenience in this scheme is that we have to figure out which quadrature point corresponds to the vertex we consider at present, as they may or may not be ordered in the same order.    


This inconvenience could be avoided if finite elements have support points on vertices (which the one here has; for the concept of support points, see  [2.x.252]  "support points"). For such a case, one could construct a custom quadrature rule using  [2.x.253]  The first  [2.x.254]  quadrature points will then correspond to the vertices of the cell and are ordered consistent with  [2.x.255] , taking into account that support points for vector elements will be duplicated  [2.x.256]  times.    


Another point worth explaining about this short function is the way in which the triangulation class exports information about its vertices: through the  [2.x.257]  function, it advertises how many vertices there are in the triangulation. Not all of them are actually in use all the time -- some are left-overs from cells that have been coarsened previously and remain in existence since deal.II never changes the number of a vertex once it has come into existence, even if vertices with lower number go away. Secondly, the location returned by  [2.x.258]  is not only a read-only object of type  [2.x.259] , but in fact a reference that can be written to. This allows to move around the nodes of a mesh with relative ease, but it is worth pointing out that it is the responsibility of an application program using this feature to make sure that the resulting cells are still useful, i.e. are not distorted so much that the cell is degenerated (indicated, for example, by negative Jacobians). Note that we do not have any provisions in this function to actually ensure this, we just have faith.    


After this lengthy introduction, here are the full 20 or so lines of code: 

[1.x.208] 




[1.x.209]  [1.x.210] 




At the beginning of our computations, we needed to set up initial values of the history variables, such as the existing stresses in the material, that we store in each quadrature point. As mentioned above, we use the  [2.x.260]  for this that is available in each cell.    


To put this into larger perspective, we note that if we had previously available stresses in our model (which we assume do not exist for the purpose of this program), then we would need to interpolate the field of preexisting stresses to the quadrature points. Likewise, if we were to simulate elasto-plastic materials with hardening/softening, then we would have to store additional history variables like the present yield stress of the accumulated plastic strains in each quadrature points. Pre-existing hardening or weakening would then be implemented by interpolating these variables in the present function as well. 

[1.x.211] 



For good measure, we set all user pointers of all cells, whether ours of not, to the null pointer. This way, if we ever access the user pointer of a cell which we should not have accessed, a segmentation fault will let us know that this should not have happened: 







[1.x.212] 



Next, allocate the quadrature objects that are within the responsibility of this processor. This, of course, equals the number of cells that belong to this processor times the number of quadrature points our quadrature formula has on each cell. Since the `resize()` function does not actually shrink the amount of allocated memory if the requested new size is smaller than the old size, we resort to a trick to first free all memory, and then reallocate it: we declare an empty vector as a temporary variable and then swap the contents of the old vector and this temporary variable. This makes sure that the `quadrature_point_history` is now really empty, and we can let the temporary variable that now holds the previous contents of the vector go out of scope and be destroyed. In the next step we can then re-allocate as many elements as we need, with the vector default-initializing the `PointHistory` objects, which includes setting the stress variables to zero. 

[1.x.213] 



Finally loop over all cells again and set the user pointers from the cells that belong to the present processor to point to the first quadrature point objects corresponding to this cell in the vector of such objects: 

[1.x.214] 



At the end, for good measure make sure that our count of elements was correct and that we have both used up all objects we allocated previously, and not point to any objects beyond the end of the vector. Such defensive programming strategies are always good checks to avoid accidental errors and to guard against future changes to this function that forget to update all uses of a variable at the same time. Recall that constructs using the  [2.x.261]  macro are optimized away in optimized mode, so do not affect the run time of optimized runs: 

[1.x.215] 




[1.x.216]  [1.x.217] 




At the end of each time step, we should have computed an incremental displacement update so that the material in its new configuration accommodates for the difference between the external body and boundary forces applied during this time step minus the forces exerted through preexisting internal stresses. In order to have the preexisting stresses available at the next time step, we therefore have to update the preexisting stresses with the stresses due to the incremental displacement computed during the present time step. Ideally, the resulting sum of internal stresses would exactly counter all external forces. Indeed, a simple experiment can make sure that this is so: if we choose boundary conditions and body forces to be time independent, then the forcing terms (the sum of external forces and internal stresses) should be exactly zero. If you make this experiment, you will realize from the output of the norm of the right hand side in each time step that this is almost the case: it is not exactly zero, since in the first time step the incremental displacement and stress updates were computed relative to the undeformed mesh, which was then deformed. In the second time step, we again compute displacement and stress updates, but this time in the deformed mesh -- there, the resulting updates are very small but not quite zero. This can be iterated, and in each such iteration the residual, i.e. the norm of the right hand side vector, is reduced; if one makes this little experiment, one realizes that the norm of this residual decays exponentially with the number of iterations, and after an initial very rapid decline is reduced by roughly a factor of about 3.5 in each iteration (for one testcase I looked at, other testcases, and other numbers of unknowns change the factor, but not the exponential decay). 




In a sense, this can then be considered as a quasi-timestepping scheme to resolve the nonlinear problem of solving large-deformation elasticity on a mesh that is moved along in a Lagrangian manner.    


Another complication is that the existing (old) stresses are defined on the old mesh, which we will move around after updating the stresses. If this mesh update involves rotations of the cell, then we need to also rotate the updated stress, since it was computed relative to the coordinate system of the old cell.    


Thus, what we need is the following: on each cell which the present processor owns, we need to extract the old stress from the data stored with each quadrature point, compute the stress update, add the two together, and then rotate the result together with the incremental rotation computed from the incremental displacement at the present quadrature point. We will detail these steps below: 

[1.x.218] 



First, set up an  [2.x.262]  object by which we will evaluate the incremental displacements and the gradients thereof at the quadrature points, together with a vector that will hold this information: 

[1.x.219] 



Then loop over all cells and do the job in the cells that belong to our subdomain: 

[1.x.220] 



Next, get a pointer to the quadrature point history data local to the present cell, and, as a defensive measure, make sure that this pointer is within the bounds of the global array: 

[1.x.221] 



Then initialize the  [2.x.263]  object on the present cell, and extract the gradients of the displacement at the quadrature points for later computation of the strains 

[1.x.222] 



Then loop over the quadrature points of this cell: 

[1.x.223] 



On each quadrature point, compute the strain increment from the gradients, and multiply it by the stress-strain tensor to get the stress update. Then add this update to the already existing strain at this point: 

[1.x.224] 



Finally, we have to rotate the result. For this, we first have to compute a rotation matrix at the present quadrature point from the incremental displacements. In fact, it can be computed from the gradients, and we already have a function for that purpose: 

[1.x.225] 



Note that the result, a rotation matrix, is in general an antisymmetric tensor of rank 2, so we must store it as a full tensor. 




With this rotation matrix, we can compute the rotated tensor by contraction from the left and right, after we expand the symmetric tensor  [2.x.264]  into a full tensor: 

[1.x.226] 



Note that while the result of the multiplication of these three matrices should be symmetric, it is not due to floating point round off: we get an asymmetry on the order of 1e-16 of the off-diagonal elements of the result. When assigning the result to a  [2.x.265] , the constructor of that class checks the symmetry and realizes that it isn't exactly symmetric; it will then raise an exception. To avoid that, we explicitly symmetrize the result to make it exactly symmetric. 




The result of all these operations is then written back into the original place: 

[1.x.227] 



This ends the project specific namespace  [2.x.266] . The rest is as usual and as already shown in  [2.x.267] : A  [2.x.268]  function that initializes and terminates PETSc, calls the classes that do the actual work, and makes sure that we catch all exceptions that propagate up to this point: 

[1.x.228] 

[1.x.229][1.x.230] 




Running the program takes a good while if one uses debug mode; it takes about eleven minutes on my i7 desktop. Fortunately, the version compiled with optimizations is much faster; the program only takes about a minute and a half after recompiling with the command <tt>make release</tt> on the same machine, a much more reasonable time. 


If run, the program prints the following output, explaining what it is doing during all that time: 

[1.x.231] 

In other words, it is computing on 12,000 cells and with some 52,000 unknowns. Not a whole lot, but enough for a coupled three-dimensional problem to keep a computer busy for a while. At the end of the day, this is what we have for output: 

[1.x.232] 




If we visualize these files with VisIt or Paraview, we get to see the full picture of the disaster our forced compression wreaks on the cylinder (colors in the images encode the norm of the stress in the material): 


 [2.x.269]  


 [2.x.270]  


As is clearly visible, as we keep compressing the cylinder, it starts to bow out near the fully constrained bottom surface and, after about eight time units, buckle in an azimuthally symmetric manner. 


Although the result appears plausible for the symmetric geometry and loading, it is yet to be established whether or not the computation is fully converged. In order to see whether it is, we ran the program again with one more global refinement at the beginning and with the time step halved. This would have taken a very long time on a single machine, so we used a proper workstation and ran it on 16 processors in parallel. The beginning of the output now looks like this: 

[1.x.233] 

That's quite a good number of unknowns, given that we are in 3d. The output of this program are 16 files for each time step: 

[1.x.234] 




Here are first the mesh on which we compute as well as the partitioning for the 16 processors: 


 [2.x.271]  


Finally, here is the same output as we have shown before for the much smaller sequential case: 

 [2.x.272]  


 [2.x.273]  


As before, we observe that at high axial compression the cylinder begins to buckle, but this time ultimately collapses on itself. In contrast to our first run, towards the end of the simulation the deflection pattern becomes nonsymmetric (the central bulge deflects laterally). The model clearly does not provide for this (all our forces and boundary deflections are symmetric) but the effect is probably physically correct anyway: in reality, small inhomogeneities in the body's material properties would lead it to buckle to one side to evade the forcing; in numerical simulations, small perturbations such as numerical round-off or an inexact solution of a linear system by an iterative solver could have the same effect. Another typical source for asymmetries in adaptive computations is that only a certain fraction of cells is refined in each step, which may lead to asymmetric meshes even if the original coarse mesh was symmetric. 


If one compares this with the previous run, the results both qualitatively and quantitatively different. The previous computation was therefore certainly not converged, though we can't say for sure anything about the present one. One would need an even finer computation to find out. However, the point may be moot: looking at the last picture in detail, it is pretty obvious that not only is the linear small deformation model we chose completely inadequate, but for a realistic simulation we would also need to make sure that the body does not intersect itself during deformation (if we continued compressing the cylinder we would observe some self-intersection). Without such a formulation we cannot expect anything to make physical sense, even if it produces nice pictures! 


[1.x.235][1.x.236] 


The program as is does not really solve an equation that has many applications in practice: quasi-static material deformation based on a purely elastic law is almost boring. However, the program may serve as the starting point for more interesting experiments, and that indeed was the initial motivation for writing it. Here are some suggestions of what the program is missing and in what direction it may be extended: 

[1.x.237][1.x.238] 


 The most obvious extension is to use a more realistic material model for large-scale quasistatic deformation. The natural choice for this would be plasticity, in which a nonlinear relationship between stress and strain replaces equation [1.x.239]. Plasticity models are usually rather complicated to program since the stress-strain dependence is generally non-smooth. The material can be thought of being able to withstand only a maximal stress (the yield stress) after which it starts to &ldquo;flow&rdquo;. A mathematical description to this can be given in the form of a variational inequality, which alternatively can be treated as minimizing the elastic energy [1.x.240] subject to the constraint [1.x.241] on the stress. This extension makes the problem to be solved in each time step nonlinear, so we need another loop within each time step. 

Without going into further details of this model, we refer to the excellent book by Simo and Hughes on &ldquo;Computational Inelasticity&rdquo; for a comprehensive overview of computational strategies for solving plastic models. Alternatively, a brief but concise description of an algorithm for plasticity is given in an article by S. Commend, A. Truty, and Th. Zimmermann;  [2.x.274] . 


[1.x.242][1.x.243] 


The formulation we have chosen, i.e. using piecewise (bi-, tri-)linear elements for all components of the displacement vector, and treating the stress as a variable dependent on the displacement is appropriate for most materials. However, this so-called displacement-based formulation becomes unstable and exhibits spurious modes for incompressible or nearly-incompressible materials. While fluids are usually not elastic (in most cases, the stress depends on velocity gradients, not displacement gradients, although there are exceptions such as electro-rheologic fluids), there are a few solids that are nearly incompressible, for example rubber. Another case is that many plasticity models ultimately let the material become incompressible, although this is outside the scope of the present program. 

Incompressibility is characterized by Poisson's ratio [1.x.244] where  [2.x.275]  are the Lam&eacute; constants of the material. Physical constraints indicate that  [2.x.276]  (the condition also follows from mathematical stability considerations). If  [2.x.277]  approaches  [2.x.278] , then the material becomes incompressible. In that case, pure displacement-based formulations are no longer appropriate for the solution of such problems, and stabilization techniques have to be employed for a stable and accurate solution. The book and paper cited above give indications as to how to do this, but there is also a large volume of literature on this subject; a good start to get an overview of the topic can be found in the references of the paper by H.-Y. Duan and Q. Lin;  [2.x.279] . 


[1.x.245][1.x.246] 


In the present form, the program only refines the initial mesh a number of times, but then never again. For any kind of realistic simulation, one would want to extend this so that the mesh is refined and coarsened every few time steps instead. This is not hard to do, in fact, but has been left for future tutorial programs or as an exercise, if you wish. 

The main complication one has to overcome is that one has to transfer the data that is stored in the quadrature points of the cells of the old mesh to the new mesh, preferably by some sort of projection scheme. The general approach to this would go like this: 

- At the beginning, the data is only available in the quadrature points of   individual cells, not as a finite element field that is defined everywhere. 

- So let us find a finite element field that [1.x.247] defined everywhere so   that we can later interpolate it to the quadrature points of the new   mesh. In general, it will be difficult to find a continuous finite element   field that matches the values in the quadrature points exactly because the   number of degrees of freedom of these fields does not match the number of   quadrature points there are, and the nodal values of this global field will   either be over- or underdetermined. But it is usually not very difficult to   find a discontinuous field that matches the values in the quadrature points;   for example, if you have a QGauss(2) quadrature formula (i.e. 4 points per   cell in 2d, 8 points in 3d), then one would use a finite element of kind   FE_DGQ(1), i.e. bi-/tri-linear functions as these have 4 degrees of freedom   per cell in 2d and 8 in 3d. 

- There are functions that can make this conversion from individual points to   a global field simpler. The following piece of pseudo-code should help if   you use a QGauss(2) quadrature formula. Note that the multiplication by the   projection matrix below takes a vector of scalar components, i.e., we can only   convert one set of scalars at a time from the quadrature points to the degrees   of freedom and vice versa. So we need to store each component of stress separately,   which requires  [2.x.280]  vectors. We'll store this set of vectors in a 2D array to   make it easier to read off components in the same way you would the stress tensor.   Thus, we'll loop over the components of stress on each cell and store   these values in the global history field. (The prefix  [2.x.281]    indicates that we work with quantities related to the history variables defined   in the quadrature points.)   [1.x.248] 



- Now that we have a global field, we can refine the mesh and transfer the   history_field vector as usual using the SolutionTransfer class. This will   interpolate everything from the old to the new mesh. 

- In a final step, we have to get the data back from the now interpolated   global field to the quadrature points on the new mesh. The following code   will do that:   [1.x.249] 



It becomes a bit more complicated once we run the program in parallel, since then each process only stores this data for the cells it owned on the old mesh. That said, using a parallel vector for  [2.x.282]  will do the trick if you put a call to  [2.x.283]  after the transfer from quadrature points into the global vector. 


[1.x.250][1.x.251] 


At present, the program makes no attempt to make sure that a cell, after moving its vertices at the end of the time step, still has a valid geometry (i.e. that its Jacobian determinant is positive and bounded away from zero everywhere). It is, in fact, not very hard to set boundary values and forcing terms in such a way that one gets distorted and inverted cells rather quickly. Certainly, in some cases of large deformation, this is unavoidable with a mesh of finite mesh size, but in some other cases this should be preventable by appropriate mesh refinement and/or a reduction of the time step size. The program does not do that, but a more sophisticated version definitely should employ some sort of heuristic defining what amount of deformation of cells is acceptable, and what isn't. [1.x.252] [1.x.253]  [2.x.284]  

 [2.x.285] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32] 



 [2.x.3]  

[1.x.33] 

 [2.x.4]  Support for particles exists in deal.II primarily due to the initial   efforts of Rene Gassmoeller. Please acknowledge this work by citing   the publication  [2.x.5]  if you use particle functionality in your   own work. 

[1.x.34] [1.x.35][1.x.36] 


The finite element method in general, and deal.II in particular, were invented to solve partial differential equations -- in other words, to solve [continuum mechanics](https://en.wikipedia.org/wiki/Continuum_mechanics) problems. On the other hand, sometimes one wants to solve problems in which it is useful to track individual objects ("particles") and how their positions evolve. If this simply leads to a set of ordinary differential equations, for example if you want to track the positions of the planets in the solar system over time, then deal.II is clearly not your right tool. On the other hand, if this evolution is due to the interaction with the solution of partial differential equation, or if having a mesh to determine which particles interact with others (such as in the [smoothed particle hydrodynamics (SPH)](https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics) method), then deal.II has support for you. 

The case we will consider here is how electrically charged particles move through an electric field. As motivation, we will consider [cathode rays](https://en.wikipedia.org/wiki/Cathode_ray): Electrons emitted by a heated piece of metal that is negatively charged (the "cathode"), and that are then accelerated by an electric field towards the positively charged electrode (the "anode"). The anode is typically ring-shaped so that the majority of electrons can fly through the hole in the form of an electron beam. In the olden times, they might then have illuminated the screen of a TV built from a [cathode ray tube](https://en.wikipedia.org/wiki/Cathode-ray_tube). Today, instead, electron beams are useful in [X-ray machines](https://en.wikipedia.org/wiki/X-ray_tube), [electron beam lithography](https://en.wikipedia.org/wiki/Electron-beam_lithography), [electron beam welding](https://en.wikipedia.org/wiki/Electron-beam_welding), and a number of other areas. 

The equations we will then consider are as follows: First, we need to describe the electric field. This is most easily accomplished by noting that the electric potential  [2.x.6]  satisfied the equation [1.x.37] where  [2.x.7]  is the dielectric constant of vacuum, and  [2.x.8]  is the charge density. This is augmented by boundary conditions that we will choose as follows: 

[1.x.38] 

In other words, we prescribe voltages  [2.x.9]  and  [2.x.10]  at the two electrodes and insulating (Neumann) boundary conditions elsewhere. Since the dynamics of the particles are purely due to the electric field  [2.x.11] , we could as well have prescribed  [2.x.12]  and  [2.x.13]  at the two electrodes -- all that matters is the voltage difference at the two electrodes. 

Given this electric potential  [2.x.14]  and the electric field  [2.x.15] , we can describe the trajectory of the  [2.x.16] th particle using the differential equation [1.x.39] where  [2.x.17]  are the mass and electric charge of each particle. In practice, it is convenient to write this as a system of first-order differential equations in the position  [2.x.18]  and velocity  [2.x.19] : 

[1.x.40] 

The deal.II class we will use to deal with particles,  [2.x.20]  stores particles in a way so that the position  [2.x.21]  is part of the  [2.x.22]  data structures. (It stores particles sorted by cell they are in, and consequently needs to know where each particle is.) The velocity  [2.x.23] , on the other hand, is of no concern to  [2.x.24]  and consequently we will store it as a "property" of each particle that we will update in each time step. Properties can also be used to store any other quantity we might care about each particle: its charge, or if they were larger than just an electron, its color, mass, attitude in space, chemical composition, etc. 

There remain two things to discuss to complete the model: Where particles start and what the charge density  [2.x.25]  is. 

First, historically, cathode rays used very large electric fields to pull electrons out of the metal. This produces only a relatively small current. One can do better by heating the cathode: a statistical fraction of electrons in that case have enough thermal energy to leave the metal; the electric field then just has to be strong enough to pull them away from the attraction of their host body. We will model this in the following way: We will create a new particle if (i) the electric field points away from the electrode, i.e., if  [2.x.26]  where  [2.x.27]  is the normal vector at a face pointing out of the domain (into the electrode), and (ii) the electric field exceeds a threshold value  [2.x.28] . This is surely not a sufficiently accurate model for what really happens, but is good enough for our current tutorial program. 

Second, in principle we would have to model the charge density via [1.x.41] 

 [2.x.29]  The issue now is that in reality, a cathode ray tube in an old television yields a current of somewhere around a few milli-Amperes. In the much higher energy beams of particle accelerators, the current may only be a few nano-Ampere. But an Ampere is  [2.x.30]  electrons flowing per second. Now, as you will see in the results section, we really only simulate a few microseconds ( [2.x.31]  seconds), but that still results in very very large numbers of electrons -- far more than we can hope to simulate with a program as small as the current one. As a consequence, let us presume that each particle represents  [2.x.32]  electrons. Then the particle mass and charge are also  [2.x.33]  and  [2.x.34]  and the equations we have to solve are [1.x.42] which is of course exactly the same as above. On the other hand, the charge density for these "clumps" of electrons is given by [1.x.43] It is this form that we will implement in the program, where  [2.x.35]  is chosen rather large in the program to ensure that the particles actually affect the electric field. (This may not be realistic in practice: In most cases, there are just not enough electrons to actually affect the overall electric field. But realism is not our goal here.) 




 [2.x.36]  One may wonder why the equation for the electric field (or, rather, the electric potential) has no time derivative whereas the equations for the electron positions do. In essence, this is a modeling assumption: We assume that the particles move so slowly that at any given time the electric field is in equilibrium. This is saying, in other words, that the velocity of the electrons is much less than the speed of light. In yet other words, we can rephrase this in terms of the electrode voltage  [2.x.37] : Since every volt of electric potential accelerates electrons by approximately 600 km/s (neglecting relativistic effects), requiring  [2.x.38]  is equivalent to saying that  [2.x.39] . Under this assumption (and the assumption that the total number of electrons is small), one can also neglect the creation of magnetic fields by the moving charges, which would otherwise also affect the movement of the electrons. 


[1.x.44][1.x.45] 


The equations outlined above form a set of coupled differential equations. Let us bring them all together in one place again to make that clear: 

[1.x.46] 

Because of the awkward dependence of the electric potential on the particle locations, we don't want to solve this as a coupled system but instead use a decoupled approach where we first solve for the potential in each time step and then the particle locations. (One could also do it the other way around, of course.) This is very much in the same spirit as we do in  [2.x.40] ,  [2.x.41] , and  [2.x.42] , to name just a few, and can all be understood in the context of the operator splitting methods discussed in  [2.x.43] . 

So, if we denote by an upper index  [2.x.44]  the time step, and if we use a simple time discretization for the ODE, then this means that we have to solve the following set of equations in each time step: 

[1.x.47] 

There are of course many better ways to do a time discretization (for example the simple [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration)) but this isn't the point of the tutorial program, and so we will be content with what we have here. (We will comment on a piece of this puzzle in the [1.x.48] section of this program, however.) 

There remains the question of how we should choose the time step size  [2.x.45] . The limitation here is that the  [2.x.46]  class needs to keep track of which cell each particle is in. This is particularly an issue if we are running computations in parallel (say, in  [2.x.47] ) because in that case every process only stores those cells it owns plus one layer of "ghost cells". That's not relevant here, but in general we should make sure that over the course of each time step, a particle moves only from one cell to any of its immediate neighbors (face, edge, or vertex neighbors). If we can ensure that, then  [2.x.48]  is guaranteed to be able to figure out which cell a particle ends up in. To do this, a useful rule of thumb is that we should choose the time step so that for all particles the expected distance the particle moves by is less than one cell diameter: [1.x.49] or equivalently [1.x.50] Here,  [2.x.49]  is the length of the shortest edge of the cell on which particle  [2.x.50]  is located -- in essence, a measure of the size of a cell. 

On the other hand, a particle might already be at the boundary of one cell and the neighboring cell might be once further refined. So then the time to cross that *neighboring* cell would actually be half the amount above, suggesting [1.x.51] 

But even that is not good enough: The formula above updates the particle positions in each time using the formula [1.x.52] that is, using the *current* velocity  [2.x.51] . But we don't have the current velocity yet at the time when we need to choose  [2.x.52]  -- which is after we have updated the potential  [2.x.53]  but before we update the velocity from  [2.x.54]  to  [2.x.55] . All we have is  [2.x.56] . So we need an additional safety factor for our final choice: [1.x.53] How large should  [2.x.57]  be? That depends on how much of underestimate  [2.x.58]  might be compared to  [2.x.59] , and that is actually quite easy to assess: A particle created in one time step with zero velocity will roughly pick up equal velocity increments in each successive time step if the electric field it encounters along the way were roughly constant. So the maximal difference between  [2.x.60]  and  [2.x.61]  would be a factor of two. As a consequence, we will choose  [2.x.62] . 

There is only one other case we ought to consider: What happens in the very first time step? There, any particles to be moved along have just been created, but they have a zero velocity. So we don't know what velocity we should choose for them. Of course, in all other time steps there are also particles that have just been created, but in general, the particles with the highest velocity limit the time step size and so the newly created particles with their zero velocity don't matter. But if we *only* have such particles? 

In that case, we can use the following approximation: If a particle starts at  [2.x.63] , then the update formula tells us that [1.x.54] and consequently [1.x.55] which we can write as [1.x.56] Not wanting to move a particle by more than  [2.x.64]  then implies that we should choose the time step as [1.x.57] Using the same argument about neighboring cells possibly being smaller by a factor of two then leads to the final formula for time step zero: [1.x.58] 

Strictly speaking, we would have to evaluate the electric potential  [2.x.65]  at the location of each particle, but a good enough approximation is to use the maximum of the values at the vertices of the respective cell. (Why the vertices and not the midpoint? Because the gradient of the solution of the Laplace equation, i.e., the electric field, is largest in corner singularities which are located at the vertices of cells.) This has the advantage that we can make good use of the FEValues functionality which can recycle pre-computed material as long as the quadrature points are the same from one cell to the next. 

We could always run this kind of scheme to estimate the difference between  [2.x.66]  and  [2.x.67] , but it relies on evaluating the electric field  [2.x.68]  on each cell, and that is expensive. As a consequence, we will limit this approach to the very first time step. 


[1.x.59][1.x.60] 


Having discussed the time discretization, the discussion of the spatial discretization is going to be short: We use quadratic finite elements, i.e., the space  [2.x.69] , to approximate the electric potential  [2.x.70] . The mesh is adapted a couple of times during the initial time step. All of this is entirely standard if you have read  [2.x.71] , and the implementation does not provide for any kind of surprise. 




[1.x.61][1.x.62] 


Adding and moving particles is, in practice, not very difficult in deal.II. To add one, the `create_particles()` function of this program simply uses a code snippet of the following form: 

[1.x.63] 

In other words, it is not all that different from inserting an object into a  [2.x.72]  or  [2.x.73]  Create the object, set its properties (here, the current location, its reference cell location, and its id) and call `insert_particle`. The only thing that may be surprising is the reference location: In order to evaluate things such as  [2.x.74] , it is necessary to evaluate finite element fields at locations  [2.x.75] . But this requires evaluating the finite element shape functions at points on the refence cell  [2.x.76] . To make this efficient, every particle doesn't just store its location and the cell it is on, but also what location that point corresponds to in the cell's reference coordinate system. 

Updating a particle's position is then no more difficult: One just has to call 

[1.x.64] 

We do this in the `move_particles()` function. The only difference is that we then have to tell the  [2.x.77]  class to also find what cell that position corresponds to (and, when computing in parallel, which process owns this cell). For efficiency reason, this is most easily done after updating all particles' locations, and is achieved via the  [2.x.78]  function. 

There are, of course, times where a particle may leave the domain in question. In that case,  [2.x.79]  can not find a surrounding cell and simply deletes the particle. But, it is often useful to track the number of particles that have been lost this way, and for this the  [2.x.80]  class offers a "signal" that one can attach to. We show how to do this in the constructor of the main class to count how many particles were lost in each time step. Specifically, the way this works is that the  [2.x.81]  class has a "signal" to which one can attach a function that will be executed whenever the signal is triggered. Here, this looks as follows: 

[1.x.65] 

That's a bit of a mouthful, but what's happening is this: We declare a lambda function that "captures" the `this` pointer (so that we can access member functions of the surrounding object inside the lambda function), and that takes two arguments: 

- A reference to the particle that has been "lost". 

- A reference to the cell it was on last. The lambda function then simply calls the  [2.x.82]  function with these arguments. When we attach this lambda function to the signal, the  [2.x.83]  function will trigger the signal for every particle for which it can't find a new home. This gives us the chance to record where the particle is, and to record statistics on it. 




 [2.x.84]  In this tutorial program, we insert particles by hand and at   locations we specifically choose based on conditions that include   the solution of the electrostatic problem. But there are other cases   where one primarily wants to use particles as passive objects, for   example to trace and visualize the flow field of a fluid flow   problem. In those cases, there are numerous functions in the    [2.x.85]  namespace that can generate particles   automatically. One of the functions of this namespace is also used   in the  [2.x.86]  tutorial program, for example. 


[1.x.66][1.x.67] 


The test case here is not meant to be a realistic depiction of a cathode ray tube, but it has the right general characteristics and the point is, in any case, only to demonstrate how one would implement deal.II codes that use particles. 

The following picture shows the geometry that we're going to use: 

 [2.x.87]  

In this picture, the parts of the boundary marked in red and blue are the cathode, held at an electric potential  [2.x.88] . The part of the cathode shown in red is the part that is heated, leading to electrons leaving the metal and then being accelerated by the electric field (a few electric field lines are also shown). The green part of the boundary is the anode, held at  [2.x.89] . The rest of the boundary satisfies a Neumann boundary condition. 

This setup mimicks real devices. The re-entrant corner results in an electric potential  [2.x.90]  whose derivative (the electric field  [2.x.91] ) has a singularity -- in other words, it becomes very large in the vicinity of the corner, allowing it to rip electrons away from the metal. These electrons are then accelerated towards the (green) anode which has a hole in the middle through which the electrons can escape the device and fly on to hit the screen, where they excite the "phosphor" to then emit the light that we see from these old-style TV screens. The non-heated part of the cathode is not subject to the emission of electrons -- in the code, we will mark this as the "focussing element" of the tube, because its negative electric voltage repels the electrons and makes sure that they do not just fly away from the heated part of the cathode perpendicular to the boundary, but in fact bend their paths towards the anode on the right. 

The electric field lines also shown in the picture illustrate that the electric field connects the negative and positive electrodes, respectively. The accelerating force the electrons experience is along these field lines. Finally, the picture shows the mesh used in the computation, illustrating that there are singularities at the tip of the re-rentrant corner as well as at all places where the boundary conditions change; these singularities are visible because the mesh is refined in these locations. 

Of practical interest is to figure out which fraction of the electrons emitted from the cathode actually make it through the hole in the anode -- electrons that just bounce into the anode itself are not actually doing anything useful other than converting eletricity into heat. As a consequence, in the `track_lost_particle()` function (which is called for each particle that leaves the domain, see above), we will estimate where it might have left the domain and report this in the output. 




 [2.x.92]  It is worth repeating that neither the geometry used here, nor in fact any other aspect of this program is intended to represent anything even half-way realistic. Tutorial programs are our tools to teach how deal.II works, and we often use situations for which we have some kind of intuition since this helps us interpret the output of a program, but that's about the extent to which we intend the program to do anything of use besides being a teaching tool. [1.x.68] [1.x.69] 


[1.x.70]  [1.x.71] 




The majority of the include files used in this program are well known from  [2.x.93]  and similar programs: 







[1.x.72] 



The ones that are new are only the following three: The first declares the DiscreteTime class that helps us keep track of time in a time-dependent simulation. The latter two provide all of the particle functionality, namely a way to keep track of particles located on a mesh (the  [2.x.94]  class) and the ability to output these particles' locations and their properties for the purposes of visualization (the  [2.x.95]  class). 

[1.x.73] 




[1.x.74]  [1.x.75] 




As is customary, we put everything that corresponds to the details of the program into a namespace of its own. At the top, we define a few constants for which we would rather use symbolic names than hard-coded numbers. 




Specifically, we define numbers for  [2.x.96]  "boundary indicators" for the various parts of the geometry, as well as the physical properties of electrons and other specifics of the setup we use here. 




For the boundary indicators, let us start enumerating at some random value 101. The principle here is to use numbers that are *uncommon*. If there are pre-defined boundary indicators previously set by the `GridGenerator` functions, they will likely be small integers starting from zero, but not in this rather randomly chosen range. Using numbers such as those below avoids the possibility for conflicts, and also reduces the temptation to just spell these numbers out in the program (because you will probably never remember which is which, whereas you might have been tempted if they had started at 0). 

[1.x.76] 




[1.x.77]  [1.x.78] 




The following is then the main class of this program. It has, fundamentally, the same structure as  [2.x.97]  and many other tutorial programs. This includes the majority of the member functions (with the purpose of the rest probably self-explanatory from their names) as well as only a small number of member variables beyond those of  [2.x.98] , all of which are related to dealing with particles. 

[1.x.79] 




[1.x.80]  [1.x.81] 





[1.x.82]  [1.x.83] 




So then let us get started on the implementation. What the constructor does is really only a straight-forward initialization of all of the member variables at the top. The only two worth mentioning are the `particle_handler`, which is handed a reference to the triangulation on which the particles will live (currently of course still empty, but the particle handler stores the reference and will use it once particles are added -- which happens after the triangulation is built). The other piece of information it gets is how many "properties" each particle needs to store. Here, all we need each particle to remember is its current velocity, i.e., a vector with `dim` components. There are, however, other intrinsic properties that each particle has and that the  [2.x.99]  class automatically and always makes sure are available; in particular, these are the current location of a particle, the cell it is on, it's reference location within that cell, and the particle's ID.    


The only other variable of interest is `time`, an object of type DiscreteTime. It keeps track of the current time we are in a time-dependent simulation, and is initialized with the start time (zero) and end time ( [2.x.100] ). We will later set the time step size in `update_timestep_size()`.    


The body of the constructor consists of a piece of code we have already discussed in the introduction. Namely, we make sure that the `track_lost_particle()` function is called by the `particle_handler` object every time a particle leaves the domain. 

[1.x.84] 




[1.x.85]  [1.x.86] 




The next function is then responsible for generating the mesh on which we want to solve. Recall how the domain looks like:  [2.x.101]  We subdivide this geometry into a mesh of  [2.x.102]  cells that looks like this:  [2.x.103]  The way this is done is by first defining where the  [2.x.104]  vertices are located -- here, we say that they are on integer points with the middle one on the left side moved to the right by a value of `delta=0.5`.    


In the following, we then have to say which vertices together form the 8 cells. The following code is then entirely equivalent to what we also do in  [2.x.105] : 

[1.x.88] 



With these arrays out of the way, we can move to slightly higher higher-level data structures. We create a vector of CellData objects that store for each cell to be created the vertices in question as well as the  [2.x.106]  "material id" (which we will here simply set to zero since we don't use it in the program).      


This information is then handed to the  [2.x.107]  function, and the mesh is twice globally refined. 

[1.x.89] 



The remaining part of the function loops over all cells and their faces, and if a face is at the boundary determines which boundary indicator should be applied to it. The various conditions should make sense if you compare the code with the picture of the geometry above.      


Once done with this step, we refine the mesh once more globally. 

[1.x.90] 




[1.x.91]  [1.x.92] 




The next function in this program deals with setting up the various objects related to solving the partial differential equations. It is in essence a copy of the corresponding function in  [2.x.108]  and requires no further discussion. 

[1.x.93] 




[1.x.94]  [1.x.95] 




The function that computes the matrix entries is again in essence a copy of the corresponding function in  [2.x.109] : 

[1.x.96] 



The only interesting part of this function is how it forms the right hand side of the linear system. Recall that the right hand side of the PDE is [1.x.97] where we have used  [2.x.110]  to index the particles here to avoid confusion with the shape function  [2.x.111] ;  [2.x.112]  is the position of the  [2.x.113] th particle.          


When multiplied by a test function  [2.x.114]  and integrated over the domain results in a right hand side vector 

[1.x.98] 

Note that the final line no longer contains an integral, and consequently also no occurrence of  [2.x.115]  which would require the appearance of the `JxW` symbol in our code.          


For a given cell  [2.x.116] , this cell's contribution to the right hand side is then 

[1.x.99] 

i.e., we only have to worry about those particles that are actually located on the current cell  [2.x.117] .          


In practice, what we do here is the following: If there are any particles on the current cell, then we first obtain an iterator range pointing to the first particle of that cell as well as the particle past the last one on this cell (or the end iterator) -- i.e., a half-open range as is common for C++ functions. Knowing now the list of particles, we query their reference locations (with respect to the reference cell), evaluate the shape functions in these reference locations, and compute the force according to the formula above (without any  [2.x.118]           




 [2.x.119]  It is worth pointing out that calling the  [2.x.120]  and  [2.x.121]  functions is not very efficient on problems with a large number of particles. But it illustrates the easiest way to write this algorithm, and so we are willing to incur this cost for the moment for expository purposes. We discuss the issue in more detail in the [1.x.100] below, and use a better approach in  [2.x.122] , for example. 

[1.x.101] 



Finally, we can copy the contributions of this cell into the global matrix and right hand side vector: 

[1.x.102] 




[1.x.103]  [1.x.104] 




The function that solves the linear system is then again exactly as in  [2.x.123] : 

[1.x.105] 




[1.x.106]  [1.x.107] 




The final field-related function is the one that refines the grid. We will call it a number of times in the first time step to obtain a mesh that is well-adapted to the structure of the solution and, in particular, resolves the various singularities in the solution that are due to re-entrant corners and places where the boundary condition type changes. You might want to refer to  [2.x.124]  again for more details: 

[1.x.108] 




[1.x.109]  [1.x.110] 




Let us now turn to the functions that deal with particles. The first one is about the creation of particles. As mentioned in the introduction, we want to create a particle at points of the cathode if the the electric field  [2.x.125]  exceeds a certain threshold, i.e., if  [2.x.126] , and if furthermore the electric field points into the domain (i.e., if  [2.x.127] ). As is common in the finite element method, we evaluate fields (and their derivatives) at specific evaluation points; typically, these are "quadrature points", and so we create a "quadrature formula" that we will use to designate the points at which we want to evaluate the solution. Here, we will simply take QMidpoint implying that we will only check the threshold condition at the midpoints of faces. We then use this to initialize an object of type FEFaceValues to evaluate the solution at these points.    


All of this will then be used in a loop over all cells, their faces, and specifically those faces that are at the boundary and, moreover, the cathode part of the boundary. 

[1.x.111] 



So we have found a face on the cathode. Next, we let the FEFaceValues object compute the gradient of the solution at each "quadrature" point, and extract the electric field vector from the gradient in the form of a Tensor variable through the methods discussed in the  [2.x.128]  "vector-valued problems" documentation module. 

[1.x.112] 



Electrons can only escape the cathode if the electric field strength exceeds a threshold and, crucially, if the electric field points *into* the domain. Once we have that checked, we create a new  [2.x.129]  object at this location and insert it into the  [2.x.130]  object with a unique ID.                  


The only thing that may be not obvious here is that we also associate with this particle the location in the reference coordinates of the cell we are currently on. This is done because we will in downstream functions compute quantities such as the electric field at the location of the particle (e.g., to compute the forces that act on it when updating its position in each time step). Evaluating a finite element field at arbitrary coordinates is quite an expensive operation because shape functions are really only defined on the reference cell, and so when asking for the electric field at an arbitrary point requires us first to determine what the reference coordinates of that point are. To avoid having to do this over and over, we determine these coordinates once and for all and then store these reference coordinates directly with the particle. 

[1.x.113] 



At the end of all of these insertions, we let the `particle_handler` update some internal statistics about the particles it stores. 

[1.x.114] 




[1.x.115]  [1.x.116] 




The second particle-related function is the one that moves the particles in each time step. To do this, we have to loop over all cells, the particles in each cell, and evaluate the electric field at each of the particles' positions.    


The approach used here is conceptually the same used in the `assemble_system()` function: We loop over all cells, find the particles located there (with the same caveat about the inefficiency of the algorithm used here to find these particles), and use FEPointEvaluation object to evaluate the gradient at these positions: 

[1.x.117] 



Then we can ask the FEPointEvaluation object for the gradients of the solution (i.e., the electric field  [2.x.131] ) at these locations and loop over the individual particles: 

[1.x.118] 



Having now obtained the electric field at the location of one of the particles, we use this to update first the velocity and then the position. To do so, let us first get the old velocity out of the properties stored with the particle, compute the acceleration, update the velocity, and store this new velocity again in the properties of the particle. Recall that this corresponds to the first of the following set of update equations discussed in the introduction: 

[1.x.119] 



[1.x.120] 



With the new velocity, we can then also update the location of the particle and tell the particle about it. 

[1.x.121] 



Having updated the locations and properties (i.e., velocities) of all particles, we need to make sure that the `particle_handler` again knows which cells they are in, and what their locations in the coordinate system of the reference cell are. The following function does that. (It also makes sure that, in parallel computations, particles are moved from one processor to another processor if a particle moves from the subdomain owned by the former to the subdomain owned by the latter.) 

[1.x.122] 




[1.x.123]  [1.x.124] 




The final particle-related function is the one that is called whenever a particle is lost from the simulation. This typically happens if it leaves the domain. If that happens, this function is called both the cell (which we can ask for its new location) and the cell it was previously on. The function then keeps track of updating the number of particles lost in this time step, the total number of lost particles, and then estimates whether the particle left through the hole in the middle of the anode. We do so by first checking whether the cell it was in last had an  [2.x.132]  coordinate to the left of the right boundary (located at  [2.x.133] ) and the particle now has a position to the right of the right boundary. If that is so, we compute a direction vector of its motion that is normalized so that the  [2.x.134]  component of the direction vector is equal to  [2.x.135] . With this direction vector, we can compute where it would have intersected the line  [2.x.136] . If this intersect is between  [2.x.137]  and  [2.x.138] , then we claim that the particle left through the hole and increment a counter. 

[1.x.125] 




[1.x.126]  [1.x.127] 




As discussed at length in the introduction, we need to respect a time step condition whereby particles can not move further than one cell in one time step. To ensure that this is the case, we again first compute the maximal speed of all particles on each cell, and divide the cell size by that speed. We then compute the next time step size as the minimum of this quantity over all cells, using the safety factor discussed in the introduction, and set this as the desired time step size using the  [2.x.139]  function. 

[1.x.128] 



As mentioned in the introduction, we have to treat the very first time step differently since there, particles are not available yet or do not yet have the information associated that we need for the computation of a reasonable step length. The formulas below follow the discussion in the introduction. 

[1.x.129] 




[1.x.130]  [1.x.131] 




The final function implementing pieces of the overall algorithm is the one that generates graphical output. In the current context, we want to output both the electric potential field as well as the particle locations and velocities. But we also want to output the electric field, i.e., the gradient of the solution.    


deal.II has a general way how one can compute derived quantities from the solution and output those as well. Here, this is the electric field, but it could also be some other quantity -- say, the norm of the electric field, or in fact anything else one could want to compute from the solution  [2.x.140]  or its derivatives. This general solution uses the DataPostprocessor class and, in cases like the one here where we want to output a quantity that represents a vector field, the DataPostprocessorVector class.    


Rather than try and explain how this class works, let us simply refer to the documentation of the DataPostprocessorVector class that has essentially this case as a well-documented example. 

[1.x.132] 



With this, the `output_results()` function becomes relatively straightforward: We use the DataOut class as we have in almost every one of the previous tutorial programs to output the solution (the "electric potential") and we use the postprocessor defined above to also output its gradient (the "electric field"). This all is then written into a file in VTU format after also associating the current time and time step number with this file. 

[1.x.133] 



Output the particle positions and properties is not more complicated. The  [2.x.141]  class plays the role of the DataOut class for particles, and all we have to do is tell that class where to take particles from and how to interpret the `dim` components of the properties -- namely, as a single vector indicating the velocity, rather than as `dim` scalar properties. The rest is then the same as above: 

[1.x.134] 




[1.x.135]  [1.x.136] 




The last member function of the principal class of this program is then the driver. At the top, it refines the mesh a number of times by solving the problem (with not particles yet created) on a sequence of finer and finer meshes. 

[1.x.137] 



do a few refinement cycles up front 

[1.x.138] 



Now do the loop over time. The sequence of steps follows closely the outline of the algorithm discussed in the introduction. As discussed in great detail in the documentation of the DiscreteTime class, while we move the field and particle information forward by one time step, the time stored in the `time` variable is not consistent with where (some of) these quantities are (in the diction of DiscreteTime, this is the "update stage"). The call to `time.advance_time()` makes everything consistent again by setting the `time` variable to the time at which the field and particles already are, and once we are in this "consistent stage", we can generate graphical output and write information about the current state of the simulation to screen. 

[1.x.139] 




[1.x.140]  [1.x.141] 




The final function of the program is then again the `main()` function. It is unchanged in all tutorial programs since  [2.x.142]  and so there is nothing new to discuss: 

[1.x.142] 

[1.x.143][1.x.144] 


When this program is run, it produces output that looks as follows: ``` Timestep 1   Field degrees of freedom:                                 4989   Total number of particles in simulation:  20   Number of particles lost this time step:  0 

  Now at t=2.12647e-07, dt=2.12647e-07. 

Timestep 2   Field degrees of freedom:                 4989   Total number of particles in simulation:  24   Number of particles lost this time step:  0 

  Now at t=4.14362e-07, dt=2.01715e-07. 

Timestep 3   Field degrees of freedom:                 4989   Total number of particles in simulation:  28   Number of particles lost this time step:  0 

  Now at t=5.96019e-07, dt=1.81657e-07. 

Timestep 4   Field degrees of freedom:                 4989   Total number of particles in simulation:  32   Number of particles lost this time step:  0 

  Now at t=7.42634e-07, dt=1.46614e-07. 


... 


  Timestep 1000   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  6   Fraction of particles lost through anode: 0.0601266 

  Now at t=4.93276e-05, dt=4.87463e-08. 

Timestep 1001   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0601266 

  Now at t=4.93759e-05, dt=4.82873e-08. 


... 


Timestep 2091   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0503338 

  Now at t=9.99237e-05, dt=4.26254e-08. 

Timestep 2092   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  0   Fraction of particles lost through anode: 0.0503338 

  Now at t=9.99661e-05, dt=4.24442e-08. 

Timestep 2093   Field degrees of freedom:                 4989   Total number of particles in simulation:  44   Number of particles lost this time step:  2   Fraction of particles lost through anode: 0.050308 

  Now at t=0.0001, dt=3.38577e-08. ``` 

Picking a random few time steps, we can visualize the solution in the form of streamlines for the electric field and dots for the electrons:  [2.x.143]  

That said, a more appropriate way to visualize the results of this program are by creating a video that shows how these electrons move, and how the electric field changes in response to their motion: 

[1.x.145] 



What you can see here is how the "focus element" of the boundary with its negative voltage repels the electrons and makes sure that they do not just fly away perpendicular from the cathode (as they do in the initial part of their trajectories). It also shows how the electric field lines move around over time, in response to the charges flying by -- in other words, the feedback the particles have on the electric field that itself drives the motion of the electrons. 

The movie suggests that electrons move in "bunches" or "bursts". One element of this appearance is an artifact of how the movie was created: Every frame of the movie corresponds to one time step, but the time step length varies. More specifically, the fastest particle moving through the smallest cell determines the length of the time step (see the discussion in the introduction), and consequently time steps are small whenever a (fast) particle moves through the small cells at the right edge of the domain; time steps are longer again once the particle has left the domain. This slowing-accelerating effect can easily be visualized by plotting the time step length shown in the screen output. 

The second part of this is real, however: The simulation creates a large group of particles in the beginning, and fewer after about the 300th time step. This is probably because of the negative charge of the particles in the simulation: They reduce the magnitude of the electric field at the (also negatively charged electrode) and consequently reduce the number of points on the cathode at which the magnitude exceeds the threshold necessary to draw an electron out of the electrode. 


[1.x.146] [1.x.147][1.x.148] 


[1.x.149][1.x.150] 


The `assemble_system()`, `move_particles()`, and `update_timestep_size()` functions all call  [2.x.144]  and  [2.x.145]  that query information about the particles located on the current cell. While this is convenient, it's also inefficient. To understand why this is so, one needs to know how particles are stored in  [2.x.146]  namely, in a data structure in which particles are ordered in some kind of linear fashion sorted by the cell they are on. Consequently, in order to find the particles associated with a given cell, these functions need to search for the first (and possibly last) particle on a given cell -- an effort that costs  [2.x.147]  operations where  [2.x.148]  is the number of particles. But this is repeated on every cell; assuming that for large computations, the number of cells and particles are roughly proportional, the accumulated cost of these function calls is then  [2.x.149]  and consequently larger than the  [2.x.150]  cost that we should shoot for with all parts of a program. 

We can make this cheaper, though. First, instead of calling  [2.x.151]  we might first call  [2.x.152]  and then compute the number of particles on a cell by just computing the distance of the last to the first particle on the current cell: 

[1.x.151] 

The first of these calls is of course still  [2.x.153] , but at least the second call only takes a compute time proportional to the number of particles on the current cell and so, when accumulated over all cells, has a cost of  [2.x.154] . 

But we can even get rid of the first of these calls with some proper algorithm design. That's because particles are ordered in the same way as cells, and so we can just walk them as we move along on the cells. The following outline of an algorithm does this: 

[1.x.152] 



In this code, we touch every cell exactly once and we never have to search the big data structure for the first or last particle on each cell. As a consequence, the algorithm costs a total of  [2.x.155]  for a complete sweep of all particles and all cells. 

It would not be very difficult to implement this scheme for all three of the functions in this program that have this issue. 


[1.x.153][1.x.154] 


The program already computes the fraction of the electrons that leave the domain through the hole in the anode. But there are other quantities one might be interested in. For example, the average velocity of these particles. It would not be very difficult to obtain each particle's velocity from its properties, in the same way as we do in the `move_particles()` function, and compute statistics from it. 


[1.x.155][1.x.156] 


As discussed above, there is a varying time difference between different frames of the video because we create output for every time step. A better way to create movies would be to generate a new output file in fixed time intervals, regardless of how many time steps lie between each such point. 


[1.x.157][1.x.158] 


The problem we are considering in this program is a coupled, multiphysics problem. But the way we solve it is by first computing the (electric) potential field and then update the particle locations. This is what is called an "operator-splitting method", a concept we will investigate in more detail in  [2.x.156] . 

While it is awkward to think of a way to solve this problem that does not involve splitting the problem into a PDE piece and a particles piece, one *can* (and probably should!) think of a better way to update the particle locations. Specifically, the equations we use to update the particle location are 

[1.x.159] 

This corresponds to a simple forward-Euler time discretization -- a method of first order accuracy in the time step size  [2.x.157]  that we know we should avoid because we can do better. Rather, one might want to consider a scheme such as the [leapfrog scheme](https://en.wikipedia.org/wiki/Leapfrog_integration) or more generally [symplectic integrators](https://en.wikipedia.org/wiki/Symplectic_integrator) such as the [Verlet scheme](https://en.wikipedia.org/wiki/Verlet_integration). 


[1.x.160][1.x.161] 


In release mode, the program runs in about 3.5 minutes on one of the author's laptops at the time of writing this. That's acceptable. But what if we wanted to make the simulation three-dimensional? If we wanted to not use a maximum of around 100 particles at any given time (as happens with the parameters used here) but 100,000? If we needed a substantially finer mesh? 

In those cases, one would want to run the program not just on a single processor, but in fact on as many as one has available. This requires parallelization both the PDE solution as well as over particles. In practice, while there are substantial challenges to making this efficient and scale well, these challenges are all addressed in deal.II itself. For example,  [2.x.158]  shows how to parallelize the finite element part, and  [2.x.159]  shows how one can then also parallelize the particles part. [1.x.162] [1.x.163]  [2.x.160]  

 [2.x.161] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33] 

[1.x.34] [1.x.35][1.x.36] 


 [2.x.3]  

This program is devoted to two aspects: the use of mixed finite elements -- in particular Raviart-Thomas elements -- and using block matrices to define solvers, preconditioners, and nested versions of those that use the substructure of the system matrix. The equation we are going to solve is again the Poisson equation, though with a matrix-valued coefficient: [1.x.37] 

 [2.x.4]  is assumed to be uniformly positive definite, i.e., there is  [2.x.5]  such that the eigenvalues  [2.x.6]  of  [2.x.7]  satisfy  [2.x.8] . The use of the symbol  [2.x.9]  instead of the usual  [2.x.10]  for the solution variable will become clear in the next section. 

After discussing the equation and the formulation we are going to use to solve it, this introduction will cover the use of block matrices and vectors, the definition of solvers and preconditioners, and finally the actual test case we are going to solve. 

We are going to extend this tutorial program in  [2.x.11]  to solve not only the mixed Laplace equation, but add another equation that describes the transport of a mixture of two fluids. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.12]  module. 


[1.x.38][1.x.39] 


In the form above, the Poisson equation (i.e., the Laplace equation with a nonzero right hand side) is generally considered a good model equation for fluid flow in porous media. Of course, one typically models fluid flow through the [1.x.40] or, if fluid velocities are slow or the viscosity is large, the [1.x.41] (which we cover in  [2.x.13] ). In the first of these two models, the forces that act are inertia and viscous friction, whereas in the second it is only viscous friction -- i.e., forces that one fluid particle exerts on a nearby one. This is appropriate if you have free flow in a large domain, say a pipe, a river, or in the air. On the other hand, if the fluid is confined in pores, then friction forces exerted by the pore walls on the fluid become more and more important and internal viscous friction becomes less and less important. Modeling this then first leads to the [1.x.42] if both effects are important, and in the limit of very small pores to the [1.x.43]. The latter is just a different name for the Poisson or Laplace equation, connotating it with the area to which one wants to apply it: slow flow in a porous medium. In essence it says that the velocity is proportional to the negative pressure gradient that drives the fluid through the porous medium. 

The Darcy equation models this pressure that drives the flow. (Because the solution variable is a pressure, we here use the name  [2.x.14]  instead of the name  [2.x.15]  more commonly used for the solution of partial differential equations.) Typical applications of this view of the Laplace equation are then modeling groundwater flow, or the flow of hydrocarbons in oil reservoirs. In these applications,  [2.x.16]  is the permeability tensor, i.e., a measure for how much resistance the soil or rock matrix asserts on the fluid flow. 

In the applications named above, a desirable feature for a numerical scheme is that it should be locally conservative, i.e., that whatever flows into a cell also flows out of it (or the difference is equal to the integral over the source terms over each cell, if the sources are nonzero). However, as it turns out, the usual discretizations of the Laplace equation (such as those used in  [2.x.17] ,  [2.x.18] , or  [2.x.19] ) do not satisfy this property. But, one can achieve this by choosing a different formulation of the problem and a particular combination of finite element spaces. 


[1.x.44][1.x.45] 


To this end, one first introduces a second variable, called the velocity,  [2.x.20] . By its definition, the velocity is a vector in the negative direction of the pressure gradient, multiplied by the permeability tensor. If the permeability tensor is proportional to the unit matrix, this equation is easy to understand and intuitive: the higher the permeability, the higher the velocity; and the velocity is proportional to the gradient of the pressure, going from areas of high pressure to areas of low pressure (thus the negative sign). 

With this second variable, one then finds an alternative version of the Laplace equation, called the [1.x.46]: [1.x.47] 

Here, we have multiplied the equation defining the velocity  [2.x.21]  by  [2.x.22]  because this makes the set of equations symmetric: one of the equations has the gradient, the second the negative divergence, and these two are of course adjoints of each other, resulting in a symmetric bilinear form and a consequently symmetric system matrix under the common assumption that  [2.x.23]  is a symmetric tensor. 

The weak formulation of this problem is found by multiplying the two equations with test functions and integrating some terms by parts: [1.x.48] 

where [1.x.49] 

Here,  [2.x.24]  is the outward normal vector at the boundary. Note how in this formulation, Dirichlet boundary values of the original problem are incorporated in the weak form. 

To be well-posed, we have to look for solutions and test functions in the space  [2.x.25]  for  [2.x.26] , [2.x.27] , and  [2.x.28]  for  [2.x.29] . It is a well-known fact stated in almost every book on finite element theory that if one chooses discrete finite element spaces for the approximation of  [2.x.30]  inappropriately, then the resulting discrete problem is instable and the discrete solution will not converge to the exact solution. (Some details on the problem considered here -- which falls in the class of "saddle-point problems" 

-- can be found on the Wikipedia page on the [1.x.50].) 

To overcome this, a number of different finite element pairs for  [2.x.31]  have been developed that lead to a stable discrete problem. One such pair is to use the Raviart-Thomas spaces  [2.x.32]  for the velocity  [2.x.33]  and discontinuous elements of class  [2.x.34]  for the pressure  [2.x.35] . For details about these spaces, we refer in particular to the book on mixed finite element methods by Brezzi and Fortin, but many other books on the theory of finite elements, for example the classic book by Brenner and Scott, also state the relevant results. In any case, with appropriate choices of function spaces, the discrete formulation reads as follows: Find  [2.x.36]  so that [1.x.51] 




Before continuing, let us briefly pause and show that the choice of function spaces above provides us with the desired local conservation property. In particular, because the pressure space consists of discontinuous piecewise polynomials, we can choose the test function  [2.x.37]  as the function that is equal to one on any given cell  [2.x.38]  and zero everywhere else. If we also choose  [2.x.39]  everywhere (remember that the weak form above has to hold for [1.x.52] discrete test functions  [2.x.40] ), then putting these choices of test functions into the weak formulation above implies in particular that [1.x.53] 

which we can of course write in more explicit form as [1.x.54] 

Applying the divergence theorem results in the fact that  [2.x.41]  has to satisfy, for every choice of cell  [2.x.42] , the relationship [1.x.55] 

If you now recall that  [2.x.43]  was the velocity, then the integral on the left is exactly the (discrete) flux across the boundary of the cell  [2.x.44] . The statement is then that the flux must be equal to the integral over the sources within  [2.x.45] . In particular, if there are no sources (i.e.,  [2.x.46]  in  [2.x.47] ), then the statement is that [1.x.56] flux is zero, i.e., whatever flows into a cell must flow out of it through some other part of the cell boundary. This is what we call [1.x.57] because it holds for every cell. 

On the other hand, the usual continuous  [2.x.48]  elements would not result in this kind of property when used for the pressure (as, for example, we do in  [2.x.49] ) because one can not choose a discrete test function  [2.x.50]  that is one on a cell  [2.x.51]  and zero everywhere else: It would be discontinuous and consequently not in the finite element space. (Strictly speaking, all we can say is that the proof above would not work for continuous elements. Whether these elements might still result in local conservation is a different question as one could think that a different kind of proof might still work; in reality, however, the property really does not hold.) 




[1.x.58][1.x.59] 


The deal.II library (of course) implements Raviart-Thomas elements  [2.x.52]  of arbitrary order  [2.x.53] , as well as discontinuous elements  [2.x.54] . If we forget about their particular properties for a second, we then have to solve a discrete problem [1.x.60] 

with the bilinear form and right hand side as stated above, and  [2.x.55] ,  [2.x.56] . Both  [2.x.57]  and  [2.x.58]  are from the space  [2.x.59] , where  [2.x.60]  is itself a space of  [2.x.61] -dimensional functions to accommodate for the fact that the flow velocity is vector-valued. The necessary question then is: how do we do this in a program? 

Vector-valued elements have already been discussed in previous tutorial programs, the first time and in detail in  [2.x.62] . The main difference there was that the vector-valued space  [2.x.63]  is uniform in all its components: the  [2.x.64]  components of the displacement vector are all equal and from the same function space. What we could therefore do was to build  [2.x.65]  as the outer product of the  [2.x.66]  times the usual  [2.x.67]  finite element space, and by this make sure that all our shape functions have only a single non-zero vector component. Instead of dealing with vector-valued shape functions, all we did in  [2.x.68]  was therefore to look at the (scalar) only non-zero component and use the  [2.x.69]  call to figure out which component this actually is. 

This doesn't work with Raviart-Thomas elements: following from their construction to satisfy certain regularity properties of the space  [2.x.70] , the shape functions of  [2.x.71]  are usually nonzero in all their vector components at once. For this reason, were  [2.x.72]  applied to determine the only nonzero component of shape function  [2.x.73] , an exception would be generated. What we really need to do is to get at  [2.x.74] all [2.x.75]  vector components of a shape function. In deal.II diction, we call such finite elements  [2.x.76] non-primitive [2.x.77] , whereas finite elements that are either scalar or for which every vector-valued shape function is nonzero only in a single vector component are called  [2.x.78] primitive [2.x.79] . 

So what do we have to do for non-primitive elements? To figure this out, let us go back in the tutorial programs, almost to the very beginnings. There, we learned that we use the  [2.x.80]  class to determine the values and gradients of shape functions at quadrature points. For example, we would call  [2.x.81]  to obtain the value of the  [2.x.82] th shape function on the quadrature point with number  [2.x.83] . Later, in  [2.x.84]  and other tutorial programs, we learned that this function call also works for vector-valued shape functions (of primitive finite elements), and that it returned the value of the only non-zero component of shape function  [2.x.85]  at quadrature point  [2.x.86] . 

For non-primitive shape functions, this is clearly not going to work: there is no single non-zero vector component of shape function  [2.x.87] , and the call to  [2.x.88]  would consequently not make much sense. However, deal.II offers a second function call,  [2.x.89]  that returns the value of the  [2.x.90]  at quadrature point  [2.x.91]  is an index between zero and the number of vector components of the present finite element; for example, the element we will use to describe velocities and pressures is going to have  [2.x.92]  components. It is worth noting that this function call can also be used for primitive shape functions: it will simply return zero for all components except one; for non-primitive shape functions, it will in general return a non-zero value for more than just one component. 

We could now attempt to rewrite the bilinear form above in terms of vector components. For example, in 2d, the first term could be rewritten like this (note that  [2.x.93] ): [1.x.61] 

If we implemented this, we would get code like this: 

[1.x.62] 



This is, at best, tedious, error prone, and not dimension independent. There are obvious ways to make things dimension independent, but in the end, the code is simply not pretty. What would be much nicer is if we could simply extract the  [2.x.94]  and  [2.x.95]  components of a shape function  [2.x.96] . In the program we do that in the following way: 

[1.x.63] 



This is, in fact, not only the first term of the bilinear form, but the whole thing (sans boundary contributions). 

What this piece of code does is, given an  [2.x.97]  object, to extract the values of the first  [2.x.98]  components of shape function  [2.x.99]  at quadrature points  [2.x.100] , that is the velocity components of that shape function. Put differently, if we write shape functions  [2.x.101]  as the tuple  [2.x.102] , then the function returns the velocity part of this tuple. Note that the velocity is of course a  [2.x.103] -dimensional tensor, and that the function returns a corresponding object. Similarly, where we subscript with the pressure extractor, we extract the scalar pressure component. The whole mechanism is described in more detail in the  [2.x.104]  module. 

In practice, it turns out that we can do a bit better if we evaluate the shape functions, their gradients and divergences only once per outermost loop, and store the result, as this saves us a few otherwise repeated computations (it is possible to save even more repeated operations by calculating all relevant quantities in advance and then only inserting the results in the actual loop, see  [2.x.105]  for a realization of that approach). The final result then looks like this, working in every space dimension: 

[1.x.64] 



This very closely resembles the form in which we have originally written down the bilinear form and right hand side. 

There is one final term that we have to take care of: the right hand side contained the term  [2.x.106] , constituting the weak enforcement of pressure boundary conditions. We have already seen in  [2.x.107]  how to deal with face integrals: essentially exactly the same as with domain integrals, except that we have to use the FEFaceValues class instead of  [2.x.108] . To compute the boundary term we then simply have to loop over all boundary faces and integrate there. The mechanism works in the same way as above, i.e. the extractor classes also work on FEFaceValues objects: 

[1.x.65] 



You will find the exact same code as above in the sources for the present program. We will therefore not comment much on it below. 


[1.x.66][1.x.67] 


After assembling the linear system we are faced with the task of solving it. The problem here is that the matrix possesses two undesirable properties: 

- It is [1.x.68],   i.e., it has both positive and negative eigenvalues.   We don't want to prove this property here, but note that this is true   for all matrices of the form    [2.x.109]    such as the one here where  [2.x.110]  is positive definite. 

- The matrix has a zero block at the bottom right (there is no term in   the bilinear form that couples the pressure  [2.x.111]  with the   pressure test function  [2.x.112] ). 

At least it is symmetric, but the first issue above still means that the Conjugate Gradient method is not going to work since it is only applicable to problems in which the matrix is symmetric and positive definite. We would have to resort to other iterative solvers instead, such as MinRes, SymmLQ, or GMRES, that can deal with indefinite systems. However, then the next problem immediately surfaces: Due to the zero block, there are zeros on the diagonal and none of the usual, "simple" preconditioners (Jacobi, SSOR) will work as they require division by diagonal elements. 

For the matrix sizes we expect to run with this program, the by far simplest approach would be to just use a direct solver (in particular, the SparseDirectUMFPACK class that is bundled with deal.II).  [2.x.113]  goes this route and shows that solving [1.x.69] linear system can be done in just 3 or 4 lines of code. 

But then, this is a tutorial: We teach how to do things. Consequently, in the following, we will introduce some techniques that can be used in cases like these. Namely, we will consider the linear system as not consisting of one large matrix and vectors, but we will want to decompose matrices into [1.x.70] that correspond to the individual operators that appear in the system. We note that the resulting solver is not optimal -- there are much better ways to efficiently compute the system, for example those explained in the results section of  [2.x.114]  or the one we use in  [2.x.115]  for a problem similar to the current one. Here, our goal is simply to introduce new solution techniques and how they can be implemented in deal.II. 


[1.x.71][1.x.72] 


In view of the difficulties using standard solvers and preconditioners mentioned above, let us take another look at the matrix. If we sort our degrees of freedom so that all velocity come before all pressure variables, then we can subdivide the linear system  [2.x.116]  into the following blocks: [1.x.73] 

where  [2.x.117]  are the values of velocity and pressure degrees of freedom, respectively,  [2.x.118]  is the mass matrix on the velocity space,  [2.x.119]  corresponds to the negative divergence operator, and  [2.x.120]  is its transpose and corresponds to the gradient. 

By block elimination, we can then re-order this system in the following way (multiply the first row of the system by  [2.x.121]  and then subtract the second row from it): [1.x.74] 

Here, the matrix  [2.x.122]  (called the [1.x.75] of  [2.x.123] ) is obviously symmetric and, owing to the positive definiteness of  [2.x.124]  and the fact that  [2.x.125]  has full column rank,  [2.x.126]  is also positive definite. 

Consequently, if we could compute  [2.x.127] , we could apply the Conjugate Gradient method to it. However, computing  [2.x.128]  is expensive because it requires us to compute the inverse of the (possibly large) matrix  [2.x.129] ; and  [2.x.130]  is in fact also a full matrix because even though  [2.x.131]  is sparse, its inverse  [2.x.132]  will generally be a dense matrix. On the other hand, the CG algorithm doesn't require us to actually have a representation of  [2.x.133] : It is sufficient to form matrix-vector products with it. We can do so in steps, using the fact that matrix products are associative (i.e., we can set parentheses in such a way that the product is more convenient to compute): To compute  [2.x.134] , we  [2.x.135]    [2.x.136]  compute  [2.x.137] ;   [2.x.138]  solve  [2.x.139]  for  [2.x.140] , using the CG method applied to the   positive definite and symmetric mass matrix  [2.x.141] ;   [2.x.142]  compute  [2.x.143]  to obtain  [2.x.144] .  [2.x.145]  Note how we evaluate the expression  [2.x.146]  right to left to avoid matrix-matrix products; this way, all we have to do is evaluate matrix-vector products. 

In the following, we will then have to come up with ways to represent the matrix  [2.x.147]  so that it can be used in a Conjugate Gradient solver, as well as to define ways in which we can precondition the solution of the linear system involving  [2.x.148] , and deal with solving linear systems with the matrix  [2.x.149]  (the second step above). 

 [2.x.150]  The key point in this consideration is to recognize that to implement an iterative solver such as CG or GMRES, we never actually need the actual [1.x.76] of a matrix! All that is required is that we can form matrix-vector products. The same is true for preconditioners. In deal.II we encode this requirement by only requiring that matrices and preconditioners given to solver classes have a  [2.x.151]  member function that does the matrix-vector product. How a class chooses to implement this function is not important to the solver. Consequently, classes can implement it by, for example, doing a sequence of products and linear solves as discussed above. 


[1.x.77][1.x.78] 


deal.II includes support for describing such linear operations in a very general way. This is done with the LinearOperator class that, like  [2.x.152]  "the MatrixType concept", defines a minimal interface for [1.x.79] a linear operation to a vector: 

[1.x.80] 

The key difference between a LinearOperator and an ordinary matrix is however that a LinearOperator does not allow any further access to the underlying object. All you can do with a LinearOperator is to apply its "action" to a vector! We take the opportunity to introduce the LinearOperator concept at this point because it is a very useful tool that allows you to construct complex solvers and preconditioners in a very intuitive manner. 

As a first example let us construct a LinearOperator object that represents  [2.x.153] . This means that whenever the  [2.x.154]  function of this operator is called it has to solve a linear system. This requires us to specify a solver (and corresponding) preconditioner. Assuming that  [2.x.155]  is a reference to the upper left block of the system matrix we can write: 

[1.x.81] 

Rather than using a SolverControl we use the ReductionControl class here that stops iterations when either an absolute tolerance is reached (for which we choose  [2.x.156] ) or when the residual is reduced by a certain factor (here,  [2.x.157] ). In contrast the SolverControl class only checks for absolute tolerances. We have to use ReductionControl in our case to work around a minor issue: The right hand sides that we  will feed to  [2.x.158]  are essentially formed by residuals that naturally decrease vastly in norm as the outer iterations progress. This makes control by an absolute tolerance very error prone. 

We now have a LinearOperator  [2.x.159]  that we can use to construct more complicated operators such as the Schur complement  [2.x.160] . Assuming that  [2.x.161]  is a reference to the upper right block constructing a LinearOperator  [2.x.162]  is a matter of two lines: 

[1.x.82] 

Here, the multiplication of three LinearOperator objects yields a composite object  [2.x.163]  function first applies  [2.x.164] , then  [2.x.165]  (i.e. solving an equation with  [2.x.166] ), and finally  [2.x.167]  to any given input vector. In that sense  [2.x.168]  is similar to the following code: 

[1.x.83] 

( [2.x.169]  are two temporary vectors). The key point behind this approach is the fact that we never actually create an inner product of matrices. Instead, whenever we have to perform a matrix vector multiplication with  [2.x.170]  we simply run all individual  [2.x.171]  operations in above sequence. 

 [2.x.172]  We could have achieved the same goal of creating a "matrix like" object by implementing a specialized class  [2.x.173]  that provides a suitable  [2.x.174]  function. Skipping over some details this might have looked like the following: 

[1.x.84] 

Even though both approaches are exactly equivalent, the LinearOperator class has a big advantage over this manual approach. It provides so-called [1.x.85][1.x.86]: Mathematically, we think about  [2.x.175]  as being the composite matrix  [2.x.176]  and the LinearOperator class allows you to write this out more or less verbatim, 

[1.x.87] 

The manual approach on the other hand obscures this fact. 

All that is left for us to do now is to form the right hand sides of the two equations defining  [2.x.177]  and  [2.x.178] , and then solve them with the Schur complement matrix and the mass matrix, respectively. For example the right hand side of the first equation reads  [2.x.179] . This could be implemented as follows: 

[1.x.88] 

Again, this is a perfectly valid approach, but the fact that deal.II requires us to manually resize the final and temporary vector, and that every operation takes up a new line makes this hard to read. This is the point where a second class in the linear operator framework can will help us. Similarly in spirit to LinearOperator, a PackagedOperation stores a "computation": 

[1.x.89] 

The class allows [1.x.90] of expressions involving vectors and linear operators. This is done by storing the computational expression and only performing the computation when either the object is converted to a vector object, or  [2.x.180]  (or  [2.x.181]  is invoked by hand. Assuming that  [2.x.182]  are the two vectors of the right hand side we can simply write: 

[1.x.91] 

Here,  [2.x.183]  is a PackagedOperation that [1.x.92] the computation we specified. It does not create a vector with the actual result immediately. 

With these prerequisites at hand, solving for  [2.x.184]  and  [2.x.185]  is a matter of creating another solver and inverse: 

[1.x.93] 



 [2.x.186]  The functionality that we developed in this example step by hand is already readily available in the library. Have a look at schur_complement(), condense_schur_rhs(), and postprocess_schur_solution(). 


[1.x.94][1.x.95] 


One may ask whether it would help if we had a preconditioner for the Schur complement  [2.x.187] . The general answer, as usual, is: of course. The problem is only, we don't know anything about this Schur complement matrix. We do not know its entries, all we know is its action. On the other hand, we have to realize that our solver is expensive since in each iteration we have to do one matrix-vector product with the Schur complement, which means that we have to do invert the mass matrix once in each iteration. 

There are different approaches to preconditioning such a matrix. On the one extreme is to use something that is cheap to apply and therefore has no real impact on the work done in each iteration. The other extreme is a preconditioner that is itself very expensive, but in return really brings down the number of iterations required to solve with  [2.x.188] . 

We will try something along the second approach, as much to improve the performance of the program as to demonstrate some techniques. To this end, let us recall that the ideal preconditioner is, of course,  [2.x.189] , but that is unattainable. However, how about [1.x.96] 

as a preconditioner? That would mean that every time we have to do one preconditioning step, we actually have to solve with  [2.x.190] . At first, this looks almost as expensive as solving with  [2.x.191]  right away. However, note that in the inner iteration, we do not have to calculate  [2.x.192] , but only the inverse of its diagonal, which is cheap. 

Thankfully, the LinearOperator framework makes this very easy to write out. We already used a Jacobi preconditioner ( [2.x.193] ) for the  [2.x.194]  matrix earlier. So all that is left to do is to write out how the approximate Schur complement should look like: 

[1.x.97] 

Note how this operator differs in simply doing one Jacobi sweep (i.e. multiplying with the inverses of the diagonal) instead of multiplying with the full  [2.x.195] . (This is how a single Jacobi preconditioner step with  [2.x.196]  is defined: it is the multiplication with the inverse of the diagonal of  [2.x.197] ; in other words, the operation  [2.x.198]  on a vector  [2.x.199]  is exactly what PreconditionJacobi does.) 

With all this we almost have the preconditioner completed: it should be the inverse of the approximate Schur complement. We implement this again by creating a linear operator with inverse_operator() function. This time however we would like to choose a relatively modest tolerance for the CG solver (that inverts  [2.x.200] ). The reasoning is that  [2.x.201] , so we actually do not need to invert it exactly. This, however creates a subtle problem:  [2.x.202]  will be used in the final outer CG iteration to create an orthogonal basis. But for this to work, it must be precisely the same linear operation for every invocation. We ensure this by using an IterationNumberControl that allows us to fix the number of CG iterations that are performed to a fixed small number (in our case 30): 

[1.x.98] 



That's all! 

Obviously, applying this inverse of the approximate Schur complement is a very expensive preconditioner, almost as expensive as inverting the Schur complement itself. We can expect it to significantly reduce the number of outer iterations required for the Schur complement. In fact it does: in a typical run on 7 times refined meshes using elements of order 0, the number of outer iterations drops from 592 to 39. On the other hand, we now have to apply a very expensive preconditioner 25 times. A better measure is therefore simply the run-time of the program: on a current laptop (as of January 2019), it drops from 3.57 to 2.05 seconds for this test case. That doesn't seem too impressive, but the savings become more pronounced on finer meshes and with elements of higher order. For example, an seven times refined mesh and using elements of order 2 (which amounts to about 0.4 million degrees of freedom) yields an improvement of 1134 to 83 outer iterations, at a runtime of 168 seconds to 40 seconds. Not earth shattering, but significant. 


[1.x.99][1.x.100] 


In this tutorial program, we will solve the Laplace equation in mixed formulation as stated above. Since we want to monitor convergence of the solution inside the program, we choose right hand side, boundary conditions, and the coefficient so that we recover a solution function known to us. In particular, we choose the pressure solution [1.x.101] 

and for the coefficient we choose the unit matrix  [2.x.203]  for simplicity. Consequently, the exact velocity satisfies [1.x.102] 

This solution was chosen since it is exactly divergence free, making it a realistic test case for incompressible fluid flow. By consequence, the right hand side equals  [2.x.204] , and as boundary values we have to choose  [2.x.205] . 

For the computations in this program, we choose  [2.x.206] . You can find the resulting solution in the [1.x.103], after the commented program. [1.x.104] [1.x.105] 


[1.x.106]  [1.x.107] 




Since this program is only an adaptation of  [2.x.207] , there is not much new stuff in terms of header files. In deal.II, we usually list include files in the order base-lac-grid-dofs-fe-numerics, followed by C++ standard include files: 

[1.x.108] 



The only two new header files that deserve some attention are those for the LinearOperator and PackagedOperation classes: 

[1.x.109] 



This is the only significant new header, namely the one in which the Raviart-Thomas finite element is declared: 

[1.x.110] 



Finally, as a bonus in this program, we will use a tensorial coefficient. Since it may have a spatial dependence, we consider it a tensor-valued function. The following include file provides the  [2.x.208]  class that offers such functionality: 

[1.x.111] 



The last step is as in all previous programs: We put all of the code relevant to this program into a namespace. (This idea was first introduced in  [2.x.209] .) 

[1.x.112] 




[1.x.113]  [1.x.114] 




Again, since this is an adaptation of  [2.x.210] , the main class is almost the same as the one in that tutorial program. In terms of member functions, the main differences are that the constructor takes the degree of the Raviart-Thomas element as an argument (and that there is a corresponding member variable to store this value) and the addition of the  [2.x.211]  function in which, no surprise, we will compute the difference between the exact and the numerical solution to determine convergence of our computations: 

[1.x.115] 



The second difference is that the sparsity pattern, the system matrix, and solution and right hand side vectors are now blocked. What this means and what one can do with such objects is explained in the introduction to this program as well as further down below when we explain the linear solvers and preconditioners for this problem: 

[1.x.116] 




[1.x.117]  [1.x.118] 




Our next task is to define the right hand side of our problem (i.e., the scalar right hand side for the pressure in the original Laplace equation), boundary values for the pressure, and a function that describes both the pressure and the velocity of the exact solution for later computations of the error. Note that these functions have one, one, and  [2.x.212]  components, respectively, and that we pass the number of components down to the  [2.x.213]  base class. For the exact solution, we only declare the function that actually returns the entire solution vector (i.e. all components of it) at once. Here are the respective declarations: 

[1.x.119] 



And then we also have to define these respective functions, of course. Given our discussion in the introduction of how the solution should look, the following computations should be straightforward: 

[1.x.120] 




[1.x.121]  [1.x.122] 




In addition to the other equation data, we also want to use a permeability tensor, or better -- because this is all that appears in the weak form -- the inverse of the permeability tensor,  [2.x.214] . For the purpose of verifying the exactness of the solution and determining convergence orders, this tensor is more in the way than helpful. We will therefore simply set it to the identity matrix.      


However, a spatially varying permeability tensor is indispensable in real-life porous media flow simulations, and we would like to use the opportunity to demonstrate the technique to use tensor valued functions.      


Possibly unsurprisingly, deal.II also has a base class not only for scalar and generally vector-valued functions (the  [2.x.215]  base class) but also for functions that return tensors of fixed dimension and rank, the  [2.x.216]  template. Here, the function under consideration returns a dim-by-dim matrix, i.e. a tensor of rank 2 and dimension  [2.x.217] . We then choose the template arguments of the base class appropriately.      


The interface that the  [2.x.218]  class provides is essentially equivalent to the  [2.x.219]  class. In particular, there exists a  [2.x.220]  function that takes a list of points at which to evaluate the function, and returns the values of the function in the second argument, a list of tensors: 

[1.x.123] 



The implementation is less interesting. As in previous examples, we add a check to the beginning of the class to make sure that the sizes of input and output parameters are the same (see  [2.x.221]  for a discussion of this technique). Then we loop over all evaluation points, and for each one set the output tensor to the identity matrix.      


There is an oddity at the top of the function (the `(void)points;` statement) that is worth discussing. The values we put into the output `values` array does not actually depend on the `points` arrays of coordinates at which the function is evaluated. In other words, the `points` argument is in fact unused, and we could have just not given it a name if we had wanted. But we want to use the `points` object for checking that the `values` object has the correct size. The problem is that in release mode, `AssertDimension` is defined as a macro that expands to nothing; the compiler will then complain that the `points` object is unused. The idiomatic approach to silencing this warning is to have a statement that evaluates (reads) variable but doesn't actually do anything: That's what `(void)points;` does: It reads from `points`, and then casts the result of the read to `void`, i.e., nothing. This statement is, in other words, completely pointless and implies no actual action except to explain to the compiler that yes, this variable is in fact used even in release mode. (In debug mode, the `AssertDimension` macro expands to something that reads from the variable, and so the funny statement would not be necessary in debug mode.) 

[1.x.124] 




[1.x.125]  [1.x.126] 





[1.x.127]  [1.x.128] 




In the constructor of this class, we first store the value that was passed in concerning the degree of the finite elements we shall use (a degree of zero, for example, means to use RT(0) and DG(0)), and then construct the vector valued element belonging to the space  [2.x.222]  described in the introduction. The rest of the constructor is as in the early tutorial programs.    


The only thing worth describing here is the constructor call of the  [2.x.223]  class to which this variable belongs has a number of different constructors that all refer to binding simpler elements together into one larger element. In the present case, we want to couple a single RT(degree) element with a single DQ(degree) element. The constructor to  [2.x.224]  that does this requires us to specify first the first base element (the  [2.x.225]  object of given degree) and then the number of copies for this base element, and then similarly the kind and number of  [2.x.226]  elements. Note that the Raviart-Thomas element already has  [2.x.227]  vector components, so that the coupled element will have  [2.x.228]  vector components, the first  [2.x.229]  of which correspond to the velocity variable whereas the last one corresponds to the pressure.    


It is also worth comparing the way we constructed this element from its base elements, with the way we have done so in  [2.x.230] : there, we have built it as  [2.x.231] , i.e. we have simply used  [2.x.232]  element, one copy for the displacement in each coordinate direction. 

[1.x.129] 




[1.x.130]  [1.x.131] 




This next function starts out with well-known functions calls that create and refine a mesh, and then associate degrees of freedom with it: 

[1.x.132] 



However, then things become different. As mentioned in the introduction, we want to subdivide the matrix into blocks corresponding to the two different kinds of variables, velocity and pressure. To this end, we first have to make sure that the indices corresponding to velocities and pressures are not intermingled: First all velocity degrees of freedom, then all pressure DoFs. This way, the global matrix separates nicely into a  [2.x.233]  system. To achieve this, we have to renumber degrees of freedom based on their vector component, an operation that conveniently is already implemented: 

[1.x.133] 



The next thing is that we want to figure out the sizes of these blocks so that we can allocate an appropriate amount of space. To this end, we call the  [2.x.234]  function that counts how many shape functions are non-zero for a particular vector component. We have  [2.x.235]  vector components, and  [2.x.236]  will count how many shape functions belong to each of these components.      


There is one problem here. As described in the documentation of that function, it [1.x.134] to put the number of  [2.x.237] -velocity shape functions into  [2.x.238] , the number of  [2.x.239] -velocity shape functions into  [2.x.240]  (and similar in 3d), and the number of pressure shape functions into  [2.x.241] . But, the Raviart-Thomas element is special in that it is non- [2.x.242]  "primitive", i.e., for Raviart-Thomas elements all velocity shape functions are nonzero in all components. In other words, the function cannot distinguish between  [2.x.243]  and  [2.x.244]  velocity functions because there [1.x.135] no such distinction. It therefore puts the overall number of velocity into each of  [2.x.245] ,  [2.x.246] . On the other hand, the number of pressure variables equals the number of shape functions that are nonzero in the dim-th component.      


Using this knowledge, we can get the number of velocity shape functions from any of the first  [2.x.247]  elements of  [2.x.248] , and then use this below to initialize the vector and matrix block sizes, as well as create output.      




 [2.x.249]  If you find this concept difficult to understand, you may want to consider using the function  [2.x.250]  instead, as we do in the corresponding piece of code in  [2.x.251] . You might also want to read up on the difference between  [2.x.252]  "blocks" and  [2.x.253]  "components" in the glossary. 

[1.x.136] 



The next task is to allocate a sparsity pattern for the matrix that we will create. We use a compressed sparsity pattern like in the previous steps, but as  [2.x.254]  is a block matrix we use the class  [2.x.255]  instead of just  [2.x.256] . This block sparsity pattern has four blocks in a  [2.x.257]  pattern. The blocks' sizes depend on  [2.x.258] , which hold the number of velocity and pressure variables. In the second step we have to instruct the block system to update its knowledge about the sizes of the blocks it manages; this happens with the  [2.x.259]  call. 

[1.x.137] 



We use the compressed block sparsity pattern in the same way as the non-block version to create the sparsity pattern and then the system matrix: 

[1.x.138] 



Then we have to resize the solution and right hand side vectors in exactly the same way as the block compressed sparsity pattern: 

[1.x.139] 




[1.x.140]  [1.x.141] 




Similarly, the function that assembles the linear system has mostly been discussed already in the introduction to this example. At its top, what happens are all the usual steps, with the addition that we do not only allocate quadrature and  [2.x.260]  objects for the cell terms, but also for face terms. After that, we define the usual abbreviations for variables, and the allocate space for the local matrix and right hand side contributions, and the array that holds the global numbers of the degrees of freedom local to the present cell. 

[1.x.142] 



The next step is to declare objects that represent the source term, pressure boundary value, and coefficient in the equation. In addition to these objects that represent continuous functions, we also need arrays to hold their values at the quadrature points of individual cells (or faces, for the boundary values). Note that in the case of the coefficient, the array has to be one of matrices. 

[1.x.143] 



Finally, we need a couple of extractors that we will use to get at the velocity and pressure components of vector-valued shape functions. Their function and use is described in detail in the  [2.x.261]  report. Essentially, we will use them as subscripts on the FEValues objects below: the FEValues object describes all vector components of shape functions, while after subscription, it will only refer to the velocities (a set of  [2.x.262]  components starting at component zero) or the pressure (a scalar component located at position  [2.x.263] ): 

[1.x.144] 



With all this in place, we can go on with the loop over all cells. The body of this loop has been discussed in the introduction, and will not be commented any further here: 

[1.x.145] 



The final step in the loop over all cells is to transfer local contributions into the global matrix and right hand side vector. Note that we use exactly the same interface as in previous examples, although we now use block matrices and vectors instead of the regular ones. In other words, to the outside world, block objects have the same interface as matrices and vectors, but they additionally allow to access individual blocks. 

[1.x.146] 




[1.x.147]  [1.x.148] 




The linear solvers and preconditioners we use in this example have been discussed in significant detail already in the introduction. We will therefore not discuss the rationale for our approach here any more, but rather only comment on some remaining implementational aspects. 





[1.x.149]  [1.x.150] 




As already outlined in the introduction, the solve function consists essentially of two steps. First, we have to form the first equation involving the Schur complement and solve for the pressure (component 1 of the solution). Then, we can reconstruct the velocities from the second equation (component 0 of the solution). 

[1.x.151] 



As a first step we declare references to all block components of the matrix, the right hand side and the solution vector that we will need. 

[1.x.152] 



Then, we will create corresponding LinearOperator objects and create the  [2.x.264]  operator: 

[1.x.153] 



This allows us to declare the Schur complement  [2.x.265]  and the approximate Schur complement  [2.x.266] : 

[1.x.154] 



We now create a preconditioner out of  [2.x.267]  that applies a fixed number of 30 (inexpensive) CG iterations: 

[1.x.155] 



Now on to the first equation. The right hand side of it is  [2.x.268] , which is what we compute in the first few lines. We then solve the first equation with a CG solver and the preconditioner we just declared. 

[1.x.156] 



After we have the pressure, we can compute the velocity. The equation reads  [2.x.269] , and we solve it by first computing the right hand side, and then multiplying it with the object that represents the inverse of the mass matrix: 

[1.x.157] 




[1.x.158]  [1.x.159] 





[1.x.160]  [1.x.161] 




After we have dealt with the linear solver and preconditioners, we continue with the implementation of our main class. In particular, the next task is to compute the errors in our numerical solution, in both the pressures as well as velocities.    


To compute errors in the solution, we have already introduced the  [2.x.270]  function in  [2.x.271]  and  [2.x.272] . However, there we only dealt with scalar solutions, whereas here we have a vector-valued solution with components that even denote different quantities and may have different orders of convergence (this isn't the case here, by choice of the used finite elements, but is frequently the case in mixed finite element applications). What we therefore have to do is to `mask' the components that we are interested in. This is easily done: the  [2.x.273]  function takes as one of its arguments a pointer to a weight function (the parameter defaults to the null pointer, meaning unit weights). What we have to do is to pass a function object that equals one in the components we are interested in, and zero in the other ones. For example, to compute the pressure error, we should pass a function that represents the constant vector with a unit value in component  [2.x.274] , whereas for the velocity the constant vector should be one in the first  [2.x.275]  components, and zero in the location of the pressure.    


In deal.II, the  [2.x.276]  does exactly this: it wants to know how many vector components the function it is to represent should have (in our case this would be  [2.x.277] , for the joint velocity-pressure space) and which individual or range of components should be equal to one. We therefore define two such masks at the beginning of the function, following by an object representing the exact solution and a vector in which we will store the cellwise errors as computed by  [2.x.278] : 

[1.x.162] 



As already discussed in  [2.x.279] , we have to realize that it is impossible to integrate the errors exactly. All we can do is approximate this integral using quadrature. This actually presents a slight twist here: if we naively chose an object of type  [2.x.280]  as one may be inclined to do (this is what we used for integrating the linear system), one realizes that the error is very small and does not follow the expected convergence curves at all. What is happening is that for the mixed finite elements used here, the Gauss points happen to be superconvergence points in which the pointwise error is much smaller (and converges with higher order) than anywhere else. These are therefore not particularly good points for integration. To avoid this problem, we simply use a trapezoidal rule and iterate it  [2.x.281]  times in each coordinate direction (again as explained in  [2.x.282] ): 

[1.x.163] 



With this, we can then let the library compute the errors and output them to the screen: 

[1.x.164] 




[1.x.165]  [1.x.166] 




The last interesting function is the one in which we generate graphical output. Note that all velocity components get the same solution name "u". Together with using  [2.x.283]  this will cause  [2.x.284]  to generate a vector representation of the individual velocity components, see  [2.x.285]  or the  [2.x.286]  "Generating graphical output" section of the  [2.x.287]  module for more information. Finally, it seems inappropriate for higher order elements to only show a single bilinear quadrilateral per cell in the graphical output. We therefore generate patches of size (degree+1)x(degree+1) to capture the full information content of the solution. See the  [2.x.288]  tutorial program for more information on this. 

[1.x.167] 




[1.x.168]  [1.x.169] 




This is the final function of our main class. It's only job is to call the other functions in their natural order: 

[1.x.170] 




[1.x.171]  [1.x.172] 




The main function we stole from  [2.x.289]  instead of  [2.x.290] . It is almost equal to the one in  [2.x.291]  (apart from the changed class names, of course), the only exception is that we pass the degree of the finite element space to the constructor of the mixed Laplace problem (here, we use zero-th order elements). 

[1.x.173] 

[1.x.174][1.x.175] 


[1.x.176][1.x.177] 




If we run the program as is, we get this output for the  [2.x.292]  mesh we use (for a total of 1024 cells with 1024 pressure degrees of freedom since we use piecewise constants, and 2112 velocities because the Raviart-Thomas element defines one degree per freedom per face and there are  [2.x.293]  faces parallel to the  [2.x.294] -axis and the same number parallel to the  [2.x.295] -axis): 

[1.x.178] 



The fact that the number of iterations is so small, of course, is due to the good (but expensive!) preconditioner we have developed. To get confidence in the solution, let us take a look at it. The following three images show (from left to right) the x-velocity, the y-velocity, and the pressure: 

 [2.x.296]  




Let us start with the pressure: it is highest at the left and lowest at the right, so flow will be from left to right. In addition, though hardly visible in the graph, we have chosen the pressure field such that the flow left-right flow first channels towards the center and then outward again. Consequently, the x-velocity has to increase to get the flow through the narrow part, something that can easily be seen in the left image. The middle image represents inward flow in y-direction at the left end of the domain, and outward flow in y-direction at the right end of the domain. 




As an additional remark, note how the x-velocity in the left image is only continuous in x-direction, whereas the y-velocity is continuous in y-direction. The flow fields are discontinuous in the other directions. This very obviously reflects the continuity properties of the Raviart-Thomas elements, which are, in fact, only in the space H(div) and not in the space  [2.x.297] . Finally, the pressure field is completely discontinuous, but that should not surprise given that we have chosen  [2.x.298]  as the finite element for that solution component. 




[1.x.179][1.x.180] 




The program offers two obvious places where playing and observing convergence is in order: the degree of the finite elements used (passed to the constructor of the  [2.x.299] ), and the refinement level (determined in  [2.x.300] ). What one can do is to change these values and observe the errors computed later on in the course of the program run. 




If one does this, one finds the following pattern for the  [2.x.301]  error in the pressure variable:  [2.x.302]  

The theoretically expected convergence orders are very nicely reflected by the experimentally observed ones indicated in the last row of the table. 




One can make the same experiment with the  [2.x.303]  error in the velocity variables:  [2.x.304]  The result concerning the convergence order is the same here. 




[1.x.181] [1.x.182][1.x.183] 


[1.x.184][1.x.185] 


Realistic flow computations for ground water or oil reservoir simulations will not use a constant permeability. Here's a first, rather simple way to change this situation: we use a permeability that decays very rapidly away from a central flowline until it hits a background value of 0.001. This is to mimic the behavior of fluids in sandstone: in most of the domain, the sandstone is homogeneous and, while permeable to fluids, not overly so; on the other stone, the stone has cracked, or faulted, along one line, and the fluids flow much easier along this large crack. Here is how we could implement something like this: 

[1.x.186] 

Remember that the function returns the inverse of the permeability tensor. 




With a significantly higher mesh resolution, we can visualize this, here with x- and y-velocity: 

 [2.x.305]  

It is obvious how fluids flow essentially only along the middle line, and not anywhere else. 




Another possibility would be to use a random permeability field. A simple way to achieve this would be to scatter a number of centers around the domain and then use a permeability field that is the sum of (negative) exponentials for each of these centers. Flow would then try to hop from one center of high permeability to the next one. This is an entirely unscientific attempt at describing a random medium, but one possibility to implement this behavior would look like this: 

[1.x.187] 



A piecewise constant interpolation of the diagonal elements of the inverse of this tensor (i.e., of  [2.x.306] ) looks as follows: 

 [2.x.307]  


With a permeability field like this, we would get x-velocities and pressures as follows: 

 [2.x.308]  

We will use these permeability fields again in  [2.x.309]  and  [2.x.310] . 


[1.x.188][1.x.189] 


As mentioned in the introduction, the Schur complement solver used here is not the best one conceivable (nor is it intended to be a particularly good one). Better ones can be found in the literature and can be built using the same block matrix techniques that were introduced here. We pick up on this theme again in  [2.x.311] , where we first build a Schur complement solver for the Stokes equation as we did here, and then in the [1.x.190] section discuss better ways based on solving the system as a whole but preconditioning based on individual blocks. We will also come back to this in  [2.x.312] . [1.x.191] [1.x.192]  [2.x.313]  

 [2.x.314] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38] 

[1.x.39][1.x.40] [1.x.41] 


This program grew out of a student project by Yan Li at Texas A&amp;M University. Most of the work for this program is by her. 

In this project, we propose a numerical simulation for two phase flow problems in porous media. This problem includes one elliptic equation and one nonlinear, time dependent transport equation. This is therefore also the first time-dependent tutorial program (besides the somewhat strange time-dependence of  [2.x.3]  " [2.x.4] "). 

The equations covered here are an extension of the material already covered in  [2.x.5] . In particular, they fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.6]  module. 


[1.x.42][1.x.43] 


Modeling of two phase flow in porous media is important for both environmental remediation and the management of petroleum and groundwater reservoirs. Practical situations involving two phase flow include the dispersal of a nonaqueous phase liquid in an aquifer, or the joint movement of a mixture of fluids such as oil and water in a reservoir. Simulation models, if they are to provide realistic predictions, must accurately account for these effects. 

To derive the governing equations, consider two phase flow in a reservoir  [2.x.7]  under the assumption that the movement of fluids is dominated by viscous effects; i.e. we neglect the effects of gravity, compressibility, and capillary pressure. Porosity will be considered to be constant. We will denote variables referring to either of the two phases using subscripts  [2.x.8]  and  [2.x.9] , short for water and oil. The derivation of the equations holds for other pairs of fluids as well, however. 

The velocity with which molecules of each of the two phases move is determined by Darcy's law that states that the velocity is proportional to the pressure gradient: [1.x.44] 

where  [2.x.10]  is the velocity of phase  [2.x.11] ,  [2.x.12]  is the permeability tensor,  [2.x.13]  is the relative permeability of phase  [2.x.14] ,  [2.x.15]  is the pressure and  [2.x.16]  is the viscosity of phase  [2.x.17] . Finally,  [2.x.18]  is the saturation (volume fraction), i.e. a function with values between 0 and 1 indicating the composition of the mixture of fluids. In general, the coefficients  [2.x.19]  may be spatially dependent variables, and we will always treat them as non-constant functions in the following. 

We combine Darcy's law with the statement of conservation of mass for each phase, [1.x.45] with a source term for each phase. By summing over the two phases, we can express the governing equations in terms of the so-called pressure equation: [1.x.46] 

Here,  [2.x.20]  is the sum source term, and [1.x.47] is the total mobility. 

So far, this looks like an ordinary stationary, Poisson-like equation that we can solve right away with the techniques of the first few tutorial programs (take a look at  [2.x.21] , for example, for something very similar). However, we have not said anything yet about the saturation, which of course is going to change as the fluids move around. 

The second part of the equations is the description of the dynamics of the saturation, i.e., how the relative concentration of the two fluids changes with time. The saturation equation for the displacing fluid (water) is given by the following conservation law: [1.x.48] 

which can be rewritten by using the product rule of the divergence operator in the previous equation: [1.x.49] 

Here,  [2.x.22]  is the total influx introduced above, and  [2.x.23]  is the flow rate of the displacing fluid (water). These two are related to the fractional flow  [2.x.24]  in the following way: [1.x.50] where the fractional flow is often parameterized via the (heuristic) expression [1.x.51] Putting it all together yields the saturation equation in the following, advected form: [1.x.52] 

where  [2.x.25]  is the total velocity [1.x.53] Note that the advection equation contains the term  [2.x.26]  rather than  [2.x.27]  to indicate that the saturation is not simply transported along; rather, since the two phases move with different velocities, the saturation can actually change even in the advected coordinate system. To see this, rewrite  [2.x.28]  to observe that the [1.x.54] velocity with which the phase with saturation  [2.x.29]  is transported is  [2.x.30]  whereas the other phase is transported at velocity  [2.x.31] .  [2.x.32]  is consequently often referred to as the [1.x.55]. 

In summary, what we get are the following two equations: [1.x.56] 

Here,  [2.x.33]  are now time dependent functions: while at every time instant the flow field is in equilibrium with the pressure (i.e. we neglect dynamic accelerations), the saturation is transported along with the flow and therefore changes over time, in turn affected the flow field again through the dependence of the first equation on  [2.x.34] . 

This set of equations has a peculiar character: one of the two equations has a time derivative, the other one doesn't. This corresponds to the character that the pressure and velocities are coupled through an instantaneous constraint, whereas the saturation evolves over finite time scales. 

Such systems of equations are called Differential Algebraic Equations (DAEs), since one of the equations is a differential equation, the other is not (at least not with respect to the time variable) and is therefore an "algebraic" equation. (The notation comes from the field of ordinary differential equations, where everything that does not have derivatives with respect to the time variable is necessarily an algebraic equation.) This class of equations contains pretty well-known cases: for example, the time dependent Stokes and Navier-Stokes equations (where the algebraic constraint is that the divergence of the flow field,  [2.x.35] , must be zero) as well as the time dependent Maxwell equations (here, the algebraic constraint is that the divergence of the electric displacement field equals the charge density,  [2.x.36]  and that the divergence of the magnetic flux density is zero:  [2.x.37] ); even the quasistatic model of  [2.x.38]  falls into this category. We will see that the different character of the two equations will inform our discretization strategy for the two equations. 


[1.x.57][1.x.58] 


In the reservoir simulation community, it is common to solve the equations derived above by going back to the first order, mixed formulation. To this end, we re-introduce the total velocity  [2.x.39]  and write the equations in the following form: [1.x.59] 

This formulation has the additional benefit that we do not have to express the total velocity  [2.x.40]  appearing in the transport equation as a function of the pressure, but can rather take the primary variable for it. Given the saddle point structure of the first two equations and their similarity to the mixed Laplace formulation we have introduced in  [2.x.41] , it will come as no surprise that we will use a mixed discretization again. 

But let's postpone this for a moment. The first business we have with these equations is to think about the time discretization. In reservoir simulation, there is a rather standard algorithm that we will use here. It first solves the pressure using an implicit equation, then the saturation using an explicit time stepping scheme. The algorithm is called IMPES for IMplicit Pressure Explicit Saturation and was first proposed a long time ago: by Sheldon et al. in 1959 and Stone and Gardner in 1961 (J. W. Sheldon, B. Zondek and W. T. Cardwell: [1.x.60], Trans. SPE AIME, 216 (1959), pp. 290-296; H. L. Stone and A. O. Gardner Jr: [1.x.61], Trans. SPE AIME, 222 (1961), pp. 92-104). In a slightly modified form, this algorithm can be written as follows: for each time step, solve [1.x.62] 

where  [2.x.42]  is the length of a time step. Note how we solve the implicit pressure-velocity system that only depends on the previously computed saturation  [2.x.43] , and then do an explicit time step for  [2.x.44]  that only depends on the previously known  [2.x.45]  and the just computed  [2.x.46] . This way, we never have to iterate for the nonlinearities of the system as we would have if we used a fully implicit method. (In a more modern perspective, this should be seen as an "operator splitting" method.  [2.x.47]  has a long description of the idea behind this.) 

We can then state the problem in weak form as follows, by multiplying each equation with test functions  [2.x.48] ,  [2.x.49] , and  [2.x.50]  and integrating terms by parts: [1.x.63] 

Note that in the first term, we have to prescribe the pressure  [2.x.51]  on the boundary  [2.x.52]  as boundary values for our problem.  [2.x.53]  denotes the unit outward normal vector to  [2.x.54] , as usual. 

For the saturation equation, we obtain after integrating by parts [1.x.64] 

Using the fact that  [2.x.55] , we can rewrite the cell term to get an equation as follows: [1.x.65] 

We introduce an object of type DiscreteTime in order to keep track of the current value of time and time step in the code. This class encapsulates many complexities regarding adjusting time step size and stopping at a specified final time. 




[1.x.66][1.x.67] 


In each time step, we then apply the mixed finite method of  [2.x.56]  " [2.x.57] " to the velocity and pressure. To be well-posed, we choose Raviart-Thomas spaces  [2.x.58]  for  [2.x.59]  and discontinuous elements of class  [2.x.60]  for  [2.x.61] . For the saturation, we will also choose  [2.x.62]  spaces. 

Since we have discontinuous spaces, we have to think about how to evaluate terms on the interfaces between cells, since discontinuous functions are not really defined there. In particular, we have to give a meaning to the last term on the left hand side of the saturation equation. To this end, let us define that we want to evaluate it in the following sense: [1.x.68] 

where  [2.x.63]  denotes the inflow boundary and  [2.x.64]  is the outflow part of the boundary. The quantities  [2.x.65]  then correspond to the values of these variables on the present cell, whereas  [2.x.66]  (needed on the inflow part of the boundary of  [2.x.67] ) are quantities taken from the neighboring cell. Some more context on discontinuous element techniques and evaluation of fluxes can also be found in  [2.x.68]  and  [2.x.69] b. 


[1.x.69][1.x.70] 


The linear solvers used in this program are a straightforward extension of the ones used in  [2.x.70]  (but without LinearOperator). Essentially, we simply have to extend everything from two to three solution components. If we use the discrete spaces mentioned above and put shape functions into the bilinear forms, we arrive at the following linear system to be solved for time step  [2.x.71] : [1.x.71] where the individual matrices and vectors are defined as follows using shape functions  [2.x.72]  (of type Raviart Thomas  [2.x.73] ) for velocities and  [2.x.74]  (of type  [2.x.75] ) for both pressures and saturations: [1.x.72] 



 [2.x.76]  Due to historical accidents, the role of matrices  [2.x.77]  and  [2.x.78]  has been reverted in this program compared to  [2.x.79] . In other words, here  [2.x.80]  refers to the divergence and  [2.x.81]  to the gradient operators when it was the other way around in  [2.x.82] . 

The system above presents a complication: Since the matrix  [2.x.83]  depends on  [2.x.84]  implicitly (the velocities are needed to determine which parts of the boundaries  [2.x.85]  of cells are influx or outflux parts), we can only assemble this matrix after we have solved for the velocities. 

The solution scheme then involves the following steps:  [2.x.86]     [2.x.87] Solve for the pressure  [2.x.88]  using the Schur complement   technique introduced in  [2.x.89] . 

   [2.x.90] Solve for the velocity  [2.x.91]  as also discussed in    [2.x.92] . 

   [2.x.93] Compute the term  [2.x.94] , using   the just computed velocities. 

   [2.x.95] Solve for the saturation  [2.x.96] .  [2.x.97]  

In this scheme, we never actually build the matrix  [2.x.98] , but rather generate the right hand side of the third equation once we are ready to do so. 

In the program, we use a variable  [2.x.99]  to store the solution of the present time step. At the end of each step, we copy its content, i.e. all three of its block components, into the variable  [2.x.100]  for use in the next time step. 


[1.x.73][1.x.74] 


A general rule of thumb in hyperbolic transport equations like the equation we have to solve for the saturation equation is that if we use an explicit time stepping scheme, then we should use a time step such that the distance that a particle can travel within one time step is no larger than the diameter of a single cell. In other words, here, we should choose [1.x.75] Fortunately, we are in a position where we can do that: we only need the time step when we want to assemble the right hand side of the saturation equation, which is after we have already solved for  [2.x.101] . All we therefore have to do after solving for the velocity is to loop over all quadrature points in the domain and determine the maximal magnitude of the velocity. We can then set the time step for the saturation equation to [1.x.76] 

Why is it important to do this? If we don't, then we will end up with lots of places where our saturation is larger than one or less than zero, as can easily be verified. (Remember that the saturation corresponds to something like the water fraction in the fluid mixture, and therefore must physically be between 0 and 1.) On the other hand, if we choose our time step according to the criterion listed above, this only happens very very infrequently &mdash; in fact only once for the entire run of the program. However, to be on the safe side, however, we run a function  [2.x.102]  at the end of each time step, that simply projects the saturation back onto the interval  [2.x.103] , should it have gotten out of the physical range. This is useful since the functions  [2.x.104]  and  [2.x.105]  do not represent anything physical outside this range, and we should not expect the program to do anything useful once we have negative saturations or ones larger than one. 

Note that we will have similar restrictions on the time step also in  [2.x.106]  and  [2.x.107]  where we solve the time dependent wave equation, another hyperbolic problem. We will also come back to the issue of time step choice below in the section on [1.x.77]. 


[1.x.78][1.x.79] 


For simplicity, this program assumes that there is no source,  [2.x.108] , and that the heterogeneous porous medium is isotropic  [2.x.109] . The first one of these is a realistic assumption in oil reservoirs: apart from injection and production wells, there are usually no mechanisms for fluids to appear or disappear out of the blue. The second one is harder to justify: on a microscopic level, most rocks are isotropic, because they consist of a network of interconnected pores. However, this microscopic scale is out of the range of today's computer simulations, and we have to be content with simulating things on the scale of meters. On that scale, however, fluid transport typically happens through a network of cracks in the rock, rather than through pores. However, cracks often result from external stress fields in the rock layer (for example from tectonic faulting) and the cracks are therefore roughly aligned. This leads to a situation where the permeability is often orders of magnitude larger in the direction parallel to the cracks than perpendicular to the cracks. A problem typically faces in reservoir simulation, however, is that the modeler doesn't know the direction of cracks because oil reservoirs are not accessible to easy inspection. The only solution in that case is to assume an effective, isotropic permeability. 

Whatever the matter, both of these restrictions, no sources and isotropy, would be easy to lift with a few lines of code in the program. 

Next, for simplicity, our numerical simulation will be done on the unit cell  [2.x.110]  for  [2.x.111] . Our initial conditions are  [2.x.112] ; in the oil reservoir picture, where  [2.x.113]  would indicate the water saturation, this means that the reservoir contains pure oil at the beginning. Note that we do not need any initial conditions for pressure or velocity, since the equations do not contain time derivatives of these variables. Finally, we impose the following pressure boundary conditions: [1.x.80] Since the pressure and velocity solve a mixed form Poisson equation, the imposed pressure leads to a resulting flow field for the velocity. On the other hand, this flow field determines whether a piece of the boundary is of inflow or outflow type, which is of relevance because we have to impose boundary conditions for the saturation on the inflow part of the boundary, [1.x.81] On this inflow boundary, we impose the following saturation values: [1.x.82] 

In other words, we have pure water entering the reservoir at the left, whereas the other parts of the boundary are in contact with undisturbed parts of the reservoir and whenever influx occurs on these boundaries, pure oil will enter. 

In our simulations, we choose the total mobility as [1.x.83] where we use  [2.x.114]  for the viscosity. In addition, the fractional flow of water is given by [1.x.84] 

 [2.x.115]  Coming back to this testcase in  [2.x.116]  several years later revealed an oddity in the setup of this testcase. To this end, consider that we can rewrite the advection equation for the saturation as  [2.x.117] . Now, at the initial time, we have  [2.x.118] , and with the given choice of function  [2.x.119] , we happen to have  [2.x.120] . In other words, at  [2.x.121] , the equation reduces to  [2.x.122]  for all  [2.x.123] , so the saturation is zero everywhere and it is going to stay zero everywhere! This is despite the fact that  [2.x.124]  is not necessarily zero: the combined fluid is moving, but we've chosen our partial flux  [2.x.125]  in such a way that infinitesimal amounts of wetting fluid also only move at infinitesimal speeds (i.e., they stick to the medium more than the non-wetting phase in which they are embedded). That said, how can we square this with the knowledge that wetting fluid is invading from the left, leading to the flow patterns seen in the [1.x.85]? That's where we get into mathematics: Equations like the transport equation we are considering here have infinitely many solutions, but only one of them is physical: the one that results from the so-called viscosity limit, called the [1.x.86]. The thing is that with discontinuous elements we arrive at this viscosity limit because using a numerical flux introduces a finite amount of artificial viscosity into the numerical scheme. On the other hand, in  [2.x.126] , we use an artificial viscosity that is proportional to  [2.x.127]  on every cell, which at the initial time is zero. Thus, the saturation there is zero and remains zero; the solution we then get is [1.x.87] solution of the advection equation, but the method does not converge to the viscosity solution without further changes. We will therefore use a different initial condition in that program. 


Finally, to come back to the description of the testcase, we will show results for computations with the two permeability functions introduced at the end of the results section of  [2.x.128]  " [2.x.129] ":  [2.x.130]     [2.x.131] A function that models a single, winding crack that snakes through the   domain. In analogy to  [2.x.132] , but taking care of the slightly   different geometry we have here, we describe this by the following function:   [1.x.88]   Taking the maximum is necessary to ensure that the ratio between maximal and   minimal permeability remains bounded. If we don't do that, permeabilities   will span many orders of magnitude. On the other hand, the ratio between   maximal and minimal permeability is a factor in the condition number of the   Schur complement matrix, and if too large leads to problems for which our   linear solvers will no longer converge properly. 

   [2.x.133] A function that models a somewhat random medium. Here, we choose   [1.x.89] 

  where the centers  [2.x.134]  are  [2.x.135]  randomly chosen locations inside   the domain. This function models a domain in which there are  [2.x.136]  centers of   higher permeability (for example where rock has cracked) embedded in a   matrix of more pristine, unperturbed background rock. Note that here we have   cut off the permeability function both above and below to ensure a bounded   condition number.  [2.x.137]  [1.x.90] [1.x.91] 

This program is an adaptation of  [2.x.138]  and includes some technique of DG methods from  [2.x.139] . A good part of the program is therefore very similar to  [2.x.140]  and we will not comment again on these parts. Only the new stuff will be discussed in more detail. 





[1.x.92]  [1.x.93] 




All of these include files have been used before: 

[1.x.94] 



In this program, we use a tensor-valued coefficient. Since it may have a spatial dependence, we consider it a tensor-valued function. The following include file provides the  [2.x.141]  class that offers such functionality: 

[1.x.95] 



Additionally, we use the class  [2.x.142]  to perform operations related to time incrementation. 

[1.x.96] 



The last step is as in all previous programs: 

[1.x.97] 




[1.x.98]  [1.x.99] 




This is the main class of the program. It is close to the one of  [2.x.143] , but with a few additional functions:    


 [2.x.144]   [2.x.145]  [2.x.146]  assembles the right hand side of the saturation equation. As explained in the introduction, this can't be integrated into  [2.x.147]  since it depends on the velocity that is computed in the first part of the time step.    


 [2.x.148]  [2.x.149]  does as its name suggests. This function is used in the computation of the time step size.    


 [2.x.150]  [2.x.151]  resets all saturation degrees of freedom with values less than zero to zero, and all those with saturations greater than one to one.   [2.x.152]     


The rest of the class should be pretty much obvious. The  [2.x.153]  variable stores the viscosity  [2.x.154]  that enters several of the formulas in the nonlinear equations. The variable  [2.x.155]  keeps track of the time information within the simulation. 

[1.x.100] 




[1.x.101]  [1.x.102] 





[1.x.103]  [1.x.104] 




At present, the right hand side of the pressure equation is simply the zero function. However, the rest of the program is fully equipped to deal with anything else, if this is desired: 

[1.x.105] 




[1.x.106]  [1.x.107] 




The next are pressure boundary values. As mentioned in the introduction, we choose a linear pressure field: 

[1.x.108] 




[1.x.109]  [1.x.110] 




Then we also need boundary values on the inflow portions of the boundary. The question whether something is an inflow part is decided when assembling the right hand side, we only have to provide a functional description of the boundary values. This is as explained in the introduction: 

[1.x.111] 




[1.x.112]  [1.x.113] 




Finally, we need initial data. In reality, we only need initial data for the saturation, but we are lazy, so we will later, before the first time step, simply interpolate the entire solution for the previous time step from a function that contains all vector components.    


We therefore simply create a function that returns zero in all components. We do that by simply forward every function to the  [2.x.156]  class. Why not use that right away in the places of this program where we presently use the  [2.x.157]  class? Because this way it is simpler to later go back and choose a different function for initial values. 

[1.x.114] 




[1.x.115]  [1.x.116] 




As announced in the introduction, we implement two different permeability tensor fields. Each of them we put into a namespace of its own, so that it will be easy later to replace use of one by the other in the code. 





[1.x.117]  [1.x.118] 




The first function for the permeability was the one that models a single curving crack. It was already used at the end of  [2.x.158] , and its functional form is given in the introduction of the present tutorial program. As in some previous programs, we have to declare a (seemingly unnecessary) default constructor of the KInverse class to avoid warnings from some compilers: 

[1.x.119] 




[1.x.120]  [1.x.121] 




This function does as announced in the introduction, i.e. it creates an overlay of exponentials at random places. There is one thing worth considering for this class. The issue centers around the problem that the class creates the centers of the exponentials using a random function. If we therefore created the centers each time we create an object of the present type, we would get a different list of centers each time. That's not what we expect from classes of this type: they should reliably represent the same function.    


The solution to this problem is to make the list of centers a static member variable of this class, i.e. there exists exactly one such variable for the entire program, rather than for each object of this type. That's exactly what we are going to do.    


The next problem, however, is that we need a way to initialize this variable. Since this variable is initialized at the beginning of the program, we can't use a regular member function for that since there may not be an object of this type around at the time. The C++ standard therefore says that only non-member and static member functions can be used to initialize a static variable. We use the latter possibility by defining a function  [2.x.159]  that computes the list of center points when called.    


Note that this class works just fine in both 2d and 3d, with the only difference being that we use more points in 3d: by experimenting we find that we need more exponentials in 3d than in 2d (we have more ground to cover, after all, if we want to keep the distance between centers roughly equal), so we choose 40 in 2d and 100 in 3d. For any other dimension, the function does presently not know what to do so simply throws an exception indicating exactly this. 

[1.x.122] 




[1.x.123]  [1.x.124] 




There are two more pieces of data that we need to describe, namely the inverse mobility function and the saturation curve. Their form is also given in the introduction: 

[1.x.125] 




[1.x.126]  [1.x.127] 




The linear solvers we use are also completely analogous to the ones used in  [2.x.160] . The following classes are therefore copied verbatim from there. Note that the classes here are not only copied from  [2.x.161] , but also duplicate classes in deal.II. In a future version of this example, they should be replaced by an efficient method, though. There is a single change: if the size of a linear system is small, i.e. when the mesh is very coarse, then it is sometimes not sufficient to set a maximum of  [2.x.162]  CG iterations before the solver in the  [2.x.163]  function converges. (This is, of course, a result of numerical round-off, since we know that on paper, the CG method converges in at most  [2.x.164]  steps.) As a consequence, we set the maximum number of iterations equal to the maximum of the size of the linear system and 200. 

[1.x.128] 




[1.x.129]  [1.x.130] 




Here now the implementation of the main class. Much of it is actually copied from  [2.x.165] , so we won't comment on it in much detail. You should try to get familiar with that program first, then most of what is happening here should be mostly clear. 





[1.x.131]  [1.x.132] 




First for the constructor. We use  [2.x.166]  spaces. For initializing the DiscreteTime object, we don't set the time step size in the constructor because we don't have its value yet. The time step size is initially set to zero, but it will be computed before it is needed to increment time, as described in a subsection of the introduction. The time object internally prevents itself from being incremented when  [2.x.167] , forcing us to set a non-zero desired size for  [2.x.168]  before advancing time. 

[1.x.133] 




[1.x.134]  [1.x.135] 




This next function starts out with well-known functions calls that create and refine a mesh, and then associate degrees of freedom with it. It does all the same things as in  [2.x.169] , just now for three components instead of two. 

[1.x.136] 




[1.x.137]  [1.x.138] 




This is the function that assembles the linear system, or at least everything except the (1,3) block that depends on the still-unknown velocity computed during this time step (we deal with this in  [2.x.170] ). Much of it is again as in  [2.x.171] , but we have to deal with some nonlinearity this time.  However, the top of the function is pretty much as usual (note that we set matrix and right hand side to zero at the beginning &mdash; something we didn't have to do for stationary problems since there we use each matrix object only once and it is empty at the beginning anyway).    


Note that in its present form, the function uses the permeability implemented in the  [2.x.172]  class. Switching to the single curved crack permeability function is as simple as just changing the namespace name. 

[1.x.139] 



Here's the first significant difference: We have to get the values of the saturation function of the previous time step at the quadrature points. To this end, we can use the  [2.x.173]  (previously already used in  [2.x.174] ,  [2.x.175]  and  [2.x.176] ), a function that takes a solution vector and returns a list of function values at the quadrature points of the present cell. In fact, it returns the complete vector-valued solution at each quadrature point, i.e. not only the saturation but also the velocities and pressure: 

[1.x.140] 



Then we also have to get the values of the pressure right hand side and of the inverse permeability tensor at the quadrature points: 

[1.x.141] 



With all this, we can now loop over all the quadrature points and shape functions on this cell and assemble those parts of the matrix and right hand side that we deal with in this function. The individual terms in the contributions should be self-explanatory given the explicit form of the bilinear form stated in the introduction: 

[1.x.142] 



Next, we also have to deal with the pressure boundary values. This, again is as in  [2.x.177] : 

[1.x.143] 



The final step in the loop over all cells is to transfer local contributions into the global matrix and right hand side vector: 

[1.x.144] 



So much for assembly of matrix and right hand side. Note that we do not have to interpolate and apply boundary values since they have all been taken care of in the weak form already. 










[1.x.145]  [1.x.146] 




As explained in the introduction, we can only evaluate the right hand side of the saturation equation once the velocity has been computed. We therefore have this separate function to this end. 

[1.x.147] 



First for the cell terms. These are, following the formulas in the introduction,  [2.x.178] , where  [2.x.179]  is the saturation component of the test function: 

[1.x.148] 



Secondly, we have to deal with the flux parts on the face boundaries. This was a bit more involved because we first have to determine which are the influx and outflux parts of the cell boundary. If we have an influx boundary, we need to evaluate the saturation on the other side of the face (or the boundary values, if we are at the boundary of the domain).          


All this is a bit tricky, but has been explained in some detail already in  [2.x.180] . Take a look there how this is supposed to work! 

[1.x.149] 




[1.x.150]  [1.x.151] 




After all these preparations, we finally solve the linear system for velocity and pressure in the same way as in  [2.x.181] . After that, we have to deal with the saturation equation (see below): 

[1.x.152] 



First the pressure, using the pressure Schur complement of the first two equations: 

[1.x.153] 



Now the velocity: 

[1.x.154] 



Finally, we have to take care of the saturation equation. The first business we have here is to determine the time step using the formula in the introduction. Knowing the shape of our domain and that we created the mesh by regular subdivision of cells, we can compute the diameter of each of our cells quite easily (in fact we use the linear extensions in coordinate directions of the cells, not the diameter). Note that we will learn a more general way to do this in  [2.x.182] , where we use the  [2.x.183]  function.      


The maximal velocity we compute using a helper function to compute the maximal velocity defined below, and with all this we can evaluate our new time step length. We use the method  [2.x.184]  to suggest the new calculated value of the time step to the DiscreteTime object. In most cases, the time object uses the exact provided value to increment time. It some case, the step size may be modified further by the time object. For example, if the calculated time increment overshoots the end time, it is truncated accordingly. 

[1.x.155] 



The next step is to assemble the right hand side, and then to pass everything on for solution. At the end, we project back saturations onto the physically reasonable range: 

[1.x.156] 




[1.x.157]  [1.x.158] 




There is nothing surprising here. Since the program will do a lot of time steps, we create an output file only every fifth time step and skip all other time steps at the top of the file already.    


When creating file names for output close to the bottom of the function, we convert the number of the time step to a string representation that is padded by leading zeros to four digits. We do this because this way all output file names have the same length, and consequently sort well when creating a directory listing. 

[1.x.159] 




[1.x.160]  [1.x.161] 




In this function, we simply run over all saturation degrees of freedom and make sure that if they should have left the physically reasonable range, that they be reset to the interval  [2.x.185] . To do this, we only have to loop over all saturation components of the solution vector; these are stored in the block 2 (block 0 are the velocities, block 1 are the pressures).    


It may be instructive to note that this function almost never triggers when the time step is chosen as mentioned in the introduction. However, if we choose the timestep only slightly larger, we get plenty of values outside the proper range. Strictly speaking, the function is therefore unnecessary if we choose the time step small enough. In a sense, the function is therefore only a safety device to avoid situations where our entire solution becomes unphysical because individual degrees of freedom have become unphysical a few time steps earlier. 

[1.x.162] 




[1.x.163]  [1.x.164] 




The following function is used in determining the maximal allowable time step. What it does is to loop over all quadrature points in the domain and find what the maximal magnitude of the velocity is. 

[1.x.165] 




[1.x.166]  [1.x.167] 




This is the final function of our main class. Its brevity speaks for itself. There are only two points worth noting: First, the function projects the initial values onto the finite element space at the beginning; the  [2.x.186]  function doing this requires an argument indicating the hanging node constraints. We have none in this program (we compute on a uniformly refined mesh), but the function requires the argument anyway, of course. So we have to create a constraint object. In its original state, constraint objects are unsorted, and have to be sorted (using the  [2.x.187]  function) before they can be used. This is what we do here, and which is why we can't simply call the  [2.x.188]  function with an anonymous temporary object  [2.x.189]  as the second argument.    


The second point worth mentioning is that we only compute the length of the present time step in the middle of solving the linear system corresponding to each time step. We can therefore output the present time of a time step only at the end of the time step. We increment time by calling the method  [2.x.190]  inside the loop. Since we are reporting the time and dt after we increment it, we have to call the method  [2.x.191]  instead of  [2.x.192]  After many steps, when the simulation reaches the end time, the last dt is chosen by the DiscreteTime class in such a way that the last step finishes exactly at the end time. 

[1.x.168] 




[1.x.169]  [1.x.170] 




That's it. In the main function, we pass the degree of the finite element space to the constructor of the TwoPhaseFlowProblem object.  Here, we use zero-th degree elements, i.e.  [2.x.193] . The rest is as in all the other programs. 

[1.x.171] 

[1.x.172][1.x.173] 


The code as presented here does not actually compute the results found on the web page. The reason is, that even on a decent computer it runs more than a day. If you want to reproduce these results, modify the end time of the DiscreteTime object to `250` within the constructor of TwoPhaseFlowProblem. 

If we run the program, we get the following kind of output: 

[1.x.174] 

As we can see, the time step is pretty much constant right from the start, which indicates that the velocities in the domain are not strongly dependent on changes in saturation, although they certainly are through the factor  [2.x.194]  in the pressure equation. 

Our second observation is that the number of CG iterations needed to solve the pressure Schur complement equation drops from 22 to 17 between the first and the second time step (in fact, it remains around 17 for the rest of the computations). The reason is actually simple: Before we solve for the pressure during a time step, we don't reset the  [2.x.195]  variable to zero. The pressure (and the other variables) therefore have the previous time step's values at the time we get into the CG solver. Since the velocities and pressures don't change very much as computations progress, the previous time step's pressure is actually a good initial guess for this time step's pressure. Consequently, the number of iterations we need once we have computed the pressure once is significantly reduced. 

The final observation concerns the number of iterations needed to solve for the saturation, i.e. one. This shouldn't surprise us too much: the matrix we have to solve with is the mass matrix. However, this is the mass matrix for the  [2.x.196]  element of piecewise constants where no element couples with the degrees of freedom on neighboring cells. The matrix is therefore a diagonal one, and it is clear that we should be able to invert this matrix in a single CG iteration. 


With all this, here are a few movies that show how the saturation progresses over time. First, this is for the single crack model, as implemented in the  [2.x.197]  class: 

 [2.x.198]  

As can be seen, the water rich fluid snakes its way mostly along the high-permeability zone in the middle of the domain, whereas the rest of the domain is mostly impermeable. This and the next movie are generated using  [2.x.199] , leading to a  [2.x.200]  mesh with some 16,000 cells and about 66,000 unknowns in total. 


The second movie shows the saturation for the random medium model of class  [2.x.201] , where we have randomly distributed centers of high permeability and fluid hops from one of these zones to the next: 

 [2.x.202]  


Finally, here is the same situation in three space dimensions, on a mesh with  [2.x.203] , which produces a mesh of some 32,000 cells and 167,000 degrees of freedom: 

 [2.x.204]  

To repeat these computations, all you have to do is to change the line 

[1.x.175] 

in the main function to 

[1.x.176] 

The visualization uses a cloud technique, where the saturation is indicated by colored but transparent clouds for each cell. This way, one can also see somewhat what happens deep inside the domain. A different way of visualizing would have been to show isosurfaces of the saturation evolving over time. There are techniques to plot isosurfaces transparently, so that one can see several of them at the same time like the layers of an onion. 

So why don't we show such isosurfaces? The problem lies in the way isosurfaces are computed: they require that the field to be visualized is continuous, so that the isosurfaces can be generated by following contours at least across a single cell. However, our saturation field is piecewise constant and discontinuous. If we wanted to plot an isosurface for a saturation  [2.x.205] , chances would be that there is no single point in the domain where that saturation is actually attained. If we had to define isosurfaces in that context at all, we would have to take the interfaces between cells, where one of the two adjacent cells has a saturation greater than and the other cell a saturation less than 0.5. However, it appears that most visualization programs are not equipped to do this kind of transformation. 


[1.x.177] [1.x.178][1.x.179] 


There are a number of areas where this program can be improved. Three of them are listed below. All of them are, in fact, addressed in a tutorial program that forms the continuation of the current one:  [2.x.206] . 


[1.x.180][1.x.181] 


At present, the program is not particularly fast: the 2d random medium computation took about a day for the 1,000 or so time steps. The corresponding 3d computation took almost two days for 800 time steps. The reason why it isn't faster than this is twofold. First, we rebuild the entire matrix in every time step, although some parts such as the  [2.x.207] ,  [2.x.208] , and  [2.x.209]  blocks never change. 

Second, we could do a lot better with the solver and preconditioners. Presently, we solve the Schur complement  [2.x.210]  with a CG method, using  [2.x.211]  as a preconditioner. Applying this preconditioner is expensive, since it involves solving a linear system each time. This may have been appropriate for  [2.x.212]  " [2.x.213] ", where we have to solve the entire problem only once. However, here we have to solve it hundreds of times, and in such cases it is worth considering a preconditioner that is more expensive to set up the first time, but cheaper to apply later on. 

One possibility would be to realize that the matrix we use as preconditioner,  [2.x.214]  is still sparse, and symmetric on top of that. If one looks at the flow field evolve over time, we also see that while  [2.x.215]  changes significantly over time, the pressure hardly does and consequently  [2.x.216] . In other words, the matrix for the first time step should be a good preconditioner also for all later time steps.  With a bit of back-and-forthing, it isn't hard to actually get a representation of it as a SparseMatrix object. We could then hand it off to the SparseMIC class to form a sparse incomplete Cholesky decomposition. To form this decomposition is expensive, but we have to do it only once in the first time step, and can then use it as a cheap preconditioner in the future. We could do better even by using the SparseDirectUMFPACK class that produces not only an incomplete, but a complete decomposition of the matrix, which should yield an even better preconditioner. 

Finally, why use the approximation  [2.x.217]  to precondition  [2.x.218] ? The latter matrix, after all, is the mixed form of the Laplace operator on the pressure space, for which we use linear elements. We could therefore build a separate matrix  [2.x.219]  on the side that directly corresponds to the non-mixed formulation of the Laplacian, for example using the bilinear form  [2.x.220] . We could then form an incomplete or complete decomposition of this non-mixed matrix and use it as a preconditioner of the mixed form. 

Using such techniques, it can reasonably be expected that the solution process will be faster by at least an order of magnitude. 


[1.x.182][1.x.183] 


In the introduction we have identified the time step restriction [1.x.184] that has to hold globally, i.e. for all  [2.x.221] . After discretization, we satisfy it by choosing [1.x.185] 

This restriction on the time step is somewhat annoying: the finer we make the mesh the smaller the time step; in other words, we get punished twice: each time step is more expensive to solve and we have to do more time steps. 

This is particularly annoying since the majority of the additional work is spent solving the implicit part of the equations, i.e. the pressure-velocity system, whereas it is the hyperbolic transport equation for the saturation that imposes the time step restriction. 

To avoid this bottleneck, people have invented a number of approaches. For example, they may only re-compute the pressure-velocity field every few time steps (or, if you want, use different time step sizes for the pressure/velocity and saturation equations). This keeps the time step restriction on the cheap explicit part while it makes the solution of the implicit part less frequent. Experiments in this direction are certainly worthwhile; one starting point for such an approach is the paper by Zhangxin Chen, Guanren Huan and Baoyan Li: [1.x.186], Transport in Porous Media, 54 (2004), pp. 361&mdash;376. There are certainly many other papers on this topic as well, but this one happened to land on our desk a while back. 




[1.x.187][1.x.188] 


Adaptivity would also clearly help. Looking at the movies, one clearly sees that most of the action is confined to a relatively small part of the domain (this particularly obvious for the saturation, but also holds for the velocities and pressures). Adaptivity can therefore be expected to keep the necessary number of degrees of freedom low, or alternatively increase the accuracy. 

On the other hand, adaptivity for time dependent problems is not a trivial thing: we would have to change the mesh every few time steps, and we would have to transport our present solution to the next mesh every time we change it (something that the SolutionTransfer class can help with). These are not insurmountable obstacles, but they do require some additional coding and more than we felt comfortable was worth packing into this tutorial program. [1.x.189] [1.x.190]  [2.x.222]  

 [2.x.223] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44] 

 [2.x.4]  

[1.x.45] 




[1.x.46] [1.x.47][1.x.48] 


This program deals with the Stokes system of equations which reads as follows in non-dimensionalized form: [1.x.49] 

where  [2.x.5]  denotes the velocity of a fluid,  [2.x.6]  is its pressure,  [2.x.7]  are external forces, and  [2.x.8]   is the rank-2 tensor of symmetrized gradients; a component-wise definition of it is  [2.x.9] . 

The Stokes equations describe the steady-state motion of a slow-moving, viscous fluid such as honey, rocks in the earth mantle, or other cases where inertia does not play a significant role. If a fluid is moving fast enough that inertia forces are significant compared to viscous friction, the Stokes equations are no longer valid; taking into account inertia effects then leads to the nonlinear Navier-Stokes equations. However, in this tutorial program, we will focus on the simpler Stokes system. 

Note that when deriving the more general compressible Navier-Stokes equations, the diffusion is modeled as the divergence of the stress tensor [1.x.50] 

where  [2.x.10]  is the viscosity of the fluid. With the assumption of  [2.x.11]  (assume constant viscosity and non-dimensionalize the equation by dividing out  [2.x.12] ) and assuming incompressibility ( [2.x.13] ), we arrive at the formulation from above: [1.x.51] 

A different formulation uses the Laplace operator ( [2.x.14] ) instead of the symmetrized gradient. A big difference here is that the different components of the velocity do not couple. If you assume additional regularity of the solution  [2.x.15]  (second partial derivatives exist and are continuous), the formulations are equivalent: [1.x.52] 

This is because the  [2.x.16] th entry of   [2.x.17]  is given by: [1.x.53] 

If you can not assume the above mentioned regularity, or if your viscosity is not a constant, the equivalence no longer holds. Therefore, we decided to stick with the more physically accurate symmetric tensor formulation in this tutorial. 


To be well-posed, we will have to add boundary conditions to the equations. What boundary conditions are readily possible here will become clear once we discuss the weak form of the equations. 

The equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.18]  module. 


[1.x.54][1.x.55] 


The weak form of the equations is obtained by writing it in vector form as [1.x.56] 

forming the dot product from the left with a vector-valued test function  [2.x.19]  and integrating over the domain  [2.x.20] , yielding the following set of equations: [1.x.57] 

which has to hold for all test functions  [2.x.21] . 

A generally good rule of thumb is that if one [1.x.58] reduce how many derivatives are taken on any variable in the formulation, then one [1.x.59] in fact do that using integration by parts. (This is motivated by the theory of [1.x.60], and in particular the difference between strong and [1.x.61].) We have already done that for the Laplace equation, where we have integrated the second derivative by parts to obtain the weak formulation that has only one derivative on both test and trial function. 

In the current context, we integrate by parts the second term: [1.x.62] 

Likewise, we integrate by parts the first term to obtain [1.x.63] 

where the scalar product between two tensor-valued quantities is here defined as [1.x.64] 

Using this, we have now reduced the requirements on our variables to first derivatives for  [2.x.22]  and no derivatives at all for  [2.x.23] . 

Because the scalar product between a general tensor like  [2.x.24]  and a symmetric tensor like  [2.x.25]  equals the scalar product between the symmetrized forms of the two, we can also write the bilinear form above as follows: [1.x.65] 

We will deal with the boundary terms in the next section, but it is already clear from the domain terms [1.x.66] 

of the bilinear form that the Stokes equations yield a symmetric bilinear form, and consequently a symmetric (if indefinite) system matrix. 


[1.x.67][1.x.68] 


 [2.x.26]  ( [2.x.27]  

The weak form just derived immediately presents us with different possibilities for imposing boundary conditions:  [2.x.28]   [2.x.29] Dirichlet velocity boundary conditions: On a part      [2.x.30]  we may impose Dirichlet conditions     on the velocity  [2.x.31] : 

    [1.x.69] 

    Because test functions  [2.x.32]  come from the tangent space of     the solution variable, we have that  [2.x.33]  on  [2.x.34]      and consequently that     [1.x.70] 

    In other words, as usual, strongly imposed boundary values do not     appear in the weak form. 

    It is noteworthy that if we impose Dirichlet boundary values on the entire     boundary, then the pressure is only determined up to a constant. An     algorithmic realization of that would use similar tools as have been seen in      [2.x.35] . 

 [2.x.36] Neumann-type or natural boundary conditions: On the rest of the boundary      [2.x.37] , let us re-write the     boundary terms as follows:     [1.x.71] 

    In other words, on the Neumann part of the boundary we can     prescribe values for the total stress:     [1.x.72] 

    If the boundary is subdivided into Dirichlet and Neumann parts      [2.x.38] , this then leads to the following weak form:     [1.x.73] 




 [2.x.39] Robin-type boundary conditions: Robin boundary conditions are a mixture of     Dirichlet and Neumann boundary conditions. They would read     [1.x.74] 

    with a rank-2 tensor (matrix)  [2.x.40] . The associated weak form is     [1.x.75] 



 [2.x.41] Partial boundary conditions: It is possible to combine Dirichlet and     Neumann boundary conditions by only enforcing each of them for certain     components of the velocity. For example, one way to impose artificial     boundary conditions is to require that the flow is perpendicular to the     boundary, i.e. the tangential component  [2.x.42]  be zero, thereby constraining      [2.x.43] -1 components of the velocity. The remaining component can     be constrained by requiring that the normal component of the normal     stress be zero, yielding the following set of boundary conditions:     [1.x.76] 



    An alternative to this is when one wants the flow to be [1.x.77]     rather than perpendicular to the boundary (in deal.II, the      [2.x.44]  function can do this for     you). This is frequently the case for problems with a free boundary     (e.g. at the surface of a river or lake if vertical forces of the flow are     not large enough to actually deform the surface), or if no significant     friction is exerted by the boundary on the fluid (e.g. at the interface     between earth mantle and earth core where two fluids meet that are     stratified by different densities but that both have small enough     viscosities to not introduce much tangential stress on each other).     In formulas, this means that     [1.x.78] 

    the first condition (which needs to be imposed strongly) fixing a single     component of the velocity, with the second (which would be enforced in the     weak form) fixing the remaining two components.  [2.x.45]  

Despite this wealth of possibilities, we will only use Dirichlet and (homogeneous) Neumann boundary conditions in this tutorial program. 


[1.x.79][1.x.80] 


As developed above, the weak form of the equations with Dirichlet and Neumann boundary conditions on  [2.x.46]  and  [2.x.47]  reads like this: find  [2.x.48]  so that [1.x.81] 

for all test functions  [2.x.49] . 

These equations represent a symmetric [1.x.82]. It is well known that then a solution only exists if the function spaces in which we search for a solution have to satisfy certain conditions, typically referred to as the Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions. The continuous function spaces above satisfy these. However, when we discretize the equations by replacing the continuous variables and test functions by finite element functions in finite dimensional spaces  [2.x.50] , we have to make sure that  [2.x.51]  also satisfy the LBB conditions. This is similar to what we had to do in  [2.x.52] . 

For the Stokes equations, there are a number of possible choices to ensure that the finite element spaces are compatible with the LBB condition. A simple and accurate choice that we will use here is  [2.x.53] , i.e. use elements one order higher for the velocities than for the pressures. 

This then leads to the following discrete problem: find  [2.x.54]  so that [1.x.83] 

for all test functions  [2.x.55] . Assembling the linear system associated with this problem follows the same lines used in  [2.x.56]  " [2.x.57] ",  [2.x.58] , and explained in detail in the  [2.x.59]  module. 




[1.x.84][1.x.85] 


The weak form of the discrete equations naturally leads to the following linear system for the nodal values of the velocity and pressure fields: [1.x.86] 

Like in  [2.x.60]  and  [2.x.61] , we will solve this system of equations by forming the Schur complement, i.e. we will first find the solution  [2.x.62]  of [1.x.87] 

and then [1.x.88] 

The way we do this is pretty much exactly like we did in these previous tutorial programs, i.e. we use the same classes  [2.x.63]  and  [2.x.64]  again. There are two significant differences, however: 

 [2.x.65]   [2.x.66]  First, in the mixed Laplace equation we had to deal with the question of how to precondition the Schur complement  [2.x.67] , which was spectrally equivalent to the Laplace operator on the pressure space (because  [2.x.68]  represents the gradient operator,  [2.x.69]  its adjoint  [2.x.70] , and  [2.x.71]  the identity (up to the material parameter  [2.x.72] ), so  [2.x.73]  is something like  [2.x.74] ). Consequently, the matrix is badly conditioned for small mesh sizes and we had to come up with an elaborate preconditioning scheme for the Schur complement. 

 [2.x.75]  Second, every time we multiplied with  [2.x.76]  we had to solve with the mass matrix  [2.x.77] . This wasn't particularly difficult, however, since the mass matrix is always well conditioned and so simple to invert using CG and a little bit of preconditioning.  [2.x.78]  In other words, preconditioning the inner solver for  [2.x.79]  was simple whereas preconditioning the outer solver for  [2.x.80]  was complicated. 

Here, the situation is pretty much exactly the opposite. The difference stems from the fact that the matrix at the heart of the Schur complement does not stem from the identity operator but from a variant of the Laplace operator,  [2.x.81]  (where  [2.x.82]  is the symmetric gradient) acting on a vector field. In the investigation of this issue we largely follow the paper D. Silvester and A. Wathen: "Fast iterative solution of stabilised Stokes systems part II. Using general block preconditioners." (SIAM J. Numer. Anal., 31 (1994), pp. 1352-1367), which is available online [1.x.89]. Principally, the difference in the matrix at the heart of the Schur complement has two consequences: 

 [2.x.83]   [2.x.84]  First, it makes the outer preconditioner simple: the Schur complement corresponds to the operator  [2.x.85]  on the pressure space; forgetting about the fact that we deal with symmetric gradients instead of the regular one, the Schur complement is something like  [2.x.86] , which, even if not mathematically entirely concise, is spectrally equivalent to the identity operator (a heuristic argument would be to commute the operators into  [2.x.87] ). It turns out that it isn't easy to solve this Schur complement in a straightforward way with the CG method: using no preconditioner, the condition number of the Schur complement matrix depends on the size ratios of the largest to the smallest cells, and one still needs on the order of 50-100 CG iterations. However, there is a simple cure: precondition with the mass matrix on the pressure space and we get down to a number between 5-15 CG iterations, pretty much independently of the structure of the mesh (take a look at the [1.x.90] of this program to see that indeed the number of CG iterations does not change as we refine the mesh). 

So all we need in addition to what we already have is the mass matrix on the pressure variables and we will store it in a separate object. 




 [2.x.88]  While the outer preconditioner has become simpler compared to the mixed Laplace case discussed in  [2.x.89] , the issue of the inner solver has become more complicated. In the mixed Laplace discretization, the Schur complement has the form  [2.x.90] . Thus, every time we multiplied with the Schur complement, we had to solve a linear system  [2.x.91] ; this isn't too complicated there, however, since the mass matrix  [2.x.92]  on the pressure space is well-conditioned. 


On the other hand, for the Stokes equation we consider here, the Schur complement is  [2.x.93]  where the matrix  [2.x.94]  is related to the Laplace operator (it is, in fact, the matrix corresponding to the bilinear form  [2.x.95] ). Thus, solving with  [2.x.96]  is a lot more complicated: the matrix is badly conditioned and we know that we need many iterations unless we have a very good preconditioner. What is worse, we have to solve with  [2.x.97]  every time we multiply with the Schur complement, which is 5-15 times using the preconditioner described above. 

Because we have to solve with  [2.x.98]  several times, it pays off to spend a bit more time once to create a good preconditioner for this matrix. So here's what we're going to do: if in 2d, we use the ultimate preconditioner, namely a direct sparse LU decomposition of the matrix. This is implemented using the SparseDirectUMFPACK class that uses the UMFPACK direct solver to compute the decomposition. To use it, you will have to build deal.II with UMFPACK support (which is the default); see the [1.x.91] for instructions. With this, the inner solver converges in one iteration. 

In 2d, we can do this sort of thing because even reasonably large problems rarely have more than a few 100,000 unknowns with relatively few nonzero entries per row. Furthermore, the bandwidth of matrices in 2d is  [2.x.99]  and therefore moderate. For such matrices, sparse factors can be computed in a matter of a few seconds. (As a point of reference, computing the sparse factors of a matrix of size  [2.x.100]  and bandwidth  [2.x.101]  takes  [2.x.102]  operations. In 2d, this is  [2.x.103] ; though this is a higher complexity than, for example, assembling the linear system which takes  [2.x.104] , the constant for computing the decomposition is so small that it doesn't become the dominating factor in the entire program until we get to very large %numbers of unknowns in the high 100,000s or more.) 

The situation changes in 3d, because there we quickly have many more unknowns and the bandwidth of matrices (which determines the number of nonzero entries in sparse LU factors) is  [2.x.105] , and there are many more entries per row as well. This makes using a sparse direct solver such as UMFPACK inefficient: only for problem sizes of a few 10,000 to maybe 100,000 unknowns can a sparse decomposition be computed using reasonable time and memory resources. 

What we do in that case is to use an incomplete LU decomposition (ILU) as a preconditioner, rather than actually computing complete LU factors. As it so happens, deal.II has a class that does this: SparseILU. Computing the ILU takes a time that only depends on the number of nonzero entries in the sparse matrix (or that we are willing to fill in the LU factors, if these should be more than the ones in the matrix), but is independent of the bandwidth of the matrix. It is therefore an operation that can efficiently also be computed in 3d. On the other hand, an incomplete LU decomposition, by definition, does not represent an exact inverse of the matrix  [2.x.106] . Consequently, preconditioning with the ILU will still require more than one iteration, unlike preconditioning with the sparse direct solver. The inner solver will therefore take more time when multiplying with the Schur complement: an unavoidable trade-off.  [2.x.107]  

In the program below, we will make use of the fact that the SparseILU and SparseDirectUMFPACK classes have a very similar interface and can be used interchangeably. All that we need is a switch class that, depending on the dimension, provides a type that is either of the two classes mentioned above. This is how we do that: 

[1.x.92] 



From here on, we can refer to the type <code>typename  [2.x.108]  and automatically get the correct preconditioner class. Because of the similarity of the interfaces of the two classes, we will be able to use them interchangeably using the same syntax in all places. 


[1.x.93][1.x.94] 


The discussions above showed *one* way in which the linear system that results from the Stokes equations can be solved, and because the tutorial programs are teaching tools that makes sense. But is this the way this system of equations *should* be solved? 

The answer to this is no. The primary bottleneck with the approach, already identified above, is that we have to repeatedly solve linear systems with  [2.x.109]  inside the Schur complement, and because we don't have a good preconditioner for the Schur complement, these solves just have to happen too often. A better approach is to use a block decomposition, which is based on an observation of Silvester and Wathen  [2.x.110]  and explained in much greater detail in  [2.x.111]  . An implementation of this alternative approach is discussed below, in the section on a [1.x.95] in the results section of this program. 


[1.x.96][1.x.97] 


Above, we have claimed that the linear system has the form [1.x.98] 

i.e., in particular that there is a zero block at the bottom right of the matrix. This then allowed us to write the Schur complement as  [2.x.112] . But this is not quite correct. 

Think of what would happen if there are constraints on some pressure variables (see the  [2.x.113]  "Constraints on degrees of freedom" documentation module), for example because we use adaptively refined meshes and continuous pressure finite elements so that there are hanging nodes. Another cause for such constraints are Dirichlet boundary conditions on the pressure. Then the AffineConstraints class, upon copying the local contributions to the matrix into the global linear system will zero out rows and columns corresponding to constrained degrees of freedom and put a positive entry on the diagonal. (You can think of this entry as being one for simplicity, though in reality it is a value of the same order of magnitude as the other matrix entries.) In other words, the bottom right block is really not empty at all: It has a few entries on the diagonal, one for each constrained pressure degree of freedom, and a correct description of the linear system we have to solve is that it has the form [1.x.99] 

where  [2.x.114]  is the zero matrix with the exception of the positive diagonal entries for the constrained degrees of freedom. The correct Schur complement would then in fact be the matrix  [2.x.115]  instead of the one stated above. 

Thinking about this makes us, first, realize that the resulting Schur complement is now indefinite because  [2.x.116]  is symmetric and positive definite whereas  [2.x.117]  is a positive semidefinite, and subtracting the latter from the former may no longer be positive definite. This is annoying because we could no longer employ the Conjugate Gradient method on this true Schur complement. That said, we could fix the issue in  [2.x.118]  by simply putting *negative* values onto the diagonal for the constrained pressure variables -- because we really only put something nonzero to ensure that the resulting matrix is not singular; we really didn't care whether that entry is positive or negative. So if the entries on the diagonal of  [2.x.119]  were negative, then  [2.x.120]  would again be a symmetric and positive definite matrix. 

But, secondly, the code below doesn't actually do any of that: It happily solves the linear system with the wrong Schur complement  [2.x.121]  that just ignores the issue altogether. Why does this even work? To understand why this is so, recall that when writing local contributions into the global matrix,  [2.x.122]  zeros out the rows and columns that correspond to constrained degrees of freedom. This means that  [2.x.123]  has some zero rows, and  [2.x.124]  zero columns. As a consequence, if one were to multiply out what the entries of  [2.x.125]  are, one would realize that it has zero rows and columns for all constrained pressure degrees of freedom, including a zero on the diagonal. The nonzero entries of  [2.x.126]  would fit into exactly those zero diagonal locations, and ensure that  [2.x.127]  is invertible. Not doing so, strictly speaking, means that  [2.x.128]  remains singular: It is symmetric and positive definite on the subset of non-constrained pressure degrees of freedom, and simply the zero matrix on the constrained pressures. Why does the Conjugate Gradient method work for this matrix? Because  [2.x.129]  also makes sure that the right hand side entries that correspond to these zero rows of the matrix are *also* zero, i.e., the right hand side is compatible. 

What this means is that whatever the values of the solution vector for these constrained pressure degrees of freedom, these rows will always have a zero residual and, if one were to consider what the CG algorithm does internally, just never produce any updates to the solution vector. In other words, the CG algorithm just *ignores* these rows, despite the fact that the matrix is singular. This only works because these degrees of freedom are entirely decoupled from the rest of the linear system (because the entire row and corresponding column are zero). At the end of the solution process, the constrained pressure values in the solution vector therefore remain exactly as they were when we started the call to the solver; they are finally overwritten with their correct values when we call  [2.x.130]  after the CG solver is done. 

The upshot of this discussion is that the assumption that the bottom right block of the big matrix is zero is a bit simplified, but that just going with it does not actually lead to any practical problems worth addressing. 


[1.x.100][1.x.101] 


The domain, right hand side and boundary conditions we implement below relate to a problem in geophysics: there, one wants to compute the flow field of magma in the earth's interior under a mid-ocean rift. Rifts are places where two continental plates are very slowly drifting apart (a few centimeters per year at most), leaving a crack in the earth crust that is filled with magma from below. Without trying to be entirely realistic, we model this situation by solving the following set of equations and boundary conditions on the domain  [2.x.131] : [1.x.102] 

and using natural boundary conditions  [2.x.132]  everywhere else. In other words, at the left part of the top surface we prescribe that the fluid moves with the continental plate to the left at speed  [2.x.133] , that it moves to the right on the right part of the top surface, and impose natural flow conditions everywhere else. If we are in 2d, the description is essentially the same, with the exception that we omit the second component of all vectors stated above. 

As will become apparent in the [1.x.103], the flow field will pull material from below and move it to the left and right ends of the domain, as expected. The discontinuity of velocity boundary conditions will produce a singularity in the pressure at the center of the top surface that sucks material all the way to the top surface to fill the gap left by the outward motion of material at this location. 


[1.x.104][1.x.105] 


[1.x.106][1.x.107] 


In all the previous tutorial programs, we used the AffineConstraints object merely for handling hanging node constraints (with exception of  [2.x.134] ). However, the class can also be used to implement Dirichlet boundary conditions, as we will show in this program, by fixing some node values  [2.x.135] . Note that these are inhomogeneous constraints, and we have to pay some special attention to that. The way we are going to implement this is to first read in the boundary values into the AffineConstraints object by using the call 

[1.x.108] 



very similar to how we were making the list of boundary nodes before (note that we set Dirichlet conditions only on boundaries with boundary flag 1). The actual application of the boundary values is then handled by the AffineConstraints object directly, without any additional interference. 

We could then proceed as before, namely by filling the matrix, and then calling a condense function on the constraints object of the form 

[1.x.109] 



Note that we call this on the system matrix and system right hand side simultaneously, since resolving inhomogeneous constraints requires knowledge about both the matrix entries and the right hand side. For efficiency reasons, though, we choose another strategy: all the constraints collected in the AffineConstraints object can be resolved on the fly while writing local data into the global matrix, by using the call 

[1.x.110] 



This technique is further discussed in the  [2.x.136]  tutorial program. All we need to know here is that this functions does three things at once: it writes the local data into the global matrix and right hand side, it distributes the hanging node constraints and additionally implements (inhomogeneous) Dirichlet boundary conditions. That's nice, isn't it? 

We can conclude that the AffineConstraints class provides an alternative to using  [2.x.137]  for implementing Dirichlet boundary conditions. 


[1.x.111][1.x.112][1.x.113] 

Frequently, a sparse matrix contains a substantial amount of elements that actually are zero when we are about to start a linear solve. Such elements are introduced when we eliminate constraints or implement Dirichlet conditions, where we usually delete all entries in constrained rows and columns, i.e., we set them to zero. The fraction of elements that are present in the sparsity pattern, but do not really contain any information, can be up to one fourth of the total number of elements in the matrix for the 3D application considered in this tutorial program. Remember that matrix-vector products or preconditioners operate on all the elements of a sparse matrix (even those that are zero), which is an inefficiency we will avoid here. 

An advantage of directly resolving constrained degrees of freedom is that we can avoid having most of the entries that are going to be zero in our sparse matrix &mdash; we do not need constrained entries during matrix construction (as opposed to the traditional algorithms, which first fill the matrix, and only resolve constraints afterwards). This will save both memory and time when forming matrix-vector products. The way we are going to do that is to pass the information about constraints to the function that generates the sparsity pattern, and then set a <tt>false</tt> argument specifying that we do not intend to use constrained entries: 

[1.x.114] 

This functions obviates, by the way, also the call to the <tt>condense()</tt> function on the sparsity pattern. 


[1.x.115][1.x.116] 


The program developed below has seen a lot of TLC. We have run it over and over under profiling tools (mainly [1.x.117]'s cachegrind and callgrind tools, as well as the KDE [1.x.118] program for visualization) to see where the bottlenecks are. This has paid off: through this effort, the program has become about four times as fast when considering the runtime of the refinement cycles zero through three, reducing the overall number of CPU instructions executed from 869,574,060,348 to 199,853,005,625. For higher refinement levels, the gain is probably even larger since some algorithms that are not  [2.x.138]  have been eliminated. 

Essentially, there are currently two algorithms in the program that do not scale linearly with the number of degrees of freedom: renumbering of degrees of freedom (which is  [2.x.139] , and the linear solver (which is  [2.x.140] ). As for the first, while reordering degrees of freedom may not scale linearly, it is an indispensable part of the overall algorithm as it greatly improves the quality of the sparse ILU, easily making up for the time spent on computing the renumbering; graphs and timings to demonstrate this are shown in the documentation of the DoFRenumbering namespace, also underlining the choice of the Cuthill-McKee reordering algorithm chosen below. 

As for the linear solver: as mentioned above, our implementation here uses a Schur complement formulation. This is not necessarily the very best choice but demonstrates various important techniques available in deal.II. The question of which solver is best is again discussed in the [1.x.119] of this program, along with code showing alternative solvers and a comparison of their results. 

Apart from this, many other algorithms have been tested and improved during the creation of this program. For example, in building the sparsity pattern, we originally used a (now no longer existing) BlockCompressedSparsityPattern object that added one element at a time; however, its data structures were poorly adapted for the large numbers of nonzero entries per row created by our discretization in 3d, leading to a quadratic behavior. Replacing the internal algorithms in deal.II to set many elements at a time, and using a BlockCompressedSimpleSparsityPattern (which has, as of early 2015, been in turn replaced by BlockDynamicSparsityPattern) as a better adapted data structure, removed this bottleneck at the price of a slightly higher memory consumption. Likewise, the implementation of the decomposition step in the SparseILU class was very inefficient and has been replaced by one that is about 10 times faster. Even the vmult function of the SparseILU has been improved to save about twenty percent of time. Small improvements were applied here and there. Moreover, the AffineConstraints object has been used to eliminate a lot of entries in the sparse matrix that are eventually going to be zero, see [1.x.120]. 

A profile of how many CPU instructions are spent at the various different places in the program during refinement cycles zero through three in 3d is shown here: 

 [2.x.141]  

As can be seen, at this refinement level approximately three quarters of the instruction count is spent on the actual solver (the  [2.x.142]  calls on the left, the  [2.x.143]  call in the middle for the Schur complement solve, and another box representing the multiplications with SparseILU and SparseMatrix in the solve for [1.x.121]). About one fifth of the instruction count is spent on matrix assembly and sparse ILU computation (box in the lower right corner) and the rest on other things. Since floating point operations such as in the  [2.x.144]  calls typically take much longer than many of the logical operations and table lookups in matrix assembly, the fraction of the run time taken up by matrix assembly is actually significantly less than the fraction of instructions, as will become apparent in the comparison we make in the results section. 

For higher refinement levels, the boxes representing the solver as well as the blue box at the top right stemming from reordering algorithm are going to grow at the expense of the other parts of the program, since they don't scale linearly. The fact that at this moderate refinement level (3168 cells and 93176 degrees of freedom) the linear solver already makes up about three quarters of the instructions is a good sign that most of the algorithms used in this program are well-tuned and that major improvements in speeding up the program are most likely not to come from hand-optimizing individual aspects but by changing solver algorithms. We will address this point in the discussion of results below as well. 

As a final point, and as a point of reference, the following picture also shows how the profile looked at an early stage of optimizing this program: 

 [2.x.145]  

As mentioned above, the runtime of this version was about four times as long as for the first profile, with the SparseILU decomposition taking up about 30% of the instruction count, and operations an early, inefficient version of DynamicSparsityPattern about 10%. Both these bottlenecks have since been completely removed. [1.x.122] [1.x.123] 


[1.x.124]  [1.x.125] 




As usual, we start by including some well-known files: 

[1.x.126] 



Then we need to include the header file for the sparse direct solver UMFPACK: 

[1.x.127] 



This includes the library for the incomplete LU factorization that will be used as a preconditioner in 3D: 

[1.x.128] 



This is C++: 

[1.x.129] 



As in all programs, the namespace dealii is included: 

[1.x.130] 




[1.x.131]  [1.x.132] 




As explained in the introduction, we are going to use different preconditioners for two and three space dimensions, respectively. We distinguish between them by the use of the spatial dimension as a template parameter. See  [2.x.146]  for details on templates. We are not going to create any preconditioner object here, all we do is to create class that holds a local alias determining the preconditioner class so we can write our program in a dimension-independent way. 

[1.x.133] 



In 2D, we are going to use a sparse direct solver as preconditioner: 

[1.x.134] 



And the ILU preconditioning in 3D, called by SparseILU: 

[1.x.135] 




[1.x.136]  [1.x.137] 




This is an adaptation of  [2.x.147] , so the main class and the data types are nearly the same as used there. The only difference is that we have an additional member  [2.x.148] , that is used for preconditioning the Schur complement, and a corresponding sparsity pattern  [2.x.149] . In addition, instead of relying on LinearOperator, we implement our own InverseMatrix class.    


In this example we also use adaptive grid refinement, which is handled in analogy to  [2.x.150] . According to the discussion in the introduction, we are also going to use the AffineConstraints object for implementing Dirichlet boundary conditions. Hence, we change the name  [2.x.151] . 

[1.x.138] 



This one is new: We shall use a so-called shared pointer structure to access the preconditioner. Shared pointers are essentially just a convenient form of pointers. Several shared pointers can point to the same object (just like regular pointers), but when the last shared pointer object to point to a preconditioner object is deleted (for example if a shared pointer object goes out of scope, if the class of which it is a member is destroyed, or if the pointer is assigned a different preconditioner object) then the preconditioner object pointed to is also destroyed. This ensures that we don't have to manually track in how many places a preconditioner object is still referenced, it can never create a memory leak, and can never produce a dangling pointer to an already destroyed object: 

[1.x.139] 




[1.x.140]  [1.x.141] 




As in  [2.x.152]  and most other example programs, the next task is to define the data for the PDE: For the Stokes problem, we are going to use natural boundary values on parts of the boundary (i.e. homogeneous Neumann-type) for which we won't have to do anything special (the homogeneity implies that the corresponding terms in the weak form are simply zero), and boundary conditions on the velocity (Dirichlet-type) on the rest of the boundary, as described in the introduction.    


In order to enforce the Dirichlet boundary values on the velocity, we will use the  [2.x.153]  function as usual which requires us to write a function object with as many components as the finite element has. In other words, we have to define the function on the  [2.x.154] -space, but we are going to filter out the pressure component when interpolating the boundary values. 




The following function object is a representation of the boundary values described in the introduction: 

[1.x.142] 



We implement similar functions for the right hand side which for the current example is simply zero: 

[1.x.143] 




[1.x.144]  [1.x.145] 




The linear solvers and preconditioners are discussed extensively in the introduction. Here, we create the respective objects that will be used. 





[1.x.146]  [1.x.147] The  [2.x.155]  class represents the data structure for an inverse matrix. Unlike  [2.x.156] , we implement this with a class instead of the helper function inverse_linear_operator() we will apply this class to different kinds of matrices that will require different preconditioners (in  [2.x.157]  we only used a non-identity preconditioner for the mass matrix). The types of matrix and preconditioner are passed to this class via template parameters, and matrix and preconditioner objects of these types will then be passed to the constructor when an  [2.x.158]  object is created. The member function  [2.x.159]  is obtained by solving a linear system: 

[1.x.148] 



This is the implementation of the  [2.x.160]  function. 




In this class we use a rather large tolerance for the solver control. The reason for this is that the function is used very frequently, and hence, any additional effort to make the residual in the CG solve smaller makes the solution more expensive. Note that we do not only use this class as a preconditioner for the Schur complement, but also when forming the inverse of the Laplace matrix &ndash; which is hence directly responsible for the accuracy of the solution itself, so we can't choose a too large tolerance, either. 

[1.x.149] 




[1.x.150]  [1.x.151] 




This class implements the Schur complement discussed in the introduction. It is in analogy to  [2.x.161] .  Though, we now call it with a template parameter  [2.x.162]  in order to access that when specifying the respective type of the inverse matrix class. As a consequence of the definition above, the declaration  [2.x.163]  now contains the second template parameter for a preconditioner class as above, which affects the  [2.x.164]  as well. 

[1.x.152] 




[1.x.153]  [1.x.154] 





[1.x.155]  [1.x.156] 




The constructor of this class looks very similar to the one of  [2.x.165] . The constructor initializes the variables for the polynomial degree, triangulation, finite element system and the dof handler. The underlying polynomial functions are of order  [2.x.166]  for the vector-valued velocity components and of order  [2.x.167]  for the pressure.  This gives the LBB-stable element pair  [2.x.168] , often referred to as the Taylor-Hood element.    


Note that we initialize the triangulation with a MeshSmoothing argument, which ensures that the refinement of cells is done in a way that the approximation of the PDE solution remains well-behaved (problems arise if grids are too unstructured), see the documentation of  [2.x.169]  for details. 

[1.x.157] 




[1.x.158]  [1.x.159] 




Given a mesh, this function associates the degrees of freedom with it and creates the corresponding matrices and vectors. At the beginning it also releases the pointer to the preconditioner object (if the shared pointer pointed at anything at all at this point) since it will definitely not be needed any more after this point and will have to be re-computed after assembling the matrix, and unties the sparse matrices from their sparsity pattern objects.    


We then proceed with distributing degrees of freedom and renumbering them: In order to make the ILU preconditioner (in 3D) work efficiently, it is important to enumerate the degrees of freedom in such a way that it reduces the bandwidth of the matrix, or maybe more importantly: in such a way that the ILU is as close as possible to a real LU decomposition. On the other hand, we need to preserve the block structure of velocity and pressure already seen in  [2.x.170]  and  [2.x.171] . This is done in two steps: First, all dofs are renumbered to improve the ILU and then we renumber once again by components. Since  [2.x.172]  does not touch the renumbering within the individual blocks, the basic renumbering from the first step remains. As for how the renumber degrees of freedom to improve the ILU: deal.II has a number of algorithms that attempt to find orderings to improve ILUs, or reduce the bandwidth of matrices, or optimize some other aspect. The DoFRenumbering namespace shows a comparison of the results we obtain with several of these algorithms based on the testcase discussed here in this tutorial program. Here, we will use the traditional Cuthill-McKee algorithm already used in some of the previous tutorial programs.  In the [1.x.160] we're going to discuss this issue in more detail. 




There is one more change compared to previous tutorial programs: There is no reason in sorting the  [2.x.173]  velocity components individually. In fact, rather than first enumerating all  [2.x.174] -velocities, then all  [2.x.175] -velocities, etc, we would like to keep all velocities at the same location together and only separate between velocities (all components) and pressures. By default, this is not what the  [2.x.176]  function does: it treats each vector component separately; what we have to do is group several components into "blocks" and pass this block structure to that function. Consequently, we allocate a vector  [2.x.177]  with as many elements as there are components and describe all velocity components to correspond to block 0, while the pressure component will form block 1: 

[1.x.161] 



Now comes the implementation of Dirichlet boundary conditions, which should be evident after the discussion in the introduction. All that changed is that the function already appears in the setup functions, whereas we were used to see it in some assembly routine. Further down below where we set up the mesh, we will associate the top boundary where we impose Dirichlet boundary conditions with boundary indicator 1.  We will have to pass this boundary indicator as second argument to the function below interpolating boundary values.  There is one more thing, though.  The function describing the Dirichlet conditions was defined for all components, both velocity and pressure. However, the Dirichlet conditions are to be set for the velocity only.  To this end, we use a ComponentMask that only selects the velocity components. The component mask is obtained from the finite element by specifying the particular components we want. Since we use adaptively refined grids, the affine constraints object needs to be first filled with hanging node constraints generated from the DoF handler. Note the order of the two functions &mdash; we first compute the hanging node constraints, and then insert the boundary values into the constraints object. This makes sure that we respect H<sup>1</sup> conformity on boundaries with hanging nodes (in three space dimensions), where the hanging node needs to dominate the Dirichlet boundary values. 

[1.x.162] 



In analogy to  [2.x.178] , we count the dofs in the individual components. We could do this in the same way as there, but we want to operate on the block structure we used already for the renumbering: The function  [2.x.179]  does the same as  [2.x.180] , but now grouped as velocity and pressure block via  [2.x.181] . 

[1.x.163] 



The next task is to allocate a sparsity pattern for the system matrix we will create and one for the preconditioner matrix. We could do this in the same way as in  [2.x.182] , i.e. directly build an object of type SparsityPattern through  [2.x.183]  However, there is a major reason not to do so: In 3D, the function  [2.x.184]  yields a conservative but rather large number for the coupling between the individual dofs, so that the memory initially provided for the creation of the sparsity pattern of the matrix is far too much -- so much actually that the initial sparsity pattern won't even fit into the physical memory of most systems already for moderately-sized 3D problems, see also the discussion in  [2.x.185] . Instead, we first build temporary objects that use a different data structure that doesn't require allocating more memory than necessary but isn't suitable for use as a basis of SparseMatrix or BlockSparseMatrix objects; in a second step we then copy these objects into objects of type BlockSparsityPattern. This is entirely analogous to what we already did in  [2.x.186]  and  [2.x.187] . In particular, we make use of the fact that we will never write into the  [2.x.188]  block of the system matrix and that this is the only block to be filled for the preconditioner matrix.      


All this is done inside new scopes, which means that the memory of  [2.x.189]  will be released once the information has been copied to  [2.x.190] . 

[1.x.164] 



Finally, the system matrix, the preconsitioner matrix, the solution and the right hand side vector are created from the block structure similar to the approach in  [2.x.191] : 

[1.x.165] 




[1.x.166]  [1.x.167] 




The assembly process follows the discussion in  [2.x.192]  and in the introduction. We use the well-known abbreviations for the data structures that hold the local matrices, right hand side, and global numbering of the degrees of freedom for the present cell. 

[1.x.168] 



Next, we need two objects that work as extractors for the FEValues object. Their use is explained in detail in the report on  [2.x.193]  : 

[1.x.169] 



As an extension over  [2.x.194]  and  [2.x.195] , we include a few optimizations that make assembly much faster for this particular problem. The improvements are based on the observation that we do a few calculations too many times when we do as in  [2.x.196] : The symmetric gradient actually has  [2.x.197]  different values per quadrature point, but we extract it  [2.x.198]  times from the FEValues object - for both the loop over  [2.x.199]  and the inner loop over  [2.x.200] . In 3d, that means evaluating it  [2.x.201]  instead of  [2.x.202]  times, a not insignificant difference.      


So what we're going to do here is to avoid such repeated calculations by getting a vector of rank-2 tensors (and similarly for the divergence and the basis function value on pressure) at the quadrature point prior to starting the loop over the dofs on the cell. First, we create the respective objects that will hold these values. Then, we start the loop over all cells and the loop over the quadrature points, where we first extract these values. There is one more optimization we implement here: the local matrix (as well as the global one) is going to be symmetric, since all the operations involved are symmetric with respect to  [2.x.203]  and  [2.x.204] . This is implemented by simply running the inner loop not to  [2.x.205] , the index of the outer loop. 

[1.x.170] 



Now finally for the bilinear forms of both the system matrix and the matrix we use for the preconditioner. Recall that the formulas for these two are 

[1.x.171] 

and 

[1.x.172] 

respectively, where  [2.x.206]  and  [2.x.207]  are the velocity and pressure components of the  [2.x.208] th shape function. The various terms above are then easily recognized in the following implementation: 

[1.x.173] 



Note that in the implementation of (1) above, `operator*` is overloaded for symmetric tensors, yielding the scalar product between the two tensors.                  


For the right-hand side we use the fact that the shape functions are only non-zero in one component (because our elements are primitive).  Instead of multiplying the tensor representing the dim+1 values of shape function i with the whole right-hand side vector, we only look at the only non-zero component. The function  [2.x.209]  will return which component this shape function lives in (0=x velocity, 1=y velocity, 2=pressure in 2d), which we use to pick out the correct component of the right-hand side vector to multiply with. 

[1.x.174] 



Before we can write the local data into the global matrix (and simultaneously use the AffineConstraints object to apply Dirichlet boundary conditions and eliminate hanging node constraints, as we discussed in the introduction), we have to be careful about one thing, though. We have only built half of the local matrices because of symmetry, but we're going to save the full matrices in order to use the standard functions for solving. This is done by flipping the indices in case we are pointing into the empty part of the local matrices. 

[1.x.175] 



Before we're going to solve this linear system, we generate a preconditioner for the velocity-velocity matrix, i.e.,  [2.x.210]  in the system matrix. As mentioned above, this depends on the spatial dimension. Since the two classes described by the  [2.x.211]  alias have the same interface, we do not have to do anything different whether we want to use a sparse direct solver or an ILU: 

[1.x.176] 




[1.x.177]  [1.x.178] 




After the discussion in the introduction and the definition of the respective classes above, the implementation of the  [2.x.212]  function is rather straight-forward and done in a similar way as in  [2.x.213] . To start with, we need an object of the  [2.x.214]  class that represents the inverse of the matrix A. As described in the introduction, the inverse is generated with the help of an inner preconditioner of type  [2.x.215] . 

[1.x.179] 



This is as in  [2.x.216] . We generate the right hand side  [2.x.217]  for the Schur complement and an object that represents the respective linear operation  [2.x.218] , now with a template parameter indicating the preconditioner - in accordance with the definition of the class. 

[1.x.180] 



The usual control structures for the solver call are created... 

[1.x.181] 



Now to the preconditioner to the Schur complement. As explained in the introduction, the preconditioning is done by a mass matrix in the pressure variable.        


Actually, the solver needs to have the preconditioner in the form  [2.x.219] , so we need to create an inverse operation. Once again, we use an object of the class  [2.x.220] , which implements the  [2.x.221]  operation that is needed by the solver.  In this case, we have to invert the pressure mass matrix. As it already turned out in earlier tutorial programs, the inversion of a mass matrix is a rather cheap and straight-forward operation (compared to, e.g., a Laplace matrix). The CG method with ILU preconditioning converges in 5-10 steps, independently on the mesh size.  This is precisely what we do here: We choose another ILU preconditioner and take it along to the InverseMatrix object via the corresponding template parameter.  A CG solver is then called within the vmult operation of the inverse matrix.        


An alternative that is cheaper to build, but needs more iterations afterwards, would be to choose a SSOR preconditioner with factor 1.2. It needs about twice the number of iterations, but the costs for its generation are almost negligible. 

[1.x.182] 



With the Schur complement and an efficient preconditioner at hand, we can solve the respective equation for the pressure (i.e. block 0 in the solution vector) in the usual way: 

[1.x.183] 



After this first solution step, the hanging node constraints have to be distributed to the solution in order to achieve a consistent pressure field. 

[1.x.184] 



As in  [2.x.222] , we finally need to solve for the velocity equation where we plug in the solution to the pressure equation. This involves only objects we already know - so we simply multiply  [2.x.223]  by  [2.x.224] , subtract the right hand side and multiply by the inverse of  [2.x.225] . At the end, we need to distribute the constraints from hanging nodes in order to obtain a consistent flow field: 

[1.x.185] 




[1.x.186]  [1.x.187] 




The next function generates graphical output. In this example, we are going to use the VTK file format.  We attach names to the individual variables in the problem:  [2.x.226]  components of velocity and  [2.x.227]  to the pressure.    


Not all visualization programs have the ability to group individual vector components into a vector to provide vector plots; in particular, this holds for some VTK-based visualization programs. In this case, the logical grouping of components into vectors should already be described in the file containing the data. In other words, what we need to do is provide our output writers with a way to know which of the components of the finite element logically form a vector (with  [2.x.228]  components in  [2.x.229]  space dimensions) rather than letting them assume that we simply have a bunch of scalar fields.  This is achieved using the members of the  [2.x.230]  namespace: as with the filename, we create a vector in which the first  [2.x.231]  components refer to the velocities and are given the tag  [2.x.232]  we finally push one tag  [2.x.233]  to describe the grouping of the pressure variable. 




The rest of the function is then the same as in  [2.x.234] . 

[1.x.188] 




[1.x.189]  [1.x.190] 




This is the last interesting function of the  [2.x.235]  class.  As indicated by its name, it takes the solution to the problem and refines the mesh where this is needed. The procedure is the same as in the respective step in  [2.x.236] , with the exception that we base the refinement only on the change in pressure, i.e., we call the Kelly error estimator with a mask object of type ComponentMask that selects the single scalar component for the pressure that we are interested in (we get such a mask from the finite element class by specifying the component we want). Additionally, we do not coarsen the grid again: 

[1.x.191] 




[1.x.192]  [1.x.193] 




The last step in the Stokes class is, as usual, the function that generates the initial grid and calls the other functions in the respective order.    


We start off with a rectangle of size  [2.x.237]  (in 2d) or  [2.x.238]  (in 3d), placed in  [2.x.239]  as  [2.x.240]  or  [2.x.241] , respectively. It is natural to start with equal mesh size in each direction, so we subdivide the initial rectangle four times in the first coordinate direction. To limit the scope of the variables involved in the creation of the mesh to the range where we actually need them, we put the entire block between a pair of braces: 

[1.x.194] 



A boundary indicator of 1 is set to all boundaries that are subject to Dirichlet boundary conditions, i.e.  to faces that are located at 0 in the last coordinate direction. See the example description above for details. 

[1.x.195] 



We then apply an initial refinement before solving for the first time. In 3D, there are going to be more degrees of freedom, so we refine less there: 

[1.x.196] 



As first seen in  [2.x.242] , we cycle over the different refinement levels and refine (except for the first cycle), setup the degrees of freedom and matrices, assemble, solve and create output: 

[1.x.197] 




[1.x.198]  [1.x.199] 




The main function is the same as in  [2.x.243] . We pass the element degree as a parameter and choose the space dimension at the well-known template slot. 

[1.x.200] 

[1.x.201] [1.x.202][1.x.203] 


[1.x.204][1.x.205] 


[1.x.206][1.x.207] 


Running the program with the space dimension set to 2 in the  [2.x.244]  function yields the following output (in "release mode",  [2.x.245]  

[1.x.208] 



The entire computation above takes about 2 seconds on a reasonably quick (for 2015 standards) machine. 

What we see immediately from this is that the number of (outer) iterations does not increase as we refine the mesh. This confirms the statement in the introduction that preconditioning the Schur complement with the mass matrix indeed yields a matrix spectrally equivalent to the identity matrix (i.e. with eigenvalues bounded above and below independently of the mesh size or the relative sizes of cells). In other words, the mass matrix and the Schur complement are spectrally equivalent. 

In the images below, we show the grids for the first six refinement steps in the program.  Observe how the grid is refined in regions where the solution rapidly changes: On the upper boundary, we have Dirichlet boundary conditions that are -1 in the left half of the line and 1 in the right one, so there is an abrupt change at  [2.x.246] . Likewise, there are changes from Dirichlet to Neumann data in the two upper corners, so there is need for refinement there as well: 

 [2.x.247]  

Finally, following is a plot of the flow field. It shows fluid transported along with the moving upper boundary and being replaced by material coming from below: 

 [2.x.248]  

This plot uses the capability of VTK-based visualization programs (in this case of VisIt) to show vector data; this is the result of us declaring the velocity components of the finite element in use to be a set of vector components, rather than independent scalar components in the  [2.x.249]  function of this tutorial program. 




[1.x.209][1.x.210] 


In 3d, the screen output of the program looks like this: 

[1.x.211] 



Again, we see that the number of outer iterations does not increase as we refine the mesh. Nevertheless, the compute time increases significantly: for each of the iterations above separately, it takes about 0.14 seconds, 0.63 seconds, 4.8 seconds, 35 seconds, 2 minutes and 33 seconds, and 13 minutes and 12 seconds. This overall superlinear (in the number of unknowns) increase in runtime is due to the fact that our inner solver is not  [2.x.250] : a simple experiment shows that as we keep refining the mesh, the average number of ILU-preconditioned CG iterations to invert the velocity-velocity block  [2.x.251]  increases. 

We will address the question of how possibly to improve our solver [1.x.212]. 

As for the graphical output, the grids generated during the solution look as follow: 

 [2.x.252]  

Again, they show essentially the location of singularities introduced by boundary conditions. The vector field computed makes for an interesting graph: 

 [2.x.253]  

The isocontours shown here as well are those of the pressure variable, showing the singularity at the point of discontinuous velocity boundary conditions. 




[1.x.213][1.x.214] 


As explained during the generation of the sparsity pattern, it is important to have the numbering of degrees of freedom in mind when using preconditioners like incomplete LU decompositions. This is most conveniently visualized using the distribution of nonzero elements in the stiffness matrix. 

If we don't do anything special to renumber degrees of freedom (i.e., without using  [2.x.254]  but with using  [2.x.255]  to ensure that degrees of freedom are appropriately sorted into their corresponding blocks of the matrix and vector), then we get the following image after the first adaptive refinement in two dimensions: 

 [2.x.256]  

In order to generate such a graph, you have to insert a piece of code like the following to the end of the setup step. 

[1.x.215] 



It is clearly visible that the nonzero entries are spread over almost the whole matrix.  This makes preconditioning by ILU inefficient: ILU generates a Gaussian elimination (LU decomposition) without fill-in elements, which means that more tentative fill-ins left out will result in a worse approximation of the complete decomposition. 

In this program, we have thus chosen a more advanced renumbering of components.  The renumbering with  [2.x.257]  and grouping the components into velocity and pressure yields the following output: 

 [2.x.258]  

It is apparent that the situation has improved a lot. Most of the elements are now concentrated around the diagonal in the (0,0) block in the matrix. Similar effects are also visible for the other blocks. In this case, the ILU decomposition will be much closer to the full LU decomposition, which improves the quality of the preconditioner. (It may be interesting to note that the sparse direct solver UMFPACK does some %internal renumbering of the equations before actually generating a sparse LU decomposition; that procedure leads to a very similar pattern to the one we got from the Cuthill-McKee algorithm.) 

Finally, we want to have a closer look at a sparsity pattern in 3D. We show only the (0,0) block of the matrix, again after one adaptive refinement. Apart from the fact that the matrix size has increased, it is also visible that there are many more entries in the matrix. Moreover, even for the optimized renumbering, there will be a considerable amount of tentative fill-in elements. This illustrates why UMFPACK is not a good choice in 3D - a full decomposition needs many new entries that  eventually won't fit into the physical memory (RAM): 

 [2.x.259]  




[1.x.216][1.x.217] 


[1.x.218][1.x.219][1.x.220] 

We have seen in the section of computational results that the number of outer iterations does not depend on the mesh size, which is optimal in a sense of scalability. This does, however, not apply to the solver as a whole, as mentioned above: We did not look at the number of inner iterations when generating the inverse of the matrix  [2.x.260]  and the mass matrix  [2.x.261] . Of course, this is unproblematic in the 2D case where we precondition  [2.x.262]  with a direct solver and the  [2.x.263]  operation of the inverse matrix structure will converge in one single CG step, but this changes in 3D where we only use an ILU preconditioner.  There, the number of required preconditioned CG steps to invert  [2.x.264]  increases as the mesh is refined, and each  [2.x.265]  operation involves on average approximately 14, 23, 36, 59, 75 and 101 inner CG iterations in the refinement steps shown above. (On the other hand, the number of iterations for applying the inverse pressure mass matrix is always around five, both in two and three dimensions.)  To summarize, most work is spent on solving linear systems with the same matrix  [2.x.266]  over and over again. What makes this look even worse is the fact that we actually invert a matrix that is about 95 percent the size of the total system matrix and stands for 85 percent of the non-zero entries in the sparsity pattern. Hence, the natural question is whether it is reasonable to solve a linear system with matrix  [2.x.267]  for about 15 times when calculating the solution to the block system. 

The answer is, of course, that we can do that in a few other (most of the time better) ways. Nevertheless, it has to be remarked that an indefinite system as the one at hand puts indeed much higher demands on the linear algebra than standard elliptic problems as we have seen in the early tutorial programs. The improvements are still rather unsatisfactory, if one compares with an elliptic problem of similar size. Either way, we will introduce below a number of improvements to the linear solver, a discussion that we will re-consider again with additional options in the  [2.x.268]  program. 

[1.x.221][1.x.222][1.x.223] A first attempt to improve the speed of the linear solution process is to choose a dof reordering that makes the ILU being closer to a full LU decomposition, as already mentioned in the in-code comments. The DoFRenumbering namespace compares several choices for the renumbering of dofs for the Stokes equations. The best result regarding the computing time was found for the King ordering, which is accessed through the call  [2.x.269]  With that program, the inner solver needs considerably less operations, e.g. about 62 inner CG iterations for the inversion of  [2.x.270]  at cycle 4 compared to about 75 iterations with the standard Cuthill-McKee-algorithm. Also, the computing time at cycle 4 decreased from about 17 to 11 minutes for the  [2.x.271]  call. However, the King ordering (and the orderings provided by the  [2.x.272]  namespace in general) has a serious drawback - it uses much more memory than the in-build deal versions, since it acts on abstract graphs rather than the geometry provided by the triangulation. In the present case, the renumbering takes about 5 times as much memory, which yields an infeasible algorithm for the last cycle in 3D with 1.2 million unknowns. 

[1.x.224][1.x.225] 

Another idea to improve the situation even more would be to choose a preconditioner that makes CG for the (0,0) matrix  [2.x.273]  converge in a mesh-independent number of iterations, say 10 to 30. We have seen such a candidate in  [2.x.274] : multigrid. 

[1.x.226][1.x.227] 

[1.x.228] Even with a good preconditioner for  [2.x.275] , we still need to solve of the same linear system repeatedly (with different right hand sides, though) in order to make the Schur complement solve converge. The approach we are going to discuss here is how inner iteration and outer iteration can be combined. If we persist in calculating the Schur complement, there is no other possibility. 

The alternative is to attack the block system at once and use an approximate Schur complement as efficient preconditioner. The idea is as follows: If we find a block preconditioner  [2.x.276]  such that the matrix [1.x.229] 

is simple, then an iterative solver with that preconditioner will converge in a few iterations. Using the Schur complement  [2.x.277] , one finds that [1.x.230] 

would appear to be a good choice since [1.x.231] 

This is the approach taken by the paper by Silvester and Wathen referenced to in the introduction (with the exception that Silvester and Wathen use right preconditioning). In this case, a Krylov-based iterative method would converge in one step only if exact inverses of  [2.x.278]  and  [2.x.279]  were applied, since all the eigenvalues are one (and the number of iterations in such a method is bounded by the number of distinct eigenvalues). Below, we will discuss the choice of an adequate solver for this problem. First, we are going to have a closer look at the implementation of the preconditioner. 

Since  [2.x.280]  is aimed to be a preconditioner only, we shall use approximations to the inverse of the Schur complement  [2.x.281]  and the matrix  [2.x.282] . Hence, the Schur complement will be approximated by the pressure mass matrix  [2.x.283] , and we use a preconditioner to  [2.x.284]  (without an InverseMatrix class around it) for approximating  [2.x.285] . 

Here comes the class that implements the block Schur complement preconditioner. The  [2.x.286]  operation for block vectors according to the derivation above can be specified by three successive operations: 

[1.x.232] 



Since we act on the whole block system now, we have to live with one disadvantage: we need to perform the solver iterations on the full block system instead of the smaller pressure space. 

Now we turn to the question which solver we should use for the block system. The first observation is that the resulting preconditioned matrix cannot be solved with CG since it is neither positive definite nor symmetric. 

The deal.II libraries implement several solvers that are appropriate for the problem at hand. One choice is the solver  [2.x.287]  "BiCGStab", which was used for the solution of the unsymmetric advection problem in  [2.x.288] . The second option, the one we are going to choose, is  [2.x.289]  "GMRES" (generalized minimum residual). Both methods have their pros and cons - there are problems where one of the two candidates clearly outperforms the other, and vice versa. [1.x.233]'s article on the GMRES method gives a comparative presentation. A more comprehensive and well-founded comparison can be read e.g. in the book by J.W. Demmel (Applied Numerical Linear Algebra, SIAM, 1997, section 6.6.6). 

For our specific problem with the ILU preconditioner for  [2.x.290] , we certainly need to perform hundreds of iterations on the block system for large problem sizes (we won't beat CG!). Actually, this disfavors GMRES: During the GMRES iterations, a basis of Krylov vectors is successively built up and some operations are performed on these vectors. The more vectors are in this basis, the more operations and memory will be needed. The number of operations scales as  [2.x.291]  and memory as  [2.x.292] , where  [2.x.293]  is the number of vectors in the Krylov basis and  [2.x.294]  the size of the (block) matrix. To not let these demands grow excessively, deal.II limits the size  [2.x.295]  of the basis to 30 vectors by default. Then, the basis is rebuilt. This implementation of the GMRES method is called GMRES(k), with default  [2.x.296] . What we have gained by this restriction, namely a bound on operations and memory requirements, will be compensated by the fact that we use an incomplete basis - this will increase the number of required iterations. 

BiCGStab, on the other hand, won't get slower when many iterations are needed (one iteration uses only results from one preceding step and not all the steps as GMRES). Besides the fact the BiCGStab is more expensive per step since two matrix-vector products are needed (compared to one for CG or GMRES), there is one main reason which makes BiCGStab not appropriate for this problem: The preconditioner applies the inverse of the pressure mass matrix by using the InverseMatrix class. Since the application of the inverse matrix to a vector is done only in approximative way (an exact inverse is too expensive), this will also affect the solver. In the case of BiCGStab, the Krylov vectors will not be orthogonal due to that perturbation. While this is uncritical for a small number of steps (up to about 50), it ruins the performance of the solver when these perturbations have grown to a significant magnitude in the coarse of iterations. 

We did some experiments with BiCGStab and found it to be faster than GMRES up to refinement cycle 3 (in 3D), but it became very slow for cycles 4 and 5 (even slower than the original Schur complement), so the solver is useless in this situation. Choosing a sharper tolerance for the inverse matrix class ( [2.x.297]  instead of  [2.x.298] ) made BiCGStab perform well also for cycle 4, but did not change the failure on the very large problems. 

GMRES is of course also effected by the approximate inverses, but it is not as sensitive to orthogonality and retains a relatively good performance also for large sizes, see the results below. 

With this said, we turn to the realization of the solver call with GMRES with  [2.x.299]  temporary vectors: 

[1.x.234] 



Obviously, one needs to add the include file  [2.x.300]  "<lac/solver_gmres.h>" in order to make this run. We call the solver with a BlockVector template in order to enable GMRES to operate on block vectors and matrices. Note also that we need to set the (1,1) block in the system matrix to zero (we saved the pressure mass matrix there which is not part of the problem) after we copied the information to another matrix. 

Using the Timer class, we collect some statistics that compare the runtime of the block solver with the one from the problem implementation above. Besides the solution with the two options we also check if the solutions of the two variants are close to each other (i.e. this solver gives indeed the same solution as we had before) and calculate the infinity norm of the vector difference. 

Let's first see the results in 2D: 

[1.x.235] 



We see that there is no huge difference in the solution time between the block Schur complement preconditioner solver and the Schur complement itself. The reason is simple: we used a direct solve as preconditioner for  [2.x.301]  - so we cannot expect any gain by avoiding the inner iterations. We see that the number of iterations has slightly increased for GMRES, but all in all the two choices are fairly similar. 

The picture of course changes in 3D: 

[1.x.236] 



Here, the block preconditioned solver is clearly superior to the Schur complement, but the advantage gets less for more mesh points. This is because GMRES(k) scales worse with the problem size than CG, as we discussed above.  Nonetheless, the improvement by a factor of 3-6 for moderate problem sizes is quite impressive. 


[1.x.237][1.x.238] 

An ultimate linear solver for this problem could be imagined as a combination of an optimal preconditioner for  [2.x.302]  (e.g. multigrid) and the block preconditioner described above, which is the approach taken in the  [2.x.303]  and  [2.x.304]  tutorial programs (where we use an algebraic multigrid method) and  [2.x.305]  (where we use a geometric multigrid method). 


[1.x.239][1.x.240] 

Another possibility that can be taken into account is to not set up a block system, but rather solve the system of velocity and pressure all at once. The options are direct solve with UMFPACK (2D) or GMRES with ILU preconditioning (3D). It should be straightforward to try that. 




[1.x.241][1.x.242] 


The program can of course also serve as a basis to compute the flow in more interesting cases. The original motivation to write this program was for it to be a starting point for some geophysical flow problems, such as the movement of magma under places where continental plates drift apart (for example mid-ocean ridges). Of course, in such places, the geometry is more complicated than the examples shown above, but it is not hard to accommodate for that. 

For example, by using the following modification of the boundary values function 

[1.x.243] 

and the following way to generate the mesh as the domain  [2.x.306]  

[1.x.244] 

then we get images where the fault line is curved:  [2.x.307]  [1.x.245] [1.x.246]  [2.x.308]  

 [2.x.309] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22] 

[1.x.23] [1.x.24][1.x.25] 


 [2.x.3]  

This is the first of a number of tutorial programs that will finally cover "real" time-dependent problems, not the slightly odd form of time dependence found in  [2.x.4]  or the DAE model of  [2.x.5] . In particular, this program introduces the wave equation in a bounded domain. Later,  [2.x.6]  will consider an example of absorbing boundary conditions, and  [2.x.7]  " [2.x.8] " a kind of nonlinear wave equation producing solutions called solitons. 

The wave equation in its prototypical form reads as follows: find  [2.x.9]  that satisfies [1.x.26] 

Note that since this is an equation with second-order time derivatives, we need to pose two initial conditions, one for the value and one for the time derivative of the solution. 

Physically, the equation describes the motion of an elastic medium. In 2-d, one can think of how a membrane moves if subjected to a force. The Dirichlet boundary conditions above indicate that the membrane is clamped at the boundary at a height  [2.x.10]  (this height might be moving as well &mdash; think of people holding a blanket and shaking it up and down). The first initial condition equals the initial deflection of the membrane, whereas the second one gives its velocity. For example, one could think of pushing the membrane down with a finger and then letting it go at  [2.x.11]  (nonzero deflection but zero initial velocity), or hitting it with a hammer at  [2.x.12]  (zero deflection but nonzero velocity). Both cases would induce motion in the membrane. 


[1.x.27][1.x.28] 


[1.x.29][1.x.30] 

There is a long-standing debate in the numerical analysis community over whether a discretization of time dependent equations should involve first discretizing the time variable leading to a stationary PDE at each time step that is then solved using standard finite element techniques (this is called the Rothe method), or whether one should first discretize the spatial variables, leading to a large system of ordinary differential equations that can then be handled by one of the usual ODE solvers (this is called the method of lines). 

Both of these methods have advantages and disadvantages. Traditionally, people have preferred the method of lines, since it allows to use the very well developed machinery of high-order ODE solvers available for the rather stiff ODEs resulting from this approach, including step length control and estimation of the temporal error. 

On the other hand, Rothe's method becomes awkward when using higher-order time stepping method, since one then has to write down a PDE that couples the solution of the present time step not only with that at the previous time step, but possibly also even earlier solutions, leading to a significant number of terms. 

For these reasons, the method of lines was the method of choice for a long time. However, it has one big drawback: if we discretize the spatial variable first, leading to a large ODE system, we have to choose a mesh once and for all. If we are willing to do this, then this is a legitimate and probably superior approach. 

If, on the other hand, we are looking at the wave equation and many other time dependent problems, we find that the character of a solution changes as time progresses. For example, for the wave equation, we may have a single wave travelling through the domain, where the solution is smooth or even constant in front of and behind the wave &mdash; adaptivity would be really useful for such cases, but the key is that the area where we need to refine the mesh changes from time step to time step! 

If we intend to go that way, i.e. choose a different mesh for each time step (or set of time steps), then the method of lines is not appropriate any more: instead of getting one ODE system with a number of variables equal to the number of unknowns in the finite element mesh, our number of unknowns now changes all the time, a fact that standard ODE solvers are certainly not prepared to deal with at all. On the other hand, for the Rothe method, we just get a PDE for each time step that we may choose to discretize independently of the mesh used for the previous time step; this approach is not without perils and difficulties, but at least is a sensible and well-defined procedure. 

For all these reasons, for the present program, we choose to use the Rothe method for discretization, i.e. we first discretize in time and then in space. We will not actually use adaptive meshes at all, since this involves a large amount of additional code, but we will comment on this some more in the [1.x.31]. 


[1.x.32][1.x.33] 


Given these considerations, here is how we will proceed: let us first define a simple time stepping method for this second order problem, and then in a second step do the spatial discretization, i.e. we will follow Rothe's approach. 

For the first step, let us take a little detour first: in order to discretize a second time derivative, we can either discretize it directly, or we can introduce an additional variable and transform the system into a first order system. In many cases, this turns out to be equivalent, but dealing with first order systems is often simpler. To this end, let us introduce [1.x.34] and call this variable the [1.x.35] for obvious reasons. We can then reformulate the original wave equation as follows: [1.x.36] 

The advantage of this formulation is that it now only contains first time derivatives for both variables, for which it is simple to write down time stepping schemes. Note that we do not have boundary conditions for  [2.x.13]  at first. However, we could enforce  [2.x.14]  on the boundary. It turns out in numerical examples that this is actually necessary: without doing so the solution doesn't look particularly wrong, but the Crank-Nicolson scheme does not conserve energy if one doesn't enforce these boundary conditions. 

With this formulation, let us introduce the following time discretization where a superscript  [2.x.15]  indicates the number of a time step and  [2.x.16]  is the length of the present time step: [1.x.37] Note how we introduced a parameter  [2.x.17]  here. If we chose  [2.x.18] , for example, the first equation would reduce to  [2.x.19] , which is well-known as the forward or explicit Euler method. On the other hand, if we set  [2.x.20] , then we would get  [2.x.21] , which corresponds to the backward or implicit Euler method. Both these methods are first order accurate methods. They are simple to implement, but they are not really very accurate. 

The third case would be to choose  [2.x.22] . The first of the equations above would then read  [2.x.23] . This method is known as the Crank-Nicolson method and has the advantage that it is second order accurate. In addition, it has the nice property that it preserves the energy in the solution (physically, the energy is the sum of the kinetic energy of the particles in the membrane plus the potential energy present due to the fact that it is locally stretched; this quantity is a conserved one in the continuous equation, but most time stepping schemes do not conserve it after time discretization). Since  [2.x.24]  also appears in the equation for  [2.x.25] , the Crank-Nicolson scheme is also implicit. 

In the program, we will leave  [2.x.26]  as a parameter, so that it will be easy to play with it. The results section will show some numerical evidence comparing the different schemes. 

The equations above (called the [1.x.38] equations because we have only discretized the time, but not space), can be simplified a bit by eliminating  [2.x.27]  from the first equation and rearranging terms. We then get [1.x.39] In this form, we see that if we are given the solution  [2.x.28]  of the previous timestep, that we can then solve for the variables  [2.x.29]  separately, i.e. one at a time. This is convenient. In addition, we recognize that the operator in the first equation is positive definite, and the second equation looks particularly simple. 


[1.x.40][1.x.41] 


We have now derived equations that relate the approximate (semi-discrete) solution  [2.x.30]  and its time derivative  [2.x.31]  at time  [2.x.32]  with the solutions  [2.x.33]  of the previous time step at  [2.x.34] . The next step is to also discretize the spatial variable using the usual finite element methodology. To this end, we multiply each equation with a test function, integrate over the entire domain, and integrate by parts where necessary. This leads to [1.x.42] 

It is then customary to approximate  [2.x.35] , where  [2.x.36]  are the shape functions used for the discretization of the  [2.x.37] -th time step and  [2.x.38]  are the unknown nodal values of the solution. Similarly,  [2.x.39] . Finally, we have the solutions of the previous time step,  [2.x.40]  and  [2.x.41] . Note that since the solution of the previous time step has already been computed by the time we get to time step  [2.x.42] ,  [2.x.43]  are known. Furthermore, note that the solutions of the previous step may have been computed on a different mesh, so we have to use shape functions  [2.x.44] . 

If we plug these expansions into above equations and test with the test functions from the present mesh, we get the following linear system: [1.x.43] where [1.x.44] 



If we solve these two equations, we can move the solution one step forward and go on to the next time step. 

It is worth noting that if we choose the same mesh on each time step (as we will in fact do in the program below), then we have the same shape functions on time step  [2.x.45]  and  [2.x.46] , i.e.  [2.x.47] . Consequently, we get  [2.x.48]  and  [2.x.49] . On the other hand, if we had used different shape functions, then we would have to compute integrals that contain shape functions defined on two meshes. This is a somewhat messy process that we omit here, but that is treated in some detail in  [2.x.50] . 

Under these conditions (i.e. a mesh that doesn't change), one can optimize the solution procedure a bit by basically eliminating the solution of the second linear system. We will discuss this in the introduction of the  [2.x.51]  " [2.x.52] " program. 

[1.x.45][1.x.46] 


One way to compare the quality of a time stepping scheme is to see whether the numerical approximation preserves conservation properties of the continuous equation. For the wave equation, the natural quantity to look at is the energy. By multiplying the wave equation by  [2.x.53] , integrating over  [2.x.54] , and integrating by parts where necessary, we find that [1.x.47] By consequence, in absence of body forces and constant boundary values, we get that [1.x.48] is a conserved quantity, i.e. one that doesn't change with time. We will compute this quantity after each time step. It is straightforward to see that if we replace  [2.x.55]  by its finite element approximation, and  [2.x.56]  by the finite element approximation of the velocity  [2.x.57] , then [1.x.49] As we will see in the results section, the Crank-Nicolson scheme does indeed conserve the energy, whereas neither the forward nor the backward Euler scheme do. 


[1.x.50][1.x.51] 


One of the reasons why the wave equation is nasty to solve numerically is that explicit time discretizations are only stable if the time step is small enough. In particular, it is coupled to the spatial mesh width  [2.x.58] . For the lowest order discretization we use here, the relationship reads [1.x.52] where  [2.x.59]  is the wave speed, which in our formulation of the wave equation has been normalized to one. Consequently, unless we use the implicit schemes with  [2.x.60] , our solutions will not be numerically stable if we violate this restriction. Implicit schemes do not have this restriction for stability, but they become inaccurate if the time step is too large. 

This condition was first recognized by Courant, Friedrichs, and Lewy &mdash; in 1928, long before computers became available for numerical computations! (This result appeared in the German language article R. Courant, K. Friedrichs and H. Lewy: [1.x.53], Mathematische Annalen, vol. 100, no. 1, pages 32-74, 1928.) This condition on the time step is most frequently just referred to as the [1.x.54] condition. Intuitively, the CFL condition says that the time step must not be larger than the time it takes a wave to cross a single cell. 

In the program, we will refine the square  [2.x.61]  seven times uniformly, giving a mesh size of  [2.x.62] , which is what we set the time step to. The fact that we set the time step and mesh size individually in two different places is error prone: it is too easy to refine the mesh once more but forget to also adjust the time step.  [2.x.63]  " [2.x.64] " shows a better way how to keep these things in sync. 


[1.x.55][1.x.56] 


Although the program has all the hooks to deal with nonzero initial and boundary conditions and body forces, we take a simple case where the domain is a square  [2.x.65]  and [1.x.57] 

This corresponds to a membrane initially at rest and clamped all around, where someone is waving a part of the clamped boundary once up and down, thereby shooting a wave into the domain. [1.x.58] [1.x.59] 


[1.x.60]  [1.x.61] 




We start with the usual assortment of include files that we've seen in so many of the previous tests: 

[1.x.62] 



Here are the only three include files of some new interest: The first one is already used, for example, for the  [2.x.66]  and  [2.x.67]  functions. However, we here use another function in that class,  [2.x.68]  to compute our initial values as the  [2.x.69]  projection of the continuous initial values. Furthermore, we use  [2.x.70]  to generate the integrals  [2.x.71] . These were previously always generated by hand in  [2.x.72]  or similar functions in application code. However, we're too lazy to do that here, so simply use a library function: 

[1.x.63] 



In a very similar vein, we are also too lazy to write the code to assemble mass and Laplace matrices, although it would have only taken copying the relevant code from any number of previous tutorial programs. Rather, we want to focus on the things that are truly new to this program and therefore use the  [2.x.73]  and  [2.x.74]  functions. They are declared here: 

[1.x.64] 



Finally, here is an include file that contains all sorts of tool functions that one sometimes needs. In particular, we need the  [2.x.75]  class that, given an integer argument, returns a string representation of it. It is particularly useful since it allows for a second parameter indicating the number of digits to which we want the result padded with leading zeros. We will use this to write output files that have the form  [2.x.76]  denotes the number of the time step and always consists of three digits even if we are still in the single or double digit time steps. 

[1.x.65] 



The last step is as in all previous programs: 

[1.x.66] 




[1.x.67]  [1.x.68] 




Next comes the declaration of the main class. It's public interface of functions is like in most of the other tutorial programs. Worth mentioning is that we now have to store four matrices instead of one: the mass matrix  [2.x.77] , the Laplace matrix  [2.x.78] , the matrix  [2.x.79]  used for solving for  [2.x.80] , and a copy of the mass matrix with boundary conditions applied used for solving for  [2.x.81] . Note that it is a bit wasteful to have an additional copy of the mass matrix around. We will discuss strategies for how to avoid this in the section on possible improvements.    


Likewise, we need solution vectors for  [2.x.82]  as well as for the corresponding vectors at the previous time step,  [2.x.83] . The  [2.x.84]  will be used for whatever right hand side vector we have when solving one of the two linear systems in each time step. These will be solved in the two functions  [2.x.85]  and  [2.x.86] .    


Finally, the variable  [2.x.87]  is used to indicate the parameter  [2.x.88]  that is used to define which time stepping scheme to use, as explained in the introduction. The rest is self-explanatory. 

[1.x.69] 




[1.x.70]  [1.x.71] 




Before we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial and boundary values for both the solution  [2.x.89]  and its time derivative  [2.x.90] , as well as a right hand side class. We do so using classes derived from the Function class template that has been used many times before, so the following should not be a surprise.    


Let's start with initial values and choose zero for both the value  [2.x.91]  as well as its time derivative, the velocity  [2.x.92] : 

[1.x.72] 



Secondly, we have the right hand side forcing term. Boring as we are, we choose zero here as well: 

[1.x.73] 



Finally, we have boundary values for  [2.x.93]  and  [2.x.94] . They are as described in the introduction, one being the time derivative of the other: 

[1.x.74] 




[1.x.75]  [1.x.76] 




The implementation of the actual logic is actually fairly short, since we relegate things like assembling the matrices and right hand side vectors to the library. The rest boils down to not much more than 130 lines of actual code, a significant fraction of which is boilerplate code that can be taken from previous example programs (e.g. the functions that solve linear systems, or that generate output).    


Let's start with the constructor (for an explanation of the choice of time step, see the section on Courant, Friedrichs, and Lewy in the introduction): 

[1.x.77] 




[1.x.78]  [1.x.79] 




The next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to  [2.x.95] : 

[1.x.80] 



Then comes a block where we have to initialize the 3 matrices we need in the course of the program: the mass matrix, the Laplace matrix, and the matrix  [2.x.96]  used when solving for  [2.x.97]  in each time step.      


When setting up these matrices, note that they all make use of the same sparsity pattern object. Finally, the reason why matrices and sparsity patterns are separate objects in deal.II (unlike in many other finite element or linear algebra classes) becomes clear: in a significant fraction of applications, one has to hold several matrices that happen to have the same sparsity pattern, and there is no reason for them not to share this information, rather than re-building and wasting memory on it several times.      


After initializing all of these matrices, we call library functions that build the Laplace and mass matrices. All they need is a DoFHandler object and a quadrature formula object that is to be used for numerical integration. Note that in many respects these functions are better than what we would usually do in application programs, for example because they automatically parallelize building the matrices if multiple processors are available in a machine: for more information see the documentation of WorkStream or the  [2.x.98]  "Parallel computing with multiple processors" module. The matrices for solving linear systems will be filled in the run() method because we need to re-apply boundary conditions every time step. 

[1.x.81] 



The rest of the function is spent on setting vector sizes to the correct value. The final line closes the hanging node constraints object. Since we work on a uniformly refined mesh, no constraints exist or have been computed (i.e. there was no need to call  [2.x.99]  as in other programs), but we need a constraints object in one place further down below anyway. 

[1.x.82] 




[1.x.83]  [1.x.84] 




The next two functions deal with solving the linear systems associated with the equations for  [2.x.100]  and  [2.x.101] . Both are not particularly interesting as they pretty much follow the scheme used in all the previous tutorial programs.    


One can make little experiments with preconditioners for the two matrices we have to invert. As it turns out, however, for the matrices at hand here, using Jacobi or SSOR preconditioners reduces the number of iterations necessary to solve the linear system slightly, but due to the cost of applying the preconditioner it is no win in terms of run-time. It is not much of a loss either, but let's keep it simple and just do without: 

[1.x.85] 




[1.x.86]  [1.x.87] 




Likewise, the following function is pretty much what we've done before. The only thing worth mentioning is how here we generate a string representation of the time step number padded with leading zeros to 3 character length using the  [2.x.102]  function's second argument. 

[1.x.88] 



Like  [2.x.103] , since we write output at every time step (and the system we have to solve is relatively easy), we instruct DataOut to use the zlib compression algorithm that is optimized for speed instead of disk usage since otherwise plotting the output becomes a bottleneck: 

[1.x.89] 




[1.x.90]  [1.x.91] 




The following is really the only interesting function of the program. It contains the loop over all time steps, but before we get to that we have to set up the grid, DoFHandler, and matrices. In addition, we have to somehow get started with initial values. To this end, we use the  [2.x.104]  function that takes an object that describes a continuous function and computes the  [2.x.105]  projection of this function onto the finite element space described by the DoFHandler object. Can't be any simpler than that: 

[1.x.92] 



The next thing is to loop over all the time steps until we reach the end time ( [2.x.106]  in this case). In each time step, we first have to solve for  [2.x.107] , using the equation  [2.x.108]   [2.x.109]   [2.x.110] . Note that we use the same mesh for all time steps, so that  [2.x.111]  and  [2.x.112] . What we therefore have to do first is to add up  [2.x.113]  and the forcing terms, and put the result into the  [2.x.114]  vector. (For these additions, we need a temporary vector that we declare before the loop to avoid repeated memory allocations in each time step.)      


The one thing to realize here is how we communicate the time variable to the object describing the right hand side: each object derived from the Function class has a time field that can be set using the  [2.x.115]  and read by  [2.x.116]  In essence, using this mechanism, all functions of space and time are therefore considered functions of space evaluated at a particular time. This matches well what we typically need in finite element programs, where we almost always work on a single time step at a time, and where it never happens that, for example, one would like to evaluate a space-time function for all times at any given spatial location. 

[1.x.93] 



After so constructing the right hand side vector of the first equation, all we have to do is apply the correct boundary values. As for the right hand side, this is a space-time function evaluated at a particular time, which we interpolate at boundary nodes and then use the result to apply boundary values as we usually do. The result is then handed off to the solve_u() function: 

[1.x.94] 



The matrix for solve_u() is the same in every time steps, so one could think that it is enough to do this only once at the beginning of the simulation. However, since we need to apply boundary values to the linear system (which eliminate some matrix rows and columns and give contributions to the right hand side), we have to refill the matrix in every time steps before we actually apply boundary data. The actual content is very simple: it is the sum of the mass matrix and a weighted Laplace matrix: 

[1.x.95] 



The second step, i.e. solving for  [2.x.117] , works similarly, except that this time the matrix on the left is the mass matrix (which we copy again in order to be able to apply boundary conditions, and the right hand side is  [2.x.118]  plus forcing terms. Boundary values are applied in the same way as before, except that now we have to use the BoundaryValuesV class: 

[1.x.96] 



Finally, after both solution components have been computed, we output the result, compute the energy in the solution, and go on to the next time step after shifting the present solution into the vectors that hold the solution at the previous time step. Note the function  [2.x.119]  that can compute  [2.x.120]  and  [2.x.121]  in one step, saving us the expense of a temporary vector and several lines of code: 

[1.x.97] 




[1.x.98]  [1.x.99] 




What remains is the main function of the program. There is nothing here that hasn't been shown in several of the previous programs: 

[1.x.100] 

[1.x.101][1.x.102] 


When the program is run, it produces the following output: 

[1.x.103] 



What we see immediately is that the energy is a constant at least after  [2.x.122]  (until which the boundary source term  [2.x.123]  is nonzero, injecting energy into the system). 

In addition to the screen output, the program writes the solution of each time step to an output file. If we process them adequately and paste them into a movie, we get the following: 

 [2.x.124]  

The movie shows the generated wave nice traveling through the domain and back, being reflected at the clamped boundary. Some numerical noise is trailing the wave, an artifact of a too-large mesh size that can be reduced by reducing the mesh width and the time step. 


[1.x.104] [1.x.105][1.x.106] 


If you want to explore a bit, try out some of the following things:  [2.x.125]     [2.x.126] Varying  [2.x.127] . This gives different time stepping schemes, some of   which are stable while others are not. Take a look at how the energy   evolves. 

   [2.x.128] Different initial and boundary conditions, right hand sides. 

   [2.x.129] More complicated domains or more refined meshes. Remember that the time   step needs to be bounded by the mesh width, so changing the mesh should   always involve also changing the time step. We will come back to this issue   in  [2.x.130] . 

   [2.x.131] Variable coefficients: In real media, the wave speed is often   variable. In particular, the "real" wave equation in realistic media would   read   [1.x.107]   where  [2.x.132]  is the density of the material, and  [2.x.133]  is related to the   stiffness coefficient. The wave speed is then  [2.x.134] . 

  To make such a change, we would have to compute the mass and Laplace   matrices with a variable coefficient. Fortunately, this isn't too hard: the   functions  [2.x.135]  and    [2.x.136]  have additional default parameters that can   be used to pass non-constant coefficient functions to them. The required   changes are therefore relatively small. On the other hand, care must be   taken again to make sure the time step is within the allowed range. 

   [2.x.137] In the in-code comments, we discussed the fact that the matrices for   solving for  [2.x.138]  and  [2.x.139]  need to be reset in every time because of   boundary conditions, even though the actual content does not change. It is   possible to avoid copying by not eliminating columns in the linear systems,   which is implemented by appending a  [2.x.140]  argument to the call:   [1.x.108] 



   [2.x.141] deal.II being a library that supports adaptive meshes it would of course be   nice if this program supported change the mesh every few time steps. Given the   structure of the solution &mdash; a wave that travels through the domain &mdash;   it would seem appropriate if we only refined the mesh where the wave currently is,   and not simply everywhere. It is intuitively clear that we should be able to   save a significant amount of cells this way. (Though upon further thought one   realizes that this is really only the case in the initial stages of the simulation.   After some time, for wave phenomena, the domain is filled with reflections of   the initial wave going in every direction and filling every corner of the domain.   At this point, there is in general little one can gain using local mesh   refinement.) 

  To make adaptively changing meshes possible, there are basically two routes.   The "correct" way would be to go back to the weak form we get using Rothe's   method. For example, the first of the two equations to be solved in each time   step looked like this:   [1.x.109]   Now, note that we solve for  [2.x.142]  on mesh  [2.x.143] , and   consequently the test functions  [2.x.144]  have to be from the space    [2.x.145]  as well. As discussed in the introduction, terms like    [2.x.146]  then require us to integrate the solution of the   previous step (which may have been computed on a different mesh    [2.x.147] ) against the test functions of the current mesh,   leading to a matrix  [2.x.148] . This process of integrating shape   functions from different meshes is, at best, awkward. It can be done   but because it is difficult to ensure that  [2.x.149]  and    [2.x.150]  differ by at most one level of refinement, one   has to recursively match cells from both meshes. It is feasible to   do this, but it leads to lengthy and not entirely obvious code. 

  The second approach is the following: whenever we change the mesh,   we simply interpolate the solution from the last time step on the old   mesh to the new mesh, using the SolutionTransfer class. In other words,   instead of the equation above, we would solve   [1.x.110]   where  [2.x.151]  interpolates a given function onto mesh  [2.x.152] .   This is a much simpler approach because, in each time step, we no   longer have to worry whether  [2.x.153]  were computed on the   same mesh as we are using now or on a different mesh. Consequently,   the only changes to the code necessary are the addition of a function   that computes the error, marks cells for refinement, sets up a   SolutionTransfer object, transfers the solution to the new mesh, and   rebuilds matrices and right hand side vectors on the new mesh. Neither   the functions building the matrices and right hand sides, nor the   solvers need to be changed. 

  While this second approach is, strictly speaking,   not quite correct in the Rothe framework (it introduces an addition source   of error, namely the interpolation), it is nevertheless what   almost everyone solving time dependent equations does. We will use this   method in  [2.x.154] , for example.  [2.x.155]  [1.x.111] [1.x.112]  [2.x.156]  

 [2.x.157] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20] 

[1.x.21] [1.x.22][1.x.23] 


This program grew out of a student project by Xing Jin at Texas A&amp;M University. Most of the work for this program is by her. Some of the work on this tutorial program has been funded by NSF under grant DMS-0604778. 

The program is part of a project that aims to simulate thermoacoustic tomography imaging. In thermoacoustic tomography, pulsed electromagnetic energy is delivered into biological issues. Tissues absorb some of this energy and those parts of the tissue that absorb the most energy generate thermoacoustic waves through thermoelastic expansion. For imaging, one uses that different kinds of tissue, most importantly healthy and diseased tissue, absorb different amounts of energy and therefore expand at different rates. The experimental setup is to measure the amplitude of the pressure waves generated by these sources on the surface of the tissue and try to reconstruct the source distributions, which is indicative for the distribution of absorbers and therefore of different kinds of tissue. Part of this project is to compare simulated data with actual measurements, so one has to solve the "forward problem", i.e. the wave equation that describes the propagation of pressure waves in tissue. This program is therefore a continuation of  [2.x.3]  " [2.x.4] ", where the wave equation was first introduced. 


[1.x.24][1.x.25] 


The temperature at a given location, neglecting thermal diffusion, can be stated as 

[1.x.26] 

Here  [2.x.5]  is the density;  [2.x.6]  is the specific heat;  [2.x.7]  is the temperature rise due to the delivered microwave energy; and  [2.x.8]  is the heating function defined as the thermal energy per time and volume transformed from deposited microwave energy. 

Let us assume that tissues have heterogeneous dielectric properties but homogeneous acoustic properties. The basic acoustic generation equation in an acoustically homogeneous medium can be described as follows: if  [2.x.9]  is the vector-valued displacement, then tissue certainly reacts to changes in pressure by acceleration: [1.x.27] Furthermore, it contracts due to excess pressure and expands based on changes in temperature: [1.x.28] Here,  [2.x.10]  is a thermoexpansion coefficient. 

Let us now make the assumption that heating only happens on a time scale much shorter than wave propagation through tissue (i.e. the temporal length of the microwave pulse that heats the tissue is much shorter than the time it takes a wave to cross the domain). In that case, the heating rate  [2.x.11]  can be written as  [2.x.12]  (where  [2.x.13]  is a map of absorption strengths for microwave energy and  [2.x.14]  is the Dirac delta function), which together with the first equation above will yield an instantaneous jump in the temperature  [2.x.15]  at time  [2.x.16] . Using this assumption, and taking all equations together, we can rewrite and combine the above as follows: [1.x.29] where  [2.x.17] . 

This somewhat strange equation with the derivative of a Dirac delta function on the right hand side can be rewritten as an initial value problem as follows: [1.x.30] 

(A derivation of this transformation into an initial value problem is given at the end of this introduction as an appendix.) 

In the inverse problem, it is the initial condition  [2.x.18]  that one would like to recover, since it is a map of absorption strengths for microwave energy, and therefore presumably an indicator to discern healthy from diseased tissue. 

In real application, the thermoacoustic source is very small as compared to the medium.  The propagation path of the thermoacoustic waves can then be approximated as from the source to the infinity. Furthermore, detectors are only a limited distance from the source. One only needs to evaluate the values when the thermoacoustic waves pass through the detectors, although they do continue beyond. This is therefore a problem where we are only interested in a small part of an infinite medium, and we do not want waves generated somewhere to be reflected at the boundary of the domain which we consider interesting. Rather, we would like to simulate only that part of the wave field that is contained inside the domain of interest, and waves that hit the boundary of that domain to simply pass undisturbed through the boundary. In other words, we would like the boundary to absorb any waves that hit it. 

In general, this is a hard problem: Good absorbing boundary conditions are nonlinear and/or numerically very expensive. We therefore opt for a simple first order approximation to absorbing boundary conditions that reads [1.x.31] Here,  [2.x.19]  is the normal derivative at the boundary. It should be noted that this is not a particularly good boundary condition, but it is one of the very few that are reasonably simple to implement. 


[1.x.32][1.x.33] 


As in  [2.x.20] , one first introduces a second variable, which is defined as the derivative of the pressure potential: [1.x.34] 

With the second variable, one then transforms the forward problem into two separate equations: [1.x.35] 

with initial conditions: [1.x.36] 

Note that we have introduced a right hand side  [2.x.21]  here to show how to derive these formulas in the general case, although in the application to the thermoacoustic problem  [2.x.22] . 

The semi-discretized, weak version of this model, using the general  [2.x.23]  scheme introduced in  [2.x.24]  is then: [1.x.37] 

where  [2.x.25]  is an arbitrary test function, and where we have used the absorbing boundary condition to integrate by parts: absorbing boundary conditions are incorporated into the weak form by using [1.x.38] 

From this we obtain the discrete model by introducing a finite number of shape functions, and get [1.x.39] 

The matrices  [2.x.26]  and  [2.x.27]  are here as in  [2.x.28] , and the boundary mass matrix [1.x.40] results from the use of absorbing boundary conditions. 

Above two equations can be rewritten in a matrix form with the pressure and its derivative as an unknown vector: [1.x.41] 

where [1.x.42] 

By simple transformations, one then obtains two equations for the pressure potential and its derivative, just as in the previous tutorial program: [1.x.43] 




[1.x.44][1.x.45] 


Compared to  [2.x.29] , this programs adds the treatment of a simple absorbing boundary conditions. In addition, it deals with data obtained from actual experimental measurements. To this end, we need to evaluate the solution at points at which the experiment also evaluates a real pressure field. We will see how to do that using the  [2.x.30]  function further down below. 




[1.x.46][1.x.47] 


In the derivation of the initial value problem for the wave equation, we initially found that the equation had the derivative of a Dirac delta function as a right hand side: [1.x.48] In order to see how to transform this single equation into the usual statement of a PDE with initial conditions, let us make the assumption that the physically quite reasonable medium is at rest initially, i.e.  [2.x.31]  for  [2.x.32] . Next, let us form the indefinite integral with respect to time of both sides: [1.x.49] This immediately leads to the statement [1.x.50] where  [2.x.33]  is such that  [2.x.34] . Next, we form the (definite) integral over time from  [2.x.35]  to  [2.x.36]  to find [1.x.51] If we use the property of the delta function that  [2.x.37] , and assume that  [2.x.38]  is a continuous function in time, we find as we let  [2.x.39]  go to zero that [1.x.52] In other words, using that  [2.x.40] , we retrieve the initial condition [1.x.53] At the same time, we know that for every  [2.x.41]  the delta function is zero, so for  [2.x.42]  we get the equation [1.x.54] Consequently, we have obtained a representation of the wave equation and one initial condition from the original somewhat strange equation. 

Finally, because we here have an equation with two time derivatives, we still need a second initial condition. To this end, let us go back to the equation [1.x.55] and integrate it in time from  [2.x.43]  to  [2.x.44] . This leads to [1.x.56] Using integration by parts of the form [1.x.57] where we use that  [2.x.45]  and inserting  [2.x.46] , we see that in fact [1.x.58] 

Now, let  [2.x.47] . Assuming that  [2.x.48]  is a continuous function in time, we see that [1.x.59] and consequently [1.x.60] However, we have assumed that  [2.x.49] . Consequently, we obtain as the second initial condition that [1.x.61] completing the system of equations. [1.x.62] [1.x.63] 


[1.x.64]  [1.x.65] 




The following have all been covered previously: 

[1.x.66] 



This is the only new one: We will need a library function defined in the namespace GridTools that computes the minimal cell diameter. 

[1.x.67] 



The last step is as in all previous programs: 

[1.x.68] 




[1.x.69]  [1.x.70] 




The first part of the main class is exactly as in  [2.x.50]  (except for the name): 

[1.x.71] 



Here's what's new: first, we need that boundary mass matrix  [2.x.51]  that came out of the absorbing boundary condition. Likewise, since this time we consider a realistic medium, we must have a measure of the wave speed  [2.x.52]  that will enter all the formulas with the Laplace matrix (which we still define as  [2.x.53] ): 

[1.x.72] 



The last thing we have to take care of is that we wanted to evaluate the solution at a certain number of detector locations. We need an array to hold these locations, declared here and filled in the constructor: 

[1.x.73] 




[1.x.74]  [1.x.75] 




As usual, we have to define our initial values, boundary conditions, and right hand side functions. Things are a bit simpler this time: we consider a problem that is driven by initial conditions, so there is no right hand side function (though you could look up in  [2.x.54]  to see how this can be done). Secondly, there are no boundary conditions: the entire boundary of the domain consists of absorbing boundary conditions. That only leaves initial conditions, and there things are simple too since for this particular application only nonzero initial conditions for the pressure are prescribed, not for the velocity (which is zero at the initial time).    


So this is all we need: a class that specifies initial conditions for the pressure. In the physical setting considered in this program, these are small absorbers, which we model as a series of little circles where we assume that the pressure surplus is one, whereas no absorption and therefore no pressure surplus is everywhere else. This is how we do things (note that if we wanted to expand this program to not only compile but also to run, we would have to initialize the sources with three-dimensional source locations): 

[1.x.76] 




[1.x.77]  [1.x.78] 




Let's start again with the constructor. Setting the member variables is straightforward. We use the acoustic wave speed of mineral oil (in millimeters per microsecond, a common unit in experimental biomedical imaging) since this is where many of the experiments we want to compare the output with are made in. The Crank-Nicolson scheme is used again, i.e. theta is set to 0.5. The time step is later selected to satisfy  [2.x.55] : here we initialize it to an invalid number. 

[1.x.79] 



The second task in the constructor is to initialize the array that holds the detector locations. The results of this program were compared with experiments in which the step size of the detector spacing is 2.25 degree, corresponding to 160 detector locations. The radius of the scanning circle is selected to be half way between the center and the boundary to avoid that the remaining reflections from the imperfect boundary condition spoils our numerical results.      


The locations of the detectors are then calculated in clockwise order. Note that the following of course only works if we are computing in 2d, a condition that we guard with an assertion. If we later wanted to run the same program in 3d, we would have to add code here for the initialization of detector locations in 3d. Due to the assertion, there is no way we can forget to do this. 

[1.x.80] 




[1.x.81]  [1.x.82] 




The following system is pretty much what we've already done in  [2.x.56] , but with two important differences. First, we have to create a circular (or spherical) mesh around the origin, with a radius of 1. This nothing new: we've done so before in  [2.x.57]  and  [2.x.58] , where we also explain how the PolarManifold or SphericalManifold object places new points on concentric circles when a cell is refined, which we will use here as well.    


One thing we had to make sure is that the time step satisfies the CFL condition discussed in the introduction of  [2.x.59] . Back in that program, we ensured this by hand by setting a timestep that matches the mesh width, but that was error prone because if we refined the mesh once more we would also have to make sure the time step is changed. Here, we do that automatically: we ask a library function for the minimal diameter of any cell. Then we set  [2.x.60] . The only problem is: what exactly is  [2.x.61] ? The point is that there is really no good theory on this question for the wave equation. It is known that for uniformly refined meshes consisting of rectangles,  [2.x.62]  is the minimal edge length. But for meshes on general quadrilaterals, the exact relationship appears to be unknown, i.e. it is unknown what properties of cells are relevant for the CFL condition. The problem is that the CFL condition follows from knowledge of the smallest eigenvalue of the Laplace matrix, and that can only be computed analytically for simply structured meshes.    


The upshot of all this is that we're not quite sure what exactly we should take for  [2.x.63] . The function  [2.x.64]  computes the minimal diameter of all cells. If the cells were all squares or cubes, then the minimal edge length would be the minimal diameter divided by  [2.x.65] . We simply generalize this, without theoretical justification, to the case of non-uniform meshes.    


The only other significant change is that we need to build the boundary mass matrix. We will comment on this further down below. 

[1.x.83] 



The second difference, as mentioned, to  [2.x.66]  is that we need to build the boundary mass matrix that grew out of the absorbing boundary conditions.      


A first observation would be that this matrix is much sparser than the regular mass matrix, since none of the shape functions with purely interior support contribute to this matrix. We could therefore optimize the storage pattern to this situation and build up a second sparsity pattern that only contains the nonzero entries that we need. There is a trade-off to make here: first, we would have to have a second sparsity pattern object, so that costs memory. Secondly, the matrix attached to this sparsity pattern is going to be smaller and therefore requires less memory; it would also be faster to perform matrix-vector multiplications with it. The final argument, however, is the one that tips the scale: we are not primarily interested in performing matrix-vector with the boundary matrix alone (though we need to do that for the right hand side vector once per time step), but mostly wish to add it up to the other matrices used in the first of the two equations since this is the one that is going to be multiplied with once per iteration of the CG method, i.e. significantly more often. It is now the case that the  [2.x.67]  class allows to add one matrix to another, but only if they use the same sparsity pattern (the reason being that we can't add nonzero entries to a matrix after the sparsity pattern has been created, so we simply require that the two matrices have the same sparsity pattern).      


So let's go with that: 

[1.x.84] 



The second thing to do is to actually build the matrix. Here, we need to integrate over faces of cells, so first we need a quadrature object that works on  [2.x.68]  dimensional objects. Secondly, the FEFaceValues variant of FEValues that works on faces, as its name suggest. And finally, the other variables that are part of the assembly machinery. All of this we put between curly braces to limit the scope of these variables to where we actually need them.      


The actual act of assembling the matrix is then fairly straightforward: we loop over all cells, over all faces of each of these cells, and then do something only if that particular face is at the boundary of the domain. Like this: 

[1.x.85] 




[1.x.86]  [1.x.87] 




The following two functions, solving the linear systems for the pressure and the velocity variable, are taken pretty much verbatim (with the exception of the change of name from  [2.x.69]  to  [2.x.70]  of the primary variable) from  [2.x.71] : 

[1.x.88] 




[1.x.89]  [1.x.90] 




The same holds here: the function is from  [2.x.72] . 

[1.x.91] 




[1.x.92]  [1.x.93] 




This function that does most of the work is pretty much again like in  [2.x.73] , though we make things a bit clearer by using the vectors G1 and G2 mentioned in the introduction. Compared to the overall memory consumption of the program, the introduction of a few temporary vectors isn't doing much harm.    


The only changes to this function are: first, that we do not have to project initial values for the velocity  [2.x.74] , since we know that it is zero. And second that we evaluate the solution at the detector locations computed in the constructor. This is done using the  [2.x.75]  function. These values are then written to a file that we open at the beginning of the function. 

[1.x.94] 




[1.x.95]  [1.x.96] 




What remains is the main function of the program. There is nothing here that hasn't been shown in several of the previous programs: 

[1.x.97] 

[1.x.98][1.x.99] 


The program writes both graphical data for each time step as well as the values evaluated at each detector location to disk. We then draw them in plots. Experimental data were also collected for comparison. Currently our experiments have only been done in two dimensions by circularly scanning a single detector. The tissue sample here is a thin slice in the  [2.x.76]  plane ( [2.x.77] ), and we assume that signals from other  [2.x.78]  directions won't contribute to the data. Consequently, we only have to compare our experimental data with two dimensional simulated data. 

[1.x.100][1.x.101] 


This movie shows the thermoacoustic waves generated by a single small absorber propagating in the medium (in our simulation, we assume the medium is mineral oil, which has a acoustic speed of 1.437  [2.x.79] ): 

 [2.x.80]  

For a single absorber, we of course have to change the  [2.x.81]  class accordingly. 

Next, let us compare experimental and computational results. The visualization uses a technique long used in seismology, where the data of each detector is plotted all in one graph. The way this is done is by offsetting each detector's signal a bit compared to the previous one. For example, here is a plot of the first four detectors (from bottom to top, with time in microseconds running from left to right) using the source setup used in the program, to make things a bit more interesting compared to the present case of only a single source: 

 [2.x.82]  

One thing that can be seen, for example, is that the arrival of the second and fourth signals shifts to earlier times for greater detector numbers (i.e. the topmost ones), but not the first and the third; this can be interpreted to mean that the origin of these signals must be closer to the latter detectors than to the former ones. 

If we stack not only 4, but all 160 detectors in one graph, the individual lines blur, but where they run together they create a pattern of darker or lighter grayscales.  The following two figures show the results obtained at the detector locations stacked in that way. The left figure is obtained from experiments, and the right is the simulated data. In the experiment, a single small strong absorber was embedded in weaker absorbing tissue: 

 [2.x.83]  

It is obvious that the source location is closer to the detectors at angle  [2.x.84] . All the other signals that can be seen in the experimental data result from the fact that there are weak absorbers also in the rest of the tissue, which surrounds the signals generated by the small strong absorber in the center. On the other hand, in the simulated data, we only simulate the small strong absorber. 

In reality, detectors have limited bandwidth. The thermoacoustic waves passing through the detector will therefore be filtered. By using a high-pass filter (implemented in MATLAB and run against the data file produced by this program), the simulated results can be made to look closer to the experimental data: 

 [2.x.85]  

In our simulations, we see spurious signals behind the main wave that result from numerical artifacts. This problem can be alleviated by using finer mesh, resulting in the following plot: 

 [2.x.86]  




[1.x.102][1.x.103] 


To further verify the program, we will also show simulation results for multiple absorbers. This corresponds to the case that is actually implemented in the program. The following movie shows the propagation of the generated thermoacoustic waves in the medium by multiple absorbers: 

 [2.x.87]  

Experimental data and our simulated data are compared in the following two figures:  [2.x.88]  

Note that in the experimental data, the first signal (i.e. the left-most dark line) results from absorption at the tissue boundary, and therefore reaches the detectors first and before any of the signals from the interior. This signal is also faintly visible at the end of the traces, around 30  [2.x.89] , which indicates that the signal traveled through the entire tissue to reach detectors at the other side, after all the signals originating from the interior have reached them. 

As before, the numerical result better matches experimental ones by applying a bandwidth filter that matches the actual behavior of detectors (left) and by choosing a finer mesh (right): 

 [2.x.90]  

One of the important differences between the left and the right figure is that the curves look much less "angular" at the right. The angularity comes from the fact that while waves in the continuous equation travel equally fast in all directions, this isn't the case after discretization: there, waves that travel diagonal to cells move at slightly different speeds to those that move parallel to mesh lines. This anisotropy leads to wave fronts that aren't perfectly circular (and would produce sinusoidal signals in the stacked plots), but are bulged out in certain directions. To make things worse, the circular mesh we use (see for example  [2.x.91]  for a view of the coarse mesh) is not isotropic either. The net result is that the signal fronts are not sinusoidal unless the mesh is sufficiently fine. The right image is a lot better in this respect, though artifacts in the form of trailing spurious waves can still be seen. [1.x.104] [1.x.105]  [2.x.92]  

 [2.x.93] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26] 

[1.x.27][1.x.28] [1.x.29] 


This program grew out of a student project by Ivan Christov at Texas A&amp;M University. Most of the work for this program is by him. 

The goal of this program is to solve the sine-Gordon soliton equation in 1, 2 or 3 spatial dimensions. The motivation for solving this equation is that very little is known about the nature of the solutions in 2D and 3D, even though the 1D case has been studied extensively. 

Rather facetiously, the sine-Gordon equation's moniker is a pun on the so-called Klein-Gordon equation, which is a relativistic version of the Schrödinger equation for particles with non-zero mass. The resemblance is not just superficial, the sine-Gordon equation has been shown to model some unified-field phenomena such as interaction of subatomic particles (see, e.g., Perring &amp; Skyrme in Nuclear %Physics [1.x.30]) and the Josephson (quantum) effect in superconductor junctions (see, e.g., [1.x.31]). Furthermore, from the mathematical standpoint, since the sine-Gordon equation is "completely integrable," it is a candidate for study using the usual methods such as the inverse scattering transform. Consequently, over the years, many interesting solitary-wave, and even stationary, solutions to the sine-Gordon equation have been found. In these solutions, particles correspond to localized features. For more on the sine-Gordon equation, the inverse scattering transform and other methods for finding analytical soliton equations, the reader should consult the following "classical" references on the subject: G. L. Lamb's [1.x.32] (Chapter 5, Section 2) and G. B. Whitham's [1.x.33] (Chapter 17, Sections 10-13). 

 [2.x.3]  We will cover a separate nonlinear equation from quantum   mechanics, the Nonlinear Schr&ouml;dinger Equation, in  [2.x.4] . 

[1.x.34][1.x.35] 

The sine-Gordon initial-boundary-value problem (IBVP) we wish to solve consists of the following equations: [1.x.36] It is a nonlinear equation similar to the wave equation we discussed in  [2.x.5]  and  [2.x.6] . We have chosen to enforce zero Neumann boundary conditions in order for waves to reflect off the boundaries of our domain. It should be noted, however, that Dirichlet boundary conditions are not appropriate for this problem. Even though the solutions to the sine-Gordon equation are localized, it only makes sense to specify (Dirichlet) boundary conditions at  [2.x.7] , otherwise either a solution does not exist or only the trivial solution  [2.x.8]  exists. 

However, the form of the equation above is not ideal for numerical discretization. If we were to discretize the second-order time derivative directly and accurately, then  we would need a large stencil (i.e., several time steps would need to be kept in the memory), which could become expensive. Therefore, in complete analogy to what we did in  [2.x.9]  and  [2.x.10] , we split the second-order (in time) sine-Gordon equation into a system of two first-order (in time) equations, which we call the split, or velocity, formulation. To this end, by setting  [2.x.11] , it is easy to see that the sine-Gordon equation is equivalent to [1.x.37] 

[1.x.38][1.x.39] 

Now, we can discretize the split formulation in time using the  [2.x.12] -method, which has a stencil of only two time steps. By choosing a  [2.x.13] , the latter discretization allows us to choose from a continuum of schemes. In particular, if we pick  [2.x.14]  or  [2.x.15] , we obtain the first-order accurate explicit or implicit Euler method, respectively. Another important choice is  [2.x.16] , which gives the second-order accurate Crank-Nicolson scheme. Henceforth, a superscript  [2.x.17]  denotes the values of the variables at the  [2.x.18]  time step, i.e. at  [2.x.19] , where  [2.x.20]  is the (fixed) time step size. Thus, the split formulation of the time-discretized sine-Gordon equation becomes [1.x.40] 

We can simplify the latter via a bit of algebra. Eliminating  [2.x.21]  from the first equation and rearranging, we obtain [1.x.41] 

It may seem as though we can just proceed to discretize the equations in space at this point. While this is true for the second equation (which is linear in  [2.x.22] ), this would not work for all  [2.x.23]  since the first equation above is nonlinear. Therefore, a nonlinear solver must be implemented, then the equations can be discretized in space and solved. 

To this end, we can use Newton's method. Given the nonlinear equation  [2.x.24] , we produce successive approximations to  [2.x.25]  as follows: [1.x.42] The iteration can be initialized with the old time step, i.e.  [2.x.26] , and eventually it will produce a solution to the first equation of the split formulation (see above). For the time discretization of the sine-Gordon equation under consideration here, we have that [1.x.43] Notice that while  [2.x.27]  is a function,  [2.x.28]  is an operator. 

[1.x.44][1.x.45] 

With hindsight, we choose both the solution and the test space to be  [2.x.29] . Hence, multiplying by a test function  [2.x.30]  and integrating, we obtain the following variational (or weak) formulation of the split formulation (including the nonlinear solver for the first equation) at each time step: [1.x.46] Note that the we have used integration by parts and the zero Neumann boundary conditions on all terms involving the Laplacian operator. Moreover,  [2.x.31]  and  [2.x.32]  are as defined above, and  [2.x.33]  denotes the usual  [2.x.34]  inner product over the domain  [2.x.35] , i.e.  [2.x.36] . Finally, notice that the first equation is, in fact, the definition of an iterative procedure, so it is solved multiple times during each time step until a stopping criterion is met. 

[1.x.47][1.x.48] 

Using the Finite Element Method, we discretize the variational formulation in space. To this end, let  [2.x.37]  be a finite-dimensional  [2.x.38] -conforming finite element space ( [2.x.39] ) with nodal basis  [2.x.40] . Now, we can expand all functions in the weak formulation (see above) in terms of the nodal basis. Henceforth, we shall denote by a capital letter the vector of coefficients (in the nodal basis) of a function denoted by the same letter in lower case; e.g.,  [2.x.41]  where  [2.x.42]  and  [2.x.43] . Thus, the finite-dimensional version of the variational formulation requires that we solve the following matrix equations at each time step: [1.x.49] 

Above, the matrix  [2.x.44]  and the vector  [2.x.45]  denote the discrete versions of the gadgets discussed above, i.e., [1.x.50] Again, note that the first matrix equation above is, in fact, the definition of an iterative procedure, so it is solved multiple times until a stopping criterion is met. Moreover,  [2.x.46]  is the mass matrix, i.e.  [2.x.47] ,  [2.x.48]  is the Laplace matrix, i.e.  [2.x.49] ,  [2.x.50]  is the nonlinear term in the equation that defines our auxiliary velocity variable, i.e.  [2.x.51] , and  [2.x.52]  is the nonlinear term in the Jacobian matrix of  [2.x.53] , i.e.  [2.x.54] . 

What solvers can we use for the first equation? Let's look at the matrix we have to invert: [1.x.51] for some  [2.x.55]  that depends on the present and previous solution. First, note that the matrix is symmetric. In addition, if the time step  [2.x.56]  is small enough, i.e. if  [2.x.57] , then the matrix is also going to be positive definite. In the program below, this will always be the case, so we will use the Conjugate Gradient method together with the SSOR method as preconditioner. We should keep in mind, however, that this will fail if we happen to use a bigger time step. Fortunately, in that case the solver will just throw an exception indicating a failure to converge, rather than silently producing a wrong result. If that happens, then we can simply replace the CG method by something that can handle indefinite symmetric systems. The GMRES solver is typically the standard method for all "bad" linear systems, but it is also a slow one. Possibly better would be a solver that utilizes the symmetry, such as, for example, SymmLQ, which is also implemented in deal.II. 

This program uses a clever optimization over  [2.x.58]  and  [2.x.59]  " [2.x.60] ": If you read the above formulas closely, it becomes clear that the velocity  [2.x.61]  only ever appears in products with the mass matrix. In  [2.x.62]  and  [2.x.63] , we were, therefore, a bit wasteful: in each time step, we would solve a linear system with the mass matrix, only to multiply the solution of that system by  [2.x.64]  again in the next time step. This can, of course, be avoided, and we do so in this program. 


[1.x.52][1.x.53] 


There are a few analytical solutions for the sine-Gordon equation, both in 1D and 2D. In particular, the program as is computes the solution to a problem with a single kink-like solitary wave initial condition.  This solution is given by Leibbrandt in \e Phys. \e Rev. \e Lett. \b 41(7), and is implemented in the  [2.x.65]  class. 

It should be noted that this closed-form solution, strictly speaking, only holds for the infinite-space initial-value problem (not the Neumann initial-boundary-value problem under consideration here). However, given that we impose \e zero Neumann boundary conditions, we expect that the solution to our initial-boundary-value problem would be close to the solution of the infinite-space initial-value problem, if reflections of waves off the boundaries of our domain do \e not occur. In practice, this is of course not the case, but we can at least assume that this were so. 

The constants  [2.x.66]  and  [2.x.67]  in the 2D solution and  [2.x.68] ,  [2.x.69]  and  [2.x.70]  in the 3D solution are called the B&auml;cklund transformation parameters. They control such things as the orientation and steepness of the kink. For the purposes of testing the code against the exact solution, one should choose the parameters so that the kink is aligned with the grid. 

The solutions that we implement in the  [2.x.71]  class are these:  [2.x.72]     [2.x.73] In 1D:   [1.x.54]   where we choose  [2.x.74] . 

  In 1D, more interesting analytical solutions are known. Many of them are   listed on http://mathworld.wolfram.com/Sine-GordonEquation.html . 

   [2.x.75] In 2D:   [1.x.55]   where  [2.x.76]  is defined as   [1.x.56]   and where we choose  [2.x.77] . 

   [2.x.78] In 3D:   [1.x.57]   where  [2.x.79]  is defined as   [1.x.58]   and where we choose  [2.x.80] .  [2.x.81]  


Since it makes it easier to play around, the  [2.x.82]  class that is used to set &mdash; surprise! &mdash; the initial values of our simulation simply queries the class that describes the exact solution for the value at the initial time, rather than duplicating the effort to implement a solution function. [1.x.59] [1.x.60] 


[1.x.61]  [1.x.62] 




For an explanation of the include files, the reader should refer to the example programs  [2.x.83]  through  [2.x.84] . They are in the standard order, which is  [2.x.85]  --  [2.x.86]  (since each of these categories roughly builds upon previous ones), then a few C++ headers for file input/output and string streams. 

[1.x.63] 



The last step is as in all previous programs: 

[1.x.64] 




[1.x.65]  [1.x.66] 




The entire algorithm for solving the problem is encapsulated in this class. As in previous example programs, the class is declared with a template parameter, which is the spatial dimension, so that we can solve the sine-Gordon equation in one, two or three spatial dimensions. For more on the dimension-independent class-encapsulation of the problem, the reader should consult  [2.x.87]  and  [2.x.88] .    


Compared to  [2.x.89]  and  [2.x.90] , there isn't anything newsworthy in the general structure of the program (though there is of course in the inner workings of the various functions!). The most notable difference is the presence of the two new functions  [2.x.91]  and  [2.x.92]  that compute the nonlinear contributions to the system matrix and right-hand side of the first equation, as discussed in the Introduction. In addition, we have to have a vector  [2.x.93]  that contains the nonlinear update to the solution vector in each Newton step.    


As also mentioned in the introduction, we do not store the velocity variable in this program, but the mass matrix times the velocity. This is done in the  [2.x.94]  variable (the "x" is intended to stand for "times").    


Finally, the  [2.x.95]  variable stores the number of time steps to be taken each time before graphical output is to be generated. This is of importance when using fine meshes (and consequently small time steps) where we would run lots of time steps and create lots of output files of solutions that look almost the same in subsequent files. This only clogs up our visualization procedures and we should avoid creating more output than we are really interested in. Therefore, if this variable is set to a value  [2.x.96]  bigger than one, output is generated only every  [2.x.97] th time step. 

[1.x.67] 




[1.x.68]  [1.x.69] 




In the following two classes, we first implement the exact solution for 1D, 2D, and 3D mentioned in the introduction to this program. This space-time solution may be of independent interest if one wanted to test the accuracy of the program by comparing the numerical against the analytic solution (note however that the program uses a finite domain, whereas these are analytic solutions for an unbounded domain). This may, for example, be done using the  [2.x.98]  function. Note, again (as was already discussed in  [2.x.99] ), how we describe space-time functions as spatial functions that depend on a time variable that can be set and queried using the  [2.x.100]  and  [2.x.101]  member functions of the FunctionTime base class of the Function class. 

[1.x.70] 



In the second part of this section, we provide the initial conditions. We are lazy (and cautious) and don't want to implement the same functions as above a second time. Rather, if we are queried for initial conditions, we create an object  [2.x.102] , set it to the correct time, and let it compute whatever values the exact solution has at that time: 

[1.x.71] 




[1.x.72]  [1.x.73] 




Let's move on to the implementation of the main class, as it implements the algorithm outlined in the introduction. 





[1.x.74]  [1.x.75] 




This is the constructor of the  [2.x.103]  class. It specifies the desired polynomial degree of the finite elements, associates a  [2.x.104]  object (just as in the example programs  [2.x.105]  and  [2.x.106] ), initializes the current or initial time, the final time, the time step size, and the value of  [2.x.107]  for the time stepping scheme. Since the solutions we compute here are time-periodic, the actual value of the start-time doesn't matter, and we choose it so that we start at an interesting time.    


Note that if we were to chose the explicit Euler time stepping scheme ( [2.x.108] ), then we must pick a time step  [2.x.109] , otherwise the scheme is not stable and oscillations might arise in the solution. The Crank-Nicolson scheme ( [2.x.110] ) and the implicit Euler scheme ( [2.x.111] ) do not suffer from this deficiency, since they are unconditionally stable. However, even then the time step should be chosen to be on the order of  [2.x.112]  in order to obtain a good solution. Since we know that our mesh results from the uniform subdivision of a rectangle, we can compute that time step easily; if we had a different domain, the technique in  [2.x.113]  using  [2.x.114]  would work as well. 

[1.x.76] 




[1.x.77]  [1.x.78] 




This function creates a rectangular grid in  [2.x.115]  dimensions and refines it several times. Also, all matrix and vector members of the  [2.x.116]  class are initialized to their appropriate sizes once the degrees of freedom have been assembled. Like  [2.x.117] , we use  [2.x.118]  functions to generate a mass matrix  [2.x.119]  and a Laplace matrix  [2.x.120]  and store them in the appropriate variables for the remainder of the program's life. 

[1.x.79] 




[1.x.80]  [1.x.81] 




This function assembles the system matrix and right-hand side vector for each iteration of Newton's method. The reader should refer to the Introduction for the explicit formulas for the system matrix and right-hand side.    


Note that during each time step, we have to add up the various contributions to the matrix and right hand sides. In contrast to  [2.x.121]  and  [2.x.122] , this requires assembling a few more terms, since they depend on the solution of the previous time step or previous nonlinear step. We use the functions  [2.x.123]  and  [2.x.124]  to do this, while the present function provides the top-level logic. 

[1.x.82] 



First we assemble the Jacobian matrix  [2.x.125] , where  [2.x.126]  is stored in the vector  [2.x.127]  for convenience. 

[1.x.83] 



Next we compute the right-hand side vector. This is just the combination of matrix-vector products implied by the description of  [2.x.128]  in the introduction. 

[1.x.84] 




[1.x.85]  [1.x.86] 




This function computes the vector  [2.x.129] , which appears in the nonlinear term in both equations of the split formulation. This function not only simplifies the repeated computation of this term, but it is also a fundamental part of the nonlinear iterative solver that we use when the time stepping is implicit (i.e.  [2.x.130] ). Moreover, we must allow the function to receive as input an "old" and a "new" solution. These may not be the actual solutions of the problem stored in  [2.x.131] , but are simply the two functions we linearize about. For the purposes of this function, let us call the first two arguments  [2.x.132]  and  [2.x.133]  in the documentation of this class below, respectively.    


As a side-note, it is perhaps worth investigating what order quadrature formula is best suited for this type of integration. Since  [2.x.134]  is not a polynomial, there are probably no quadrature formulas that can integrate these terms exactly. It is usually sufficient to just make sure that the right hand side is integrated up to the same order of accuracy as the discretization scheme is, but it may be possible to improve on the constant in the asymptotic statement of convergence by choosing a more accurate quadrature formula. 

[1.x.87] 



Once we re-initialize our  [2.x.135]  instantiation to the current cell, we make use of the  [2.x.136]  routine to get the values of the "old" data (presumably at  [2.x.137] ) and the "new" data (presumably at  [2.x.138] ) at the nodes of the chosen quadrature formula. 

[1.x.88] 



Now, we can evaluate  [2.x.139]  using the desired quadrature formula. 

[1.x.89] 



We conclude by adding up the contributions of the integrals over the cells to the global integral. 

[1.x.90] 




[1.x.91]  [1.x.92] 




This is the second function dealing with the nonlinear scheme. It computes the matrix  [2.x.140] , which appears in the nonlinear term in the Jacobian of  [2.x.141] . Just as  [2.x.142] , we must allow this function to receive as input an "old" and a "new" solution, which we again call  [2.x.143]  and  [2.x.144]  below, respectively. 

[1.x.93] 



Again, first we re-initialize our  [2.x.145]  instantiation to the current cell. 

[1.x.94] 



Then, we evaluate  [2.x.146]  using the desired quadrature formula. 

[1.x.95] 



Finally, we add up the contributions of the integrals over the cells to the global integral. 

[1.x.96] 




[1.x.97]  [1.x.98] 




As discussed in the Introduction, this function uses the CG iterative solver on the linear system of equations resulting from the finite element spatial discretization of each iteration of Newton's method for the (nonlinear) first equation of the split formulation. The solution to the system is, in fact,  [2.x.147]  so it is stored in  [2.x.148]  in the  [2.x.149]  function.    


Note that we re-set the solution update to zero before solving for it. This is not necessary: iterative solvers can start from any point and converge to the correct solution. If one has a good estimate about the solution of a linear system, it may be worthwhile to start from that vector, but as a general observation it is a fact that the starting point doesn't matter very much: it has to be a very, very good guess to reduce the number of iterations by more than a few. It turns out that for this problem, using the previous nonlinear update as a starting point actually hurts convergence and increases the number of iterations needed, so we simply set it to zero.    


The function returns the number of iterations it took to converge to a solution. This number will later be used to generate output on the screen showing how many iterations were needed in each nonlinear iteration. 

[1.x.99] 




[1.x.100]  [1.x.101] 




This function outputs the results to a file. It is pretty much identical to the respective functions in  [2.x.150]  and  [2.x.151] : 

[1.x.102] 




[1.x.103]  [1.x.104] 




This function has the top-level control over everything: it runs the (outer) time-stepping loop, the (inner) nonlinear-solver loop, and outputs the solution after each time step. 

[1.x.105] 



To acknowledge the initial condition, we must use the function  [2.x.152]  to compute  [2.x.153] . To this end, below we will create an object of type  [2.x.154] ; note that when we create this object (which is derived from the  [2.x.155]  class), we set its internal time variable to  [2.x.156] , to indicate that the initial condition is a function of space and time evaluated at  [2.x.157] .      


Then we produce  [2.x.158]  by projecting  [2.x.159]  onto the grid using  [2.x.160] . We have to use the same construct using hanging node constraints as in  [2.x.161] : the  [2.x.162]  function requires a hanging node constraints object, but to be used we first need to close it: 

[1.x.106] 



For completeness, we output the zeroth time step to a file just like any other time step. 

[1.x.107] 



Now we perform the time stepping: at every time step we solve the matrix equation(s) corresponding to the finite element discretization of the problem, and then advance our solution according to the time stepping formulas we discussed in the Introduction. 

[1.x.108] 



At the beginning of each time step we must solve the nonlinear equation in the split formulation via Newton's method --- i.e. solve for  [2.x.163]  then compute  [2.x.164]  and so on. The stopping criterion for this nonlinear iteration is that  [2.x.165] . Consequently, we need to record the norm of the residual in the first iteration.          


At the end of each iteration, we output to the console how many linear solver iterations it took us. When the loop below is done, we have (an approximation of)  [2.x.166] . 

[1.x.109] 



Upon obtaining the solution to the first equation of the problem at  [2.x.167] , we must update the auxiliary velocity variable  [2.x.168] . However, we do not compute and store  [2.x.169]  since it is not a quantity we use directly in the problem. Hence, for simplicity, we update  [2.x.170]  directly: 

[1.x.110] 



Oftentimes, in particular for fine meshes, we must pick the time step to be quite small in order for the scheme to be stable. Therefore, there are a lot of time steps during which "nothing interesting happens" in the solution. To improve overall efficiency -- in particular, speed up the program and save disk space -- we only output the solution every  [2.x.171]  time steps: 

[1.x.111] 




[1.x.112]  [1.x.113] 




This is the main function of the program. It creates an object of top-level class and calls its principal function. If exceptions are thrown during the execution of the run method of the  [2.x.172]  class, we catch and report them here. For more information about exceptions the reader should consult  [2.x.173] . 

[1.x.114] 

[1.x.115][1.x.116] 

The explicit Euler time stepping scheme  ( [2.x.174] ) performs adequately for the problems we wish to solve. Unfortunately, a rather small time step has to be chosen due to stability issues ---  [2.x.175]  appears to work for most the simulations we performed. On the other hand, the Crank-Nicolson scheme ( [2.x.176] ) is unconditionally stable, and (at least for the case of the 1D breather) we can pick the time step to be as large as  [2.x.177]  without any ill effects on the solution. The implicit Euler scheme ( [2.x.178] ) is "exponentially damped," so it is not a good choice for solving the sine-Gordon equation, which is conservative. However, some of the damped schemes in the continuum that is offered by the  [2.x.179] -method were useful for eliminating spurious oscillations due to boundary effects. 

In the simulations below, we solve the sine-Gordon equation on the interval  [2.x.180]  in 1D and on the square  [2.x.181]  in 2D. In each case, the respective grid is refined uniformly 6 times, i.e.  [2.x.182] . 

[1.x.117][1.x.118] 

The first example we discuss is the so-called 1D (stationary) breather solution of the sine-Gordon equation. The breather has the following closed-form expression, as mentioned in the Introduction: [1.x.119] where  [2.x.183] ,  [2.x.184]  and  [2.x.185]  are constants. In the simulation below, we have chosen  [2.x.186] ,  [2.x.187] ,  [2.x.188] . Moreover, it is know that the period of oscillation of the breather is  [2.x.189] , hence we have chosen  [2.x.190]  and  [2.x.191]  so that we can observe three oscillations of the solution. Then, taking  [2.x.192] ,  [2.x.193]  and  [2.x.194] , the program computed the following solution. 

 [2.x.195]  

Though not shown how to do this in the program, another way to visualize the (1+1)-d solution is to use output generated by the DataOutStack class; it allows to "stack" the solutions of individual time steps, so that we get 2D space-time graphs from 1D time-dependent solutions. This produces the space-time plot below instead of the animation above. 

 [2.x.196]  

Furthermore, since the breather is an analytical solution of the sine-Gordon equation, we can use it to validate our code, although we have to assume that the error introduced by our choice of Neumann boundary conditions is small compared to the numerical error. Under this assumption, one could use the  [2.x.197]  function to compute the difference between the numerical solution and the function described by the  [2.x.198]  class of this program. For the simulation shown in the two images above, the  [2.x.199]  norm of the error in the finite element solution at each time step remained on the order of  [2.x.200] . Hence, we can conclude that the numerical method has been implemented correctly in the program. 


[1.x.120][1.x.121] 


The only analytical solution to the sine-Gordon equation in (2+1)D that can be found in the literature is the so-called kink solitary wave. It has the following closed-form expression:   [1.x.122] with   [1.x.123] where  [2.x.201] ,  [2.x.202]  and  [2.x.203]  are constants. In the simulation below we have chosen  [2.x.204] . Notice that if  [2.x.205]  the kink is stationary, hence it would make a good solution against which we can validate the program in 2D because no reflections off the boundary of the domain occur. 

The simulation shown below was performed with  [2.x.206] ,  [2.x.207] ,  [2.x.208] ,  [2.x.209]  and  [2.x.210] . The  [2.x.211]  norm of the error of the finite element solution at each time step remained on the order of  [2.x.212] , showing that the program is working correctly in 2D, as well as 1D. Unfortunately, the solution is not very interesting, nonetheless we have included a snapshot of it below for completeness. 

 [2.x.213]  

Now that we have validated the code in 1D and 2D, we move to a problem where the analytical solution is unknown. 

To this end, we rotate the kink solution discussed above about the  [2.x.214]  axis: we let   [2.x.215] . The latter results in a solitary wave that is not aligned with the grid, so reflections occur at the boundaries of the domain immediately. For the simulation shown below, we have taken  [2.x.216] ,  [2.x.217] ,  [2.x.218] ,  [2.x.219]  and  [2.x.220] . Moreover, we had to pick  [2.x.221]  because for any  [2.x.222]  oscillations arose at the boundary, which are likely due to the scheme and not the equation, thus picking a value of  [2.x.223]  a good bit into the "exponentially damped" spectrum of the time stepping schemes assures these oscillations are not created. 

 [2.x.224]  

Another interesting solution to the sine-Gordon equation (which cannot be obtained analytically) can be produced by using two 1D breathers to construct the following separable 2D initial condition: [1.x.124] where  [2.x.225] ,  [2.x.226]  as in the 1D case we discussed above. For the simulation shown below, we have chosen  [2.x.227] ,  [2.x.228] ,  [2.x.229]  and  [2.x.230] . The solution is pretty interesting 

--- it acts like a breather (as far as the pictures are concerned); however, it appears to break up and reassemble, rather than just oscillate. 

 [2.x.231]  


[1.x.125] [1.x.126][1.x.127] 


It is instructive to change the initial conditions. Most choices will not lead to solutions that stay localized (in the soliton community, such solutions are called "stationary", though the solution does change with time), but lead to solutions where the wave-like character of the equation dominates and a wave travels away from the location of a localized initial condition. For example, it is worth playing around with the  [2.x.232]  class, by replacing the call to the  [2.x.233]  class by something like this function: [1.x.128] if  [2.x.234] , and  [2.x.235]  outside this region. 

A second area would be to investigate whether the scheme is energy-preserving. For the pure wave equation, discussed in  [2.x.236]  " [2.x.237] ", this is the case if we choose the time stepping parameter such that we get the Crank-Nicolson scheme. One could do a similar thing here, noting that the energy in the sine-Gordon solution is defined as [1.x.129] (We use  [2.x.238]  instead of  [2.x.239]  in the formula to ensure that all contributions to the energy are positive, and so that decaying solutions have finite energy on unbounded domains.) 

Beyond this, there are two obvious areas: 

- Clearly, adaptivity (i.e. time-adaptive grids) would be of interest   to problems like these. Their complexity leads us to leave this out   of this program again, though the general comments in the   introduction of  [2.x.240]  " [2.x.241] " remain true. 

- Faster schemes to solve this problem. While computers today are   plenty fast enough to solve 2d and, frequently, even 3d stationary   problems within not too much time, time dependent problems present   an entirely different class of problems. We address this topic in    [2.x.242]  where we show how to solve this problem in parallel and   without assembling or inverting any matrix at all. [1.x.130] [1.x.131]  [2.x.243]  

 [2.x.244] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22] 

[1.x.23] [1.x.24][1.x.25] 


 [2.x.3]  ( [2.x.4]  


This program implements the heat equation 

[1.x.26] 

In some sense, this equation is simpler than the ones we have discussed in the preceding programs  [2.x.5] ,  [2.x.6] ,  [2.x.7] , namely the wave equation. This is due to the fact that the heat equation smoothes out the solution over time, and is consequently more forgiving in many regards. For example, when using implicit time stepping methods, we can actually take large time steps, we have less trouble with the small disturbances we introduce through adapting the mesh every few time steps, etc. 

Our goal here will be to solve the equations above using the theta-scheme that discretizes the equation in time using the following approach, where we would like  [2.x.8]  to approximate  [2.x.9]  at some time  [2.x.10] : 

[1.x.27] 

Here,  [2.x.11]  is the time step size. The theta-scheme generalizes the explicit Euler ( [2.x.12] ), implicit Euler ( [2.x.13] ) and Crank-Nicolson ( [2.x.14] ) time discretizations. Since the latter has the highest convergence order, we will choose  [2.x.15]  in the program below, but make it so that playing with this parameter remains simple. (If you are interested in playing with higher order methods, take a look at  [2.x.16] .) 

Given this time discretization, space discretization happens as it always does, by multiplying with test functions, integrating by parts, and then restricting everything to a finite dimensional subspace. This yields the following set of fully discrete equations after multiplying through with  [2.x.17] : 

[1.x.28] 

where  [2.x.18]  is the mass matrix and  [2.x.19]  is the stiffness matrix that results from discretizing the Laplacian. Bringing all known quantities to the right hand side yields the linear system we have to solve in every step: 

[1.x.29] 

The linear system on the left hand side is symmetric and positive definite, so we should have no trouble solving it with the Conjugate Gradient method. 

We can start the iteration above if we have the set of nodal coefficients  [2.x.20]  at the initial time. Here, we take the ones we get by interpolating the initial values  [2.x.21]  onto the mesh used for the first time step. We will also need to choose a time step; we will here just choose it as fixed, but clearly advanced simulators will want to choose it adaptively. We will briefly come back to this in the [1.x.30]. 


[1.x.31][1.x.32] 


When solving the wave equation and its variants in the previous few programs, we kept the mesh fixed. Just as for stationary equations, one can make a good case that this is not the smartest approach and that significant savings can be had by adapting the mesh. There are, however, significant difficulties compared to the stationary case. Let us go through them in turn: 

 [2.x.22]     [2.x.23] [1.x.33]: For stationary problems, the   general approach is "make the mesh as fine as it is necessary". For problems   with singularities, this often leads to situations where we get many levels   of refinement into corners or along interfaces. The very first tutorial to   use adaptive meshes,  [2.x.24] , is a point in case already. 

  However, for time dependent problems, we typically need to choose the time   step related to the mesh size. For explicit time discretizations, this is   obvious, since we need to respect a CFL condition that ties the time step   size to the smallest mesh size. For implicit time discretizations, no such   hard restriction exists, but in practice we still want to make the time step   smaller if we make the mesh size smaller since we typically have error   estimates of the form  [2.x.25]  where  [2.x.26]  are the   convergence orders of the time and space discretization, respectively. We   can only make the error small if we decrease both terms. Ideally, an   estimate like this would suggest to choose  [2.x.27] . Because, at   least for problems with non-smooth solutions, the error is typically   localized in the cells with the smallest mesh size, we have to indeed choose    [2.x.28] , using the [1.x.34] mesh size. 

  The consequence is that refining the mesh further in one place implies not   only the moderate additional effort of increasing the number of degrees of   freedom slightly, but also the much larger effort of having the solve the   [1.x.35] linear system more often because of the smaller time step. 

  In practice, one typically deals with this by acknowledging that we can not   make the time step arbitrarily small, and consequently can not make the   local mesh size arbitrarily small. Rather, we set a maximal level of   refinement and when we flag cells for refinement, we simply do not refine   those cells whose children would exceed this maximal level of refinement. 

  There is a similar problem in that we will choose a right hand side that   will switch on in different parts of the domain at different times. To avoid   being caught flat footed with too coarse a mesh in areas where we suddenly   need a finer mesh, we will also enforce in our program a [1.x.36] mesh   refinement level. 

   [2.x.29] [1.x.37]: Let us consider again the   semi-discrete equations we have written down above:   [1.x.38] 

  We can here consider  [2.x.30]  as data since it has presumably been computed   before. Now, let us replace   [1.x.39] 

  multiply with test functions  [2.x.31]  and integrate by parts   where necessary. In a process as outlined above, this would yield   [1.x.40] 

  Now imagine that we have changed the mesh between time steps  [2.x.32]  and    [2.x.33] . Then the problem is that the basis functions we use for  [2.x.34]  and    [2.x.35]  are different! This pertains to the terms on the right hand side,   the first of which we could more clearly write as (the second follows the   same pattern)   [1.x.41] 

  If the meshes used in these two time steps are the same, then    [2.x.36]  forms a square mass matrix    [2.x.37] . However, if the meshes are not the same, then in general the matrix   is rectangular. Worse, it is difficult to even compute these integrals   because if we loop over the cells of the mesh at time step  [2.x.38] , then we need   to evaluate  [2.x.39]  at the quadrature points of these cells, but   they do not necessarily correspond to the cells of the mesh at time step    [2.x.40]  and  [2.x.41]  is not defined via these cells; the same of   course applies if we wanted to compute the integrals via integration on the   cells of mesh  [2.x.42] . 

  In any case, what we have to face is a situation where we need to integrate   shape functions defined on two different meshes. This can be done, and is in   fact demonstrated in  [2.x.43] , but the process is at best described by the   word "awkward". 

  In practice, one does not typically want to do this. Rather, we avoid the   whole situation by interpolating the solution from the old to the new mesh   every time we adapt the mesh. In other words, rather than solving the   equations above, we instead solve the problem   [1.x.42] 

  where  [2.x.44]  is the interpolation operator onto the finite element space   used in time step  [2.x.45] . This is not the optimal approach since it introduces   an additional error besides time and space discretization, but it is a   pragmatic one that makes it feasible to do time adapting meshes.  [2.x.46]  




[1.x.43][1.x.44] 


There are a number of things one can typically get wrong when implementing a finite element code. In particular, for time dependent problems, the following are common sources of bugs: 

- The time integration, for example by getting the coefficients in front of   the terms involving the current and previous time steps wrong (e.g., mixing   up a factor  [2.x.47]  for  [2.x.48] ). 

- Handling the right hand side, for example forgetting a factor of  [2.x.49]  or    [2.x.50] . 

- Mishandling the boundary values, again for example forgetting a factor of    [2.x.51]  or  [2.x.52] , or forgetting to apply nonzero boundary values not only   to the right hand side but also to the system matrix. 

A less common problem is getting the initial conditions wrong because one can typically see that it is wrong by just outputting the first time step. In any case, in order to verify the correctness of the code, it is helpful to have a testing protocol that allows us to verify each of these components separately. This means: 

- Testing the code with nonzero initial conditions but zero right hand side   and boundary values and verifying that the time evolution is correct. 

- Then testing with zero initial conditions and boundary values but nonzero   right hand side and again ensuring correctness. 

- Finally, testing with zero initial conditions and right hand side but   nonzero boundary values. 

This sounds complicated, but fortunately, for linear partial differential equations without coefficients (or constant coefficients) like the one here, there is a fairly standard protocol that rests on the following observation: if you choose as your domain a square  [2.x.53]  (or, with slight modifications, a rectangle), then the exact solution can be written as 

[1.x.45] 

(with integer constants  [2.x.54] ) if only the initial condition, right hand side and boundary values are all of the form  [2.x.55]  as well. This is due to the fact that the function  [2.x.56]  is an eigenfunction of the Laplace operator and allows us to compute things like the time factor  [2.x.57]  analytically and, consequently, compare with what we get numerically. 

As an example, let us consider the situation where we have  [2.x.58]  and  [2.x.59] . With the claim (ansatz) of the form for  [2.x.60]  above, we get that 

[1.x.46] 

For this to be equal to  [2.x.61] , we need that 

[1.x.47] 

and due to the initial conditions,  [2.x.62] . This differential equation can be integrated to yield 

[1.x.48] 

In other words, if the initial condition is a product of sines, then the solution has exactly the same shape of a product of sines that decays to zero with a known time dependence. This is something that is easy to test if you have a sufficiently fine mesh and sufficiently small time step. 

What is typically going to happen if you get the time integration scheme wrong (e.g., by having the wrong factors of  [2.x.63]  or  [2.x.64]  in front of the various terms) is that you don't get the right temporal behavior of the solution. Double check the various factors until you get the right behavior. You may also want to verify that the temporal decay rate (as determined, for example, by plotting the value of the solution at a fixed point) does not double or halve each time you double or halve the time step or mesh size. You know that it's not the handling of the boundary conditions or right hand side because these were both zero. 

If you have so verified that the time integrator is correct, take the situation where the right hand side is nonzero but the initial conditions are zero:  [2.x.65]  and  [2.x.66] . Again, 

[1.x.49] 

and for this to be equal to  [2.x.67] , we need that 

[1.x.50] 

and due to the initial conditions,  [2.x.68] . Integrating this equation in time yields 

[1.x.51] 



Again, if you have the wrong factors of  [2.x.69]  or  [2.x.70]  in front of the right hand side terms you will either not get the right temporal behavior of the solution, or it will converge to a maximum value other than  [2.x.71] . 

Once we have verified that the time integration and right hand side handling are correct using this scheme, we can go on to verifying that we have the boundary values correct, using a very similar approach. 




[1.x.52][1.x.53] 


Solving the heat equation on a simple domain with a simple right hand side almost always leads to solutions that are exceedingly boring, since they become very smooth very quickly and then do not move very much any more. Rather, we here solve the equation on the L-shaped domain with zero Dirichlet boundary values and zero initial conditions, but as right hand side we choose 

[1.x.54] 

Here, 

[1.x.55] 

In other words, in every period of length  [2.x.72] , the right hand side first flashes on in domain 1, then off completely, then on in domain 2, then off completely again. This pattern is probably best observed via the little animation of the solution shown in the [1.x.56]. 

If you interpret the heat equation as finding the spatially and temporally variable temperature distribution of a conducting solid, then the test case above corresponds to an L-shaped body where we keep the boundary at zero temperature, and heat alternatingly in two parts of the domain. While heating is in effect, the temperature rises in these places, after which it diffuses and diminishes again. The point of these initial conditions is that they provide us with a solution that has singularities both in time (when sources switch on and off) as well as time (at the reentrant corner as well as at the edges and corners of the regions where the source acts). [1.x.57] [1.x.58] 

The program starts with the usual include files, all of which you should have seen before by now: 

[1.x.59] 



Then the usual placing of all content of this program into a namespace and the importation of the deal.II namespace into the one we will work in: 

[1.x.60] 




[1.x.61]  [1.x.62]    


The next piece is the declaration of the main class of this program. It follows the well trodden path of previous examples. If you have looked at  [2.x.73] , for example, the only thing worth noting here is that we need to build two matrices (the mass and Laplace matrix) and keep the current and previous time step's solution. We then also need to store the current time, the size of the time step, and the number of the current time step. The last of the member variables denotes the theta parameter discussed in the introduction that allows us to treat the explicit and implicit Euler methods as well as the Crank-Nicolson method and other generalizations all in one program.    


As far as member functions are concerned, the only possible surprise is that the  [2.x.74]  function takes arguments for the minimal and maximal mesh refinement level. The purpose of this is discussed in the introduction. 

[1.x.63] 




[1.x.64]  [1.x.65] 




In the following classes and functions, we implement the various pieces of data that define this problem (right hand side and boundary values) that are used in this program and for which we need function objects. The right hand side is chosen as discussed at the end of the introduction. For boundary values, we choose zero values, but this is easily changed below. 

[1.x.66] 




[1.x.67]  [1.x.68]    


It is time now for the implementation of the main class. Let's start with the constructor which selects a linear element, a time step constant at 1/500 (remember that one period of the source on the right hand side was set to 0.2 above, so we resolve each period with 100 time steps) and chooses the Crank Nicolson method by setting  [2.x.75] . 

[1.x.69] 




[1.x.70]  [1.x.71]    


The next function is the one that sets up the DoFHandler object, computes the constraints, and sets the linear algebra objects to their correct sizes. We also compute the mass and Laplace matrix here by simply calling two functions in the library.    


Note that we do not take the hanging node constraints into account when assembling the matrices (both functions have an AffineConstraints argument that defaults to an empty object). This is because we are going to condense the constraints in run() after combining the matrices for the current time-step. 

[1.x.72] 




[1.x.73]  [1.x.74]    


The next function is the one that solves the actual linear system for a single time step. There is nothing surprising here: 

[1.x.75] 




[1.x.76]  [1.x.77]    


Neither is there anything new in generating graphical output other than the fact that we tell the DataOut object what the current time and time step number is, so that this can be written into the output file: 

[1.x.78] 




[1.x.79]  [1.x.80]    


This function is the interesting part of the program. It takes care of the adaptive mesh refinement. The three tasks this function performs is to first find out which cells to refine/coarsen, then to actually do the refinement and eventually transfer the solution vectors between the two different grids. The first task is simply achieved by using the well-established Kelly error estimator on the solution. The second task is to actually do the remeshing. That involves only basic functions as well, such as the  [2.x.76]  that refines those cells with the largest estimated error that together make up 60 per cent of the error, and coarsens those cells with the smallest error that make up for a combined 40 per cent of the error. Note that for problems such as the current one where the areas where something is going on are shifting around, we want to aggressively coarsen so that we can move cells around to where it is necessary.    


As already discussed in the introduction, too small a mesh leads to too small a time step, whereas too large a mesh leads to too little resolution. Consequently, after the first two steps, we have two loops that limit refinement and coarsening to an allowable range of cells: 

[1.x.81] 



These two loops above are slightly different but this is easily explained. In the first loop, instead of calling  [2.x.77]  we may as well have called  [2.x.78] . The two calls should yield the same iterator since iterators are sorted by level and there should not be any cells on levels higher than on level  [2.x.79] . In fact, this very piece of code makes sure that this is the case. 




As part of mesh refinement we need to transfer the solution vectors from the old mesh to the new one. To this end we use the SolutionTransfer class and we have to prepare the solution vectors that should be transferred to the new grid (we will lose the old grid once we have done the refinement so the transfer has to happen concurrently with refinement). At the point where we call this function, we will have just computed the solution, so we no longer need the old_solution variable (it will be overwritten by the solution just after the mesh may have been refined, i.e., at the end of the time step; see below). In other words, we only need the one solution vector, and we copy it to a temporary object where it is safe from being reset when we further down below call  [2.x.80] .      


Consequently, we initialize a SolutionTransfer object by attaching it to the old DoF handler. We then prepare the triangulation and the data vector for refinement (in this order). 

[1.x.82] 



Now everything is ready, so do the refinement and recreate the DoF structure on the new grid, and finally initialize the matrix structures and the new vectors in the  [2.x.81]  function. Next, we actually perform the interpolation of the solution from old to new grid. The final step is to apply the hanging node constraints to the solution vector, i.e., to make sure that the values of degrees of freedom located on hanging nodes are so that the solution is continuous. This is necessary since SolutionTransfer only operates on cells locally, without regard to the neighborhoof. 

[1.x.83] 




[1.x.84]  [1.x.85]    


This is the main driver of the program, where we loop over all time steps. At the top of the function, we set the number of initial global mesh refinements and the number of initial cycles of adaptive mesh refinement by repeating the first time step a few times. Then we create a mesh, initialize the various objects we will work with, set a label for where we should start when re-running the first time step, and interpolate the initial solution onto out mesh (we choose the zero function here, which of course we could do in a simpler way by just setting the solution vector to zero). We also output the initial time step once.    




 [2.x.82]  If you're an experienced programmer, you may be surprised that we use a  [2.x.83]  statement in this piece of code!  [2.x.84]  statements are not particularly well liked any more since Edsgar Dijkstra, one of the greats of computer science, wrote a letter in 1968 called "Go To Statement considered harmful" (see [1.x.86]). The author of this code subscribes to this notion whole-heartedly:  [2.x.85]  is hard to understand. In fact, deal.II contains virtually no occurrences: excluding code that was essentially transcribed from books and not counting duplicated code pieces, there are 3 locations in about 600,000 lines of code at the time this note is written; we also use it in 4 tutorial programs, in exactly the same context as here. Instead of trying to justify the occurrence here, let's first look at the code and we'll come back to the issue at the end of function. 

[1.x.87] 



Then we start the main loop until the computed time exceeds our end time of 0.5. The first task is to build the right hand side of the linear system we need to solve in each time step. Recall that it contains the term  [2.x.86] . We put these terms into the variable system_rhs, with the help of a temporary vector: 

[1.x.88] 



The second piece is to compute the contributions of the source terms. This corresponds to the term  [2.x.87] . The following code calls  [2.x.88]  to compute the vectors  [2.x.89] , where we set the time of the right hand side (source) function before we evaluate it. The result of this all ends up in the forcing_terms variable: 

[1.x.89] 



Next, we add the forcing terms to the ones that come from the time stepping, and also build the matrix  [2.x.90]  that we have to invert in each time step. The final piece of these operations is to eliminate hanging node constrained degrees of freedom from the linear system: 

[1.x.90] 



There is one more operation we need to do before we can solve it: boundary values. To this end, we create a boundary value object, set the proper time to the one of the current time step, and evaluate it as we have done many times before. The result is used to also set the correct boundary values in the linear system: 

[1.x.91] 



With this out of the way, all we have to do is solve the system, generate graphical data, and... 

[1.x.92] 



...take care of mesh refinement. Here, what we want to do is (i) refine the requested number of times at the very beginning of the solution procedure, after which we jump to the top to restart the time iteration, (ii) refine every fifth time step after that.          


The time loop and, indeed, the main part of the program ends with starting into the next time step by setting old_solution to the solution we have just computed. 

[1.x.93] 



Now that you have seen what the function does, let us come back to the issue of the  [2.x.91] . In essence, what the code does is something like this:  [2.x.92]  Here, the condition "happy with the result" is whether we'd like to keep the current mesh or would rather refine the mesh and start over on the new mesh. We could of course replace the use of the  [2.x.93]  by the following:  [2.x.94]  This has the advantage of getting rid of the  [2.x.95]  but the disadvantage of having to duplicate the code that implements the "solve timestep" and "postprocess" operations in two different places. This could be countered by putting these parts of the code (sizable chunks in the actual implementation above) into their own functions, but a  [2.x.96]  loop with a  [2.x.97]  statement is not really all that much easier to read or understand than a  [2.x.98] . 




In the end, one might simply agree that [1.x.96]  [2.x.99]  statements are a bad idea but be pragmatic and state that there may be occasions where they can help avoid code duplication and awkward control flow. This may be one of these places, and it matches the position Steve McConnell takes in his excellent book "Code Complete"  [2.x.100]  about good programming practices (see the mention of this book in the introduction of  [2.x.101] ) that spends a surprising ten pages on the question of  [2.x.102]  in general. 










[1.x.97]  [1.x.98] 




Having made it this far,  there is, again, nothing much to discuss for the main function of this program: it looks like all such functions since  [2.x.103] . 

[1.x.99] 

[1.x.100][1.x.101] 


As in many of the tutorials, the actual output of the program matters less than how we arrived there. Nonetheless, here it is: 

[1.x.102] 



Maybe of more interest is a visualization of the solution and the mesh on which it was computed: 

 [2.x.104]  

The movie shows how the two sources switch on and off and how the mesh reacts to this. It is quite obvious that the mesh as is is probably not the best we could come up with. We'll get back to this in the next section. 


[1.x.103] [1.x.104][1.x.105] 


There are at least two areas where one can improve this program significantly: adaptive time stepping and a better choice of the mesh. 

[1.x.106][1.x.107] 


Having chosen an implicit time stepping scheme, we are not bound by any CFL-like condition on the time step. Furthermore, because the time scales on which change happens on a given cell in the heat equation are not bound to the cells diameter (unlike the case with the wave equation, where we had a fixed speed of information transport that couples the temporal and spatial scales), we can choose the time step as we please. Or, better, choose it as we deem necessary for accuracy. 

Looking at the solution, it is clear that the action does not happen uniformly over time: a lot is changing around the time we switch on a source, things become less dramatic once a source is on for a little while, and we enter a long phase of decline when both sources are off. During these times, we could surely get away with a larger time step than before without sacrificing too much accuracy. 

The literature has many suggestions on how to choose the time step size adaptively. Much can be learned, for example, from the way ODE solvers choose their time steps. One can also be inspired by a posteriori error estimators that can, ideally, be written in a way that the consist of a temporal and a spatial contribution to the overall error. If the temporal one is too large, we should choose a smaller time step. Ideas in this direction can be found, for example, in the PhD thesis of a former principal developer of deal.II, Ralf Hartmann, published by the University of Heidelberg, Germany, in 2002. 


[1.x.108][1.x.109] 


We here use one of the simpler time stepping methods, namely the second order in time Crank-Nicolson method. However, more accurate methods such as Runge-Kutta methods are available and should be used as they do not represent much additional effort. It is not difficult to implement this for the current program, but a more systematic treatment is also given in  [2.x.105] . 


[1.x.110][1.x.111] 


If you look at the meshes in the movie above, it is clear that they are not particularly well suited to the task at hand. In fact, they look rather random. 

There are two factors at play. First, there are some islands where cells have been refined but that are surrounded by non-refined cells (and there are probably also a few occasional coarsened islands). These are not terrible, as they most of the time do not affect the approximation quality of the mesh, but they also don't help because so many of their additional degrees of freedom are in fact constrained by hanging node constraints. That said, this is easy to fix: the Triangulation class takes an argument to its constructor indicating a level of "mesh smoothing". Passing one of many possible flags, this instructs the triangulation to refine some additional cells, or not to refine some cells, so that the resulting mesh does not have these artifacts. 

The second problem is more severe: the mesh appears to lag the solution. The underlying reason is that we only adapt the mesh once every fifth time step, and only allow for a single refinement in these cases. Whenever a source switches on, the solution had been very smooth in this area before and the mesh was consequently rather coarse. This implies that the next time step when we refine the mesh, we will get one refinement level more in this area, and five time steps later another level, etc. But this is not enough: first, we should refine immediately when a source switches on (after all, in the current context we at least know what the right hand side is), and we should allow for more than one refinement level. Of course, all of this can be done using deal.II, it just requires a bit of algorithmic thinking in how to make this work! 


[1.x.112][1.x.113] 


To increase the accuracy and resolution of your simulation in time, one typically decreases the time step size  [2.x.106] . If you start playing around with the time step in this particular example, you will notice that the solution becomes partly negative, if  [2.x.107]  is below a certain threshold. This is not what we would expect to happen (in nature). 

To get an idea of this behavior mathematically, let us consider a general, fully discrete problem: 

[1.x.114] 

The general form of the  [2.x.108] th equation then reads: 

[1.x.115] 

where  [2.x.109]  is the set of degrees of freedom that DoF  [2.x.110]  couples with (i.e., for which either the matrix  [2.x.111]  or matrix  [2.x.112]  has a nonzero entry at position  [2.x.113] ). If all coefficients fulfill the following conditions: 

[1.x.116] 

all solutions  [2.x.114]  keep their sign from the previous ones  [2.x.115] , and consequently from the initial values  [2.x.116] . See e.g. [1.x.117] for more information on positivity preservation. 

Depending on the PDE to solve and the time integration scheme used, one is able to deduce conditions for the time step  [2.x.117] . For the heat equation with the Crank-Nicolson scheme, [1.x.118] have translated it to the following ones: 

[1.x.119] 

where  [2.x.118]  denotes the mass matrix and  [2.x.119]  the stiffness matrix with  [2.x.120]  for  [2.x.121] , respectively. With  [2.x.122] , we can formulate bounds for the global time step  [2.x.123]  as follows: 

[1.x.120] 

In other words, the time step is constrained by [1.x.121] in case of a Crank-Nicolson scheme. These bounds should be considered along with the CFL condition to ensure significance of the performed simulations. 

Being unable to make the time step as small as we want to get more accuracy without losing the positivity property is annoying. It raises the question of whether we can at least [1.x.122] the minimal time step we can choose  to ensure positivity preservation in this particular tutorial. Indeed, we can use the SparseMatrix objects for both mass and stiffness that are created via the MatrixCreator functions. Iterating through each entry via SparseMatrixIterators lets us check for diagonal and off-diagonal entries to set a proper time step dynamically. For quadratic matrices, the diagonal element is stored as the first member of a row (see SparseMatrix documentation). An exemplary code snippet on how to grab the entries of interest from the  [2.x.124]  is shown below. 

[1.x.123] 



Using the information so computed, we can bound the time step via the formulas above. [1.x.124] [1.x.125]  [2.x.125]  

 [2.x.126] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34] 

[1.x.35] [1.x.36][1.x.37] 


This tutorial program attempts to show how to use  [2.x.3] -finite element methods with deal.II. It solves the Laplace equation and so builds only on the first few tutorial programs, in particular on  [2.x.4]  for dimension independent programming and  [2.x.5]  for adaptive mesh refinement. 

The  [2.x.6] -finite element method was proposed in the early 1980s by Babu&scaron;ka and Guo as an alternative to either (i) mesh refinement (i.e., decreasing the mesh parameter  [2.x.7]  in a finite element computation) or (ii) increasing the polynomial degree  [2.x.8]  used for shape functions. It is based on the observation that increasing the polynomial degree of the shape functions reduces the approximation error if the solution is sufficiently smooth. On the other hand, it is well known that even for the generally well-behaved class of elliptic problems, higher degrees of regularity can not be guaranteed in the vicinity of boundaries, corners, or where coefficients are discontinuous; consequently, the approximation can not be improved in these areas by increasing the polynomial degree  [2.x.9]  but only by refining the mesh, i.e., by reducing the mesh size  [2.x.10] . These differing means to reduce the error have led to the notion of  [2.x.11] -finite elements, where the approximating finite element spaces are adapted to have a high polynomial degree  [2.x.12]  wherever the solution is sufficiently smooth, while the mesh width  [2.x.13]  is reduced at places wherever the solution lacks regularity. It was already realized in the first papers on this method that  [2.x.14] -finite elements can be a powerful tool that can guarantee that the error is reduced not only with some negative power of the number of degrees of freedom, but in fact exponentially. 

In order to implement this method, we need several things above and beyond what a usual finite element program needs, and in particular above what we have introduced in the tutorial programs leading up to  [2.x.15] . In particular, we will have to discuss the following aspects:  [2.x.16]     [2.x.17] Instead of using the same finite element on all cells, we now will want   a collection of finite element objects, and associate each cell with one   of these objects in this collection. [2.x.18]  

   [2.x.19] Degrees of freedom will then have to be allocated on each cell depending   on what finite element is associated with this particular cell. Constraints   will have to be generated in the same way as for hanging nodes, but we now   also have to deal with the case where two neighboring cells have different   finite elements assigned. [2.x.20]  

   [2.x.21] We will need to be able to assemble cell and face contributions   to global matrices and right hand side vectors. [2.x.22]  

   [2.x.23] After solving the resulting linear system, we will want to   analyze the solution. In particular, we will want to compute error   indicators that tell us whether a given cell should be refined   and/or whether the polynomial degree of the shape functions used on   it should be increased. [2.x.24]   [2.x.25]  

We will discuss all these aspects in the following subsections of this introduction. It will not come as a big surprise that most of these tasks are already well supported by functionality provided by the deal.II, and that we will only have to provide the logic of what the program should do, not exactly how all this is going to happen. 

In deal.II, the  [2.x.26] -functionality is largely packaged into the hp-namespace. This namespace provides classes that handle  [2.x.27] -discretizations, assembling matrices and vectors, and other tasks. We will get to know many of them further down below. In addition, most of the functions in the DoFTools, and VectorTools namespaces accept  [2.x.28] -objects in addition to the non- [2.x.29] -ones. Much of the  [2.x.30] -implementation is also discussed in the  [2.x.31]  documentation module and the links found there. 

It may be worth giving a slightly larger perspective at the end of this first part of the introduction.  [2.x.32] -functionality has been implemented in a number of different finite element packages (see, for example, the list of references cited in the  [2.x.33]  "hp-paper"). However, by and large, most of these packages have implemented it only for the (i) the 2d case, and/or (ii) the discontinuous Galerkin method. The latter is a significant simplification because discontinuous finite elements by definition do not require continuity across faces between cells and therefore do not require the special treatment otherwise necessary whenever finite elements of different polynomial degree meet at a common face. In contrast, deal.II implements the most general case, i.e., it allows for continuous and discontinuous elements in 1d, 2d, and 3d, and automatically handles the resulting complexity. In particular, it handles computing the constraints (similar to hanging node constraints) of elements of different degree meeting at a face or edge. The many algorithmic and data structure techniques necessary for this are described in the  [2.x.34]  "hp-paper" for those interested in such detail. 

We hope that providing such a general implementation will help explore the potential of  [2.x.35] -methods further. 




[1.x.38][1.x.39] 


Now on again to the details of how to use the  [2.x.36] -functionality in deal.II. The first aspect we have to deal with is that now we do not have only a single finite element any more that is used on all cells, but a number of different elements that cells can choose to use. For this, deal.II introduces the concept of a [1.x.40], implemented in the class  [2.x.37]  In essence, such a collection acts like an object of type  [2.x.38] , but with a few more bells and whistles and a memory management better suited to the task at hand. As we will later see, we will also use similar quadrature collections, and &mdash; although we don't use them here &mdash; there is also the concept of mapping collections. All of these classes are described in the  [2.x.39]  overview. 

In this tutorial program, we will use continuous Lagrange elements of orders 2 through 7 (in 2d) or 2 through 5 (in 3d). The collection of used elements can then be created as follows: 

[1.x.41] 






[1.x.42][1.x.43][1.x.44] 


The next task we have to consider is what to do with the list of finite element objects we want to use. In previous tutorial programs, starting with  [2.x.40] , we have seen that the DoFHandler class is responsible for making the connection between a mesh (described by a Triangulation object) and a finite element, by allocating the correct number of degrees of freedom for each vertex, face, edge, and cell of the mesh. 

The situation here is a bit more complicated since we do not just have a single finite element object, but rather may want to use different elements on different cells. We therefore need two things: (i) a version of the DoFHandler class that can deal with this situation, and (ii) a way to tell the DoFHandler which element to use on which cell. 

The first of these two things is implemented in the [1.x.45]-mode of the DoFHandler class: rather than associating it with a triangulation and a single finite element object, it is associated with a triangulation and a finite element collection. The second part is achieved by a loop over all cells of this DoFHandler and for each cell setting the index of the finite element within the collection that shall be used on this cell. We call the index of the finite element object within the collection that shall be used on a cell the cell's [1.x.46] to indicate that this is the finite element that is active on this cell, whereas all the other elements of the collection are inactive on it. The general outline of this reads like this: 

[1.x.47] 



Dots in the call to  [2.x.41]  indicate that we will have to have some sort of strategy later on to decide which element to use on which cell; we will come back to this later. The main point here is that the first and last line of this code snippet is pretty much exactly the same as for the non- [2.x.42] -case. 

Another complication arises from the fact that this time we do not simply have hanging nodes from local mesh refinement, but we also have to deal with the case that if there are two cells with different active finite element indices meeting at a face (for example a Q2 and a Q3 element) then we have to compute additional constraints on the finite element field to ensure that it is continuous. This is conceptually very similar to how we compute hanging node constraints, and in fact the code looks exactly the same: 

[1.x.48] 

In other words, the  [2.x.43]  deals not only with hanging node constraints, but also with  [2.x.44] -constraints at the same time. 




[1.x.49][1.x.50] 


Following this, we have to set up matrices and vectors for the linear system of the correct size and assemble them. Setting them up works in exactly the same way as for the non- [2.x.45] -case. Assembling requires a bit more thought. 

The main idea is of course unchanged: we have to loop over all cells, assemble local contributions, and then copy them into the global objects. As discussed in some detail first in  [2.x.46] , deal.II has the FEValues class that pulls the finite element description, mapping, and quadrature formula together and aids in evaluating values and gradients of shape functions as well as other information on each of the quadrature points mapped to the real location of a cell. Every time we move on to a new cell we re-initialize this FEValues object, thereby asking it to re-compute that part of the information that changes from cell to cell. It can then be used to sum up local contributions to bilinear form and right hand side. 

In the context of  [2.x.47] -finite element methods, we have to deal with the fact that we do not use the same finite element object on each cell. In fact, we should not even use the same quadrature object for all cells, but rather higher order quadrature formulas for cells where we use higher order finite elements. Similarly, we may want to use higher order mappings on such cells as well. 

To facilitate these considerations, deal.II has a class  [2.x.48]  that does what we need in the current context. The difference is that instead of a single finite element, quadrature formula, and mapping, it takes collections of these objects. It's use is very much like the regular FEValues class, i.e., the interesting part of the loop over all cells would look like this: 

[1.x.51] 



In this tutorial program, we will always use a Q1 mapping, so the mapping collection argument to the  [2.x.49]  construction will be omitted. Inside the loop, we first initialize the  [2.x.50]  object for the current cell. The second, third and fourth arguments denote the index within their respective collections of the quadrature, mapping, and finite element objects we wish to use on this cell. These arguments can be omitted (and are in the program below), in which case  [2.x.51]  is used for this index. The order of these arguments is chosen in this way because one may sometimes want to pick a different quadrature or mapping object from their respective collections, but hardly ever a different finite element than the one in use on this cell, i.e., one with an index different from  [2.x.52] . The finite element collection index is therefore the last default argument so that it can be conveniently omitted. 

What this  [2.x.53]  call does is the following: the  [2.x.54]  class checks whether it has previously already allocated a non- [2.x.55] -FEValues object for this combination of finite element, quadrature, and mapping objects. If not, it allocates one. It then re-initializes this object for the current cell, after which there is now a FEValues object for the selected finite element, quadrature and mapping usable on the current cell. A reference to this object is then obtained using the call  [2.x.56] , and will be used in the usual fashion to assemble local contributions. 




[1.x.52][1.x.53] 


One of the central pieces of the adaptive finite element method is that we inspect the computed solution (a posteriori) with an indicator that tells us which are the cells where the error is largest, and then refine them. In many of the other tutorial programs, we use the KellyErrorEstimator class to get an indication of the size of the error on a cell, although we also discuss more complicated strategies in some programs, most importantly in  [2.x.57] . 

In any case, as long as the decision is only "refine this cell" or "do not refine this cell", the actual refinement step is not particularly challenging. However, here we have a code that is capable of hp-refinement, i.e., we suddenly have two choices whenever we detect that the error on a certain cell is too large for our liking: we can refine the cell by splitting it into several smaller ones, or we can increase the polynomial degree of the shape functions used on it. How do we know which is the more promising strategy? Answering this question is the central problem in  [2.x.58] -finite element research at the time of this writing. 

In short, the question does not appear to be settled in the literature at this time. There are a number of more or less complicated schemes that address it, but there is nothing like the KellyErrorEstimator that is universally accepted as a good, even if not optimal, indicator of the error. Most proposals use the fact that it is beneficial to increase the polynomial degree whenever the solution is locally smooth whereas it is better to refine the mesh wherever it is rough. However, the questions of how to determine the local smoothness of the solution as well as the decision when a solution is smooth enough to allow for an increase in  [2.x.59]  are certainly big and important ones. 

In the following, we propose a simple estimator of the local smoothness of a solution. As we will see in the results section, this estimator has flaws, in particular as far as cells with local hanging nodes are concerned. We therefore do not intend to present the following ideas as a complete solution to the problem. Rather, it is intended as an idea to approach it that merits further research and investigation. In other words, we do not intend to enter a sophisticated proposal into the fray about answers to the general question. However, to demonstrate our approach to  [2.x.60] -finite elements, we need a simple indicator that does generate some useful information that is able to drive the simple calculations this tutorial program will perform. 


[1.x.54][1.x.55] 


Our approach here is simple: for a function  [2.x.61]  to be in the Sobolev space  [2.x.62]  on a cell  [2.x.63] , it has to satisfy the condition [1.x.56] Assuming that the cell  [2.x.64]  is not degenerate, i.e., that the mapping from the unit cell to cell  [2.x.65]  is sufficiently regular, above condition is of course equivalent to [1.x.57] where  [2.x.66]  is the function  [2.x.67]  mapped back onto the unit cell  [2.x.68] . From here, we can do the following: first, let us define the Fourier series of  [2.x.69]  as [1.x.58] with Fourier vectors  [2.x.70]  in 2d,  [2.x.71]  in 3d, etc, and  [2.x.72] . The coefficients of expansion  [2.x.73]  can be obtained using  [2.x.74] -orthogonality of the exponential basis [1.x.59] that leads to the following expression [1.x.60] It becomes clear that we can then write the  [2.x.75]  norm of  [2.x.76]  as [1.x.61] In other words, if this norm is to be finite (i.e., for  [2.x.77]  to be in  [2.x.78] ), we need that [1.x.62] Put differently: the higher regularity  [2.x.79]  we want, the faster the Fourier coefficients have to go to zero. If you wonder where the additional exponent  [2.x.80]  comes from: we would like to make use of the fact that  [2.x.81]  if the sequence  [2.x.82]  for any  [2.x.83] . The problem is that we here have a summation not only over a single variable, but over all the integer multiples of  [2.x.84]  that are located inside the  [2.x.85] -dimensional sphere, because we have vector components  [2.x.86] . In the same way as we prove that the sequence  [2.x.87]  above converges by replacing the sum by an integral over the entire line, we can replace our  [2.x.88] -dimensional sum by an integral over  [2.x.89] -dimensional space. Now we have to note that between distance  [2.x.90]  and  [2.x.91] , there are, up to a constant,  [2.x.92]  modes, in much the same way as we can transform the volume element  [2.x.93]  into  [2.x.94] . Consequently, it is no longer  [2.x.95]  that has to decay as  [2.x.96] , but it is in fact  [2.x.97] . A comparison of exponents yields the result. 

We can turn this around: Assume we are given a function  [2.x.98]  of unknown smoothness. Let us compute its Fourier coefficients  [2.x.99]  and see how fast they decay. If they decay as [1.x.63] then consequently the function we had here was in  [2.x.100] . 


[1.x.64][1.x.65] 


So what do we have to do to estimate the local smoothness of  [2.x.101]  on a cell  [2.x.102] ? Clearly, the first step is to compute the Fourier coefficients of our solution. Fourier series being infinite series, we simplify our task by only computing the first few terms of the series, such that  [2.x.103]  with a cut-off  [2.x.104] . Let us parenthetically remark that we want to choose  [2.x.105]  large enough so that we capture at least the variation of those shape functions that vary the most. On the other hand, we should not choose  [2.x.106]  too large: clearly, a finite element function, being a polynomial, is in  [2.x.107]  on any given cell, so the coefficients will have to decay exponentially at one point; since we want to estimate the smoothness of the function this polynomial approximates, not of the polynomial itself, we need to choose a reasonable cutoff for  [2.x.108] . Either way, computing this series is not particularly hard: from the definition [1.x.66] we see that we can compute the coefficient  [2.x.109]  as [1.x.67] where  [2.x.110]  is the value of the  [2.x.111] th degree of freedom on this cell. In other words, we can write it as a matrix-vector product [1.x.68] with the matrix [1.x.69] This matrix is easily computed for a given number of shape functions  [2.x.112]  and Fourier modes  [2.x.113] . Consequently, finding the coefficients  [2.x.114]  is a rather trivial job. To simplify our life even further, we will use  [2.x.115]  class which does exactly this. 

The next task is that we have to estimate how fast these coefficients decay with  [2.x.116] . The problem is that, of course, we have only finitely many of these coefficients in the first place. In other words, the best we can do is to fit a function  [2.x.117]  to our data points  [2.x.118] , for example by determining  [2.x.119]  via a least-squares procedure: [1.x.70] However, the problem with this is that it leads to a nonlinear problem, a fact that we would like to avoid. On the other hand, we can transform the problem into a simpler one if we try to fit the logarithm of our coefficients to the logarithm of  [2.x.120] , like this: [1.x.71] Using the usual facts about logarithms, we see that this yields the problem [1.x.72] where  [2.x.121] . This is now a problem for which the optimality conditions  [2.x.122] , are linear in  [2.x.123] . We can write these conditions as follows: [1.x.73] This linear system is readily inverted to yield [1.x.74] and [1.x.75] 

This is nothing else but linear regression fit and to do that we will use  [2.x.124]  While we are not particularly interested in the actual value of  [2.x.125] , the formula above gives us a mean to calculate the value of the exponent  [2.x.126]  that we can then use to determine that  [2.x.127]  is in  [2.x.128]  with  [2.x.129] . 

These steps outlined above are applicable to many different scenarios, which motivated the introduction of a generic function  [2.x.130]  in deal.II, that combines all the tasks described in this section in one simple function call. We will use it in the implementation of this program. 


[1.x.76][1.x.77] 


In the formulas above, we have derived the Fourier coefficients  [2.x.131] . Because  [2.x.132]  is a vector, we will get a number of Fourier coefficients  [2.x.133]  for the same absolute value  [2.x.134] , corresponding to the Fourier transform in different directions. If we now consider a function like  [2.x.135]  then we will find lots of large Fourier coefficients in  [2.x.136] -direction because the function is non-smooth in this direction, but fast-decaying Fourier coefficients in  [2.x.137] -direction because the function is smooth there. The question that arises is this: if we simply fit our polynomial decay  [2.x.138]  to [1.x.78] Fourier coefficients, we will fit it to a smoothness [1.x.79]. Is this what we want? Or would it be better to only consider the largest coefficient  [2.x.139]  for all  [2.x.140]  with the same magnitude, essentially trying to determine the smoothness of the solution in that spatial direction in which the solution appears to be roughest? 

One can probably argue for either case. The issue would be of more interest if deal.II had the ability to use anisotropic finite elements, i.e., ones that use different polynomial degrees in different spatial directions, as they would be able to exploit the directionally variable smoothness much better. Alas, this capability does not exist at the time of writing this tutorial program. 

Either way, because we only have isotopic finite element classes, we adopt the viewpoint that we should tailor the polynomial degree to the lowest amount of regularity, in order to keep numerical efforts low. Consequently, instead of using the formula [1.x.80] To calculate  [2.x.141]  as shown above, we have to slightly modify all sums: instead of summing over all Fourier modes, we only sum over those for which the Fourier coefficient is the largest one among all  [2.x.142]  with the same magnitude  [2.x.143] , i.e., all sums above have to replaced by the following sums: [1.x.81] This is the form we will implement in the program. 


[1.x.82][1.x.83] 


One may ask whether it is a problem that we only compute the Fourier transform on the [1.x.84] (rather than the real cell) of the solution. After all, we stretch the solution by a factor  [2.x.144]  during the transformation, thereby shifting the Fourier frequencies by a factor of  [2.x.145] . This is of particular concern since we may have neighboring cells with mesh sizes  [2.x.146]  that differ by a factor of 2 if one of them is more refined than the other. The concern is also motivated by the fact that, as we will see in the results section below, the estimated smoothness of the solution should be a more or less continuous function, but exhibits jumps at locations where the mesh size jumps. It therefore seems natural to ask whether we have to compensate for the transformation. 

The short answer is "no". In the process outlined above, we attempt to find coefficients  [2.x.147]  that minimize the sum of squares of the terms [1.x.85] To compensate for the transformation means not attempting to fit a decay  [2.x.148]  with respect to the Fourier frequencies  [2.x.149]  [1.x.86], but to fit the coefficients  [2.x.150]  computed on the reference cell [1.x.87], where  [2.x.151]  is the norm of the transformation operator (i.e., something like the diameter of the cell). In other words, we would have to minimize the sum of squares of the terms [1.x.88] instead. However, using fundamental properties of the logarithm, this is simply equivalent to minimizing [1.x.89] In other words, this and the original least squares problem will produce the same best-fit exponent  [2.x.152] , though the offset will in one case be  [2.x.153]  and in the other  [2.x.154] . However, since we are not interested in the offset at all but only in the exponent, it doesn't matter whether we scale Fourier frequencies in order to account for mesh size effects or not, the estimated smoothness exponent will be the same in either case. 




[1.x.90][1.x.91] 


[1.x.92][1.x.93] 


One of the problems with  [2.x.155] -methods is that the high polynomial degree of shape functions together with the large number of constrained degrees of freedom leads to matrices with large numbers of nonzero entries in some rows. At the same time, because there are areas where we use low polynomial degree and consequently matrix rows with relatively few nonzero entries. Consequently, allocating the sparsity pattern for these matrices is a challenge: we cannot simply assemble a SparsityPattern by starting with an estimate of the bandwidth without using a lot of extra memory. 

The way in which we create a SparsityPattern for the underlying linear system is tightly coupled to the strategy we use to enforce constraints. deal.II supports handling constraints in linear systems in two ways:  [2.x.156]     [2.x.157] Assembling the matrix without regard to the constraints and applying them   afterwards with  [2.x.158]  or [2.x.159]     [2.x.160] Applying constraints as we assemble the system with    [2.x.161]   [2.x.162]  Most programs built on deal.II use the  [2.x.163]  function to allocate a DynamicSparsityPattern that takes constraints into account. The system matrix then uses a SparsityPattern copied over from the DynamicSparsityPattern. This method is explained in  [2.x.164]  and used in most tutorial programs. 

The early tutorial programs use first or second degree finite elements, so removing entries in the sparsity pattern corresponding to constrained degrees of freedom does not have a large impact on the overall number of zeros explicitly stored by the matrix. However, since as many as a third of the degrees of freedom may be constrained in an hp-discretization (and, with higher degree elements, these constraints can couple one DoF to as many as ten or twenty other DoFs), it is worthwhile to take these constraints into consideration since the resulting matrix will be much sparser (and, therefore, matrix-vector products or factorizations will be substantially faster too). 


[1.x.94][1.x.95] 


A second problem particular to  [2.x.165] -methods arises because we have so many constrained degrees of freedom: typically up to about one third of all degrees of freedom (in 3d) are constrained because they either belong to cells with hanging nodes or because they are on cells adjacent to cells with a higher or lower polynomial degree. This is, in fact, not much more than the fraction of constrained degrees of freedom in non- [2.x.166] -mode, but the difference is that each constrained hanging node is constrained not only against the two adjacent degrees of freedom, but is constrained against many more degrees of freedom. 

It turns out that the strategy presented first in  [2.x.167]  to eliminate the constraints while computing the element matrices and vectors with  [2.x.168]  is the most efficient approach also for this case. The alternative strategy to first build the matrix without constraints and then "condensing" away constrained degrees of freedom is considerably more expensive. It turns out that building the sparsity pattern by this inefficient algorithm requires at least  [2.x.169]  in the number of unknowns, whereas an ideal finite element program would of course only have algorithms that are linear in the number of unknowns. Timing the sparsity pattern creation as well as the matrix assembly shows that the algorithm presented in  [2.x.170]  (and used in the code below) is indeed faster. 

In our program, we will also treat the boundary conditions as (possibly inhomogeneous) constraints and eliminate the matrix rows and columns to those as well. All we have to do for this is to call the function that interpolates the Dirichlet boundary conditions already in the setup phase in order to tell the AffineConstraints object about them, and then do the transfer from local to global data on matrix and vector simultaneously. This is exactly what we've shown in  [2.x.171] . 




[1.x.96][1.x.97] 


The test case we will solve with this program is a re-take of the one we already look at in  [2.x.172] : we solve the Laplace equation [1.x.98] in 2d, with  [2.x.173] , and with zero Dirichlet boundary values for  [2.x.174] . We do so on the domain  [2.x.175] , i.e., a square with a square hole in the middle. 

The difference to  [2.x.176]  is of course that we use  [2.x.177] -finite elements for the solution. The test case is of interest because it has re-entrant corners in the corners of the hole, at which the solution has singularities. We therefore expect that the solution will be smooth in the interior of the domain, and rough in the vicinity of the singularities. The hope is that our refinement and smoothness indicators will be able to see this behavior and refine the mesh close to the singularities, while the polynomial degree is increased away from it. As we will see in the results section, this is indeed the case. [1.x.99] [1.x.100] 


[1.x.101]  [1.x.102] 




The first few files have already been covered in previous examples and will thus not be further commented on. 

[1.x.103] 



These are the new files we need. The first and second provide the FECollection and the [1.x.104] version of the FEValues class as described in the introduction of this program. The next one provides the functionality for automatic  [2.x.178] -adaptation, for which we will use the estimation algorithms based on decaying series expansion coefficients that are part of the last two files. 

[1.x.105] 



The last set of include files are standard C++ headers. 

[1.x.106] 



Finally, this is as in previous programs: 

[1.x.107] 




[1.x.108]  [1.x.109] 




The main class of this program looks very much like the one already used in the first few tutorial programs, for example the one in  [2.x.179] . The main difference is that we have merged the refine_grid and output_results functions into one since we will also want to output some of the quantities used in deciding how to refine the mesh (in particular the estimated smoothness of the solution).    


As far as member variables are concerned, we use the same structure as already used in  [2.x.180] , but we need collections instead of individual finite element, quadrature, and face quadrature objects. We will fill these collections in the constructor of the class. The last variable,  [2.x.181] , indicates the maximal polynomial degree of shape functions used. 

[1.x.110] 




[1.x.111]  [1.x.112]    


Next, let us define the right hand side function for this problem. It is  [2.x.182]  in 1d,  [2.x.183]  in 2d, and so on. 

[1.x.113] 




[1.x.114]  [1.x.115] 





[1.x.116]  [1.x.117] 




The constructor of this class is fairly straightforward. It associates the DoFHandler object with the triangulation, and then sets the maximal polynomial degree to 7 (in 1d and 2d) or 5 (in 3d and higher). We do so because using higher order polynomial degrees becomes prohibitively expensive, especially in higher space dimensions.    


Following this, we fill the collections of finite element, and cell and face quadrature objects. We start with quadratic elements, and each quadrature formula is chosen so that it is appropriate for the matching finite element in the  [2.x.184]  object. 

[1.x.118] 




[1.x.119]  [1.x.120] 




The destructor is unchanged from what we already did in  [2.x.185] : 

[1.x.121] 




[1.x.122]  [1.x.123]    


This function is again a verbatim copy of what we already did in  [2.x.186] . Despite function calls with exactly the same names and arguments, the algorithms used internally are different in some aspect since the dof_handler variable here is in  [2.x.187] -mode. 

[1.x.124] 




[1.x.125]  [1.x.126] 




This is the function that assembles the global matrix and right hand side vector from the local contributions of each cell. Its main working is as has been described in many of the tutorial programs before. The significant deviations are the ones necessary for [1.x.127] finite element methods. In particular, that we need to use a collection of FEValues object (implemented through the  [2.x.188]  class), and that we have to eliminate constrained degrees of freedom already when copying local contributions into global objects. Both of these are explained in detail in the introduction of this program.    


One other slight complication is the fact that because we use different polynomial degrees on different cells, the matrices and vectors holding local contributions do not have the same size on all cells. At the beginning of the loop over all cells, we therefore each time have to resize them to the correct size (given by  [2.x.189] ). Because these classes are implemented in such a way that reducing the size of a matrix or vector does not release the currently allocated memory (unless the new size is zero), the process of resizing at the beginning of the loop will only require re-allocation of memory during the first few iterations. Once we have found in a cell with the maximal finite element degree, no more re-allocations will happen because all subsequent  [2.x.190]  calls will only set the size to something that fits the currently allocated memory. This is important since allocating memory is expensive, and doing so every time we visit a new cell would take significant compute time. 

[1.x.128] 




[1.x.129]  [1.x.130] 




The function solving the linear system is entirely unchanged from previous examples. We simply try to reduce the initial residual (which equals the  [2.x.191]  norm of the right hand side) by a certain factor: 

[1.x.131] 




[1.x.132]  [1.x.133] 




After solving the linear system, we will want to postprocess the solution. Here, all we do is to estimate the error, estimate the local smoothness of the solution as described in the introduction, then write graphical output, and finally refine the mesh in both  [2.x.192]  and  [2.x.193]  according to the indicators computed before. We do all this in the same function because we want the estimated error and smoothness indicators not only for refinement, but also include them in the graphical output. 

[1.x.134] 



Let us start with computing estimated error and smoothness indicators, which each are one number for each active cell of our triangulation. For the error indicator, we use the KellyErrorEstimator class as always. 

[1.x.135] 



Estimating the smoothness is performed with the method of decaying expansion coefficients as outlined in the introduction. We will first need to create an object capable of transforming the finite element solution on every single cell into a sequence of Fourier series coefficients. The SmoothnessEstimator namespace offers a factory function for such a  [2.x.194]  object that is optimized for the process of estimating smoothness. The actual determination of the decay of Fourier coefficients on every individual cell then happens in the last function. 

[1.x.136] 



Next we want to generate graphical output. In addition to the two estimated quantities derived above, we would also like to output the polynomial degree of the finite elements used on each of the elements on the mesh.      


The way to do that requires that we loop over all cells and poll the active finite element index of them using  [2.x.195] . We then use the result of this operation and query the finite element collection for the finite element with that index, and finally determine the polynomial degree of that element. The result we put into a vector with one element per cell. The DataOut class requires this to be a vector of  [2.x.196] , even though our values are all integers, so that is what we use: 

[1.x.137] 



With now all data vectors available -- solution, estimated errors and smoothness indicators, and finite element degrees --, we create a DataOut object for graphical output and attach all data: 

[1.x.138] 



The final step in generating output is to determine a file name, open the file, and write the data into it (here, we use VTK format): 

[1.x.139] 



After this, we would like to actually refine the mesh, in both  [2.x.197]  and  [2.x.198] . The way we are going to do this is as follows: first, we use the estimated error to flag those cells for refinement that have the largest error. This is what we have always done: 

[1.x.140] 



Next we would like to figure out which of the cells that have been flagged for refinement should actually have  [2.x.199]  increased instead of  [2.x.200]  decreased. The strategy we choose here is that we look at the smoothness indicators of those cells that are flagged for refinement, and increase  [2.x.201]  for those with a smoothness larger than a certain relative threshold. In other words, for every cell for which (i) the refinement flag is set, (ii) the smoothness indicator is larger than the threshold, and (iii) we still have a finite element with a polynomial degree higher than the current one in the finite element collection, we will assign a future FE index that corresponds to a polynomial with degree one higher than it currently is. The following function is capable of doing exactly this. Absent any better strategies, we will set the threshold via interpolation between the minimal and maximal smoothness indicators on cells flagged for refinement. Since the corner singularities are strongly localized, we will favor  [2.x.202] - over  [2.x.203] -refinement quantitatively. We achieve this with a low threshold by setting a small interpolation factor of 0.2. In the same way, we deal with cells that are going to be coarsened and decrease their polynomial degree when their smoothness indicator is below the corresponding threshold determined on cells to be coarsened. 

[1.x.141] 



The above function only determines whether the polynomial degree will change via future FE indices, but does not manipulate the  [2.x.204] -refinement flags. So for cells that are flagged for both refinement categories, we prefer  [2.x.205] - over  [2.x.206] -refinement. The following function call ensures that only one of  [2.x.207] - or  [2.x.208] -refinement is imposed, and not both at once. 

[1.x.142] 



For grid adaptive refinement, we ensure a 2:1 mesh balance by limiting the difference of refinement levels of neighboring cells to one by calling  [2.x.209]  We would like to achieve something similar for the p-levels of neighboring cells: levels of future finite elements are not allowed to differ by more than a specified difference. With its default parameters, a call of  [2.x.210]  ensures that their level difference is limited to one. This will not necessarily decrease the number of hanging nodes in the domain, but makes sure that high order polynomials are not constrained to much lower polynomials on faces, e.g., fifth order to second order polynomials. 

[1.x.143] 



At the end of this procedure, we then refine the mesh. During this process, children of cells undergoing bisection inherit their mother cell's finite element index. Further, future finite element indices will turn into active ones, so that the new finite elements will be assigned to cells after the next call of  [2.x.211]  

[1.x.144] 




[1.x.145]  [1.x.146] 




The following function is used when creating the initial grid. The grid we would like to create is actually similar to the one from  [2.x.212] , i.e., the square domain with the square hole in the middle. It can be generated by excatly the same function. However, since its implementation is only a specialization of the 2d case, we will present a different way of creating this domain which is dimension independent.    


We first create a hypercube triangulation with enough cells so that it already holds our desired domain  [2.x.213] , subdivided into  [2.x.214]  cells. We then remove those cells in the center of the domain by testing the coordinate values of the vertices on each cell. In the end, we refine the so created grid globally as usual. 

[1.x.147] 




[1.x.148]  [1.x.149] 




This function implements the logic of the program, as did the respective function in most of the previous programs already, see for example  [2.x.215] .    


Basically, it contains the adaptive loop: in the first iteration create a coarse grid, and then set up the linear system, assemble it, solve, and postprocess the solution including mesh refinement. Then start over again. In the meantime, also output some information for those staring at the screen trying to figure out what the program does: 

[1.x.150] 




[1.x.151]  [1.x.152] 




The main function is again verbatim what we had before: wrap creating and running an object of the main class into a  [2.x.216]  block and catch whatever exceptions are thrown, thereby producing meaningful output if anything should go wrong: 

[1.x.153] 

[1.x.154][1.x.155] 


In this section, we discuss a few results produced from running the current tutorial program. More results, in particular the extension to 3d calculations and determining how much compute time the individual components of the program take, are given in the  [2.x.217]  "hp-paper". 

When run, this is what the program produces: 

[1.x.156] 



The first thing we learn from this is that the number of constrained degrees of freedom is on the order of 20-25% of the total number of degrees of freedom, at least on the later grids when we have elements of relatively high order (in 3d, the fraction of constrained degrees of freedom can be up to 30%). This is, in fact, on the same order of magnitude as for non- [2.x.218] -discretizations. For example, in the last step of the  [2.x.219]  program, we have 18353 degrees of freedom, 4432 of which are constrained. The difference is that in the latter program, each constrained hanging node is constrained against only the two adjacent degrees of freedom, whereas in the  [2.x.220] -case, constrained nodes are constrained against many more degrees of freedom. Note also that the current program also includes nodes subject to Dirichlet boundary conditions in the list of constraints. In cycle 0, all the constraints are actually because of boundary conditions. 

Of maybe more interest is to look at the graphical output. First, here is the solution of the problem: 

<img src="https://www.dealii.org/images/steps/developer/ [2.x.221] -solution.png"      alt="Elevation plot of the solution, showing the lack of regularity near           the interior (reentrant) corners."      width="200" height="200"> 

Secondly, let us look at the sequence of meshes generated: 

 [2.x.222]  

It is clearly visible how the mesh is refined near the corner singularities, as one would expect it. More interestingly, we should be curious to see the distribution of finite element polynomial degrees to these mesh cells, where the lightest color corresponds to degree two and the darkest one corresponds to degree seven: 

 [2.x.223]  

While this is certainly not a perfect arrangement, it does make some sense: we use low order elements close to boundaries and corners where regularity is low. On the other hand, higher order elements are used where (i) the error was at one point fairly large, i.e., mainly in the general area around the corner singularities and in the top right corner where the solution is large, and (ii) where the solution is smooth, i.e., far away from the boundary. 

This arrangement of polynomial degrees of course follows from our smoothness estimator. Here is the estimated smoothness of the solution, with darker colors indicating least smoothness and lighter indicating the smoothest areas: 

 [2.x.224]  

The primary conclusion one can draw from this is that the loss of regularity at the internal corners is a highly localized phenomenon; it only seems to impact the cells adjacent to the corner itself, so when we refine the mesh the black coloring is no longer visible. Besides the corners, this sequence of plots implies that the smoothness estimates are somewhat independent of the mesh refinement, particularly when we are far away from boundaries. It is also obvious that the smoothness estimates are independent of the actual size of the solution (see the picture of the solution above), as it should be. A point of larger concern, however, is that one realizes on closer inspection that the estimator we have overestimates the smoothness of the solution on cells with hanging nodes. This in turn leads to higher polynomial degrees in these areas, skewing the allocation of finite elements onto cells. 

We have no good explanation for this effect at the moment. One theory is that the numerical solution on cells with hanging nodes is, of course, constrained and therefore not entirely free to explore the function space to get close to the exact solution. This lack of degrees of freedom may manifest itself by yielding numerical solutions on these cells with suppressed oscillation, meaning a higher degree of smoothness. The estimator picks this signal up and the estimated smoothness overestimates the actual value. However, a definite answer to what is going on currently eludes the authors of this program. 

The bigger question is, of course, how to avoid this problem. Possibilities include estimating the smoothness not on single cells, but cell assemblies or patches surrounding each cell. It may also be possible to find simple correction factors for each cell depending on the number of constrained degrees of freedom it has. In either case, there are ample opportunities for further research on finding good  [2.x.225] -refinement criteria. On the other hand, the main point of the current program was to demonstrate using the  [2.x.226] -technology in deal.II, which is unaffected by our use of a possible sub-optimal refinement criterion. 




[1.x.157] [1.x.158][1.x.159] 


[1.x.160][1.x.161] 


This tutorial demonstrates only one particular strategy to decide between  [2.x.227] - and  [2.x.228] -adaptation. In fact, there are many more ways to automatically decide on the adaptation type, of which a few are already implemented in deal.II:  [2.x.229]     [2.x.230] [1.x.162] This is the strategy currently   implemented in this tutorial. For more information on this strategy, see   the general documentation of the  [2.x.231]  namespace. [2.x.232]  

   [2.x.233] [1.x.163] This strategy is quite similar   to the current one, but uses Legendre series expansion rather than the   Fourier one: instead of sinusoids as basis functions, this strategy uses   Legendre polynomials. Of course, since we approximate the solution using a   finite-dimensional polynomial on each cell, the expansion of the solution in   Legendre polynomials is also finite and, consequently, when we talk about the   "decay" of this expansion, we can only consider the finitely many nonzero   coefficients of this expansion, rather than think about it in asymptotic terms.   But, if we have enough of these coefficients, we can certainly think of the   decay of these coefficients as characteristic of the decay of the coefficients   of the exact solution (which is, in general, not polynomial and so will have an   infinite Legendre expansion), and considering the coefficients we have should   reveal something about the properties of the exact solution. 

  The transition from the Fourier strategy to the Legendre one is quite simple:   You just need to change the series expansion class and the corresponding   smoothness estimation function to be part of the proper namespaces    [2.x.234]  and  [2.x.235]  For the theoretical   background of this strategy, consult the general documentation of the    [2.x.236]  namespace, as well as  [2.x.237]  ,    [2.x.238]  and  [2.x.239] . [2.x.240]  

   [2.x.241] [1.x.164] The last strategy is quite different   from the other two. In theory, we know how the error will converge   after changing the discretization of the function space. With    [2.x.242] -refinement the solution converges algebraically as already pointed   out in  [2.x.243] . If the solution is sufficiently smooth, though, we   expect that the solution will converge exponentially with increasing   polynomial degree of the finite element. We can compare a proper   prediction of the error with the actual error in the following step to   see if our choice of adaptation type was justified. 

  The transition to this strategy is a bit more complicated. For this, we need   an initialization step with pure  [2.x.244] - or  [2.x.245] -refinement and we need to   transfer the predicted errors over adapted meshes. The extensive   documentation of the  [2.x.246]  function describes not   only the theoretical details of this approach, but also presents a blueprint   on how to implement this strategy in your code. For more information, see    [2.x.247]  . 

  Note that with this particular function you cannot predict the error for   the next time step in time-dependent problems. Therefore, this strategy   cannot be applied to this type of problem without further ado. Alternatively,   the following approach could be used, which works for all the other   strategies as well: start each time step with a coarse mesh, keep refining   until happy with the result, and only then move on to the next time step. [2.x.248]   [2.x.249]  

Try implementing one of these strategies into this tutorial and observe the subtle changes to the results. You will notice that all strategies are capable of identifying the singularities near the reentrant corners and will perform  [2.x.250] -refinement in these regions, while preferring  [2.x.251] -refinement in the bulk domain. A detailed comparison of these strategies is presented in  [2.x.252]  . 


[1.x.165][1.x.166] 


All functionality presented in this tutorial already works for both sequential and parallel applications. It is possible without too much effort to change to either the  [2.x.253]  or the  [2.x.254]  classes. If you feel eager to try it, we recommend reading  [2.x.255]  for the former and  [2.x.256]  for the latter case first for further background information on the topic, and then come back to this tutorial to try out your newly acquired skills. [1.x.167] [1.x.168]  [2.x.257]  

 [2.x.258] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39] 

 [2.x.3]  

[1.x.40][1.x.41][1.x.42] 

 [2.x.4]  


[1.x.43][1.x.44] [1.x.45] 

In this example, we intend to solve the multigroup diffusion approximation of the neutron transport equation. Essentially, the way to view this is as follows: In a nuclear reactor, neutrons are speeding around at different energies, get absorbed or scattered, or start a new fission event. If viewed at long enough length scales, the movement of neutrons can be considered a diffusion process. 

A mathematical description of this would group neutrons into energy bins, and consider the balance equations for the neutron fluxes in each of these bins, or energy groups. The scattering, absorption, and fission events would then be operators within the diffusion equation describing the neutron fluxes. Assume we have energy groups  [2.x.5] , where by convention we assume that the neutrons with the highest energy are in group 1 and those with the lowest energy in group  [2.x.6] . Then the neutron flux of each group satisfies the following equations: [1.x.46] 

augmented by appropriate boundary conditions. Here,  [2.x.7]  is the velocity of neutrons within group  [2.x.8] . In other words, the change in time in flux of neutrons in group  [2.x.9]  is governed by the following processes:  [2.x.10]   [2.x.11]  Diffusion  [2.x.12] . Here,  [2.x.13]  is the   (spatially variable) diffusion coefficient.  [2.x.14]  Absorption  [2.x.15]  (note the   negative sign). The coefficient  [2.x.16]  is called the [1.x.47].  [2.x.17]  Nuclear fission  [2.x.18] .   The production of neutrons of energy  [2.x.19]  is   proportional to the flux of neutrons of energy  [2.x.20]  times the   probability  [2.x.21]  that neutrons of energy  [2.x.22]  cause a fission   event times the number  [2.x.23]  of neutrons produced in each fission event   times the probability that a neutron produced in this event has energy    [2.x.24] .  [2.x.25]  is called the [1.x.48] and    [2.x.26]  the [1.x.49]. We will denote the term    [2.x.27]  as the [1.x.50] in the program.  [2.x.28]  Scattering  [2.x.29]    of neutrons of energy  [2.x.30]  producing neutrons   of energy  [2.x.31] .  [2.x.32]  is called the [1.x.51]. The case of elastic, in-group scattering  [2.x.33]  exists, too, but   we subsume this into the removal cross section. The case  [2.x.34]  is called   down-scattering, since a neutron loses energy in such an event. On the   other hand,  [2.x.35]  corresponds to up-scattering: a neutron gains energy in   a scattering event from the thermal motion of the atoms surrounding it;   up-scattering is therefore only an important process for neutrons with   kinetic energies that are already on the same order as the thermal kinetic   energy (i.e. in the sub  [2.x.36]  range).  [2.x.37]  An extraneous source  [2.x.38] .  [2.x.39]  

For realistic simulations in reactor analysis, one may want to split the continuous spectrum of neutron energies into many energy groups, often up to 100. However, if neutron energy spectra are known well enough for some type of reactor (for example Pressurized Water Reactors, PWR), it is possible to obtain satisfactory results with only 2 energy groups. 

In the program shown in this tutorial program, we provide the structure to compute with as many energy groups as desired. However, to keep computing times moderate and in order to avoid tabulating hundreds of coefficients, we only provide the coefficients for above equations for a two-group simulation, i.e.  [2.x.40] . We do, however, consider a realistic situation by assuming that the coefficients are not constant, but rather depend on the materials that are assembled into reactor fuel assemblies in rather complicated ways (see below). 


[1.x.52][1.x.53] 


If we consider all energy groups at once, we may write above equations in the following operator form: [1.x.54] 

where  [2.x.41]  are sinking, fission, and scattering operators, respectively.  [2.x.42]  here includes both the diffusion and removal terms. Note that  [2.x.43]  is symmetric, whereas  [2.x.44]  and  [2.x.45]  are not. 

It is well known that this equation admits a stable solution if all eigenvalues of the operator  [2.x.46]  are negative. This can be readily seen by multiplying the equation by  [2.x.47]  and integrating over the domain, leading to [1.x.55] 

Stability means that the solution does not grow, i.e. we want the left hand side to be less than zero, which is the case if the eigenvalues of the operator on the right are all negative. For obvious reasons, it is not very desirable if a nuclear reactor produces neutron fluxes that grow exponentially, so eigenvalue analyses are the bread-and-butter of nuclear engineers. The main point of the program is therefore to consider the eigenvalue problem [1.x.56] 

where we want to make sure that all eigenvalues are positive. Note that  [2.x.48] , being the diffusion operator plus the absorption (removal), is positive definite; the condition that all eigenvalues are positive therefore means that we want to make sure that fission and inter-group scattering are weak enough to not shift the spectrum into the negative. 

In nuclear engineering, one typically looks at a slightly different formulation of the eigenvalue problem. To this end, we do not just multiply with  [2.x.49]  and integrate, but rather multiply with  [2.x.50] . We then get the following evolution equation: [1.x.57] 

Stability is then guaranteed if the eigenvalues of the following problem are all negative: [1.x.58] 

which is equivalent to the eigenvalue problem [1.x.59] 

The typical formulation in nuclear engineering is to write this as [1.x.60] 

where  [2.x.51] . Intuitively,  [2.x.52]  is something like the multiplication factor for neutrons per typical time scale and should be less than or equal to one for stable operation of a reactor: if it is less than one, the chain reaction will die down, whereas nuclear bombs for example have a  [2.x.53] -eigenvalue larger than one. A stable reactor should have  [2.x.54] . 

For those who wonder how this can be achieved in practice without inadvertently getting slightly larger than one and triggering a nuclear bomb: first, fission processes happen on different time scales. While most neutrons are released very quickly after a fission event, a small number of neutrons are only released by daughter nuclei after several further decays, up to 10-60 seconds after the fission was initiated. If one is therefore slightly beyond  [2.x.55] , one therefore has many seconds to react until all the neutrons created in fission re-enter the fission cycle. Nevertheless, control rods in nuclear reactors absorbing neutrons -- and therefore reducing  [2.x.56]  -- are designed in such a way that they are all the way in the reactor in at most 2 seconds. 

One therefore has on the order of 10-60 seconds to regulate the nuclear reaction if  [2.x.57]  should be larger than one for some time, as indicated by a growing neutron flux. Regulation can be achieved by continuously monitoring the neutron flux, and if necessary increase or reduce neutron flux by moving neutron-absorbing control rods a few millimeters into or out of the reactor. On a longer scale, the water cooling the reactor contains boron, a good neutron absorber. Every few hours, boron concentrations are adjusted by adding boron or diluting the coolant. 

Finally, some of the absorption and scattering reactions have some stability built in; for example, higher neutron fluxes result in locally higher temperatures, which lowers the density of water and therefore reduces the number of scatterers that are necessary to moderate neutrons from high to low energies before they can start fission events themselves. 

In this tutorial program, we solve above  [2.x.58] -eigenvalue problem for two energy groups, and we are looking for the largest multiplication factor  [2.x.59] , which is proportional to the inverse of the minimum eigenvalue plus one. To solve the eigenvalue problem, we generally use a modified version of the [1.x.61]. The algorithm looks like this: 

 [2.x.60]   [2.x.61]  Initialize  [2.x.62]  and  [2.x.63]  with  [2.x.64]    and  [2.x.65]  and let  [2.x.66] . 

 [2.x.67]  Define the so-called [1.x.62] by   [1.x.63] 



 [2.x.68]  Solve for all group fluxes  [2.x.69]  using   [1.x.64] 



 [2.x.70]  Update   [1.x.65] 



 [2.x.71]  Compare  [2.x.72]  with  [2.x.73] .   If the change greater than a prescribed tolerance then set  [2.x.74]  repeat   the iteration starting at step 2, otherwise end the iteration.  [2.x.75]  

Note that in this scheme, we do not solve group fluxes exactly in each power iteration, but rather consider previously compute  [2.x.76]  only for down-scattering events  [2.x.77] . Up-scattering is only treated by using old iterators  [2.x.78] , in essence assuming that the scattering operator is triangular. This is physically motivated since up-scattering does not play a too important role in neutron scattering. In addition, practices shows that the inverse power iteration is stable even using this simplification. 

Note also that one can use lots of extrapolation techniques to accelerate the power iteration laid out above. However, none of these are implemented in this example. 


[1.x.66][1.x.67] 


One may wonder whether it is appropriate to solve for the solutions of the individual energy group equations on the same meshes. The question boils down to this: will  [2.x.79]  and  [2.x.80]  have similar smoothness properties? If this is the case, then it is appropriate to use the same mesh for the two; a typical application could be chemical combustion, where typically the concentrations of all or most chemical species change rapidly within the flame front. As it turns out, and as will be apparent by looking at the graphs shown in the results section of this tutorial program, this isn't the case here, however: since the diffusion coefficient is different for different energy groups, fast neutrons (in bins with a small group number  [2.x.81] ) have a very smooth flux function, whereas slow neutrons (in bins with a large group number) are much more affected by the local material properties and have a correspondingly rough solution if the coefficient are rough as in the case we compute here. Consequently, we will want to use different meshes to compute each energy group. 

This has two implications that we will have to consider: First, we need to find a way to refine the meshes individually. Second, assembling the source terms for the inverse power iteration, where we have to integrate solution  [2.x.82]  defined on mesh  [2.x.83]  against the shape functions defined on mesh  [2.x.84] , becomes a much more complicated task. 


[1.x.68][1.x.69] 


We use the usual paradigm: solve on a given mesh, then evaluate an error indicator for each cell of each mesh we have. Because it is so convenient, we again use the a posteriori error estimator by Kelly, Gago, Zienkiewicz and Babuska which approximates the error per cell by integrating the jump of the gradient of the solution along the faces of each cell. Using this, we obtain indicators [1.x.70] 

where  [2.x.85]  is the triangulation used in the solution of  [2.x.86] . The question is what to do with this. For one, it is clear that refining only those cells with the highest error indicators might lead to bad results. To understand this, it is important to realize that  [2.x.87]  scales with the second derivative of  [2.x.88] . In other words, if we have two energy groups  [2.x.89]  whose solutions are equally smooth but where one is larger by a factor of 10,000, for example, then only the cells of that mesh will be refined, whereas the mesh for the solution of small magnitude will remain coarse. This is probably not what one wants, since we can consider both components of the solution equally important. 

In essence, we would therefore have to scale  [2.x.90]  by an importance factor  [2.x.91]  that says how important it is to resolve  [2.x.92]  to any given accuracy. Such important factors can be computed using duality techniques (see, for example, the  [2.x.93]  tutorial program, and the reference to the book by Bangerth and Rannacher cited there). We won't go there, however, and simply assume that all energy groups are equally important, and will therefore normalize the error indicators  [2.x.94]  for group  [2.x.95]  by the maximum of the solution  [2.x.96] . We then refine the cells whose errors satisfy [1.x.71] 

and coarsen the cells where [1.x.72] 

We chose  [2.x.97]  and  [2.x.98]  in the code. Note that this will, of course, lead to different meshes for the different energy groups. 

The strategy above essentially means the following: If for energy group  [2.x.99]  there are many cells  [2.x.100]  on which the error is large, for example because the solution is globally very rough, then many cells will be above the threshold. On the other hand, if there are a few cells with large and many with small errors, for example because the solution is overall rather smooth except at a few places, then only the few cells with large errors will be refined. Consequently, the strategy allows for meshes that track the global smoothness properties of the corresponding solutions rather well. 


[1.x.73][1.x.74] 


As pointed out above, the multigroup refinement strategy results in different meshes for the different solutions  [2.x.101] . So what's the problem? In essence it goes like this: in step 3 of the eigenvalue iteration, we have form the weak form for the equation to compute  [2.x.102]  as usual by multiplication with test functions  [2.x.103]  defined on the mesh for energy group  [2.x.104] ; in the process, we have to compute the right hand side vector that contains terms of the following form: [1.x.75] 

where  [2.x.105]  is one of the coefficient functions  [2.x.106]  or  [2.x.107]  used in the right hand side of eigenvalue equation. The difficulty now is that  [2.x.108]  is defined on the mesh for energy group  [2.x.109] , i.e. it can be expanded as  [2.x.110] , with basis functions  [2.x.111]  defined on mesh  [2.x.112] . The contribution to the right hand side can therefore be written as [1.x.76] 

On the other hand, the test functions  [2.x.113]  are defined on mesh  [2.x.114] . This means that we can't just split the integral  [2.x.115]  into integrals over the cells of either mesh  [2.x.116]  or  [2.x.117] , since the respectively other basis functions may not be defined on these cells. 

The solution to this problem lies in the fact that both the meshes for  [2.x.118]  and  [2.x.119]  are derived by adaptive refinement from a common coarse mesh. We can therefore always find a set of cells, which we denote by  [2.x.120] , that satisfy the following conditions:  [2.x.121]   [2.x.122]  the union of the cells covers the entire domain, and  [2.x.123]  a cell  [2.x.124]  is active on at least   one of the two meshes.  [2.x.125]  A way to construct this set is to take each cell of coarse mesh and do the following steps: (i) if the cell is active on either  [2.x.126]  or  [2.x.127] , then add this cell to the set; (ii) otherwise, i.e. if this cell has children on both meshes, then do step (i) for each of the children of this cell. In fact, deal.II has a function  [2.x.128]  that computes exactly this set of cells that are active on at least one of two meshes. 

With this, we can write above integral as follows: [1.x.77] 

 In the code, we compute the right hand side in the function  [2.x.129] , where (among other things) we loop over the set of common most refined cells, calling the function  [2.x.130]  on each pair of these cells. 

By construction, there are now three cases to be considered:  [2.x.131]   [2.x.132]  The cell  [2.x.133]  is active on both meshes, i.e. both the basis   functions  [2.x.134]  as well as  [2.x.135]  are defined on  [2.x.136] .  [2.x.137]  The cell  [2.x.138]  is active on mesh  [2.x.139] , but not  [2.x.140] , i.e. the    [2.x.141]   are defined on  [2.x.142] , whereas the  [2.x.143]  are defined   on children of  [2.x.144] .  [2.x.145]  The cell  [2.x.146]  is active on mesh  [2.x.147] , but not  [2.x.148] , with opposite   conclusions than in (ii).  [2.x.149]  

To compute the right hand side above, we then need to have different code for these three cases, as follows:  [2.x.150]   [2.x.151]  If the cell  [2.x.152]  is active on both meshes, then we can directly   evaluate the integral. In fact, we don't even have to bother with the basis   functions  [2.x.153] , since all we need is the values of  [2.x.154]  at   the quadrature points. We can do this using the    [2.x.155]  function. This is done directly in   the  [2.x.156]  function. 

 [2.x.157]  If the cell  [2.x.158]  is active on mesh  [2.x.159] , but not  [2.x.160] , then the   basis functions  [2.x.161]  are only defined either on the children    [2.x.162] , or on children of these children if cell  [2.x.163]    is refined more than once on mesh  [2.x.164] . 

  Let us assume for a second that  [2.x.165]  is only once more refined on mesh  [2.x.166]    than on mesh  [2.x.167] . Using the fact that we use embedded finite element spaces   where each basis function on one mesh can be written as a linear combination   of basis functions on the next refined mesh, we can expand the restriction   of  [2.x.168]  to child cell  [2.x.169]  into the basis functions defined on that   child cell (i.e. on cells on which the basis functions  [2.x.170]  are   defined):   [1.x.78] 

  Here, and in the following, summation over indices appearing twice is   implied. The matrix  [2.x.171]  is the matrix that interpolated data from a cell   to its  [2.x.172] -th child. 

  Then we can write the contribution of cell  [2.x.173]  to the right hand side   component  [2.x.174]  as   [1.x.79] 

  In matrix notation, this can be written as   [1.x.80] 

  where  [2.x.175]  is   the weighted mass matrix on child  [2.x.176]  of cell  [2.x.177] . 

  The next question is what happens if a child  [2.x.178]  of  [2.x.179]  is not   active. Then, we have to apply the process recursively, i.e. we have to   interpolate the basis functions  [2.x.180]  onto child  [2.x.181]  of  [2.x.182] , then   onto child  [2.x.183]  of that cell, onto child  [2.x.184]  of that one, etc,   until we find an active cell. We then have to sum up all the contributions   from all the children, grandchildren, etc, of cell  [2.x.185] , with contributions   of the form   [1.x.81] 

  or   [1.x.82] 

  etc. We do this process recursively, i.e. if we sit on cell  [2.x.186]  and see that   it has children on grid  [2.x.187] , then we call a function    [2.x.188]  with an identity matrix; the function will   multiply it's argument from the left with the prolongation matrix; if the   cell has further children, it will call itself with this new matrix,   otherwise it will perform the integration. 

 [2.x.189]  The last case is where  [2.x.190]  is active on mesh  [2.x.191]  but not mesh    [2.x.192] . In that case, we have to express basis function  [2.x.193]  in   terms of the basis functions defined on the children of cell  [2.x.194] , rather   than  [2.x.195]  as before. This of course works in exactly the same   way. If the children of  [2.x.196]  are active on mesh  [2.x.197] , then   leading to the expression   [1.x.83] 

  In matrix notation, this expression now reads as   [1.x.84] 

  and correspondingly for cases where cell  [2.x.198]  is refined more than once on   mesh  [2.x.199] :   [1.x.85] 

  or   [1.x.86] 

  etc. In other words, the process works in exactly the same way as before,   except that we have to take the transpose of the prolongation matrices and   need to multiply it to the mass matrix from the other side.  [2.x.200]  


The expressions for cases (ii) and (iii) can be understood as repeatedly interpolating either the left or right basis functions in the scalar product  [2.x.201]  onto child cells, and then finally forming the inner product (the mass matrix) on the final cell. To make the symmetry in these cases more obvious, we can write them like this: for case (ii), we have [1.x.87] 

whereas for case (iii) we get [1.x.88] 






[1.x.89][1.x.90] 


A nuclear reactor core is composed of different types of assemblies. An assembly is essentially the smallest unit that can be moved in and out of a reactor, and is usually rectangular or square. However, assemblies are not fixed units, as they are assembled from a complex lattice of different fuel rods, control rods, and instrumentation elements that are held in place relative to each other by spacers that are permanently attached to the rods. To make things more complicated, there are different kinds of assemblies that are used at the same time in a reactor, where assemblies differ in the type and arrangement of rods they are made up of. 

Obviously, the arrangement of assemblies as well as the arrangement of rods inside them affect the distribution of neutron fluxes in the reactor (a fact that will be obvious by looking at the solution shown below in the results sections of this program). Fuel rods, for example, differ from each other in the enrichment of U-235 or Pu-239. Control rods, on the other hand, have zero fission, but nonzero scattering and absorption cross sections. 

This whole arrangement would make the description or spatially dependent material parameters very complicated. It will not become much simpler, but we will make one approximation: we merge the volume inhabited by each cylindrical rod and the surrounding water into volumes of quadratic cross section into so-called `pin cells' for which homogenized material data are obtained with nuclear database and knowledge of neutron spectrum. The homogenization makes all material data piecewise constant on the solution domain for a reactor with fresh fuel. Spatially dependent material parameters are then looked up for the quadratic assembly in which a point is located, and then for the quadratic pin cell within this assembly. 

In this tutorial program, we simulate a quarter of a reactor consisting of  [2.x.202]  assemblies. We use symmetry (Neumann) boundary conditions to reduce the problem to one quarter of the domain, and consequently only simulate a  [2.x.203]  set of assemblies. Two of them will be UO [2.x.204]  fuel, the other two of them MOX fuel. Each of these assemblies consists of  [2.x.205]  rods of different compositions. In total, we therefore create a  [2.x.206]  lattice of rods. To make things simpler later on, we reflect this fact by creating a coarse mesh of  [2.x.207]  cells (even though the domain is a square, for which we would usually use a single cell). In deal.II, each cell has a  [2.x.208]  which one may use to associated each cell with a particular number identifying the material from which this cell's volume is made of; we will use this material ID to identify which of the 8 different kinds of rods that are used in this testcase make up a particular cell. Note that upon mesh refinement, the children of a cell inherit the material ID, making it simple to track the material even after mesh refinement. 

The arrangement of the rods will be clearly visible in the images shown in the results section. The cross sections for materials and for both energy groups are taken from a OECD/NEA benchmark problem. The detailed configuration and material data is given in the code. 


[1.x.91][1.x.92] 


As a coarse overview of what exactly the program does, here is the basic layout: starting on a coarse mesh that is the same for each energy group, we compute inverse eigenvalue iterations to compute the  [2.x.209] -eigenvalue on a given set of meshes. We stop these iterations when the change in the eigenvalue drops below a certain tolerance, and then write out the meshes and solutions for each energy group for inspection by a graphics program. Because the meshes for the solutions are different, we have to generate a separate output file for each energy group, rather than being able to add all energy group solutions into the same file. 

After this, we evaluate the error indicators as explained in one of the sections above for each of the meshes, and refine and coarsen the cells of each mesh independently. Since the eigenvalue iterations are fairly expensive, we don't want to start all over on the new mesh; rather, we use the SolutionTransfer class to interpolate the solution on the previous mesh to the next one upon mesh refinement. A simple experiment will convince you that this is a lot cheaper than if we omitted this step. After doing so, we resume our eigenvalue iterations on the next set of meshes. 

The program is controlled by a parameter file, using the ParameterHandler class. We will show a parameter file in the results section of this tutorial. For the moment suffice it to say that it controls the polynomial degree of the finite elements used, the number of energy groups (even though all that is presently implemented are the coefficients for a 2-group problem), the tolerance where to stop the inverse eigenvalue iteration, and the number of refinement cycles we will do. [1.x.93] [1.x.94] 


[1.x.95]  [1.x.96] 




We start with a bunch of include files that have already been explained in previous tutorial programs. One new one is  [2.x.210] : This is the first example program that uses the Timer class. The Timer keeps track of both the elapsed wall clock time (that is, the amount of time that a clock mounted on the wall would measure) and CPU clock time (the amount of time that the current process uses on the CPUs). We will use a Timer below to measure how much CPU time each grid refinement cycle takes. 

[1.x.97] 



We use the next include file to access block vectors which provide us a convenient way to manage solution and right hand side vectors of all energy groups: 

[1.x.98] 



This include file is for transferring solutions from one mesh to another different mesh. We use it when we are initializing solutions after each mesh iteration: 

[1.x.99] 



When integrating functions defined on one mesh against shape functions defined on a different mesh, we need a function  [2.x.211]  (as discussed in the introduction) which is defined in the following header file: 

[1.x.100] 



We use a little utility class from boost to save the state of an output stream (see the  [2.x.212]  function below): 

[1.x.101] 



Here are two more C++ standard headers that we use to define list data types as well as to fine-tune the output we generate: 

[1.x.102] 



The last step is as in all previous programs: 

[1.x.103] 




[1.x.104]  [1.x.105] 




First up, we need to define a class that provides material data (including diffusion coefficients, removal cross sections, scattering cross sections, fission cross sections and fission spectra) to the main class.    


The parameter to the constructor determines for how many energy groups we set up the relevant tables. At present, this program only includes data for 2 energy groups, but a more sophisticated program may be able to initialize the data structures for more groups as well, depending on how many energy groups are selected in the parameter file.    


For each of the different coefficient types, there is one function that returns the value of this coefficient for a particular energy group (or combination of energy groups, as for the distribution cross section  [2.x.213]  or scattering cross section  [2.x.214] ). In addition to the energy group or groups, these coefficients depend on the type of fuel or control rod, as explained in the introduction. The functions therefore take an additional parameter, @p material_id, that identifies the particular kind of rod. Within this program, we use  [2.x.215]  different kinds of rods.    


Except for the scattering cross section, each of the coefficients therefore can be represented as an entry in a two-dimensional array of floating point values indexed by the energy group number as well as the material ID. The Table class template is the ideal way to store such data. Finally, the scattering coefficient depends on both two energy group indices and therefore needs to be stored in a three-dimensional array, for which we again use the Table class, where this time the first template argument (denoting the dimensionality of the array) of course needs to be three: 

[1.x.106] 



The constructor of the class is used to initialize all the material data arrays. It takes the number of energy groups as an argument (an throws an error if that value is not equal to two, since at presently only data for two energy groups is implemented; however, using this, the function remains flexible and extendable into the future). In the member initialization part at the beginning, it also resizes the arrays to their correct sizes.    


At present, material data is stored for 8 different types of material. This, as well, may easily be extended in the future. 

[1.x.107] 



Next are the functions that return the coefficient values for given materials and energy groups. All they do is to make sure that the given arguments are within the allowed ranges, and then look the respective value up in the corresponding tables: 

[1.x.108] 



The function computing the fission distribution cross section is slightly different, since it computes its value as the product of two other coefficients. We don't need to check arguments here, since this already happens when we call the two other functions involved, even though it would probably not hurt either: 

[1.x.109] 




[1.x.110]  [1.x.111] 




The first interesting class is the one that contains everything that is specific to a single energy group. To group things that belong together into individual objects, we declare a structure that holds the Triangulation and DoFHandler objects for the mesh used for a single energy group, and a number of other objects and member functions that we will discuss in the following sections.    


The main reason for this class is as follows: for both the forward problem (with a specified right hand side) as well as for the eigenvalue problem, one typically solves a sequence of problems for a single energy group each, rather than the fully coupled problem. This becomes understandable once one realizes that the system matrix for a single energy group is symmetric and positive definite (it is simply a diffusion operator), whereas the matrix for the fully coupled problem is generally nonsymmetric and not definite. It is also very large and quite full if more than a few energy groups are involved.    


Let us first look at the equation to solve in the case of an external right hand side (for the time independent case): [1.x.112] 

   


We would typically solve this equation by moving all the terms on the right hand side with  [2.x.216]  to the left hand side, and solving for  [2.x.217] . Of course, we don't know  [2.x.218]  yet, since the equations for those variables include right hand side terms involving  [2.x.219] . What one typically does in such situations is to iterate: compute [1.x.113] 

   


In other words, we solve the equation one by one, using values for  [2.x.220]  from the previous iteration  [2.x.221]  if  [2.x.222]  and already computed values for  [2.x.223]  from the present iteration if  [2.x.224] .    


When computing the eigenvalue, we do a very similar iteration, except that we have no external right hand side and that the solution is scaled after each iteration as explained in the introduction.    


In either case, these two cases can be treated jointly if all we do is to equip the following class with these abilities: (i) form the left hand side matrix, (ii) form the in-group right hand side contribution, i.e. involving the extraneous source, and (iii) form that contribution to the right hand side that stems from group  [2.x.225] . This class does exactly these tasks (as well as some book-keeping, such as mesh refinement, setting up matrices and vectors, etc). On the other hand, the class itself has no idea how many energy groups there are, and in particular how they interact, i.e. the decision of how the outer iteration looks (and consequently whether we solve an eigenvalue or a direct problem) is left to the NeutronDiffusionProblem class further down below in this program.    


So let us go through the class and its interface: 

[1.x.114] 




[1.x.115]  [1.x.116]      


The class has a good number of public member functions, since its the way it operates is controlled from the outside, and therefore all functions that do something significant need to be called from another class. Let's start off with book-keeping: the class obviously needs to know which energy group it represents, which material data to use, and from what coarse grid to start. The constructor takes this information and initializes the relevant member variables with that (see below).      


Then we also need functions that set up the linear system, i.e. correctly size the matrix and its sparsity pattern, etc, given a finite element object to use. The  [2.x.226]  function does that. Finally, for this initial block, there are two functions that return the number of active cells and degrees of freedom used in this object -- using this, we can make the triangulation and DoF handler member variables private, and do not have to grant external use to it, enhancing encapsulation: 

[1.x.117] 



Then there are functions that assemble the linear system for each iteration and the present energy group. Note that the matrix is independent of the iteration number, so only has to be computed once for each refinement cycle. The situation is a bit more involved for the right hand side that has to be updated in each inverse power iteration, and that is further complicated by the fact that computing it may involve several different meshes as explained in the introduction. To make things more flexible with regard to solving the forward or the eigenvalue problem, we split the computation of the right hand side into a function that assembles the extraneous source and in-group contributions (which we will call with a zero function as source terms for the eigenvalue problem) and one that computes contributions to the right hand side from another energy group: 

[1.x.118] 



Next we need a set of functions that actually compute the solution of a linear system, and do something with it (such as computing the fission source contribution mentioned in the introduction, writing graphical information to an output file, computing error indicators, or actually refining the grid based on these criteria and thresholds for refinement and coarsening). All these functions will later be called from the driver class  [2.x.227] , or any other class you may want to implement to solve a problem involving the neutron flux equations: 

[1.x.119] 




[1.x.120]  [1.x.121]      


As is good practice in object oriented programming, we hide most data members by making them private. However, we have to grant the class that drives the process access to the solution vector as well as the solution of the previous iteration, since in the power iteration, the solution vector is scaled in every iteration by the present guess of the eigenvalue we are looking for: 

[1.x.122] 




[1.x.123]  [1.x.124]      


The rest of the data members are private. Compared to all the previous tutorial programs, the only new data members are an integer storing which energy group this object represents, and a reference to the material data object that this object's constructor gets passed from the driver class. Likewise, the constructor gets a reference to the finite element object we are to use.      


Finally, we have to apply boundary values to the linear system in each iteration, i.e. quite frequently. Rather than interpolating them every time, we interpolate them once on each new mesh and then store them along with all the other data of this class: 

[1.x.125] 




[1.x.126]  [1.x.127]      


There is one private member function in this class. It recursively walks over cells of two meshes to compute the cross-group right hand side terms. The algorithm for this is explained in the introduction to this program. The arguments to this function are a reference to an object representing the energy group against which we want to integrate a right hand side term, an iterator to a cell of the mesh used for the present energy group, an iterator to a corresponding cell on the other mesh, and the matrix that interpolates the degrees of freedom from the coarser of the two cells to the finer one: 

[1.x.128] 




[1.x.129]  [1.x.130] 




The first few functions of this class are mostly self-explanatory. The constructor only sets a few data members and creates a copy of the given triangulation as the base for the triangulation used for this energy group. The next two functions simply return data from private data members, thereby enabling us to make these data members private. 

[1.x.131] 




[1.x.132]  [1.x.133]    


The first "real" function is the one that sets up the mesh, matrices, etc, on the new mesh or after mesh refinement. We use this function to initialize sparse system matrices, and the right hand side vector. If the solution vector has never been set before (as indicated by a zero size), we also initialize it and set it to a default value. We don't do that if it already has a non-zero size (i.e. this function is called after mesh refinement) since in that case we want to preserve the solution across mesh refinement (something we do in the  [2.x.228]  function). 

[1.x.134] 



At the end of this function, we update the list of boundary nodes and their values, by first clearing this list and the re-interpolating boundary values (remember that this function is called after first setting up the mesh, and each time after mesh refinement).      


To understand the code, it is necessary to realize that we create the mesh using the  [2.x.229]  function (in  [2.x.230] ) where we set the last parameter to  [2.x.231] . This means that boundaries of the domain are "colored", i.e. the four (or six, in 3d) sides of the domain are assigned different boundary indicators. As it turns out, the bottom boundary gets indicator zero, the top one boundary indicator one, and left and right boundaries get indicators two and three, respectively.      


In this program, we simulate only one, namely the top right, quarter of a reactor. That is, we want to interpolate boundary conditions only on the top and right boundaries, while do nothing on the bottom and left boundaries (i.e. impose natural, no-flux Neumann boundary conditions). This is most easily generalized to arbitrary dimension by saying that we want to interpolate on those boundaries with indicators 1, 3, ..., which we do in the following loop (note that calls to  [2.x.232]  are additive, i.e. they do not first clear the boundary value map): 

[1.x.135] 




[1.x.136]  [1.x.137]    


Next we need functions assembling the system matrix and right hand sides. Assembling the matrix is straightforward given the equations outlined in the introduction as well as what we've seen in previous example programs. Note the use of  [2.x.233]  to get at the kind of material from which a cell is made up of. Note also how we set the order of the quadrature formula so that it is always appropriate for the finite element in use.    


Finally, note that since we only assemble the system matrix here, we can't yet eliminate boundary values (we need the right hand side vector for this). We defer this to the  [2.x.234]  function, at which point all the information is available. 

[1.x.138] 




[1.x.139]  [1.x.140]    


As explained in the documentation of the  [2.x.235]  class, we split assembling the right hand side into two parts: the ingroup and the cross-group couplings. First, we need a function to assemble the right hand side of one specific group here, i.e. including an extraneous source (that we will set to zero for the eigenvalue problem) as well as the ingroup fission contributions.  (In-group scattering has already been accounted for with the definition of removal cross section.) The function's workings are pretty standard as far as assembling right hand sides go, and therefore does not require more comments except that we mention that the right hand side vector is set to zero at the beginning of the function -- something we are not going to do for the cross-group terms that simply add to the right hand side vector. 

[1.x.141] 




[1.x.142]  [1.x.143]    


The more interesting function for assembling the right hand side vector for the equation of a single energy group is the one that couples energy group  [2.x.236]  and  [2.x.237] . As explained in the introduction, we first have to find the set of cells common to the meshes of the two energy groups. First we call  [2.x.238]  to obtain this list of pairs of common cells from both meshes. Both cells in a pair may not be active but at least one of them is. We then hand each of these cell pairs off to a function that computes the right hand side terms recursively.    


Note that ingroup coupling is handled already before, so we exit the function early if  [2.x.239] . 

[1.x.144] 




[1.x.145]  [1.x.146]    


This is finally the function that handles assembling right hand side terms on potentially different meshes recursively, using the algorithm described in the introduction. The function takes a reference to the object representing energy group  [2.x.240] , as well as iterators to corresponding cells in the meshes for energy groups  [2.x.241]  and  [2.x.242] . At first, i.e. when this function is called from the one above, these two cells will be matching cells on two meshes; however, one of the two may be further refined, and we will call the function recursively with one of the two iterators replaced by one of the children of the original cell.    


The last argument is the matrix product matrix  [2.x.243]  from the introduction that interpolates from the coarser of the two cells to the finer one. If the two cells match, then this is the identity matrix -- exactly what we pass to this function initially.    


The function has to consider two cases: that both of the two cells are not further refined, i.e. have no children, in which case we can finally assemble the right hand side contributions of this pair of cells; and that one of the two cells is further refined, in which case we have to keep recursing by looping over the children of the one cell that is not active. These two cases will be discussed below: 

[1.x.147] 



The first case is that both cells are no further refined. In that case, we can assemble the relevant terms (see the introduction). This involves assembling the mass matrix on the finer of the two cells (in fact there are two mass matrices with different coefficients, one for the fission distribution cross section  [2.x.244]  and one for the scattering cross section  [2.x.245] ). This is straight forward, but note how we determine which of the two cells is the finer one by looking at the refinement level of the two cells: 

[1.x.148] 



Now we have all the interpolation (prolongation) matrices as well as local mass matrices, so we only have to form the product [1.x.149] or [1.x.150] depending on which of the two cells is the finer. We do this using either the matrix-vector product provided by the  [2.x.246]  function, or the product with the transpose matrix using  [2.x.247] . After doing so, we transfer the result into the global right hand side vector of energy group  [2.x.248] . 

[1.x.151] 



The alternative is that one of the two cells is further refined. In that case, we have to loop over all the children, multiply the existing interpolation (prolongation) product of matrices from the left with the interpolation from the present cell to its child (using the matrix-matrix multiplication function  [2.x.249] ), and then hand the result off to this very same function again, but with the cell that has children replaced by one of its children: 

[1.x.152] 




[1.x.153]  [1.x.154]    


In the (inverse) power iteration, we use the integrated fission source to update the  [2.x.250] -eigenvalue. Given its definition, the following function is essentially self-explanatory: 

[1.x.155] 




[1.x.156]  [1.x.157]    


Next a function that solves the linear system assembled before. Things are pretty much standard, except that we delayed applying boundary values until we get here, since in all the previous functions we were still adding up contributions the right hand side vector. 

[1.x.158] 




[1.x.159]  [1.x.160]    


Mesh refinement is split into two functions. The first estimates the error for each cell, normalizes it by the magnitude of the solution, and returns it in the vector given as an argument. The calling function collects all error indicators from all energy groups, and computes thresholds for refining and coarsening cells. 

[1.x.161] 




[1.x.162]  [1.x.163]    


The second part is to refine the grid given the error indicators compute in the previous function and error thresholds above which cells shall be refined or below which cells shall be coarsened. Note that we do not use any of the functions in  [2.x.251]  here, but rather set refinement flags ourselves.    


After setting these flags, we use the SolutionTransfer class to move the solution vector from the old to the new mesh. The procedure used here is described in detail in the documentation of that class: 

[1.x.164] 



enforce constraints to make the interpolated solution conforming on the new mesh: 

[1.x.165] 




[1.x.166]  [1.x.167]    


The last function of this class outputs meshes and solutions after each mesh iteration. This has been shown many times before. The only thing worth pointing out is the use of the  [2.x.252]  function to convert an integer into its string representation. The second argument of that function denotes how many digits we shall use -- if this value was larger than one, then the number would be padded by leading zeros. 

[1.x.168] 




[1.x.169]  [1.x.170] 




This is the main class of the program, not because it implements all the functionality (in fact, most of it is implemented in the  [2.x.253]  class) but because it contains the driving algorithm that determines what to compute and when. It is mostly as shown in many of the other tutorial programs in that it has a public  [2.x.254]  function and private functions doing all the rest. In several places, we have to do something for all energy groups, in which case we will start tasks for each group to let these things run in parallel if deal.II was configured for multithreading.  For strategies of parallelization, take a look at the  [2.x.255]  module.    


The biggest difference to previous example programs is that we also declare a nested class that has member variables for all the run-time parameters that can be passed to the program in an input file. Right now, these are the number of energy groups, the number of refinement cycles, the polynomial degree of the finite element to be used, and the tolerance used to determine when convergence of the inverse power iteration has occurred. In addition, we have a constructor of this class that sets all these values to their default values, a function  [2.x.256]  that describes to the ParameterHandler class what parameters are accepted in the input file, and a function  [2.x.257]  that can extract the values of these parameters from a ParameterHandler object. See also  [2.x.258]  for another example of using ParameterHandler. 

[1.x.171] 




[1.x.172]  [1.x.173] 




There are not that many member functions in this class since most of the functionality has been moved into the  [2.x.259]  class and is simply called from the  [2.x.260]  member function of this class. The ones that remain have self-explanatory names: 

[1.x.174] 




[1.x.175]  [1.x.176] 




Next, we have a few member variables. In particular, these are (i) a reference to the parameter object (owned by the main function of this program, and passed to the constructor of this class), (ii) an object describing the material parameters for the number of energy groups requested in the input file, and (iii) the finite element to be used by all energy groups: 

[1.x.177] 



Furthermore, we have (iv) the value of the computed eigenvalue at the present iteration. This is, in fact, the only part of the solution that is shared between all energy groups -- all other parts of the solution, such as neutron fluxes are particular to one or the other energy group, and are therefore stored in objects that describe a single energy group: 

[1.x.178] 



The last computational object (v) is an array of pointers to the energy group objects. The length of this array is, of course, equal to the number of energy groups specified in the parameter file. 

[1.x.179] 



Finally (vi) we have a file stream to which we will save summarized output. 

[1.x.180] 




[1.x.181]  [1.x.182] 




Before going on to the implementation of the outer class, we have to implement the functions of the parameters structure. This is pretty straightforward and, in fact, looks pretty much the same for all such parameters classes using the ParameterHandler capabilities. We will therefore not comment further on this: 

[1.x.183] 




[1.x.184]  [1.x.185] 




Now for the  [2.x.261]  class. The constructor and destructor have nothing of much interest: 

[1.x.186] 




[1.x.187]  [1.x.188]    


The first function of interest is the one that sets up the geometry of the reactor core. This is described in more detail in the introduction.    


The first part of the function defines geometry data, and then creates a coarse mesh that has as many cells as there are fuel rods (or pin cells, for that matter) in that part of the reactor core that we simulate. As mentioned when interpolating boundary values above, the last parameter to the  [2.x.262]  function specifies that sides of the domain shall have unique boundary indicators that will later allow us to determine in a simple way which of the boundaries have Neumann and which have Dirichlet conditions attached to them. 

[1.x.189] 



The second part of the function deals with material numbers of pin cells of each type of assembly. Here, we define four different types of assembly, for which we describe the arrangement of fuel rods in the following tables.      


The assemblies described here are taken from the benchmark mentioned in the introduction and are (in this order):  [2.x.263]   [2.x.264] 'UX' Assembly: UO2 fuel assembly with 24 guide tubes and a central Moveable Fission Chamber  [2.x.265] 'UA' Assembly: UO2 fuel assembly with 24 AIC and a central Moveable Fission Chamber  [2.x.266] 'PX' Assembly: MOX fuel assembly with 24 guide tubes and a central Moveable Fission Chamber  [2.x.267] 'R' Assembly: a reflector.   [2.x.268]       


Note that the numbers listed here and taken from the benchmark description are, in good old Fortran fashion, one-based. We will later subtract one from each number when assigning materials to individual cells to convert things into the C-style zero-based indexing. 

[1.x.190] 



After the description of the materials that make up an assembly, we have to specify the arrangement of assemblies within the core. We use a symmetric pattern that in fact only uses the 'UX' and 'PX' assemblies: 

[1.x.191] 



We are now in a position to actually set material IDs for each cell. To this end, we loop over all cells, look at the location of the cell's center, and determine which assembly and fuel rod this would be in. (We add a few checks to see that the locations we compute are within the bounds of the arrays in which we have to look up materials.) At the end of the loop, we set material identifiers accordingly: 

[1.x.192] 



With the coarse mesh so initialized, we create the appropriate number of energy group objects and let them initialize their individual meshes with the coarse mesh generated above: 

[1.x.193] 




[1.x.194]  [1.x.195]    


In the eigenvalue computation, we need to calculate total fission neutron source after each power iteration. The total power then is used to renew k-effective.    


Since the total fission source is a sum over all the energy groups, and since each of these sums can be computed independently, we actually do this in parallel. One of the problems is that the function in the  [2.x.269]  class that computes the fission source returns a value. We would like to add these values together in the loop itself: ideally, each task would compute its value and then immediately add it to the total. Concurrently summing values in this way requires two features:  [2.x.270]   [2.x.271] We need a way of storing a value such that multiple threads can read and write into concurrently in a way that prevents data races (i.e., thread-safe reading and writing). [2.x.272]   [2.x.273] We need a way to increment such a value that is also thread-safe. [2.x.274]   [2.x.275]     


The first feature is available through the template class  [2.x.276] . However, the second feature, implemented by  [2.x.277] , is only available in C++20 and later: since deal.II supports older versions of the C++ language standard we cannot use this feature yet. Hence, instead, we simply write each group's value out to an entry in a vector and sum the values at the end of the function. 

[1.x.196] 




[1.x.197]  [1.x.198]    


The next function lets the individual energy group objects refine their meshes. Much of this, again, is a task that can be done independently in parallel: first, let all the energy group objects calculate their error indicators in parallel, then compute the maximum error indicator over all energy groups and determine thresholds for refinement and coarsening of cells, and then ask all the energy groups to refine their meshes accordingly, again in parallel. 

[1.x.199] 



The destructor of  [2.x.278]  joins all threads so we know that the computation is done by the time we exit the scope. 







[1.x.200] 




[1.x.201]  [1.x.202]    


Finally, this is the function where the meat is: iterate on a sequence of meshes, and on each of them do a power iteration to compute the eigenvalue.    


Given the description of the algorithm in the introduction, there is actually not much to comment on: 

[1.x.203] 



We would like to change the output precision for just this function and restore the state of  [2.x.279]  when this function returns. Hence, we need a way to undo the output format change. Boost provides a convenient way to save the state of an output stream and restore it at the end of the current block (when the destructor of  [2.x.280]  is called) with the  [2.x.281]  class, which we use here. 

[1.x.204] 



We calculate the error below by the change in k_eff (i.e., the difference between k_eff_old, 

[1.x.205] 



We will measure the CPU time that each cycle takes below. The constructor for Timer calls  [2.x.282]  so once we create a timer we can query it for information. Since many parts of this loop are parallelized with tasks, the CPU time we measure (if we run with more than one thread) will be larger than the wall time. 

[1.x.206] 



Print out information about the simulation as well as the elapsed CPU time. We can call  [2.x.283]  without first calling  [2.x.284]  to get the elapsed CPU time at the point of calling the function. 

[1.x.207] 




[1.x.208]  [1.x.209] 




The last thing in the program in the  [2.x.285]  function. The structure is as in most other tutorial programs, with the only exception that we here handle a parameter file.  To this end, we first look at the command line arguments passed to this function: if no input file is specified on the command line, then use "project.prm", otherwise take the filename given as the first argument on the command line. 




With this, we create a ParameterHandler object, let the  [2.x.286]  class declare all the parameters it wants to see in the input file (or, take the default values, if nothing is listed in the parameter file), then read the input file, ask the parameters object to extract the values, and finally hand everything off to an object of type  [2.x.287]  for computation of the eigenvalue: 

[1.x.210] 

[1.x.211][1.x.212] 


We can run the program with the following input file: 

[1.x.213] 

The output of this program then consists of the console output, a file named `convergence_table' to record main results of mesh iteration, and the graphical output in vtu format. 

The console output looks like this: 

[1.x.214] 



We see that power iteration does converge faster after cycle 0 due to the initialization with solution from last mesh iteration. The contents of `convergence_table' are, 

[1.x.215] 

The meanings of columns are: number of mesh iteration, numbers of degrees of  freedom of fast energy group, numbers of DoFs of thermal group, converged k-effective and the ratio between maximum of fast flux and maximum of thermal one. 

The grids of fast and thermal energy groups at mesh iteration #9 look as follows: 

 [2.x.288]  &nbsp;  [2.x.289]  

We see that the grid of thermal group is much finer than the one of fast group. The solutions on these grids are, (Note: flux are normalized with total fission source equal to 1) 

 [2.x.290]  &nbsp;  [2.x.291]  

Then we plot the convergence data with polynomial order being equal to 1,2 and 3. 

 [2.x.292]  

The estimated `exact' k-effective = 0.906834721253 which is simply from last mesh iteration of polynomial order 3 minus 2e-10. We see that h-adaptive calculations deliver an algebraic convergence. And the higher polynomial order is, the faster mesh iteration converges. In our problem, we need smaller number of DoFs to achieve same accuracy with higher polynomial order. [1.x.216] [1.x.217]  [2.x.293]  

 [2.x.294] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22] 

 [2.x.3]  

[1.x.23] 

[1.x.24] In order to run this program, deal.II must be configured to use the UMFPACK sparse direct solver. Refer to the [1.x.25] for instructions how to do this. 


[1.x.26] [1.x.27][1.x.28] 




A question that comes up frequently is how to solve problems involving complex valued functions with deal.II. For many problems, instead of working with complex valued finite elements directly, it is often more convenient to split complex valued functions into their real and imaginary parts and use separate scalar finite element fields for discretizing each one of them. Basically this amounts to viewing a single complex valued equation as a system of two real valued equations. This short example demonstrates how this can be implemented in deal.II by using an  [2.x.4]  object to stack two finite element fields representing real and imaginary parts. (The opposite approach, keeping everything complex-valued, is demonstrated in a different tutorial program: see  [2.x.5]  for this.) When split into real and imaginary parts, the equations covered here fall into the class of vector-valued problems. A toplevel overview of this topic can be found in the  [2.x.6]  module. 

In addition to this discussion, we also discuss the ParameterHandler class, which provides a convenient way for reading parameters from a configuration file at runtime without the need to recompile the program code. 


[1.x.29][1.x.30] 


The original purpose of this program is to simulate the focusing properties of an ultrasound wave generated by a transducer lens with variable geometry. Recent applications in medical imaging use ultrasound waves not only for imaging purposes, but also to excite certain local effects in a material, like changes in optical properties, that can then be measured by other imaging techniques. A vital ingredient for these methods is the ability to focus the intensity of the ultrasound wave in a particular part of the material, ideally in a point, to be able to examine the properties of the material at that particular location. 

To derive a model for this problem, we think of ultrasound as a pressure wave governed by the wave equation: [1.x.31] where  [2.x.7]  is the wave speed (that for simplicity we assume to be constant),  [2.x.8] . The boundary  [2.x.9]  is divided into two parts  [2.x.10]  and  [2.x.11] , with  [2.x.12]  representing the transducer lens and  [2.x.13]  an absorbing boundary (that is, we want to choose boundary conditions on  [2.x.14]  in such a way that they imitate a larger domain). On  [2.x.15] , the transducer generates a wave of constant frequency  [2.x.16]  and constant amplitude (that we chose to be 1 here): [1.x.32] 

If there are no other (interior or boundary) sources, and since the only source has frequency  [2.x.17] , then the solution admits a separation of variables of the form  [2.x.18] . The complex-valued function  [2.x.19]  describes the spatial dependency of amplitude and phase (relative to the source) of the waves of frequency  [2.x.20] , with the amplitude being the quantity that we are interested in. By plugging this form of the solution into the wave equation, we see that for  [2.x.21]  we have [1.x.33] 



For finding suitable conditions on  [2.x.22]  that model an absorbing boundary, consider a wave of the form  [2.x.23]  with frequency  [2.x.24]  traveling in direction  [2.x.25] . In order for  [2.x.26]  to solve the wave equation,  [2.x.27]  must hold. Suppose that this wave hits the boundary in  [2.x.28]  at a right angle, i.e.  [2.x.29]  with  [2.x.30]  denoting the outer unit normal of  [2.x.31]  in  [2.x.32] . Then at  [2.x.33] , this wave satisfies the equation [1.x.34] Hence, by enforcing the boundary condition [1.x.35] waves that hit the boundary  [2.x.34]  at a right angle will be perfectly absorbed. On the other hand, those parts of the wave field that do not hit a boundary at a right angle do not satisfy this condition and enforcing it as a boundary condition will yield partial reflections, i.e. only parts of the wave will pass through the boundary as if it wasn't here whereas the remaining fraction of the wave will be reflected back into the domain. 

If we are willing to accept this as a sufficient approximation to an absorbing boundary we finally arrive at the following problem for  [2.x.35] : [1.x.36] 

This is a Helmholtz equation (similar to the one in  [2.x.36] , but this time with ''the bad sign'') with Dirichlet data on  [2.x.37]  and mixed boundary conditions on  [2.x.38] . Because of the condition on  [2.x.39] , we cannot just treat the equations for real and imaginary parts of  [2.x.40]  separately. What we can do however is to view the PDE for  [2.x.41]  as a system of two PDEs for the real and imaginary parts of  [2.x.42] , with the boundary condition on  [2.x.43]  representing the coupling terms between the two components of the system. This works along the following lines: Let  [2.x.44] , then in terms of  [2.x.45]  and  [2.x.46]  we have the following system: [1.x.37] 



For test functions  [2.x.47]  with  [2.x.48] , after the usual multiplication, integration over  [2.x.49]  and applying integration by parts, we get the weak formulation [1.x.38] 



We choose finite element spaces  [2.x.50]  and  [2.x.51]  with bases  [2.x.52]  and look for approximate solutions [1.x.39] Plugging into the variational form yields the equation system [1.x.40] In matrix notation: [1.x.41] (One should not be fooled by the right hand side being zero here, that is because we haven't included the Dirichlet boundary data yet.) Because of the alternating sign in the off-diagonal blocks, we can already see that this system is non-symmetric, in fact it is even indefinite. Of course, there is no necessity to choose the spaces  [2.x.53]  and  [2.x.54]  to be the same. However, we expect real and imaginary part of the solution to have similar properties and will therefore indeed take  [2.x.55]  in the implementation, and also use the same basis functions  [2.x.56]  for both spaces. The reason for the notation using different symbols is just that it allows us to distinguish between shape functions for  [2.x.57]  and  [2.x.58] , as this distinction plays an important role in the implementation. 


[1.x.42][1.x.43] 


For the computations, we will consider wave propagation in the unit square, with ultrasound generated by a transducer lens that is shaped like a segment of the circle with center at  [2.x.59]  and a radius slightly greater than  [2.x.60] ; this shape should lead to a focusing of the sound wave at the center of the circle. Varying  [2.x.61]  changes the "focus" of the lens and affects the spatial distribution of the intensity of  [2.x.62] , where our main concern is how well  [2.x.63]  is focused. 

In the program below, we will implement the complex-valued Helmholtz equations using the formulation with split real and imaginary parts. We will also discuss how to generate a domain that looks like a square with a slight bulge simulating the transducer (in the  [2.x.64]  function), and how to generate graphical output that not only contains the solution components  [2.x.65]  and  [2.x.66] , but also the magnitude  [2.x.67]  directly in the output file (in  [2.x.68] ). Finally, we use the ParameterHandler class to easily read parameters like the focal distance  [2.x.69] , wave speed  [2.x.70] , frequency  [2.x.71] , and a number of other parameters from an input file at run-time, rather than fixing those parameters in the source code where we would have to re-compile every time we want to change parameters. [1.x.44] [1.x.45] 


[1.x.46]  [1.x.47] 




The following header files have all been discussed before: 







[1.x.48] 



This header file contains the necessary declarations for the ParameterHandler class that we will use to read our parameters from a configuration file: 

[1.x.49] 



For solving the linear system, we'll use the sparse LU-decomposition provided by UMFPACK (see the SparseDirectUMFPACK class), for which the following header file is needed.  Note that in order to compile this tutorial program, the deal.II-library needs to be built with UMFPACK support, which is enabled by default: 

[1.x.50] 



The FESystem class allows us to stack several FE-objects to one compound, vector-valued finite element field. The necessary declarations for this class are provided in this header file: 

[1.x.51] 



Finally, include the header file that declares the Timer class that we will use to determine how much time each of the operations of our program takes: 

[1.x.52] 



As the last step at the beginning of this program, we put everything that is in this program into its namespace and, within it, make everything that is in the deal.II namespace globally available, without the need to prefix everything with  [2.x.72] : 

[1.x.53] 




[1.x.54]  [1.x.55] 




First we define a class for the function representing the Dirichlet boundary values. This has been done many times before and therefore does not need much explanation.    


Since there are two values  [2.x.73]  and  [2.x.74]  that need to be prescribed at the boundary, we have to tell the base class that this is a vector-valued function with two components, and the  [2.x.75]  function and its cousin  [2.x.76]  must return vectors with two entries. In our case the function is very simple, it just returns 1 for the real part  [2.x.77]  and 0 for the imaginary part  [2.x.78]  regardless of the point where it is evaluated. 

[1.x.56] 




[1.x.57]  [1.x.58] 




The next class is responsible for preparing the ParameterHandler object and reading parameters from an input file.  It includes a function  [2.x.79]  that declares all the necessary parameters and a  [2.x.80]  function that is called from outside to initiate the parameter reading process. 

[1.x.59] 



The constructor stores a reference to the ParameterHandler object that is passed to it: 

[1.x.60] 




[1.x.61]  [1.x.62] 




The  [2.x.81]  function declares all the parameters that our ParameterHandler object will be able to read from input files, along with their types, range conditions and the subsections they appear in. We will wrap all the entries that go into a section in a pair of braces to force the editor to indent them by one level, making it simpler to read which entries together form a section: 

[1.x.63] 



Parameters for mesh and geometry include the number of global refinement steps that are applied to the initial coarse mesh and the focal distance  [2.x.82]  of the transducer lens. For the number of refinement steps, we allow integer values in the range  [2.x.83] , where the omitted second argument to the  [2.x.84]  object denotes the half-open interval.  For the focal distance any number greater than zero is accepted: 

[1.x.64] 



The next subsection is devoted to the physical parameters appearing in the equation, which are the frequency  [2.x.85]  and wave speed  [2.x.86] . Again, both need to lie in the half-open interval  [2.x.87]  represented by calling the  [2.x.88]  class with only the left end-point as argument: 

[1.x.65] 



Last but not least we would like to be able to change some properties of the output, like filename and format, through entries in the configuration file, which is the purpose of the last subsection: 

[1.x.66] 



Since different output formats may require different parameters for generating output (like for example, postscript output needs viewpoint angles, line widths, colors etc), it would be cumbersome if we had to declare all these parameters by hand for every possible output format supported in the library. Instead, each output format has a  [2.x.89]  function, which declares all the parameters specific to that format in an own subsection. The following call of  [2.x.90]  executes  [2.x.91]  for all available output formats, so that for each format an own subsection will be created with parameters declared for that particular output format. (The actual value of the template parameter in the call,  [2.x.92]  above, does not matter here: the function does the same work independent of the dimension, but happens to be in a template-parameter-dependent class.)  To find out what parameters there are for which output format, you can either consult the documentation of the DataOutBase class, or simply run this program without a parameter file present. It will then create a file with all declared parameters set to their default values, which can conveniently serve as a starting point for setting the parameters to the values you desire. 

[1.x.67] 




[1.x.68]  [1.x.69] 




This is the main function in the ParameterReader class.  It gets called from outside, first declares all the parameters, and then reads them from the input file whose filename is provided by the caller. After the call to this function is complete, the  [2.x.93]  object can be used to retrieve the values of the parameters read in from the file: 

[1.x.70] 




[1.x.71]  [1.x.72] 




As mentioned in the introduction, the quantity that we are really after is the spatial distribution of the intensity of the ultrasound wave, which corresponds to  [2.x.94] . Now we could just be content with having  [2.x.95]  and  [2.x.96]  in our output, and use a suitable visualization or postprocessing tool to derive  [2.x.97]  from the solution we computed. However, there is also a way to output data derived from the solution in deal.II, and we are going to make use of this mechanism here. 




So far we have always used the  [2.x.98]  function to add vectors containing output data to a DataOut object.  There is a special version of this function that in addition to the data vector has an additional argument of type DataPostprocessor. What happens when this function is used for output is that at each point where output data is to be generated, the  [2.x.99]  or  [2.x.100]  function of the specified DataPostprocessor object is invoked to compute the output quantities from the values, the gradients and the second derivatives of the finite element function represented by the data vector (in the case of face related data, normal vectors are available as well). Hence, this allows us to output any quantity that can locally be derived from the values of the solution and its derivatives.  Of course, the ultrasound intensity  [2.x.101]  is such a quantity and its computation doesn't even involve any derivatives of  [2.x.102]  or  [2.x.103] . 




In practice, the DataPostprocessor class only provides an interface to this functionality, and we need to derive our own class from it in order to implement the functions specified by the interface. In the most general case one has to implement several member functions but if the output quantity is a single scalar then some of this boilerplate code can be handled by a more specialized class, DataPostprocessorScalar and we can derive from that one instead. This is what the  [2.x.104]  class does: 

[1.x.73] 



In the constructor, we need to call the constructor of the base class with two arguments. The first denotes the name by which the single scalar quantity computed by this class should be represented in output files. In our case, the postprocessor has  [2.x.105]  as output, so we use "Intensity".    


The second argument is a set of flags that indicate which data is needed by the postprocessor in order to compute the output quantities.  This can be any subset of update_values, update_gradients and update_hessians (and, in the case of face data, also update_normal_vectors), which are documented in UpdateFlags.  Of course, computation of the derivatives requires additional resources, so only the flags for data that are really needed should be given here, just as we do when we use FEValues objects. In our case, only the function values of  [2.x.106]  and  [2.x.107]  are needed to compute  [2.x.108] , so we're good with the update_values flag. 

[1.x.74] 



The actual postprocessing happens in the following function. Its input is an object that stores values of the function (which is here vector-valued) representing the data vector given to  [2.x.109]  evaluated at all evaluation points where we generate output, and some tensor objects representing derivatives (that we don't use here since  [2.x.110]  is computed from just  [2.x.111]  and  [2.x.112] ). The derived quantities are returned in the  [2.x.113]  vector. Remember that this function may only use data for which the respective update flag is specified by  [2.x.114] . For example, we may not use the derivatives here, since our implementation of  [2.x.115]  requests that only function values are provided. 

[1.x.75] 



The computation itself is straightforward: We iterate over each entry in the output vector and compute  [2.x.116]  from the corresponding values of  [2.x.117]  and  [2.x.118] . We do this by creating a complex number  [2.x.119]  and then calling  [2.x.120]  on the result. (One may be tempted to call  [2.x.121]  but in a historical quirk, the C++ committee decided that  [2.x.122]  should return the [1.x.76] of the absolute value -- thereby not satisfying the properties mathematicians require of something called a "norm".) 

[1.x.77] 




[1.x.78]  [1.x.79] 




Finally here is the main class of this program.  It's member functions are very similar to the previous examples, in particular  [2.x.123] , and the list of member variables does not contain any major surprises either. The ParameterHandler object that is passed to the constructor is stored as a reference to allow easy access to the parameters from all functions of the class.  Since we are working with vector valued finite elements, the FE object we are using is of type FESystem. 

[1.x.80] 



The constructor takes the ParameterHandler object and stores it in a reference. It also initializes the DoF-Handler and the finite element system, which consists of two copies of the scalar Q1 field, one for  [2.x.124]  and one for  [2.x.125] : 

[1.x.81] 




[1.x.82]  [1.x.83] 




Here we setup the grid for our domain.  As mentioned in the exposition, the geometry is just a unit square (in 2d) with the part of the boundary that represents the transducer lens replaced by a sector of a circle. 

[1.x.84] 



First we generate some logging output and start a timer so we can compute execution time when this function is done: 

[1.x.85] 



Then we query the values for the focal distance of the transducer lens and the number of mesh refinement steps from our ParameterHandler object: 

[1.x.86] 



Next, two points are defined for position and focal point of the transducer lens, which is the center of the circle whose segment will form the transducer part of the boundary. Notice that this is the only point in the program where things are slightly different in 2D and 3D. Even though this tutorial only deals with the 2D case, the necessary additions to make this program functional in 3D are so minimal that we opt for including them: 

[1.x.87] 



As initial coarse grid we take a simple unit square with 5 subdivisions in each direction. The number of subdivisions is chosen so that the line segment  [2.x.126]  that we want to designate as the transducer boundary is spanned by a single face. Then we step through all cells to find the faces where the transducer is to be located, which in fact is just the single edge from 0.4 to 0.6 on the x-axis. This is where we want the refinements to be made according to a circle shaped boundary, so we mark this edge with a different manifold indicator. Since we will Dirichlet boundary conditions on the transducer, we also change its boundary indicator. 

[1.x.88] 



For the circle part of the transducer lens, a SphericalManifold object is used (which, of course, in 2D just represents a circle), with center computed as above. 

[1.x.89] 



Now global refinement is executed. Cells near the transducer location will be automatically refined according to the circle shaped boundary of the transducer lens: 

[1.x.90] 



Lastly, we generate some more logging output. We stop the timer and query the number of CPU seconds elapsed since the beginning of the function: 

[1.x.91] 




[1.x.92]  [1.x.93]    


Initialization of the system matrix, sparsity patterns and vectors are the same as in previous examples and therefore do not need further comment. As in the previous function, we also output the run time of what we do here: 

[1.x.94] 




[1.x.95]  [1.x.96] 




As before, this function takes care of assembling the system matrix and right hand side vector: 

[1.x.97] 



First we query wavespeed and frequency from the ParameterHandler object and store them in local variables, as they will be used frequently throughout this function. 







[1.x.98] 



As usual, for computing integrals ordinary Gauss quadrature rule is used. Since our bilinear form involves boundary integrals on  [2.x.127] , we also need a quadrature rule for surface integration on the faces, which are  [2.x.128]  dimensional: 

[1.x.99] 



The FEValues objects will evaluate the shape functions for us.  For the part of the bilinear form that involves integration on  [2.x.129] , we'll need the values and gradients of the shape functions, and of course the quadrature weights.  For the terms involving the boundary integrals, only shape function values and the quadrature weights are necessary. 

[1.x.100] 



As usual, the system matrix is assembled cell by cell, and we need a matrix for storing the local cell contributions as well as an index vector to transfer the cell contributions to the appropriate location in the global system matrix after. 

[1.x.101] 



On each cell, we first need to reset the local contribution matrix and request the FEValues object to compute the shape functions for the current cell: 

[1.x.102] 



At this point, it is important to keep in mind that we are dealing with a finite element system with two components. Due to the way we constructed this FESystem, namely as the Cartesian product of two scalar finite element fields, each shape function has only a single nonzero component (they are, in deal.II lingo,  [2.x.130]  "primitive").  Hence, each shape function can be viewed as one of the  [2.x.131] 's or  [2.x.132] 's from the introduction, and similarly the corresponding degrees of freedom can be attributed to either  [2.x.133]  or  [2.x.134] . As we iterate through all the degrees of freedom on the current cell however, they do not come in any particular order, and so we cannot decide right away whether the DoFs with index  [2.x.135]  and  [2.x.136]  belong to the real or imaginary part of our solution.  On the other hand, if you look at the form of the system matrix in the introduction, this distinction is crucial since it will determine to which block in the system matrix the contribution of the current pair of DoFs will go and hence which quantity we need to compute from the given two shape functions.  Fortunately, the FESystem object can provide us with this information, namely it has a function  [2.x.137]  that for each local DoF index returns a pair of integers of which the first indicates to which component of the system the DoF belongs. The second integer of the pair indicates which index the DoF has in the scalar base finite element field, but this information is not relevant here. If you want to know more about this function and the underlying scheme behind primitive vector valued elements, take a look at  [2.x.138]  or the  [2.x.139]  module, where these topics are explained in depth. 

[1.x.103] 



If both DoFs  [2.x.140]  and  [2.x.141]  belong to same component, i.e. their shape functions are both  [2.x.142] 's or both  [2.x.143] 's, the contribution will end up in one of the diagonal blocks in our system matrix, and since the corresponding entries are computed by the same formula, we do not bother if they actually are  [2.x.144]  or  [2.x.145]  shape functions. We can simply compute the entry by iterating over all quadrature points and adding up their contributions, where values and gradients of the shape functions are supplied by our FEValues object. 







[1.x.104] 



You might think that we would have to specify which component of the shape function we'd like to evaluate when requesting shape function values or gradients from the FEValues object. However, as the shape functions are primitive, they have only one nonzero component, and the FEValues class is smart enough to figure out that we are definitely interested in this one nonzero component. 

[1.x.105] 



We also have to add contributions due to boundary terms. To this end, we loop over all faces of the current cell and see if first it is at the boundary, and second has the correct boundary indicator associated with  [2.x.146] , the part of the boundary where we have absorbing boundary conditions: 

[1.x.106] 



These faces will certainly contribute to the off-diagonal blocks of the system matrix, so we ask the FEFaceValues object to provide us with the shape function values on this face: 

[1.x.107] 



Next, we loop through all DoFs of the current cell to find pairs that belong to different components and both have support on the current face_no: 

[1.x.108] 



The check whether shape functions have support on a face is not strictly necessary: if we don't check for it we would simply add up terms to the local cell matrix that happen to be zero because at least one of the shape functions happens to be zero. However, we can save that work by adding the checks above. 




In either case, these DoFs will contribute to the boundary integrals in the off-diagonal blocks of the system matrix. To compute the integral, we loop over all the quadrature points on the face and sum up the contribution weighted with the quadrature weights that the face quadrature rule provides.  In contrast to the entries on the diagonal blocks, here it does matter which one of the shape functions is a  [2.x.147]  and which one is a  [2.x.148] , since that will determine the sign of the entry.  We account for this by a simple conditional statement that determines the correct sign. Since we already checked that DoF  [2.x.149]  and  [2.x.150]  belong to different components, it suffices here to test for one of them to which component it belongs. 

[1.x.109] 



Now we are done with this cell and have to transfer its contributions from the local to the global system matrix. To this end, we first get a list of the global indices of the this cells DoFs... 

[1.x.110] 



...and then add the entries to the system matrix one by one: 

[1.x.111] 



The only thing left are the Dirichlet boundary values on  [2.x.151] , which is characterized by the boundary indicator 1. The Dirichlet values are provided by the  [2.x.152]  class we defined above: 

[1.x.112] 




[1.x.113]  [1.x.114] 




As already mentioned in the introduction, the system matrix is neither symmetric nor definite, and so it is not quite obvious how to come up with an iterative solver and a preconditioner that do a good job on this matrix.  We chose instead to go a different way and solve the linear system with the sparse LU decomposition provided by UMFPACK. This is often a good first choice for 2D problems and works reasonably well even for a large number of DoFs.  The deal.II interface to UMFPACK is given by the SparseDirectUMFPACK class, which is very easy to use and allows us to solve our linear system with just 3 lines of code. 




Note again that for compiling this example program, you need to have the deal.II library built with UMFPACK support. 

[1.x.115] 



The code to solve the linear system is short: First, we allocate an object of the right type. The following  [2.x.153]  call provides the matrix that we would like to invert to the SparseDirectUMFPACK object, and at the same time kicks off the LU-decomposition. Hence, this is also the point where most of the computational work in this program happens. 

[1.x.116] 



After the decomposition, we can use  [2.x.154]  like a matrix representing the inverse of our system matrix, so to compute the solution we just have to multiply with the right hand side vector: 

[1.x.117] 




[1.x.118]  [1.x.119] 




Here we output our solution  [2.x.155]  and  [2.x.156]  as well as the derived quantity  [2.x.157]  in the format specified in the parameter file. Most of the work for deriving  [2.x.158]  from  [2.x.159]  and  [2.x.160]  was already done in the implementation of the  [2.x.161]  class, so that the output routine is rather straightforward and very similar to what is done in the previous tutorials. 

[1.x.120] 



Define objects of our  [2.x.162]  class and a DataOut object: 

[1.x.121] 



Next we query the output-related parameters from the ParameterHandler. The  [2.x.163]  call acts as a counterpart to the  [2.x.164]  call in  [2.x.165] . It collects all the output format related parameters from the ParameterHandler and sets the corresponding properties of the DataOut object accordingly. 

[1.x.122] 



Now we put together the filename from the base name provided by the ParameterHandler and the suffix which is provided by the DataOut class (the default suffix is set to the right type that matches the one set in the .prm file through parse_parameters()): 

[1.x.123] 



The solution vectors  [2.x.166]  and  [2.x.167]  are added to the DataOut object in the usual way: 

[1.x.124] 



For the intensity, we just call  [2.x.168]  again, but this with our  [2.x.169]  object as the second argument, which effectively adds  [2.x.170]  to the output data: 

[1.x.125] 



The last steps are as before. Note that the actual output format is now determined by what is stated in the input file, i.e. one can change the output format without having to re-compile this program: 

[1.x.126] 




[1.x.127]  [1.x.128] 




Here we simply execute our functions one after the other: 

[1.x.129] 




[1.x.130]  [1.x.131] 




Finally the  [2.x.171]  function of the program. It has the same structure as in almost all of the other tutorial programs. The only exception is that we define ParameterHandler and  [2.x.172]  objects, and let the latter read in the parameter values from a textfile called  [2.x.173] . The values so read are then handed over to an instance of the UltrasoundProblem class: 

[1.x.132] 

[1.x.133] [1.x.134][1.x.135] 


The current program reads its run-time parameters from an input file called  [2.x.174]  that looks like this: 

[1.x.136] 



As can be seen, we set  [2.x.175] , which amounts to a focus of the transducer lens at  [2.x.176] ,  [2.x.177] . The coarse mesh is refined 5 times, resulting in 160x160 cells, and the output is written in vtu format. The parameter reader understands many more parameters pertaining in particular to the generation of output, but we need none of these parameters here and therefore stick with their default values. 

Here's the console output of the program in debug mode: 

[1.x.137] 



(Of course, execution times will differ if you run the program locally.) The fact that most of the time is spent on assembling the system matrix and generating output is due to the many assertions that need to be checked in debug mode. In release mode these parts of the program run much faster whereas solving the linear system is hardly sped up at all: 

[1.x.138] 



The graphical output of the program looks as follows: 


 [2.x.178]  

The first two pictures show the real and imaginary parts of  [2.x.179] , whereas the last shows the intensity  [2.x.180] . One can clearly see that the intensity is focused around the focal point of the lens (0.5, 0.3), and that the focus is rather sharp in  [2.x.181] -direction but more blurred in  [2.x.182] -direction, which is a consequence of the geometry of the focusing lens, its finite aperture, and the wave nature of the problem. 

Because colorful graphics are always fun, and to stress the focusing effects some more, here is another set of images highlighting how well the intensity is actually focused in  [2.x.183] -direction: 

 [2.x.184]  


As a final note, the structure of the program makes it easy to determine which parts of the program scale nicely as the mesh is refined and which parts don't. Here are the run times for 5, 6, and 7 global refinements: 

[1.x.139] 



Each time we refine the mesh once, so the number of cells and degrees of freedom roughly quadruples from each step to the next. As can be seen, generating the grid, setting up degrees of freedom, assembling the linear system, and generating output scale pretty closely to linear, whereas solving the linear system is an operation that requires 8 times more time each time the number of degrees of freedom is increased by a factor of 4, i.e. it is  [2.x.185] . This can be explained by the fact that (using optimal ordering) the bandwidth of a finite element matrix is  [2.x.186] , and the effort to solve a banded linear system using LU decomposition is  [2.x.187] . This also explains why the program does run in 3d as well (after changing the dimension on the  [2.x.188]  object), but scales very badly and takes extraordinary patience before it finishes solving the linear system on a mesh with appreciable resolution, even though all the other parts of the program scale very nicely. 




[1.x.140] [1.x.141][1.x.142] 


An obvious possible extension for this program is to run it in 3d &mdash; after all, the world around us is three-dimensional, and ultrasound beams propagate in three-dimensional media. You can try this by simply changing the template parameter of the principal class in  [2.x.189]  and running it. This won't get you very far, though: certainly not if you do 5 global refinement steps as set in the parameter file. You'll simply run out of memory as both the mesh (with its  [2.x.190]  cells) and in particular the sparse direct solver take too much memory. You can solve with 3 global refinement steps, however, if you have a bit of time: in early 2011, the direct solve takes about half an hour. What you'll notice, however, is that the solution is completely wrong: the mesh size is simply not small enough to resolve the solution's waves accurately, and you can see this in plots of the solution. Consequently, this is one of the cases where adaptivity is indispensable if you don't just want to throw a bigger (presumably %parallel) machine at the problem. [1.x.143] [1.x.144]  [2.x.191]  

 [2.x.192] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19] 

[1.x.20] [1.x.21][1.x.22] 




[1.x.23][1.x.24] 


This example is devoted to  [2.x.3] anisotropic refinement [2.x.4] , which extends to possibilities of local refinement. In most parts, this is a modification of the  [2.x.5]  tutorial program, we use the same DG method for a linear transport equation. This program will cover the following topics:  [2.x.6]     [2.x.7]   [2.x.8] Anisotropic refinement [2.x.9] : What is the meaning of anisotropic refinement?    [2.x.10]   [2.x.11] Implementation [2.x.12] : Necessary modifications of code to work with anisotropically refined meshes.    [2.x.13]   [2.x.14] Jump indicator [2.x.15] : A simple indicator for anisotropic refinement in   the context of DG methods.  [2.x.16]  The discretization itself will not be discussed, and neither will implementation techniques not specific to anisotropic refinement used here. Please refer to  [2.x.17]  for this. 

Please note, at the moment of writing this tutorial program, anisotropic refinement is only fully implemented for discontinuous Galerkin Finite Elements. This may later change (or may already have). 




 [2.x.18]  While this program is a modification of  [2.x.19] , it is an adaptation of a version of  [2.x.20]  written early on in the history of deal.II when the MeshWorker framework wasn't available yet. Consequently, it bears little resemblance to the  [2.x.21]  as it exists now, apart from the fact that it solves the same equation with the same discretization. 




[1.x.25][1.x.26] 


All the adaptive processes in the preceding tutorial programs were based on  [2.x.22] isotropic [2.x.23]  refinement of cells, which cuts all edges in half and forms new cells of these split edges (plus some additional edges, faces and vertices, of course). In deal.II,  [2.x.24] anisotropic refinement [2.x.25]  refers to the process of splitting only part of the edges while leaving the others unchanged. Consider a simple square cell, for example: 

[1.x.27] 

After the usual refinement it will consist of four children and look like this: 

[1.x.28] 

The new anisotropic refinement may take two forms: either we can split the edges which are parallel to the horizontal x-axis, resulting in these two child cells: 

[1.x.29] 

or we can split the two edges which run along the y-axis, resulting again in two children, which look that way, however: 

[1.x.30] 

All refinement cases of cells are described by an enumeration  [2.x.26]  and the above anisotropic cases are called  [2.x.27]  and  [2.x.28]  for obvious reasons. The isotropic refinement case is called  [2.x.29]  in 2D and can be requested from the RefinementCase class via  [2.x.30]  

In 3D, there is a third axis which can be split, the z-axis, and thus we have an additional refinement case  [2.x.31]  here. Isotropic refinement will now refine a cell along the x-, y- and z-axes and thus be referred to as @p cut_xyz. Additional cases  [2.x.32]   [2.x.33]  and  [2.x.34]  exist, which refine a cell along two of the axes, but not along the third one. Given a hex cell with x-axis running to the right, y-axis 'into the page' and z-axis to the top, 

[1.x.31] 

we have the isotropic refinement case, 

[1.x.32] 

three anisotropic cases which refine only one axis: 

[1.x.33] 

and three cases which refine two of the three axes: 

[1.x.34] 

For 1D problems, anisotropic refinement can make no difference, as there is only one coordinate direction for a cell, so it is not possible to split it in any other way than isotropically. 

[1.x.35][1.x.36] 

Adaptive local refinement is used to obtain fine meshes which are well adapted to solving the problem at hand efficiently. In short, the size of cells which produce a large error is reduced to obtain a better approximation of the solution to the problem at hand. However, a lot of problems contain anisotropic features. Prominent examples are shocks or boundary layers in compressible viscous flows. An efficient mesh approximates these features with cells of higher aspect ratio which are oriented according to the mentioned features. Using only isotropic refinement, the aspect ratios of the original mesh cells are preserved, as they are inherited by the children of a cell. Thus, starting from an isotropic mesh, a boundary layer will be refined in order to catch the rapid variation of the flow field in the wall normal direction, thus leading to cells with very small edge lengths both in normal and tangential direction. Usually, much higher edge lengths in tangential direction and thus significantly less cells could be used without a significant loss in approximation accuracy. An anisotropic refinement process can modify the aspect ratio from mother to child cells by a factor of two for each refinement step. In the course of several refinements, the aspect ratio of the fine cells can be optimized, saving a considerable number of cells and correspondingly degrees of freedom and thus computational resources, memory as well as CPU time. 

[1.x.37][1.x.38] 


Most of the time, when we do finite element computations, we only consider one cell at a time, for example to calculate cell contributions to the global matrix, or to interpolate boundary values. However, sometimes we have to look at how cells are related in our algorithms. Relationships between cells come in two forms: neighborship and mother-child relationship. For the case of isotropic refinement, deal.II uses certain conventions (invariants) for cell relationships that are always maintained. For example, a refined cell always has exactly  [2.x.35]  children. And (except for the 1d case), two neighboring cells may differ by at most one refinement level: they are equally often refined or one of them is exactly once more refined, leaving exactly one hanging node on the common face. Almost all of the time these invariants are only of concern in the internal implementation of the library. However, there are cases where knowledge of them is also relevant to an application program. 

In the current context, it is worth noting that the kind of mesh refinement affects some of the most fundamental assumptions. Consequently, some of the usual code found in application programs will need modifications to exploit the features of meshes which were created using anisotropic refinement. For those interested in how deal.II evolved, it may be of interest that the loosening of such invariants required some incompatible changes. For example, the library used to have a member  [2.x.36]  that specified how many children a cell has once it is refined. For isotropic refinement, this number is equal to  [2.x.37] , as mentioned above. However, for anisotropic refinement, this number does not exist, as is can be either two or four in 2D and two, four or eight in 3D, and the member  [2.x.38]  has consequently been removed. It has now been replaced by  [2.x.39]  which specifies the [1.x.39] number of children a cell can have. How many children a refined cell has was previously available as static information, but now it depends on the actual refinement state of a cell and can be retrieved using  [2.x.40]  a call that works equally well for both isotropic and anisotropic refinement. A very similar situation can be found for faces and their subfaces: the pertinent information can be queried using  [2.x.41]  or  [2.x.42] , depending on the context. 

Another important aspect, and the most important one in this tutorial, is the treatment of neighbor-relations when assembling jump terms on the faces between cells. Looking at the documentation of the assemble_system functions in  [2.x.43]  we notice, that we need to decide if a neighboring cell is coarser, finer or on the same (refinement) level as our current cell. These decisions do not work in the same way for anisotropic refinement as the information given by the  [2.x.44] level [2.x.45]  of a cell is not enough to completely characterize anisotropic cells; for example, are the terminal children of a two-dimensional cell that is first cut in  [2.x.46] -direction and whose children are then cut in  [2.x.47] -direction on level 2, or are they on level 1 as they would be if the cell would have been refined once isotropically, resulting in the same set of finest cells? 

After anisotropic refinement, a coarser neighbor is not necessarily exactly one level below ours, but can pretty much have any level relative to the current one; in fact, it can even be on a higher level even though it is coarser. Thus the decisions have to be made on a different basis, whereas the intention of the decisions stays the same. 

In the following, we will discuss the cases that can happen when we want to compute contributions to the matrix (or right hand side) of the form [1.x.40] or similar; remember that we integrate terms like this using the FEFaceValues and FESubfaceValues classes. We will also show how to write code that works for both isotropic and anisotropic refinement: 

 [2.x.48]  

   [2.x.49]   [2.x.50] Finer neighbor [2.x.51] : If we are on an active cell and want   to integrate over a face  [2.x.52] , the first   possibility is that the neighbor behind this face is more refined,   i.e. has children occupying only part of the   common face. In this case, the face   under consideration has to be a refined one, which can determine by   asking  [2.x.53] . If this is true, we need to   loop over   all subfaces and get the neighbors' child behind this subface, so that we can   reinit an FEFaceValues object with the neighbor and an FESubfaceValues object   with our cell and the respective subface. 

  For isotropic refinement, this kind is reasonably simple because we   know that an invariant of the isotropically refined adaptive meshes   in deal.II is that neighbors can only differ by exactly one   refinement level. However, this isn't quite true any more for   anisotropically refined meshes, in particular in 3d; there,   the active cell we are interested on the other side of  [2.x.54]  might not   actually be a child of our   neighbor, but perhaps a grandchild or even a farther offspring. Fortunately,   this complexity is hidden in the internals of the library. All we need to do   is call the  [2.x.55]    function. Still, in 3D there are two cases which need special consideration:    [2.x.56]       [2.x.57]  If the neighbor is refined more than once anisotropically, it might be   that here are not two or four but actually three subfaces to   consider. Imagine   the following refinement process of the (two-dimensional) face of   the (three-dimensional) neighbor cell we are considering: first the   face is refined along x, later on only the left subface is refined along y. 

[1.x.41] 

     Here the number of subfaces is three. It is important to note the subtle   differences between, for a face,  [2.x.58]  and    [2.x.59]  The first function returns the number of   immediate children, which would be two for the above example, whereas the   second returns the number of active offspring (i.e., including children,   grandchildren, and further descendants), which is the correct three in   the example above. Using  [2.x.60]  works for   isotropic and anisotropic as well as 2D and 3D cases, so it should always be   used. It should be noted that if any of the cells behind the two   small subfaces on the left side of the rightmost image is further   refined, then the current cell (i.e. the side from which we are   viewing this common face) is going to be refined as well: this is so   because otherwise the invariant of having only one hanging node per   edge would be violated. 

     [2.x.61]  It might be, that the neighbor is coarser, but still has children which   are finer than our current cell. This situation can occur if two equally   coarse cells are refined, where one of the cells has two children at the face   under consideration and the other one four. The cells in the next graphic are   only separated from each other to show the individual refinement cases. 

[1.x.42] 



  Here, the left two cells resulted from an anisotropic bisection of   the mother cell in  [2.x.62] -direction, whereas the right four cells   resulted from a simultaneous anisotropic refinement in both the  [2.x.63] -   and  [2.x.64] -directions.   The left cell marked with # has two finer neighbors marked with +, but the   actual neighbor of the left cell is the complete right mother cell, as the   two cells marked with + are finer and their direct mother is the one   large cell.    [2.x.65]  

  However, fortunately,  [2.x.66]  takes care of   these situations by itself, if you loop over the correct number of subfaces,   in the above example this is two. The  [2.x.67]  function   takes care of this too, so that the resulting state is always correct. There   is one little caveat, however: For reiniting the neighbors FEFaceValues object   you need to know the index of the face that points toward the current   cell. Usually you assume that the neighbor you get directly is as coarse or as   fine as you, if it has children, thus this information can be obtained with    [2.x.68]  If the neighbor is coarser, however, you   would have to use the first value in  [2.x.69]    instead. In order to make this easy for you, there is    [2.x.70]  which does the correct thing for you and   returns the desired result. 

   [2.x.71]   [2.x.72] Neighbor is as fine as our cell [2.x.73] : After we ruled out all cases in   which there are finer children, we only need to decide, whether the neighbor   is coarser here. For this, there is the    [2.x.74]  function which returns a boolean. In   order to get the relevant case of a neighbor of the same coarseness we would   use  [2.x.75] . The code inside this   block can be left untouched. However, there is one thing to mention here: If   we want to use a rule, which cell should assemble certain terms on a given   face we might think of the rule presented in  [2.x.76] . We know that we have to   leave out the part about comparing our cell's level with that of the neighbor   and replace it with the test for a coarser neighbor presented above. However,   we also have to consider the possibility that neighboring cells of same   coarseness have the same index (on different levels). Thus we have to include   the case where the cells have the same index, and give an additional   condition, which of the cells should assemble the terms, e.g. we can choose   the cell with lower level. The details of this concept can be seen in the   implementation below. 

   [2.x.77]   [2.x.78] Coarser neighbor [2.x.79] : The remaining case is obvious: If there are no   refined neighbors and the neighbor is not as fine as the current cell, then it must   be coarser. Thus we can leave the old condition phrase, simply using    [2.x.80] . The  [2.x.81]    function takes care of all the complexity of anisotropic refinement combined   with possible non standard face orientation, flip and rotation on general 3D meshes. 

 [2.x.82]  

[1.x.43][1.x.44] 

When a triangulation is refined, cells which were not flagged for refinement may be refined nonetheless. This is due to additional smoothing algorithms which are either necessary or requested explicitly. In particular, the restriction that there be at most one hanging node on each edge frequently forces the refinement of additional cells neighboring ones that are already finer and are flagged for further refinement. 

However, deal.II also implements a number of algorithms that make sure that resulting meshes are smoother than just the bare minimum, for example ensuring that there are no isolated refined cells surrounded by non-refined ones, since the additional degrees of freedom on these islands would almost all be constrained by hanging node constraints. (See the documentation of the Triangulation class and its  [2.x.83]  member for more information on mesh smoothing.) 

Most of the smoothing algorithms that were originally developed for the isotropic case have been adapted to work in a very similar way for both anisotropic and isotropic refinement. There are two algorithms worth mentioning, however:  [2.x.84]     [2.x.85]   [2.x.86] : In an isotropic environment,   this algorithm tries to ensure a good approximation quality by reducing the   difference in refinement level of cells meeting at a common vertex. However,   there is no clear corresponding concept for anisotropic refinement, thus this   algorithm may not be used in combination with anisotropic refinement. This   restriction is enforced by an assertion which throws an error as soon as the   algorithm is called on a triangulation which has been refined anisotropically. 

   [2.x.87]   [2.x.88] : If refinement is introduced to   limit the number of hanging nodes, the additional cells are often not needed   to improve the approximation quality. This is especially true for DG   methods. If you set the flag  [2.x.89]  the   smoothing algorithm tries to minimize the number of probably unneeded   additional cells by using anisotropic refinement for the smoothing. If you set   this smoothing flag you might get anisotropically refined cells, even if you   never set a single refinement flag to anisotropic refinement. Be aware that   you should only use this flag, if your code respects the possibility of   anisotropic meshes. Combined with a suitable anisotropic indicator this flag   can help save additional cells and thus effort.  [2.x.90]  


[1.x.45][1.x.46] 


Using the benefits of anisotropic refinement requires an indicator to catch anisotropic features of the solution and exploit them for the refinement process. Generally the anisotropic refinement process will consist of several steps:  [2.x.91]     [2.x.92]  Calculate an error indicator.    [2.x.93]  Use the error indicator to flag cells for refinement, e.g. using a fixed   number or fraction of cells. Those cells will be flagged for isotropic   refinement automatically.    [2.x.94]  Evaluate a distinct anisotropic indicator only on the flagged cells.    [2.x.95]  Use the anisotropic indicator to set a new, anisotropic refinement flag   for cells where this is appropriate, leave the flags unchanged otherwise.    [2.x.96]  Call  [2.x.97]  to perform the   requested refinement, using the requested isotropic and anisotropic flags.  [2.x.98]  This approach is similar to the one we have used in  [2.x.99]  for hp-refinement and has the great advantage of flexibility: Any error indicator can be used in the anisotropic process, i.e. if you have quite involved a posteriori goal-oriented error indicators available you can use them as easily as a simple Kelly error estimator. The anisotropic part of the refinement process is not influenced by this choice. Furthermore, simply leaving out the third and forth steps leads to the same isotropic refinement you used to get before any anisotropic changes in deal.II or your application program. As a last advantage, working only on cells flagged for refinement results in a faster evaluation of the anisotropic indicator, which can become noticeable on finer meshes with a lot of cells if the indicator is quite involved. 

Here, we use a very simple approach which is only applicable to DG methods. The general idea is quite simple: DG methods allow the discrete solution to jump over the faces of a cell, whereas it is smooth within each cell. Of course, in the limit we expect that the jumps tend to zero as we refine the mesh and approximate the true solution better and better. Thus, a large jump across a given face indicates that the cell should be refined (at least) orthogonally to that face, whereas a small jump does not lead to this conclusion. It is possible, of course, that the exact solution is not smooth and that it also features a jump. In that case, however, a large jump over one face indicates, that this face is more or less parallel to the jump and in the vicinity of it, thus again we would expect a refinement orthogonal to the face under consideration to be effective. 

The proposed indicator calculates the average jump  [2.x.100] , i.e. the mean value of the absolute jump  [2.x.101]  of the discrete solution  [2.x.102]  over the two faces  [2.x.103] ,  [2.x.104] ,  [2.x.105]  orthogonal to coordinate direction  [2.x.106]  on the unit cell. [1.x.47] If the average jump in one direction is larger than the average of the jumps in the other directions by a certain factor  [2.x.107] , i.e. if  [2.x.108] , the cell is refined only along that particular direction  [2.x.109] , otherwise the cell is refined isotropically. 

Such a criterion is easily generalized to systems of equations: the absolute value of the jump would be replaced by an appropriate norm of the vector-valued jump. 




[1.x.48][1.x.49] 


We solve the linear transport equation presented in  [2.x.110] . The domain is extended to cover  [2.x.111]  in 2D, where the flow field  [2.x.112]  describes a counterclockwise quarter circle around the origin in the right half of the domain and is parallel to the x-axis in the left part of the domain. The inflow boundary is again located at  [2.x.113]  and along the positive part of the x-axis, and the boundary conditions are chosen as in  [2.x.114] . [1.x.50] [1.x.51] 

The deal.II include files have already been covered in previous examples and will thus not be further commented on. 

[1.x.52] 



And this again is C++: 

[1.x.53] 



The last step is as in all previous programs: 

[1.x.54] 




[1.x.55]  [1.x.56]    


The classes describing equation data and the actual assembly of individual terms are almost entirely copied from  [2.x.115] . We will comment on differences. 

[1.x.57] 



The flow field is chosen to be a quarter circle with counterclockwise flow direction and with the origin as midpoint for the right half of the domain with positive  [2.x.116]  values, whereas the flow simply goes to the left in the left part of the domain at a velocity that matches the one coming in from the right. In the circular part the magnitude of the flow velocity is proportional to the distance from the origin. This is a difference to  [2.x.117] , where the magnitude was 1 everywhere. the new definition leads to a linear variation of  [2.x.118]  along each given face of a cell. On the other hand, the solution  [2.x.119]  is exactly the same as before. 

[1.x.58] 




[1.x.59]  [1.x.60]    


This declaration of this class is utterly unaffected by our current changes. 

[1.x.61] 



Likewise, the constructor of the class as well as the functions assembling the terms corresponding to cell interiors and boundary faces are unchanged from before. The function that assembles face terms between cells also did not change because all it does is operate on two objects of type FEFaceValuesBase (which is the base class of both FEFaceValues and FESubfaceValues). Where these objects come from, i.e. how they are initialized, is of no concern to this function: it simply assumes that the quadrature points on faces or subfaces represented by the two objects correspond to the same points in physical space. 

[1.x.62] 




[1.x.63]  [1.x.64]    


This declaration is much like that of  [2.x.120] . However, we introduce a new routine (set_anisotropic_flags) and modify another one (refine_grid). 

[1.x.65] 



Again we want to use DG elements of degree 1 (but this is only specified in the constructor). If you want to use a DG method of a different degree replace 1 in the constructor by the new degree. 

[1.x.66] 



This is new, the threshold value used in the evaluation of the anisotropic jump indicator explained in the introduction. Its value is set to 3.0 in the constructor, but it can easily be changed to a different value greater than 1. 

[1.x.67] 



This is a bool flag indicating whether anisotropic refinement shall be used or not. It is set by the constructor, which takes an argument of the same name. 

[1.x.68] 



Change here for DG methods of different degrees. 

[1.x.69] 



As beta is a linear function, we can choose the degree of the quadrature for which the resulting integration is correct. Thus, we choose to use  [2.x.121]  Gauss points, which enables us to integrate exactly polynomials of degree  [2.x.122] , enough for all the integrals we will perform in this program. 

[1.x.70] 




[1.x.71]  [1.x.72]    


We proceed with the  [2.x.123]  function that implements the DG discretization. This function does the same thing as the  [2.x.124]  function from  [2.x.125]  (but without MeshWorker).  The four cases considered for the neighbor-relations of a cell are the same as the isotropic case, namely a) cell is at the boundary, b) there are finer neighboring cells, c) the neighbor is neither coarser nor finer and d) the neighbor is coarser.  However, the way in which we decide upon which case we have are modified in the way described in the introduction. 

[1.x.73] 



Case (a): The face is at the boundary. 

[1.x.74] 



Case (b): This is an internal face and the neighbor is refined (which we can test by asking whether the face of the current cell has children). In this case, we will need to integrate over the "sub-faces", i.e., the children of the face of the current cell.                  


(There is a slightly confusing corner case: If we are in 1d -- where admittedly the current program and its demonstration of anisotropic refinement is not particularly relevant -- then the faces between cells are always the same: they are just vertices. In other words, in 1d, we do not want to treat faces between cells of different level differently. The condition `face->has_children()` we check here ensures this: in 1d, this function always returns `false`, and consequently in 1d we will not ever go into this `if` branch. But we will have to come back to this corner case below in case (c).) 

[1.x.75] 



We need to know, which of the neighbors faces points in the direction of our cell. Using the @p neighbor_face_no function we get this information for both coarser and non-coarser neighbors. 

[1.x.76] 



Now we loop over all subfaces, i.e. the children and possibly grandchildren of the current face. 

[1.x.77] 



To get the cell behind the current subface we can use the  [2.x.126]  function. it takes care of all the complicated situations of anisotropic refinement and non-standard faces. 

[1.x.78] 



The remaining part of this case is unchanged. 

[1.x.79] 



Case (c). We get here if this is an internal face and if the neighbor is not further refined (or, as mentioned above, we are in 1d in which case we get here for every internal face). We then need to decide whether we want to integrate over the current face. If the neighbor is in fact coarser, then we ignore the face and instead handle it when we visit the neighboring cell and look at the current face (except in 1d, where as mentioned above this is not happening): 

[1.x.80] 



On the other hand, if the neighbor is more refined, then we have already handled the face in case (b) above (except in 1d). So for 2d and 3d, we just have to decide whether we want to handle a face between cells at the same level from the current side or from the neighboring side.  We do this by introducing a tie-breaker: We'll just take the cell with the smaller index (within the current refinement level). In 1d, we take either the coarser cell, or if they are on the same level, the one with the smaller index within that level. This leads to a complicated condition that, hopefully, makes sense given the description above: 

[1.x.81] 



Here we know, that the neighbor is not coarser so we can use the usual  [2.x.127]  function. However, we could also use the more general  [2.x.128]  function. 

[1.x.82] 



We do not need to consider a case (d), as those faces are treated 'from the other side within case (b). 

[1.x.83] 




[1.x.84]  [1.x.85]    


For this simple problem we use the simple Richardson iteration again. The solver is completely unaffected by our anisotropic changes. 

[1.x.86] 




[1.x.87]  [1.x.88]    


We refine the grid according to the same simple refinement criterion used in  [2.x.129] , namely an approximation to the gradient of the solution. 

[1.x.89] 



We approximate the gradient, 

[1.x.90] 



and scale it to obtain an error indicator. 

[1.x.91] 



Then we use this indicator to flag the 30 percent of the cells with highest error indicator to be refined. 

[1.x.92] 



Now the refinement flags are set for those cells with a large error indicator. If nothing is done to change this, those cells will be refined isotropically. If the  [2.x.130]  flag given to this function is set, we now call the set_anisotropic_flags() function, which uses the jump indicator to reset some of the refinement flags to anisotropic refinement. 

[1.x.93] 



Now execute the refinement considering anisotropic as well as isotropic refinement flags. 

[1.x.94] 



Once an error indicator has been evaluated and the cells with largest error are flagged for refinement we want to loop over the flagged cells again to decide whether they need isotropic refinement or whether anisotropic refinement is more appropriate. This is the anisotropic jump indicator explained in the introduction. 

[1.x.95] 



We want to evaluate the jump over faces of the flagged cells, so we need some objects to evaluate values of the solution on faces. 

[1.x.96] 



Now we need to loop over all active cells. 

[1.x.97] 



We only need to consider cells which are flagged for refinement. 

[1.x.98] 



The four cases of different neighbor relations seen in the assembly routines are repeated much in the same way here. 

[1.x.99] 



The neighbor is refined.  First we store the information, which of the neighbor's faces points in the direction of our current cell. This property is inherited to the children. 

[1.x.100] 



Now we loop over all subfaces, 

[1.x.101] 



get an iterator pointing to the cell behind the present subface... 

[1.x.102] 



... and reinit the respective FEFaceValues and FESubFaceValues objects. 

[1.x.103] 



We obtain the function values 

[1.x.104] 



as well as the quadrature weights, multiplied by the Jacobian determinant. 

[1.x.105] 



Now we loop over all quadrature points 

[1.x.106] 



and integrate the absolute value of the jump of the solution, i.e. the absolute value of the difference between the function value seen from the current cell and the neighboring cell, respectively. We know, that the first two faces are orthogonal to the first coordinate direction on the unit cell, the second two faces are orthogonal to the second coordinate direction and so on, so we accumulate these values into vectors with  [2.x.131]  components. 

[1.x.107] 



We also sum up the scaled weights to obtain the measure of the face. 

[1.x.108] 



Our current cell and the neighbor have the same refinement along the face under consideration. Apart from that, we do much the same as with one of the subcells in the above case. 

[1.x.109] 



Now the neighbor is actually coarser. This case is new, in that it did not occur in the assembly routine. Here, we have to consider it, but this is not overly complicated. We simply use the @p neighbor_of_coarser_neighbor function, which again takes care of anisotropic refinement and non-standard face orientation by itself. 

[1.x.110] 



Now we analyze the size of the mean jumps, which we get dividing the jumps by the measure of the respective faces. 

[1.x.111] 



Now we loop over the  [2.x.132]  coordinate directions of the unit cell and compare the average jump over the faces orthogonal to that direction with the average jumps over faces orthogonal to the remaining direction(s). If the first is larger than the latter by a given factor, we refine only along hat axis. Otherwise we leave the refinement flag unchanged, resulting in isotropic refinement. 

[1.x.112] 




[1.x.113]  [1.x.114]    


The remaining part of the program very much follows the scheme of previous tutorial programs. We output the mesh in VTU format (just as we did in  [2.x.133] , for example), and the visualization output in VTU format as we almost always do. 

[1.x.115] 



Create the rectangular domain. 

[1.x.116] 



Adjust the number of cells in different directions to obtain completely isotropic cells for the original mesh. 

[1.x.117] 



If you want to run the program in 3D, simply change the following line to  [2.x.134] . 

[1.x.118] 



First, we perform a run with isotropic refinement. 

[1.x.119] 



Now we do a second run, this time with anisotropic refinement. 

[1.x.120] 

[1.x.121][1.x.122] 




The output of this program consist of the console output, the SVG files containing the grids, and the solutions given in VTU format. 

[1.x.123] 



This text output shows the reduction in the number of cells which results from the successive application of anisotropic refinement. After the last refinement step the savings have accumulated so much that almost four times as many cells and thus degrees of freedom are needed in the isotropic case. The time needed for assembly scales with a similar factor. 

The first interesting part is of course to see how the meshes look like. On the left are the isotropically refined ones, on the right the anisotropic ones (colors indicate the refinement level of cells): 

 [2.x.135]  


The other interesting thing is, of course, to see the solution on these two sequences of meshes. Here they are, on the refinement cycles 1 and 4, clearly showing that the solution is indeed composed of [1.x.124] piecewise polynomials: 

 [2.x.136]  

We see, that the solution on the anisotropically refined mesh is very similar to the solution obtained on the isotropically refined mesh. Thus the anisotropic indicator seems to effectively select the appropriate cells for anisotropic refinement. 

The pictures also explain why the mesh is refined as it is. In the whole left part of the domain refinement is only performed along the  [2.x.137] -axis of cells. In the right part of the domain the refinement is dominated by isotropic refinement, as the anisotropic feature of the solution - the jump from one to zero - is not well aligned with the mesh where the advection direction takes a turn. However, at the bottom and closest (to the observer) parts of the quarter circle this jumps again becomes more and more aligned with the mesh and the refinement algorithm reacts by creating anisotropic cells of increasing aspect ratio. 

It might seem that the necessary alignment of anisotropic features and the coarse mesh can decrease performance significantly for real world problems. That is not wrong in general: If one were, for example, to apply anisotropic refinement to problems in which shocks appear (e.g., the equations solved in  [2.x.138] ), then it many cases the shock is not aligned with the mesh and anisotropic refinement will help little unless one also introduces techniques to move the mesh in alignment with the shocks. On the other hand, many steep features of solutions are due to boundary layers. In those cases, the mesh is already aligned with the anisotropic features because it is of course aligned with the boundary itself, and anisotropic refinement will almost always increase the efficiency of computations on adapted grids for these cases. [1.x.125] [1.x.126]  [2.x.139]  

 [2.x.140] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45][1.x.46][1.x.47][1.x.48][1.x.49][1.x.50] 

 [2.x.3]  

[1.x.51] 


[1.x.52] [1.x.53][1.x.54] 


[1.x.55][1.x.56] 


This program deals with an interesting physical problem: how does a fluid (i.e., a liquid or gas) behave if it experiences differences in buoyancy caused by temperature differences? It is clear that those parts of the fluid that are hotter (and therefore lighter) are going to rise up and those that are cooler (and denser) are going to sink down with gravity. 

In cases where the fluid moves slowly enough such that inertial effects can be neglected, the equations that describe such behavior are the Boussinesq equations that read as follows: [1.x.57] 

These equations fall into the class of vector-valued problems (a toplevel overview of this topic can be found in the  [2.x.4]  module). Here,  [2.x.5]  is the velocity field,  [2.x.6]  the pressure, and  [2.x.7]  the temperature of the fluid.  [2.x.8]  is the symmetric gradient of the velocity. As can be seen, velocity and pressure solve a Stokes equation describing the motion of an incompressible fluid, an equation we have previously considered in  [2.x.9] ; we will draw extensively on the experience we have gained in that program, in particular with regard to efficient linear Stokes solvers. 

The forcing term of the fluid motion is the buoyancy of the fluid, expressed as the product of the density  [2.x.10] , the thermal expansion coefficient  [2.x.11] , the temperature  [2.x.12]  and the gravity vector  [2.x.13]  pointing downward. (A derivation of why the right hand side looks like it looks is given in the introduction of  [2.x.14] .) While the first two equations describe how the fluid reacts to temperature differences by moving around, the third equation states how the fluid motion affects the temperature field: it is an advection diffusion equation, i.e., the temperature is attached to the fluid particles and advected along in the flow field, with an additional diffusion (heat conduction) term. In many applications, the diffusion coefficient is fairly small, and the temperature equation is in fact transport, not diffusion dominated and therefore in character more hyperbolic than elliptic; we will have to take this into account when developing a stable discretization. 

In the equations above, the term  [2.x.15]  on the right hand side denotes the heat sources and may be a spatially and temporally varying function.  [2.x.16]  and  [2.x.17]  denote the viscosity and diffusivity coefficients, which we assume constant for this tutorial program. The more general case when  [2.x.18]  depends on the temperature is an important factor in physical applications: Most materials become more fluid as they get hotter (i.e.,  [2.x.19]  decreases with  [2.x.20] ); sometimes, as in the case of rock minerals at temperatures close to their melting point,  [2.x.21]  may change by orders of magnitude over the typical range of temperatures. 

We note that the Stokes equation above could be nondimensionalized by introducing the [1.x.58]  [2.x.22]  using a typical length scale  [2.x.23] , typical temperature difference  [2.x.24] , density  [2.x.25] , thermal diffusivity  [2.x.26] , and thermal conductivity  [2.x.27] .  [2.x.28]  is a dimensionless number that describes the ratio of heat transport due to convection induced by buoyancy changes from temperature differences, and of heat transport due to thermal diffusion. A small Rayleigh number implies that buoyancy is not strong relative to viscosity and fluid motion  [2.x.29]  is slow enough so that heat diffusion  [2.x.30]  is the dominant heat transport term. On the other hand, a fluid with a high Rayleigh number will show vigorous convection that dominates heat conduction. 

For most fluids for which we are interested in computing thermal convection, the Rayleigh number is very large, often  [2.x.31]  or larger. From the structure of the equations, we see that this will lead to large pressure differences and large velocities. Consequently, the convection term in the convection-diffusion equation for  [2.x.32]  will also be very large and an accurate solution of this equation will require us to choose small time steps. Problems with large Rayleigh numbers are therefore hard to solve numerically for similar reasons that make solving the [1.x.59] hard to solve when the [1.x.60] is large. 

Note that a large Rayleigh number does not necessarily involve large velocities in absolute terms. For example, the Rayleigh number in the earth mantle is larger than  [2.x.33] . Yet the velocities are small: the material is in fact solid rock but it is so hot and under pressure that it can flow very slowly, on the order of at most a few centimeters per year. Nevertheless, this can lead to mixing over time scales of many million years, a time scale much shorter than for the same amount of heat to be distributed by thermal conductivity and a time scale of relevance to affect the evolution of the earth's interior and surface structure. 

 [2.x.34]  If you are interested in using the program as the basis for your own experiments, you will also want to take a look at its continuation in  [2.x.35] . Furthermore,  [2.x.36]  later was developed into the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic problems and that you may want to investigate before trying to morph  [2.x.37]  into something that can solve whatever you want to solve. 


[1.x.61][1.x.62] 


Since the Boussinesq equations are derived under the assumption that inertia of the fluid's motion does not play a role, the flow field is at each time entirely determined by buoyancy difference at that time, not by the flow field at previous times. This is reflected by the fact that the first two equations above are the steady state Stokes equation that do not contain a time derivative. Consequently, we do not need initial conditions for either velocities or pressure. On the other hand, the temperature field does satisfy an equation with a time derivative, so we need initial conditions for  [2.x.38] . 

As for boundary conditions: if  [2.x.39]  then the temperature satisfies a second order differential equation that requires boundary data all around the boundary for all times. These can either be a prescribed boundary temperature  [2.x.40]  (Dirichlet boundary conditions), or a prescribed thermal flux  [2.x.41] ; in this program, we will use an insulated boundary condition, i.e., prescribe no thermal flux:  [2.x.42] . 

Similarly, the velocity field requires us to pose boundary conditions. These may be no-slip no-flux conditions  [2.x.43]  on  [2.x.44]  if the fluid sticks to the boundary, or no normal flux conditions  [2.x.45]  if the fluid can flow along but not across the boundary, or any number of other conditions that are physically reasonable. In this program, we will use no normal flux conditions. 


[1.x.63][1.x.64] 


Like the equations solved in  [2.x.46] , we here have a system of differential-algebraic equations (DAE): with respect to the time variable, only the temperature equation is a differential equation whereas the Stokes system for  [2.x.47]  and  [2.x.48]  has no time-derivatives and is therefore of the sort of an algebraic constraint that has to hold at each time instant. The main difference to  [2.x.49]  is that the algebraic constraint there was a mixed Laplace system of the form [1.x.65] 

where now we have a Stokes system [1.x.66] 

where  [2.x.50]  is an operator similar to the Laplacian  [2.x.51]  applied to a vector field. 

Given the similarity to what we have done in  [2.x.52] , it may not come as a surprise that we choose a similar approach, although we will have to make adjustments for the change in operator in the top-left corner of the differential operator. 


[1.x.67][1.x.68] 


The structure of the problem as a DAE allows us to use the same strategy as we have already used in  [2.x.53] , i.e., we use a time lag scheme: we first solve the temperature equation (using an extrapolated velocity field), and then insert the new temperature solution into the right hand side of the velocity equation. The way we implement this in our code looks at things from a slightly different perspective, though. We first solve the Stokes equations for velocity and pressure using the temperature field from the previous time step, which means that we get the velocity for the previous time step. In other words, we first solve the Stokes system for time step  [2.x.54]  as [1.x.69] 

and then the temperature equation with an extrapolated velocity field to time  [2.x.55] . 

In contrast to  [2.x.56] , we'll use a higher order time stepping scheme here, namely the [1.x.70] that replaces the time derivative  [2.x.57]  by the (one-sided) difference quotient  [2.x.58]  with  [2.x.59]  the time step size. This gives the discretized-in-time temperature equation [1.x.71] 

Note how the temperature equation is solved semi-explicitly: diffusion is treated implicitly whereas advection is treated explicitly using an extrapolation (or forward-projection) of temperature and velocity, including the just-computed velocity  [2.x.60] . The forward-projection to the current time level  [2.x.61]  is derived from a Taylor expansion,  [2.x.62] . We need this projection for maintaining the order of accuracy of the BDF-2 scheme. In other words, the temperature fields we use in the explicit right hand side are second order approximations of the current temperature field &mdash; not quite an explicit time stepping scheme, but by character not too far away either. 

The introduction of the temperature extrapolation limits the time step by a [1.x.72] just like it was in  [2.x.63]  " [2.x.64] ". (We wouldn't have had that stability condition if we treated the advection term implicitly since the BDF-2 scheme is A-stable, at the price that we needed to build a new temperature matrix at each time step.) We will discuss the exact choice of time step in the [1.x.73], but for the moment of importance is that this CFL condition means that the time step size  [2.x.65]  may change from time step to time step, and that we have to modify the above formula slightly. If  [2.x.66]  are the time steps sizes of the current and previous time step, then we use the approximations 

[1.x.74] 

and 

[1.x.75] 

and above equation is generalized as follows: [1.x.76] 



where  [2.x.67]  denotes the extrapolation of velocity  [2.x.68]  and temperature  [2.x.69]  to time level  [2.x.70] , using the values at the two previous time steps. That's not an easy to read equation, but will provide us with the desired higher order accuracy. As a consistency check, it is easy to verify that it reduces to the same equation as above if  [2.x.71] . 

As a final remark we note that the choice of a higher order time stepping scheme of course forces us to keep more time steps in memory; in particular, we here will need to have  [2.x.72]  around, a vector that we could previously discard. This seems like a nuisance that we were able to avoid previously by using only a first order time stepping scheme, but as we will see below when discussing the topic of stabilization, we will need this vector anyway and so keeping it around for time discretization is essentially for free and gives us the opportunity to use a higher order scheme. 


[1.x.77][1.x.78] 


Like solving the mixed Laplace equations, solving the Stokes equations requires us to choose particular pairs of finite elements for velocities and pressure variables. Because this has already been discussed in  [2.x.73] , we only cover this topic briefly: Here, we use the stable pair  [2.x.74] . These are continuous elements, so we can form the weak form of the Stokes equation without problem by integrating by parts and substituting continuous functions by their discrete counterparts: [1.x.79] 

for all test functions  [2.x.75] . The first term of the first equation is considered as the inner product between tensors, i.e.  [2.x.76] . Because the second tensor in this product is symmetric, the anti-symmetric component of  [2.x.77]  plays no role and it leads to the entirely same form if we use the symmetric gradient of  [2.x.78]  instead. Consequently, the formulation we consider and that we implement is [1.x.80] 



This is exactly the same as what we already discussed in  [2.x.79]  and there is not much more to say about this here. 


[1.x.81][1.x.82] 


The more interesting question is what to do with the temperature advection-diffusion equation. By default, not all discretizations of this equation are equally stable unless we either do something like upwinding, stabilization, or all of this. One way to achieve this is to use discontinuous elements (i.e., the FE_DGQ class that we used, for example, in the discretization of the transport equation in  [2.x.80] , or in discretizing the pressure in  [2.x.81]  and  [2.x.82] ) and to define a flux at the interface between cells that takes into account upwinding. If we had a pure advection problem this would probably be the simplest way to go. However, here we have some diffusion as well, and the discretization of the Laplace operator with discontinuous elements is cumbersome because of the significant number of additional terms that need to be integrated on each face between cells. Discontinuous elements also have the drawback that the use of numerical fluxes introduces an additional numerical diffusion that acts everywhere, whereas we would really like to minimize the effect of numerical diffusion to a minimum and only apply it where it is necessary to stabilize the scheme. 

A better alternative is therefore to add some nonlinear viscosity to the model. Essentially, what this does is to transform the temperature equation from the form [1.x.83] 

to something like [1.x.84] 

where  [2.x.83]  is an addition viscosity (diffusion) term that only acts in the vicinity of shocks and other discontinuities.  [2.x.84]  is chosen in such a way that if  [2.x.85]  satisfies the original equations, the additional viscosity is zero. 

To achieve this, the literature contains a number of approaches. We will here follow one developed by Guermond and Popov that builds on a suitably defined residual and a limiting procedure for the additional viscosity. To this end, let us define a residual  [2.x.86]  as follows: [1.x.85] 

where we will later choose the stabilization exponent  [2.x.87]  from within the range  [2.x.88] . Note that  [2.x.89]  will be zero if  [2.x.90]  satisfies the temperature equation, since then the term in parentheses will be zero. Multiplying terms out, we get the following, entirely equivalent form: [1.x.86] 



With this residual, we can now define the artificial viscosity as a piecewise constant function defined on each cell  [2.x.91]  with diameter  [2.x.92]  separately as follows: [1.x.87] 



Here,  [2.x.93]  is a stabilization constant (a dimensional analysis reveals that it is unitless and therefore independent of scaling; we will discuss its choice in the [1.x.88]) and  [2.x.94]  is a normalization constant that must have units  [2.x.95] . We will choose it as  [2.x.96] , where  [2.x.97]  is the range of present temperature values (remember that buoyancy is driven by temperature variations, not the absolute temperature) and  [2.x.98]  is a dimensionless constant. To understand why this method works consider this: If on a particular cell  [2.x.99]  the temperature field is smooth, then we expect the residual to be small there (in fact to be on the order of  [2.x.100] ) and the stabilization term that injects artificial diffusion will there be of size  [2.x.101]  &mdash; i.e., rather small, just as we hope it to be when no additional diffusion is necessary. On the other hand, if we are on or close to a discontinuity of the temperature field, then the residual will be large; the minimum operation in the definition of  [2.x.102]  will then ensure that the stabilization has size  [2.x.103]  &mdash; the optimal amount of artificial viscosity to ensure stability of the scheme. 

Whether or not this scheme really works is a good question. Computations by Guermond and Popov have shown that this form of stabilization actually performs much better than most of the other stabilization schemes that are around (for example streamline diffusion, to name only the simplest one). Furthermore, for  [2.x.104]  they can even prove that it produces better convergence orders for the linear transport equation than for example streamline diffusion. For  [2.x.105] , no theoretical results are currently available, but numerical tests indicate that the results are considerably better than for  [2.x.106] . 

A more practical question is how to introduce this artificial diffusion into the equations we would like to solve. Note that the numerical viscosity  [2.x.107]  is temperature-dependent, so the equation we want to solve is nonlinear in  [2.x.108]  &mdash; not what one desires from a simple method to stabilize an equation, and even less so if we realize that  [2.x.109]  is nondifferentiable in  [2.x.110] . However, there is no reason to despair: we still have to discretize in time and we can treat the term explicitly. 

In the definition of the stabilization parameter, we approximate the time derivative by  [2.x.111] . This approximation makes only use of available time data and this is the reason why we need to store data of two previous time steps (which enabled us to use the BDF-2 scheme without additional storage cost). We could now simply evaluate the rest of the terms at  [2.x.112] , but then the discrete residual would be nothing else than a backward Euler approximation, which is only first order accurate. So, in case of smooth solutions, the residual would be still of the order  [2.x.113] , despite the second order time accuracy in the outer BDF-2 scheme and the spatial FE discretization. This is certainly not what we want to have (in fact, we desired to have small residuals in regions where the solution behaves nicely), so a bit more care is needed. The key to this problem is to observe that the first derivative as we constructed it is actually centered at  [2.x.114] . We get the desired second order accurate residual calculation if we evaluate all spatial terms at  [2.x.115]  by using the approximation  [2.x.116] , which means that we calculate the nonlinear viscosity as a function of this intermediate temperature,  [2.x.117] . Note that this evaluation of the residual is nothing else than a Crank-Nicholson scheme, so we can be sure that now everything is alright. One might wonder whether it is a problem that the numerical viscosity now is not evaluated at time  [2.x.118]  (as opposed to the rest of the equation). However, this offset is uncritical: For smooth solutions,  [2.x.119]  will vary continuously, so the error in time offset is  [2.x.120]  times smaller than the nonlinear viscosity itself, i.e., it is a small higher order contribution that is left out. That's fine because the term itself is already at the level of discretization error in smooth regions. 

Using the BDF-2 scheme introduced above, this yields for the simpler case of uniform time steps of size  [2.x.121] : [1.x.89] 

On the left side of this equation remains the term from the time derivative and the original (physical) diffusion which we treat implicitly (this is actually a nice term: the matrices that result from the left hand side are the mass matrix and a multiple of the Laplace matrix &mdash; both are positive definite and if the time step size  [2.x.122]  is small, the sum is simple to invert). On the right hand side, the terms in the first line result from the time derivative; in the second line is the artificial diffusion at time  [2.x.123] ; the third line contains the advection term, and the fourth the sources. Note that the artificial diffusion operates on the extrapolated temperature at the current time in the same way as we have discussed the advection works in the section on time stepping. 

The form for nonuniform time steps that we will have to use in reality is a bit more complicated (which is why we showed the simpler form above first) and reads: [1.x.90] 



After settling all these issues, the weak form follows naturally from the strong form shown in the last equation, and we immediately arrive at the weak form of the discretized equations: [1.x.91] 

for all discrete test functions  [2.x.124] . Here, the diffusion term has been integrated by parts, and we have used that we will impose no thermal flux,  [2.x.125] . 

This then results in a matrix equation of form [1.x.92] 

which given the structure of matrix on the left (the sum of two positive definite matrices) is easily solved using the Conjugate Gradient method. 




[1.x.93][1.x.94] 


As explained above, our approach to solving the joint system for velocities/pressure on the one hand and temperature on the other is to use an operator splitting where we first solve the Stokes system for the velocities and pressures using the old temperature field, and then solve for the new temperature field using the just computed velocity field. (A more extensive discussion of operator splitting methods can be found in  [2.x.126] .) 


[1.x.95][1.x.96] 


Solving the linear equations coming from the Stokes system has been discussed in great detail in  [2.x.127] . In particular, in the results section of that program, we have discussed a number of alternative linear solver strategies that turned out to be more efficient than the original approach. The best alternative identified there we to use a GMRES solver preconditioned by a block matrix involving the Schur complement. Specifically, the Stokes operator leads to a block structured matrix [1.x.97] 

and as discussed there a good preconditioner is [1.x.98] 

where  [2.x.128]  is the Schur complement of the Stokes operator  [2.x.129] . Of course, this preconditioner is not useful because we can't form the various inverses of matrices, but we can use the following as a preconditioner: [1.x.99] 

where  [2.x.130]  are approximations to the inverse matrices. In particular, it turned out that  [2.x.131]  is spectrally equivalent to the mass matrix and consequently replacing  [2.x.132]  by a CG solver applied to the mass matrix on the pressure space was a good choice. In a small deviation from  [2.x.133] , we here have a coefficient  [2.x.134]  in the momentum equation, and by the same derivation as there we should arrive at the conclusion that it is the weighted mass matrix with entries  [2.x.135]  that we should be using. 

It was more complicated to come up with a good replacement  [2.x.136] , which corresponds to the discretized symmetric Laplacian of the vector-valued velocity field, i.e.  [2.x.137] . In  [2.x.138]  we used a sparse LU decomposition (using the SparseDirectUMFPACK class) of  [2.x.139]  for  [2.x.140]  &mdash; the perfect preconditioner &mdash; in 2d, but for 3d memory and compute time is not usually sufficient to actually compute this decomposition; consequently, we only use an incomplete LU decomposition (ILU, using the SparseILU class) in 3d. 

For this program, we would like to go a bit further. To this end, note that the symmetrized bilinear form on vector fields,  [2.x.141]  is not too far away from the nonsymmetrized version,  [2.x.142]  (note that the factor 2 has disappeared in this form). The latter, however, has the advantage that the  [2.x.143]  vector components of the test functions are not coupled (well, almost, see below), i.e., the resulting matrix is block-diagonal: one block for each vector component, and each of these blocks is equal to the Laplace matrix for this vector component. So assuming we order degrees of freedom in such a way that first all  [2.x.144] -components of the velocity are numbered, then the  [2.x.145] -components, and then the  [2.x.146] -components, then the matrix  [2.x.147]  that is associated with this slightly different bilinear form has the form [1.x.100] 

where  [2.x.148]  is a Laplace matrix of size equal to the number of shape functions associated with each component of the vector-valued velocity. With this matrix, one could be tempted to define our preconditioner for the velocity matrix  [2.x.149]  as follows: [1.x.101] 

where  [2.x.150]  is a preconditioner for the Laplace matrix &mdash; something where we know very well how to build good preconditioners! 

In reality, the story is not quite as simple: To make the matrix  [2.x.151]  definite, we need to make the individual blocks  [2.x.152]  definite by applying boundary conditions. One can try to do so by applying Dirichlet boundary conditions all around the boundary, and then the so-defined preconditioner  [2.x.153]  turns out to be a good preconditioner for  [2.x.154]  if the latter matrix results from a Stokes problem where we also have Dirichlet boundary conditions on the velocity components all around the domain, i.e., if we enforce  [2.x.155] . 

Unfortunately, this "if" is an "if and only if": in the program below we will want to use no-flux boundary conditions of the form  [2.x.156]  (i.e., flow %parallel to the boundary is allowed, but no flux through the boundary). In this case, it turns out that the block diagonal matrix defined above is not a good preconditioner because it neglects the coupling of components at the boundary. A better way to do things is therefore if we build the matrix  [2.x.157]  as the vector Laplace matrix  [2.x.158]  and then apply the same boundary condition as we applied to  [2.x.159] . If this is a Dirichlet boundary condition all around the domain, the  [2.x.160]  will decouple to three diagonal blocks as above, and if the boundary conditions are of the form  [2.x.161]  then this will introduce a coupling of degrees of freedom at the boundary but only there. This, in fact, turns out to be a much better preconditioner than the one introduced above, and has almost all the benefits of what we hoped to get. 


To sum this whole story up, we can observe:  [2.x.162]     [2.x.163]  Compared to building a preconditioner from the original matrix  [2.x.164]    resulting from the symmetric gradient as we did in  [2.x.165] ,   we have to expect that the preconditioner based on the Laplace bilinear form   performs worse since it does not take into account the coupling between   vector components. 

   [2.x.166] On the other hand, preconditioners for the Laplace matrix are typically   more mature and perform better than ones for vector problems. For example,   at the time of this writing, Algebraic %Multigrid (AMG) algorithms are very   well developed for scalar problems, but not so for vector problems. 

   [2.x.167] In building this preconditioner, we will have to build up the   matrix  [2.x.168]  and its preconditioner. While this means that we   have to store an additional matrix we didn't need before, the   preconditioner  [2.x.169]  is likely going to need much less   memory than storing a preconditioner for the coupled matrix    [2.x.170] . This is because the matrix  [2.x.171]  has only a third of the   entries per row for all rows corresponding to interior degrees of   freedom, and contains coupling between vector components only on   those parts of the boundary where the boundary conditions introduce   such a coupling. Storing the matrix is therefore comparatively   cheap, and we can expect that computing and storing the   preconditioner  [2.x.172]  will also be much cheaper compared to   doing so for the fully coupled matrix.  [2.x.173]  




[1.x.102][1.x.103] 


This is the easy part: The matrix for the temperature equation has the form  [2.x.174] , where  [2.x.175]  are mass and stiffness matrices on the temperature space, and  [2.x.176]  are constants related the time stepping scheme and the current and previous time step. This being the sum of a symmetric positive definite and a symmetric positive semidefinite matrix, the result is also symmetric positive definite. Furthermore,  [2.x.177]  is a number proportional to the time step, and so becomes small whenever the mesh is fine, damping the effect of the then ill-conditioned stiffness matrix. 

As a consequence, inverting this matrix with the Conjugate Gradient algorithm, using a simple preconditioner, is trivial and very cheap compared to inverting the Stokes matrix. 




[1.x.104][1.x.105] 


[1.x.106][1.x.107] 


One of the things worth explaining up front about the program below is the use of two different DoFHandler objects. If one looks at the structure of the equations above and the scheme for their solution, one realizes that there is little commonality that keeps the Stokes part and the temperature part together. In all previous tutorial programs in which we have discussed  [2.x.178]  "vector-valued problems" we have always only used a single finite element with several vector components, and a single DoFHandler object. Sometimes, we have substructured the resulting matrix into blocks to facilitate particular solver schemes; this was, for example, the case in the  [2.x.179]  program for the Stokes equations upon which the current program is based. 

We could of course do the same here. The linear system that we would get would look like this: [1.x.108] 

The problem with this is: We never use the whole matrix at the same time. In fact, it never really exists at the same time: As explained above,  [2.x.180]  and  [2.x.181]  depend on the already computed solution  [2.x.182] , in the first case through the time step (that depends on  [2.x.183]  because it has to satisfy a CFL condition). So we can only assemble it once we've already solved the top left  [2.x.184]  block Stokes system, and once we've moved on to the temperature equation we don't need the Stokes part any more; the fact that we build an object for a matrix that never exists as a whole in memory at any given time led us to jumping through some hoops in  [2.x.185] , so let's not repeat this sort of error. Furthermore, we don't actually build the matrix  [2.x.186] : Because by the time we get to the temperature equation we already know  [2.x.187] , and because we have to assemble the right hand side  [2.x.188]  at this time anyway, we simply move the term  [2.x.189]  to the right hand side and assemble it along with all the other terms there. What this means is that there does not remain a part of the matrix where temperature variables and Stokes variables couple, and so a global enumeration of all degrees of freedom is no longer important: It is enough if we have an enumeration of all Stokes degrees of freedom, and of all temperature degrees of freedom independently. 

In essence, there is consequently not much use in putting [1.x.109] into a block matrix (though there are of course the same good reasons to do so for the  [2.x.190]  Stokes part), or, for that matter, in putting everything into the same DoFHandler object. 

But are there [1.x.110] to doing so? These exist, though they may not be obvious at first. The main problem is that if we need to create one global finite element that contains velocity, pressure, and temperature shape functions, and use this to initialize the DoFHandler. But we also use this finite element object to initialize all FEValues or FEFaceValues objects that we use. This may not appear to be that big a deal, but imagine what happens when, for example, we evaluate the residual  [2.x.191]  that we need to compute the artificial viscosity  [2.x.192] .  For this, we need the Laplacian of the temperature, which we compute using the tensor of second derivatives (Hessians) of the shape functions (we have to give the  [2.x.193]  flag to the FEValues object for this). Now, if we have a finite that contains the shape functions for velocities, pressures, and temperatures, that means that we have to compute the Hessians of [1.x.111] shape functions, including the many higher order shape functions for the velocities. That's a lot of computations that we don't need, and indeed if one were to do that (as we had in an early version of the program), assembling the right hand side took about a quarter of the overall compute time. 

So what we will do is to use two different finite element objects, one for the Stokes components and one for the temperatures. With this come two different DoFHandlers, two sparsity patterns and two matrices for the Stokes and temperature parts, etc. And whenever we have to assemble something that contains both temperature and Stokes shape functions (in particular the right hand sides of Stokes and temperature equations), then we use two FEValues objects initialized with two cell iterators that we walk in %parallel through the two DoFHandler objects associated with the same Triangulation object; for these two FEValues objects, we use of course the same quadrature objects so that we can iterate over the same set of quadrature points, but each FEValues object will get update flags only according to what it actually needs to compute. In particular, when we compute the residual as above, we only ask for the values of the Stokes shape functions, but also the Hessians of the temperature shape functions &mdash; much cheaper indeed, and as it turns out: assembling the right hand side of the temperature equation is now a component of the program that is hardly measurable. 

With these changes, timing the program yields that only the following operations are relevant for the overall run time:  [2.x.194]     [2.x.195] Solving the Stokes system: 72% of the run time.    [2.x.196] Assembling the Stokes preconditioner and computing the algebraic       multigrid hierarchy using the Trilinos ML package: 11% of the       run time.    [2.x.197] The function  [2.x.198] : 7%       of overall run time.    [2.x.199] Assembling the Stokes and temperature right hand side vectors as       well as assembling the matrices: 7%.  [2.x.200]  In essence this means that all bottlenecks apart from the algebraic multigrid have been removed. 




[1.x.112][1.x.113] 


In much the same way as we used PETSc to support our linear algebra needs in  [2.x.201]  and  [2.x.202] , we use interfaces to the [1.x.114] library (see the deal.II README file for installation instructions) in this program. Trilinos is a very large collection of everything that has to do with linear and nonlinear algebra, as well as all sorts of tools around that (and looks like it will grow in many other directions in the future as well). 

The main reason for using Trilinos, similar to our exploring PETSc, is that it is a very powerful library that provides a lot more tools than deal.II's own linear algebra library. That includes, in particular, the ability to work in %parallel on a cluster, using MPI, and a wider variety of preconditioners. In the latter class, one of the most interesting capabilities is the existence of the Trilinos ML package that implements an Algebraic Multigrid (AMG) method. We will use this preconditioner to precondition the second order operator part of the momentum equation. The ability to solve problems in %parallel will be explored in  [2.x.203] , using the same problem as discussed here. 

PETSc, which we have used in  [2.x.204]  and  [2.x.205] , is certainly a powerful library, providing a large number of functions that deal with matrices, vectors, and iterative solvers and preconditioners, along with lots of other stuff, most of which runs quite well in %parallel. It is, however, a few years old already than Trilinos, written in C, and generally not quite as easy to use as some other libraries. As a consequence, deal.II has also acquired interfaces to Trilinos, which shares a lot of the same functionality with PETSc. It is, however, a project that is several years younger, is written in C++ and by people who generally have put a significant emphasis on software design. 


[1.x.115][1.x.116] 


The case we want to solve here is as follows: we solve the Boussinesq equations described above with  [2.x.206] , i.e., a relatively slow moving fluid that has virtually no thermal diffusive conductivity and transports heat mainly through convection. On the boundary, we will require no-normal flux for the velocity ( [2.x.207] ) and for the temperature ( [2.x.208] ). This is one of the cases discussed in the introduction of  [2.x.209]  and fixes one component of the velocity while allowing flow to be %parallel to the boundary. There remain  [2.x.210]  components to be fixed, namely the tangential components of the normal stress; for these, we choose homogeneous conditions which means that we do not have to anything special. Initial conditions are only necessary for the temperature field, and we choose it to be constant zero. 

The evolution of the problem is then entirely driven by the right hand side  [2.x.211]  of the temperature equation, i.e., by heat sources and sinks. Here, we choose a setup invented in advance of a Christmas lecture: real candles are of course prohibited in U.S. class rooms, but virtual ones are allowed. We therefore choose three spherical heat sources unequally spaced close to the bottom of the domain, imitating three candles. The fluid located at these sources, initially at rest, is then heated up and as the temperature rises gains buoyancy, rising up; more fluid is dragged up and through the sources, leading to three hot plumes that rise up until they are captured by the recirculation of fluid that sinks down on the outside, replacing the air that rises due to heating. [1.x.117] [1.x.118] 


[1.x.119]  [1.x.120] 




The first step, as always, is to include the functionality of these well-known deal.II library files and some C++ header files. 

[1.x.121] 



Then we need to include some header files that provide vector, matrix, and preconditioner classes that implement interfaces to the respective Trilinos classes. In particular, we will need interfaces to the matrix and vector classes based on Trilinos as well as Trilinos preconditioners: 

[1.x.122] 



Finally, here are a few C++ headers that haven't been included yet by one of the aforelisted header files: 

[1.x.123] 



At the end of this top-matter, we import all deal.II names into the global namespace: 

[1.x.124] 




[1.x.125]  [1.x.126] 




Again, the next stage in the program is the definition of the equation data, that is, the various boundary conditions, the right hand sides and the initial condition (remember that we're about to solve a time-dependent system). The basic strategy for this definition is the same as in  [2.x.212] . Regarding the details, though, there are some differences. 




The first thing is that we don't set any inhomogeneous boundary conditions on the velocity, since as is explained in the introduction we will use no-flux conditions  [2.x.213] . So what is left are  [2.x.214]  conditions for the tangential part of the normal component of the stress tensor,  [2.x.215] ; we assume homogeneous values for these components, i.e., a natural boundary condition that requires no specific action (it appears as a zero term in the right hand side of the weak form).    


For the temperature  [2.x.216] , we assume no thermal energy flux, i.e.,  [2.x.217] . This, again, is a boundary condition that does not require us to do anything in particular.    


Secondly, we have to set initial conditions for the temperature (no initial conditions are required for the velocity and pressure, since the Stokes equations for the quasi-stationary case we consider here have no time derivatives of the velocity or pressure). Here, we choose a very simple test case, where the initial temperature is zero, and all dynamics are driven by the temperature right hand side.    


Thirdly, we need to define the right hand side of the temperature equation. We choose it to be constant within three circles (or spheres in 3d) somewhere at the bottom of the domain, as explained in the introduction, and zero outside.    


Finally, or maybe firstly, at the top of this namespace, we define the various material constants we need ( [2.x.218] , density  [2.x.219]  and the thermal expansion coefficient  [2.x.220] ): 

[1.x.127] 




[1.x.128]  [1.x.129] 




This section introduces some objects that are used for the solution of the linear equations of the Stokes system that we need to solve in each time step. Many of the ideas used here are the same as in  [2.x.221] , where Schur complement based preconditioners and solvers have been introduced, with the actual interface taken from  [2.x.222]  (in particular the discussion in the "Results" section of  [2.x.223] , in which we introduce alternatives to the direct Schur complement approach). Note, however, that here we don't use the Schur complement to solve the Stokes equations, though an approximate Schur complement (the mass matrix on the pressure space) appears in the preconditioner. 

[1.x.130] 




[1.x.131]  [1.x.132] 




This class is an interface to calculate the action of an "inverted" matrix on a vector (using the  [2.x.224]  operation) in the same way as the corresponding class in  [2.x.225] : when the product of an object of this class is requested, we solve a linear equation system with that matrix using the CG method, accelerated by a preconditioner of (templated) class  [2.x.226] .      


In a minor deviation from the implementation of the same class in  [2.x.227] , we make the  [2.x.228]  function take any kind of vector type (it will yield compiler errors, however, if the matrix does not allow a matrix-vector product with this kind of vector).      


Secondly, we catch any exceptions that the solver may have thrown. The reason is as follows: When debugging a program like this one occasionally makes a mistake of passing an indefinite or nonsymmetric matrix or preconditioner to the current class. The solver will, in that case, not converge and throw a run-time exception. If not caught here it will propagate up the call stack and may end up in  [2.x.229]  where we output an error message that will say that the CG solver failed. The question then becomes: Which CG solver? The one that inverted the mass matrix? The one that inverted the top left block with the Laplace operator? Or a CG solver in one of the several other nested places where we use linear solvers in the current code? No indication about this is present in a run-time exception because it doesn't store the stack of calls through which we got to the place where the exception was generated.      


So rather than letting the exception propagate freely up to  [2.x.230]  we realize that there is little that an outer function can do if the inner solver fails and rather convert the run-time exception into an assertion that fails and triggers a call to  [2.x.231] , allowing us to trace back in a debugger how we got to the current place. 

[1.x.133] 




[1.x.134]  [1.x.135] 




This is the implementation of the Schur complement preconditioner as described in detail in the introduction. As opposed to  [2.x.232]  and  [2.x.233] , we solve the block system all-at-once using GMRES, and use the Schur complement of the block structured matrix to build a good preconditioner instead.      


Let's have a look at the ideal preconditioner matrix  [2.x.234]  described in the introduction. If we apply this matrix in the solution of a linear system, convergence of an iterative GMRES solver will be governed by the matrix [1.x.136] which indeed is very simple. A GMRES solver based on exact matrices would converge in one iteration, since all eigenvalues are equal (any Krylov method takes at most as many iterations as there are distinct eigenvalues). Such a preconditioner for the blocked Stokes system has been proposed by Silvester and Wathen ("Fast iterative solution of stabilised Stokes systems part II.  Using general block preconditioners", SIAM J. Numer. Anal., 31 (1994), pp. 1352-1367).      


Replacing  [2.x.235]  by  [2.x.236]  keeps that spirit alive: the product  [2.x.237]  will still be close to a matrix with eigenvalues 1 with a distribution that does not depend on the problem size. This lets us hope to be able to get a number of GMRES iterations that is problem-size independent.      


The deal.II users who have already gone through the  [2.x.238]  and  [2.x.239]  tutorials can certainly imagine how we're going to implement this.  We replace the exact inverse matrices in  [2.x.240]  by some approximate inverses built from the InverseMatrix class, and the inverse Schur complement will be approximated by the pressure mass matrix  [2.x.241]  (weighted by  [2.x.242]  as mentioned in the introduction). As pointed out in the results section of  [2.x.243] , we can replace the exact inverse of  [2.x.244]  by just the application of a preconditioner, in this case on a vector Laplace matrix as was explained in the introduction. This does increase the number of (outer) GMRES iterations, but is still significantly cheaper than an exact inverse, which would require between 20 and 35 CG iterations for  [2.x.245] each [2.x.246]  outer solver step (using the AMG preconditioner).      


Having the above explanations in mind, we define a preconditioner class with a  [2.x.247]  functionality, which is all we need for the interaction with the usual solver functions further below in the program code.      


First the declarations. These are similar to the definition of the Schur complement in  [2.x.248] , with the difference that we need some more preconditioners in the constructor and that the matrices we use here are built upon Trilinos: 

[1.x.137] 



When using a  [2.x.249]  or a  [2.x.250]  the Vector is initialized using an IndexSet. IndexSet is used not only to resize the  [2.x.251]  but it also associates an index in the  [2.x.252]  with a degree of freedom (see  [2.x.253]  for a more detailed explanation). The function complete_index_set() creates an IndexSet where every valid index is part of the set. Note that this program can only be run sequentially and will throw an exception if used in parallel. 

[1.x.138] 



Next is the  [2.x.254]  function. We implement the action of  [2.x.255]  as described above in three successive steps.  In formulas, we want to compute  [2.x.256]  where  [2.x.257]  are both vectors with two block components.      


The first step multiplies the velocity part of the vector by a preconditioner of the matrix  [2.x.258] , i.e., we compute  [2.x.259] .  The resulting velocity vector is then multiplied by  [2.x.260]  and subtracted from the pressure, i.e., we want to compute  [2.x.261] . This second step only acts on the pressure vector and is accomplished by the residual function of our matrix classes, except that the sign is wrong. Consequently, we change the sign in the temporary pressure vector and finally multiply by the inverse pressure mass matrix to get the final pressure vector, completing our work on the Stokes preconditioner: 

[1.x.139] 




[1.x.140]  [1.x.141] 




The definition of the class that defines the top-level logic of solving the time-dependent Boussinesq problem is mainly based on the  [2.x.262]  tutorial program. The main differences are that now we also have to solve for the temperature equation, which forces us to have a second DoFHandler object for the temperature variable as well as matrices, right hand sides, and solution vectors for the current and previous time steps. As mentioned in the introduction, all linear algebra objects are going to use wrappers of the corresponding Trilinos functionality.    


The member functions of this class are reminiscent of  [2.x.263] , where we also used a staggered scheme that first solve the flow equations (here the Stokes equations, in  [2.x.264]  Darcy flow) and then update the advected quantity (here the temperature, there the saturation). The functions that are new are mainly concerned with determining the time step, as well as the proper size of the artificial viscosity stabilization.    


The last three variables indicate whether the various matrices or preconditioners need to be rebuilt the next time the corresponding build functions are called. This allows us to move the corresponding  [2.x.265]  into the respective function and thereby keeping our main  [2.x.266]  function clean and easy to read. 

[1.x.142] 




[1.x.143]  [1.x.144] 





[1.x.145]  [1.x.146]    


The constructor of this class is an extension of the constructor in  [2.x.267] . We need to add the various variables that concern the temperature. As discussed in the introduction, we are going to use  [2.x.268]  (Taylor-Hood) elements again for the Stokes part, and  [2.x.269]  elements for the temperature. However, by using variables that store the polynomial degree of the Stokes and temperature finite elements, it is easy to consistently modify the degree of the elements as well as all quadrature formulas used on them downstream. Moreover, we initialize the time stepping as well as the options for matrix assembly and preconditioning: 

[1.x.147] 




[1.x.148]  [1.x.149] 




Starting the real functionality of this class is a helper function that determines the maximum ( [2.x.270] ) velocity in the domain (at the quadrature points, in fact). How it works should be relatively obvious to all who have gotten to this point of the tutorial. Note that since we are only interested in the velocity, rather than using  [2.x.271]  to get the values of the entire Stokes solution (velocities and pressures) we use  [2.x.272]  to extract only the velocities part. This has the additional benefit that we get it as a Tensor<1,dim>, rather than some components in a Vector<double>, allowing us to process it right away using the  [2.x.273]  function to get the magnitude of the velocity.    


The only point worth thinking about a bit is how to choose the quadrature points we use here. Since the goal of this function is to find the maximal velocity over a domain by looking at quadrature points on each cell. So we should ask how we should best choose these quadrature points on each cell. To this end, recall that if we had a single  [2.x.274]  field (rather than the vector-valued field of higher order) then the maximum would be attained at a vertex of the mesh. In other words, we should use the QTrapezoid class that has quadrature points only at the vertices of cells.    


For higher order shape functions, the situation is more complicated: the maxima and minima may be attained at points between the support points of shape functions (for the usual  [2.x.275]  elements the support points are the equidistant Lagrange interpolation points); furthermore, since we are looking for the maximum magnitude of a vector-valued quantity, we can even less say with certainty where the set of potential maximal points are. Nevertheless, intuitively if not provably, the Lagrange interpolation points appear to be a better choice than the Gauss points.    


There are now different methods to produce a quadrature formula with quadrature points equal to the interpolation points of the finite element. One option would be to use the  [2.x.276]  function, reduce the output to a unique set of points to avoid duplicate function evaluations, and create a Quadrature object using these points. Another option, chosen here, is to use the QTrapezoid class and combine it with the QIterated class that repeats the QTrapezoid formula on a number of sub-cells in each coordinate direction. To cover all support points, we need to iterate it  [2.x.277]  times since this is the polynomial degree of the Stokes element in use: 

[1.x.150] 




[1.x.151]  [1.x.152] 




Next a function that determines the minimum and maximum temperature at quadrature points inside  [2.x.278]  when extrapolated from the two previous time steps to the current one. We need this information in the computation of the artificial viscosity parameter  [2.x.279]  as discussed in the introduction.    


The formula for the extrapolated temperature is  [2.x.280] . The way to compute it is to loop over all quadrature points and update the maximum and minimum value if the current value is bigger/smaller than the previous one. We initialize the variables that store the max and min before the loop over all quadrature points by the smallest and the largest number representable as a double. Then we know for a fact that it is larger/smaller than the minimum/maximum and that the loop over all quadrature points is ultimately going to update the initial value with the correct one.    


The only other complication worth mentioning here is that in the first time step,  [2.x.281]  is not yet available of course. In that case, we can only use  [2.x.282]  which we have from the initial temperature. As quadrature points, we use the same choice as in the previous function though with the difference that now the number of repetitions is determined by the polynomial degree of the temperature field. 

[1.x.153] 




[1.x.154]  [1.x.155] 




The last of the tool functions computes the artificial viscosity parameter  [2.x.283]  on a cell  [2.x.284]  as a function of the extrapolated temperature, its gradient and Hessian (second derivatives), the velocity, the right hand side  [2.x.285]  all on the quadrature points of the current cell, and various other parameters as described in detail in the introduction.    


There are some universal constants worth mentioning here. First, we need to fix  [2.x.286] ; we choose  [2.x.287] , a choice discussed in detail in the results section of this tutorial program. The second is the exponent  [2.x.288] ;  [2.x.289]  appears to work fine for the current program, even though some additional benefit might be expected from choosing  [2.x.290] . Finally, there is one thing that requires special casing: In the first time step, the velocity equals zero, and the formula for  [2.x.291]  is not defined. In that case, we return  [2.x.292] , a choice admittedly more motivated by heuristics than anything else (it is in the same order of magnitude, however, as the value returned for most cells on the second time step).    


The rest of the function should be mostly obvious based on the material discussed in the introduction: 

[1.x.156] 




[1.x.157]  [1.x.158]    


This is the function that sets up the DoFHandler objects we have here (one for the Stokes part and one for the temperature part) as well as set to the right sizes the various objects required for the linear algebra in this program. Its basic operations are similar to what we do in  [2.x.293] .    


The body of the function first enumerates all degrees of freedom for the Stokes and temperature systems. For the Stokes part, degrees of freedom are then sorted to ensure that velocities precede pressure DoFs so that we can partition the Stokes matrix into a  [2.x.294]  matrix. As a difference to  [2.x.295] , we do not perform any additional DoF renumbering. In that program, it paid off since our solver was heavily dependent on ILU's, whereas we use AMG here which is not sensitive to the DoF numbering. The IC preconditioner for the inversion of the pressure mass matrix would of course take advantage of a Cuthill-McKee like renumbering, but its costs are low compared to the velocity portion, so the additional work does not pay off.    


We then proceed with the generation of the hanging node constraints that arise from adaptive grid refinement for both DoFHandler objects. For the velocity, we impose no-flux boundary conditions  [2.x.296]  by adding constraints to the object that already stores the hanging node constraints matrix. The second parameter in the function describes the first of the velocity components in the total dof vector, which is zero here. The variable  [2.x.297]  denotes the boundary indicators for which to set the no flux boundary conditions; here, this is boundary indicator zero.    


After having done so, we count the number of degrees of freedom in the various blocks: 

[1.x.159] 



The next step is to create the sparsity pattern for the Stokes and temperature system matrices as well as the preconditioner matrix from which we build the Stokes preconditioner. As in  [2.x.298] , we choose to create the pattern by using the blocked version of DynamicSparsityPattern.      


So, we first release the memory stored in the matrices, then set up an object of type BlockDynamicSparsityPattern consisting of  [2.x.299]  blocks (for the Stokes system matrix and preconditioner) or DynamicSparsityPattern (for the temperature part). We then fill these objects with the nonzero pattern, taking into account that for the Stokes system matrix, there are no entries in the pressure-pressure block (but all velocity vector components couple with each other and with the pressure). Similarly, in the Stokes preconditioner matrix, only the diagonal blocks are nonzero, since we use the vector Laplacian as discussed in the introduction. This operator only couples each vector component of the Laplacian with itself, but not with the other vector components. (Application of the constraints resulting from the no-flux boundary conditions will couple vector components at the boundary again, however.)      


When generating the sparsity pattern, we directly apply the constraints from hanging nodes and no-flux boundary conditions. This approach was already used in  [2.x.300] , but is different from the one in early tutorial programs where we first built the original sparsity pattern and only then added the entries resulting from constraints. The reason for doing so is that later during assembly we are going to distribute the constraints immediately when transferring local to global dofs. Consequently, there will be no data written at positions of constrained degrees of freedom, so we can let the  [2.x.301]  function omit these entries by setting the last Boolean flag to  [2.x.302] . Once the sparsity pattern is ready, we can use it to initialize the Trilinos matrices. Since the Trilinos matrices store the sparsity pattern internally, there is no need to keep the sparsity pattern around after the initialization of the matrix. 

[1.x.160] 



The creation of the temperature matrix (or, rather, matrices, since we provide a temperature mass matrix and a temperature stiffness matrix, that will be added together for time discretization) follows the generation of the Stokes matrix &ndash; except that it is much easier here since we do not need to take care of any blocks or coupling between components. Note how we initialize the three temperature matrices: We only use the sparsity pattern for reinitialization of the first matrix, whereas we use the previously generated matrix for the two remaining reinits. The reason for doing so is that reinitialization from an already generated matrix allows Trilinos to reuse the sparsity pattern instead of generating a new one for each copy. This saves both some time and memory. 

[1.x.161] 



Lastly, we set the vectors for the Stokes solutions  [2.x.303]  and  [2.x.304] , as well as for the temperatures  [2.x.305] ,  [2.x.306]  and  [2.x.307]  (required for time stepping) and all the system right hand sides to their correct sizes and block structure: 

[1.x.162] 




[1.x.163]  [1.x.164]    


This function assembles the matrix we use for preconditioning the Stokes system. What we need are a vector Laplace matrix on the velocity components and a mass matrix weighted by  [2.x.308]  on the pressure component. We start by generating a quadrature object of appropriate order, the FEValues object that can give values and gradients at the quadrature points (together with quadrature weights). Next we create data structures for the cell matrix and the relation between local and global DoFs. The vectors  [2.x.309]  are going to hold the values of the basis functions in order to faster build up the local matrices, as was already done in  [2.x.310] . Before we start the loop over all active cells, we have to specify which components are pressure and which are velocity. 

[1.x.165] 



The creation of the local matrix is rather simple. There are only a Laplace term (on the velocity) and a mass matrix weighted by  [2.x.311]  to be generated, so the creation of the local matrix is done in two lines. Once the local matrix is ready (loop over rows and columns in the local matrix on each quadrature point), we get the local DoF indices and write the local information into the global matrix. We do this as in  [2.x.312] , i.e., we directly apply the constraints from hanging nodes locally. By doing so, we don't have to do that afterwards, and we don't also write into entries of the matrix that will actually be set to zero again later when eliminating constraints. 

[1.x.166] 




[1.x.167]  [1.x.168]    


This function generates the inner preconditioners that are going to be used for the Schur complement block preconditioner. Since the preconditioners need only to be regenerated when the matrices change, this function does not have to do anything in case the matrices have not changed (i.e., the flag  [2.x.313]  has the value  [2.x.314] ). Otherwise its first task is to call  [2.x.315]  to generate the preconditioner matrices.    


Next, we set up the preconditioner for the velocity-velocity matrix  [2.x.316] . As explained in the introduction, we are going to use an AMG preconditioner based on a vector Laplace matrix  [2.x.317]  (which is spectrally close to the Stokes matrix  [2.x.318] ). Usually, the  [2.x.319]  class can be seen as a good black-box preconditioner which does not need any special knowledge. In this case, however, we have to be careful: since we build an AMG for a vector problem, we have to tell the preconditioner setup which dofs belong to which vector component. We do this using the function  [2.x.320]  a function that generates a set of  [2.x.321]  vectors, where each one has ones in the respective component of the vector problem and zeros elsewhere. Hence, these are the constant modes on each component, which explains the name of the variable. 

[1.x.169] 



Next, we set some more options of the AMG preconditioner. In particular, we need to tell the AMG setup that we use quadratic basis functions for the velocity matrix (this implies more nonzero elements in the matrix, so that a more robust algorithm needs to be chosen internally). Moreover, we want to be able to control how the coarsening structure is build up. The way the Trilinos smoothed aggregation AMG does this is to look which matrix entries are of similar size as the diagonal entry in order to algebraically build a coarse-grid structure. By setting the parameter  [2.x.322]  to 0.02, we specify that all entries that are more than two percent of size of some diagonal pivots in that row should form one coarse grid point. This parameter is rather ad hoc, and some fine-tuning of it can influence the performance of the preconditioner. As a rule of thumb, larger values of  [2.x.323]  will decrease the number of iterations, but increase the costs per iteration. A look at the Trilinos documentation will provide more information on these parameters. With this data set, we then initialize the preconditioner with the matrix we want it to apply to.      


Finally, we also initialize the preconditioner for the inversion of the pressure mass matrix. This matrix is symmetric and well-behaved, so we can chose a simple preconditioner. We stick with an incomplete Cholesky (IC) factorization preconditioner, which is designed for symmetric matrices. We could have also chosen an SSOR preconditioner with relaxation factor around 1.2, but IC is cheaper for our example. We wrap the preconditioners into a  [2.x.324]  pointer, which makes it easier to recreate the preconditioner next time around since we do not have to care about destroying the previously used object. 

[1.x.170] 




[1.x.171]  [1.x.172]    


The time lag scheme we use for advancing the coupled Stokes-temperature system forces us to split up the assembly (and the solution of linear systems) into two step. The first one is to create the Stokes system matrix and right hand side, and the second is to create matrix and right hand sides for the temperature dofs, which depends on the result of the linear system for the velocity.    


This function is called at the beginning of each time step. In the first time step or if the mesh has changed, indicated by the  [2.x.325] , we need to assemble the Stokes matrix; on the other hand, if the mesh hasn't changed and the matrix is already available, this is not necessary and all we need to do is assemble the right hand side vector which changes in each time step.    


Regarding the technical details of implementation, not much has changed from  [2.x.326] . We reset matrix and vector, create a quadrature formula on the cells, and then create the respective FEValues object. For the update flags, we require basis function derivatives only in case of a full assembly, since they are not needed for the right hand side; as always, choosing the minimal set of flags depending on what is currently needed makes the call to  [2.x.327]  further down in the program more efficient.    


There is one thing that needs to be commented &ndash; since we have a separate finite element and DoFHandler for the temperature, we need to generate a second FEValues object for the proper evaluation of the temperature solution. This isn't too complicated to realize here: just use the temperature structures and set an update flag for the basis function values which we need for evaluation of the temperature solution. The only important part to remember here is that the same quadrature formula is used for both FEValues objects to ensure that we get matching information when we loop over the quadrature points of the two objects.    


The declarations proceed with some shortcuts for array sizes, the creation of the local matrix and right hand side as well as the vector for the indices of the local dofs compared to the global system. 

[1.x.173] 



Next we need a vector that will contain the values of the temperature solution at the previous time level at the quadrature points to assemble the source term in the right hand side of the momentum equation. Let's call this vector  [2.x.328] .      


The set of vectors we create next hold the evaluations of the basis functions as well as their gradients and symmetrized gradients that will be used for creating the matrices. Putting these into their own arrays rather than asking the FEValues object for this information each time it is needed is an optimization to accelerate the assembly process, see  [2.x.329]  for details.      


The last two declarations are used to extract the individual blocks (velocity, pressure, temperature) from the total FE system. 

[1.x.174] 



Now start the loop over all cells in the problem. We are working on two different DoFHandlers for this assembly routine, so we must have two different cell iterators for the two objects in use. This might seem a bit peculiar, since both the Stokes system and the temperature system use the same grid, but that's the only way to keep degrees of freedom in sync. The first statements within the loop are again all very familiar, doing the update of the finite element data as specified by the update flags, zeroing out the local arrays and getting the values of the old solution at the quadrature points. Then we are ready to loop over the quadrature points on the cell. 

[1.x.175] 



Next we extract the values and gradients of basis functions relevant to the terms in the inner products. As shown in  [2.x.330]  this helps accelerate assembly.              


Once this is done, we start the loop over the rows and columns of the local matrix and feed the matrix with the relevant products. The right hand side is filled with the forcing term driven by temperature in direction of gravity (which is vertical in our example).  Note that the right hand side term is always generated, whereas the matrix contributions are only updated when it is requested by the  [2.x.331]  flag. 

[1.x.176] 



The last step in the loop over all cells is to enter the local contributions into the global matrix and vector structures to the positions specified in  [2.x.332] .  Again, we let the AffineConstraints class do the insertion of the cell matrix elements to the global matrix, which already condenses the hanging node constraints. 

[1.x.177] 




[1.x.178]  [1.x.179]    


This function assembles the matrix in the temperature equation. The temperature matrix consists of two parts, a mass matrix and the time step size times a stiffness matrix given by a Laplace term times the amount of diffusion. Since the matrix depends on the time step size (which varies from one step to another), the temperature matrix needs to be updated every time step. We could simply regenerate the matrices in every time step, but this is not really efficient since mass and Laplace matrix do only change when we change the mesh. Hence, we do this more efficiently by generating two separate matrices in this function, one for the mass matrix and one for the stiffness (diffusion) matrix. We will then sum up the matrix plus the stiffness matrix times the time step size once we know the actual time step.    


So the details for this first step are very simple. In case we need to rebuild the matrix (i.e., the mesh has changed), we zero the data structures, get a quadrature formula and a FEValues object, and create local matrices, local dof indices and evaluation structures for the basis functions. 

[1.x.180] 



Now, let's start the loop over all cells in the triangulation. We need to zero out the local matrices, update the finite element evaluations, and then loop over the rows and columns of the matrices on each quadrature point, where we then create the mass matrix and the stiffness matrix (Laplace terms times the diffusion  [2.x.333] . Finally, we let the constraints object insert these values into the global matrix, and directly condense the constraints into the matrix. 

[1.x.181] 




[1.x.182]  [1.x.183]    


This function does the second part of the assembly work on the temperature matrix, the actual addition of pressure mass and stiffness matrix (where the time step size comes into play), as well as the creation of the velocity-dependent right hand side. The declarations for the right hand side assembly in this function are pretty much the same as the ones used in the other assembly routines, except that we restrict ourselves to vectors this time. We are going to calculate residuals on the temperature system, which means that we have to evaluate second derivatives, specified by the update flag  [2.x.334] .    


The temperature equation is coupled to the Stokes system by means of the fluid velocity. These two parts of the solution are associated with different DoFHandlers, so we again need to create a second FEValues object for the evaluation of the velocity at the quadrature points. 

[1.x.184] 



Next comes the declaration of vectors to hold the old and older solution values (as a notation for time levels  [2.x.335]  and  [2.x.336] , respectively) and gradients at quadrature points of the current cell. We also declare an object to hold the temperature right hand side values ( [2.x.337] ), and we again use shortcuts for the temperature basis functions. Eventually, we need to find the temperature extrema and the diameter of the computational domain which will be used for the definition of the stabilization parameter (we got the maximal velocity as an input to this function). 

[1.x.185] 



Now, let's start the loop over all cells in the triangulation. Again, we need two cell iterators that walk in parallel through the cells of the two involved DoFHandler objects for the Stokes and temperature part. Within the loop, we first set the local rhs to zero, and then get the values and derivatives of the old solution functions at the quadrature points, since they are going to be needed for the definition of the stabilization parameters and as coefficients in the equation, respectively. Note that since the temperature has its own DoFHandler and FEValues object we get the entire solution at the quadrature point (which is the scalar temperature field only anyway) whereas for the Stokes part we restrict ourselves to extracting the velocity part (and ignoring the pressure part) by using  [2.x.338] . 

[1.x.186] 



Next, we calculate the artificial viscosity for stabilization according to the discussion in the introduction using the dedicated function. With that at hand, we can get into the loop over quadrature points and local rhs vector components. The terms here are quite lengthy, but their definition follows the time-discrete system developed in the introduction of this program. The BDF-2 scheme needs one more term from the old time step (and involves more complicated factors) than the backward Euler scheme that is used for the first time step. When all this is done, we distribute the local vector into the global one (including hanging node constraints). 

[1.x.187] 




[1.x.188]  [1.x.189]    


This function solves the linear systems of equations. Following the introduction, we start with the Stokes system, where we need to generate our block Schur preconditioner. Since all the relevant actions are implemented in the class  [2.x.339] , all we have to do is to initialize the class appropriately. What we need to pass down is an  [2.x.340]  object for the pressure mass matrix, which we set up using the respective class together with the IC preconditioner we already generated, and the AMG preconditioner for the velocity-velocity matrix. Note that both  [2.x.341]  and  [2.x.342]  are only pointers, so we use  [2.x.343]  to pass down the actual preconditioner objects.    


Once the preconditioner is ready, we create a GMRES solver for the block system. Since we are working with Trilinos data structures, we have to set the respective template argument in the solver. GMRES needs to internally store temporary vectors for each iteration (see the discussion in the results section of  [2.x.344] ) &ndash; the more vectors it can use, the better it will generally perform. To keep memory demands in check, we set the number of vectors to 100. This means that up to 100 solver iterations, every temporary vector can be stored. If the solver needs to iterate more often to get the specified tolerance, it will work on a reduced set of vectors by restarting at every 100 iterations.    


With this all set up, we solve the system and distribute the constraints in the Stokes system, i.e., hanging nodes and no-flux boundary condition, in order to have the appropriate solution values even at constrained dofs. Finally, we write the number of iterations to the screen. 

[1.x.190] 



Once we know the Stokes solution, we can determine the new time step from the maximal velocity. We have to do this to satisfy the CFL condition since convection terms are treated explicitly in the temperature equation, as discussed in the introduction. The exact form of the formula used here for the time step is discussed in the results section of this program.      


There is a snatch here. The formula contains a division by the maximum value of the velocity. However, at the start of the computation, we have a constant temperature field (we start with a constant temperature, and it will be nonconstant only after the first time step during which the source acts). Constant temperature means that no buoyancy acts, and so the velocity is zero. Dividing by it will not likely lead to anything good.      


To avoid the resulting infinite time step, we ask whether the maximal velocity is very small (in particular smaller than the values we encounter during any of the following time steps) and if so rather than dividing by zero we just divide by a small value, resulting in a large but finite time step. 

[1.x.191] 



Next we set up the temperature system and the right hand side using the function  [2.x.345] .  Knowing the matrix and right hand side of the temperature equation, we set up a preconditioner and a solver. The temperature matrix is a mass matrix (with eigenvalues around one) plus a Laplace matrix (with eigenvalues between zero and  [2.x.346] ) times a small number proportional to the time step  [2.x.347] . Hence, the resulting symmetric and positive definite matrix has eigenvalues in the range  [2.x.348]  (up to constants). This matrix is only moderately ill conditioned even for small mesh sizes and we get a reasonably good preconditioner by simple means, for example with an incomplete Cholesky decomposition preconditioner (IC) as we also use for preconditioning the pressure mass matrix solver. As a solver, we choose the conjugate gradient method CG. As before, we tell the solver to use Trilinos vectors via the template argument  [2.x.349] . Finally, we solve, distribute the hanging node constraints and write out the number of iterations. 

[1.x.192] 



At the end of this function, we step through the vector and read out the maximum and minimum temperature value, which we also want to output. This will come in handy when determining the correct constant in the choice of time step as discuss in the results section of this program. 

[1.x.193] 




[1.x.194]  [1.x.195]    


This function writes the solution to a VTK output file for visualization, which is done every tenth time step. This is usually quite a simple task, since the deal.II library provides functions that do almost all the job for us. There is one new function compared to previous examples: We want to visualize both the Stokes solution and the temperature as one data set, but we have done all the calculations based on two different DoFHandler objects. Luckily, the DataOut class is prepared to deal with it. All we have to do is to not attach one single DoFHandler at the beginning and then use that for all added vector, but specify the DoFHandler to each vector separately. The rest is done as in  [2.x.350] . We create solution names (that are going to appear in the visualization program for the individual components). The first  [2.x.351]  components are the vector velocity, and then we have pressure for the Stokes part, whereas temperature is scalar. This information is read out using the DataComponentInterpretation helper class. Next, we actually attach the data vectors with their DoFHandler objects, build patches according to the degree of freedom, which are (sub-) elements that describe the data for visualization programs. Finally, we open a file (that includes the time step number) and write the vtk data into it. 

[1.x.196] 




[1.x.197]  [1.x.198]    


This function takes care of the adaptive mesh refinement. The three tasks this function performs is to first find out which cells to refine/coarsen, then to actually do the refinement and eventually transfer the solution vectors between the two different grids. The first task is simply achieved by using the well-established Kelly error estimator on the temperature (it is the temperature we're mainly interested in for this program, and we need to be accurate in regions of high temperature gradients, also to not have too much numerical diffusion). The second task is to actually do the remeshing. That involves only basic functions as well, such as the  [2.x.352]  that refines those cells with the largest estimated error that together make up 80 per cent of the error, and coarsens those cells with the smallest error that make up for a combined 10 per cent of the error.    


If implemented like this, we would get a program that will not make much progress: Remember that we expect temperature fields that are nearly discontinuous (the diffusivity  [2.x.353]  is very small after all) and consequently we can expect that a freely adapted mesh will refine further and further into the areas of large gradients. This decrease in mesh size will then be accompanied by a decrease in time step, requiring an exceedingly large number of time steps to solve to a given final time. It will also lead to meshes that are much better at resolving discontinuities after several mesh refinement cycles than in the beginning.    


In particular to prevent the decrease in time step size and the correspondingly large number of time steps, we limit the maximal refinement depth of the mesh. To this end, after the refinement indicator has been applied to the cells, we simply loop over all cells on the finest level and unselect them from refinement if they would result in too high a mesh level. 

[1.x.199] 



As part of mesh refinement we need to transfer the solution vectors from the old mesh to the new one. To this end we use the SolutionTransfer class and we have to prepare the solution vectors that should be transferred to the new grid (we will lose the old grid once we have done the refinement so the transfer has to happen concurrently with refinement). What we definitely need are the current and the old temperature (BDF-2 time stepping requires two old solutions). Since the SolutionTransfer objects only support to transfer one object per dof handler, we need to collect the two temperature solutions in one data structure. Moreover, we choose to transfer the Stokes solution, too, since we need the velocity at two previous time steps, of which only one is calculated on the fly.      


Consequently, we initialize two SolutionTransfer objects for the Stokes and temperature DoFHandler objects, by attaching them to the old dof handlers. With this at place, we can prepare the triangulation and the data vectors for refinement (in this order). 

[1.x.200] 



Now everything is ready, so do the refinement and recreate the dof structure on the new grid, and initialize the matrix structures and the new vectors in the  [2.x.354]  function. Next, we actually perform the interpolation of the solutions between the grids. We create another copy of temporary vectors for temperature (now corresponding to the new grid), and let the interpolate function do the job. Then, the resulting array of vectors is written into the respective vector member variables.      


Remember that the set of constraints will be updated for the new triangulation in the setup_dofs() call. 

[1.x.201] 



After the solution has been transferred we then enforce the constraints on the transferred solution. 

[1.x.202] 



For the Stokes vector, everything is just the same &ndash; except that we do not need another temporary vector since we just interpolate a single vector. In the end, we have to tell the program that the matrices and preconditioners need to be regenerated, since the mesh has changed. 

[1.x.203] 




[1.x.204]  [1.x.205]    


This function performs all the essential steps in the Boussinesq program. It starts by setting up a grid (depending on the spatial dimension, we choose some different level of initial refinement and additional adaptive refinement steps, and then create a cube in  [2.x.355]  dimensions and set up the dofs for the first time. Since we want to start the time stepping already with an adaptively refined grid, we perform some pre-refinement steps, consisting of all assembly, solution and refinement, but without actually advancing in time. Rather, we use the vilified  [2.x.356]  statement to jump out of the time loop right after mesh refinement to start all over again on the new mesh beginning at the  [2.x.357]  label. (The use of the  [2.x.358]  is discussed in  [2.x.359] .)    


Before we start, we project the initial values to the grid and obtain the first data for the  [2.x.360]  vector. Then, we initialize time step number and time step and start the time loop. 

[1.x.206] 



The first steps in the time loop are all obvious &ndash; we assemble the Stokes system, the preconditioner, the temperature matrix (matrices and preconditioner do actually only change in case we've remeshed before), and then do the solve. Before going on with the next time step, we have to check whether we should first finish the pre-refinement steps or if we should remesh (every fifth time step), refining up to a level that is consistent with initial refinement and pre-refinement steps. Last in the loop is to advance the solutions, i.e., to copy the solutions to the next "older" time level. 

[1.x.207] 



Do all the above until we arrive at time 100. 

[1.x.208] 




[1.x.209]  [1.x.210] 




The main function looks almost the same as in all other programs. 




There is one difference we have to be careful about. This program uses Trilinos and, typically, Trilinos is configured so that it can run in %parallel using MPI. This doesn't mean that it [1.x.211] to run in %parallel, and in fact this program (unlike  [2.x.361] ) makes no attempt at all to do anything in %parallel using MPI. Nevertheless, Trilinos wants the MPI system to be initialized. We do that be creating an object of type  [2.x.362]  that initializes MPI (if available) using the arguments given to main() (i.e.,  [2.x.363]  and  [2.x.364] ) and de-initializes it again when the object goes out of scope. 

[1.x.212] 



This program can only be run in serial. Otherwise, throw an exception. 

[1.x.213] 

[1.x.214][1.x.215] 


[1.x.216][1.x.217] 


When you run the program in 2d, the output will look something like this: <code> <pre> Number of active cells: 256 (on 5 levels) Number of degrees of freedom: 3556 (2178+289+1089) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.919118    9 CG iterations for temperature.    Temperature range: -0.16687 1.30011 

Number of active cells: 280 (on 6 levels) Number of degrees of freedom: 4062 (2490+327+1245) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.459559    9 CG iterations for temperature.    Temperature range: -0.0982971 0.598503 

Number of active cells: 520 (on 7 levels) Number of degrees of freedom: 7432 (4562+589+2281) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.229779    9 CG iterations for temperature.    Temperature range: -0.0551098 0.294493 

Number of active cells: 1072 (on 8 levels) Number of degrees of freedom: 15294 (9398+1197+4699) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.11489    9 CG iterations for temperature.    Temperature range: -0.0273524 0.156861 

Number of active cells: 2116 (on 9 levels) Number of degrees of freedom: 30114 (18518+2337+9259) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.0574449    9 CG iterations for temperature.    Temperature range: -0.014993 0.0738328 

Timestep 1:  t=0.0574449    Assembling...    Solving...    56 GMRES iterations for Stokes subsystem.    Time step: 0.0574449    9 CG iterations for temperature.    Temperature range: -0.0273934 0.14488 

... </pre> </code> 

In the beginning we refine the mesh several times adaptively and always return to time step zero to restart on the newly refined mesh. Only then do we start the actual time iteration. 

The program runs for a while. The temperature field for time steps 0, 500, 1000, 1500, 2000, 3000, 4000, and 5000 looks like this (note that the color scale used for the temperature is not always the same): 

 [2.x.365]  

The visualizations shown here were generated using a version of the example which did not enforce the constraints after transferring the mesh. 

As can be seen, we have three heat sources that heat fluid and therefore produce a buoyancy effect that lets hots pockets of fluid rise up and swirl around. By a chimney effect, the three streams are pressed together by fluid that comes from the outside and wants to join the updraft party. Note that because the fluid is initially at rest, those parts of the fluid that were initially over the sources receive a longer heating time than that fluid that is later dragged over the source by the fully developed flow field. It is therefore hotter, a fact that can be seen in the red tips of the three plumes. Note also the relatively fine features of the flow field, a result of the sophisticated transport stabilization of the temperature equation we have chosen. 

In addition to the pictures above, the following ones show the adaptive mesh and the flow field at the same time steps: 

 [2.x.366]  


[1.x.218][1.x.219] 


The same thing can of course be done in 3d by changing the template parameter to the BoussinesqFlowProblem object in  [2.x.367]  from 2 to 3, so that the output now looks like follows: 

<code> <pre> Number of active cells: 64 (on 3 levels) Number of degrees of freedom: 3041 (2187+125+729) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 2.45098    9 CG iterations for temperature.    Temperature range: -0.675683 4.94725 

Number of active cells: 288 (on 4 levels) Number of degrees of freedom: 12379 (8943+455+2981) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 1.22549    9 CG iterations for temperature.    Temperature range: -0.527701 2.25764 

Number of active cells: 1296 (on 5 levels) Number of degrees of freedom: 51497 (37305+1757+12435) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.612745    10 CG iterations for temperature.    Temperature range: -0.496942 0.847395 

Number of active cells: 5048 (on 6 levels) Number of degrees of freedom: 192425 (139569+6333+46523) 

Timestep 0:  t=0    Assembling...    Rebuilding Stokes preconditioner...    Solving...    0 GMRES iterations for Stokes subsystem.    Time step: 0.306373    10 CG iterations for temperature.    Temperature range: -0.267683 0.497739 

Timestep 1:  t=0.306373    Assembling...    Solving...    27 GMRES iterations for Stokes subsystem.    Time step: 0.306373    10 CG iterations for temperature.    Temperature range: -0.461787 0.958679 

... </pre> </code> 

Visualizing the temperature isocontours at time steps 0, 50, 100, 150, 200, 300, 400, 500, 600, 700, and 800 yields the following plots: 

 [2.x.368]  

That the first picture looks like three hedgehogs stems from the fact that our scheme essentially projects the source times the first time step size onto the mesh to obtain the temperature field in the first time step. Since the source function is discontinuous, we need to expect over- and undershoots from this project. This is in fact what happens (it's easier to check this in 2d) and leads to the crumpled appearance of the isosurfaces.  The visualizations shown here were generated using a version of the example which did not enforce the constraints after transferring the mesh. 




[1.x.220][1.x.221] 


The program as is has three parameters that we don't have much of a theoretical handle on how to choose in an optimal way. These are:  [2.x.369]     [2.x.370] The time step must satisfy a CFL condition        [2.x.371] . Here,  [2.x.372]  is       dimensionless, but what is the right value?    [2.x.373] In the computation of the artificial viscosity, [1.x.222] 

      with  [2.x.374] .       Here, the choice of the dimensionless %numbers  [2.x.375]  is of       interest.  [2.x.376]  In all of these cases, we will have to expect that the correct choice of each value depends on that of the others, and most likely also on the space dimension and polynomial degree of the finite element used for the temperature. Below we'll discuss a few numerical experiments to choose constants  [2.x.377]  and  [2.x.378] . 

Below, we will not discuss the choice of  [2.x.379] . In the program, we set it to  [2.x.380] . The reason for this value is a bit complicated and has more to do with the history of the program than reasoning: while the correct formula for the global scaling parameter  [2.x.381]  is shown above, the program (including the version shipped with deal.II 6.2) initially had a bug in that we computed  [2.x.382]  instead, where we had set the scaling parameter to one. Since we only computed on the unit square/cube where  [2.x.383] , this was entirely equivalent to using the correct formula with  [2.x.384] . Since this value for  [2.x.385]  appears to work just fine for the current program, we corrected the formula in the program and set  [2.x.386]  to a value that reproduces exactly the results we had before. We will, however, revisit this issue again in  [2.x.387] . 

Now, however, back to the discussion of what values of  [2.x.388]  and  [2.x.389]  to choose: 


[1.x.223][1.x.224][1.x.225] 


These two constants are definitely linked in some way. The reason is easy to see: In the case of a pure advection problem,  [2.x.390] , any explicit scheme has to satisfy a CFL condition of the form  [2.x.391] . On the other hand, for a pure diffusion problem,  [2.x.392] , explicit schemes need to satisfy a condition  [2.x.393] . So given the form of  [2.x.394]  above, an advection diffusion problem like the one we have to solve here will result in a condition of the form  [2.x.395] . It follows that we have to face the fact that we might want to choose  [2.x.396]  larger to improve the stability of the numerical scheme (by increasing the amount of artificial diffusion), but we have to pay a price in the form of smaller, and consequently more time steps. In practice, one would therefore like to choose  [2.x.397]  as small as possible to keep the transport problem sufficiently stabilized while at the same time trying to choose the time step as large as possible to reduce the overall amount of work. 

The find the right balance, the only way is to do a few computational experiments. Here's what we did: We modified the program slightly to allow less mesh refinement (so we don't always have to wait that long) and to choose  [2.x.398]  to eliminate the effect of the constant  [2.x.399]  (we know that solutions are stable by using this version of  [2.x.400]  as an artificial viscosity, but that we can improve things -- i.e. make the solution sharper -- by using the more complicated formula for this artificial viscosity). We then run the program for different values  [2.x.401]  and observe maximal and minimal temperatures in the domain. What we expect to see is this: If we choose the time step too big (i.e. choose a  [2.x.402]  bigger than theoretically allowed) then we will get exponential growth of the temperature. If we choose  [2.x.403]  too small, then the transport stabilization becomes insufficient and the solution will show significant oscillations but not exponential growth. 


[1.x.226][1.x.227] 


Here is what we get for  [2.x.404] , and  [2.x.405] , different choices of  [2.x.406] , and bilinear elements ( [2.x.407] ) in 2d: 

 [2.x.408]  

The way to interpret these graphs goes like this: for  [2.x.409]  and  [2.x.410] , we see exponential growth or at least large variations, but if we choose  [2.x.411]  or smaller, then the scheme is stable though a bit wobbly. For more artificial diffusion, we can choose  [2.x.412]  or smaller for  [2.x.413] ,  [2.x.414]  or smaller for  [2.x.415] , and again need  [2.x.416]  for  [2.x.417]  (this time because much diffusion requires a small time step). 

So how to choose? If we were simply interested in a large time step, then we would go with  [2.x.418]  and  [2.x.419] . On the other hand, we're also interested in accuracy and here it may be of interest to actually investigate what these curves show. To this end note that we start with a zero temperature and that our sources are positive &mdash; so we would intuitively expect that the temperature can never drop below zero. But it does, a consequence of Gibb's phenomenon when using continuous elements to approximate a discontinuous solution. We can therefore see that choosing  [2.x.420]  too small is bad: too little artificial diffusion leads to over- and undershoots that aren't diffused away. On the other hand, for large  [2.x.421] , the minimum temperature drops below zero at the beginning but then quickly diffuses back to zero. 

On the other hand, let's also look at the maximum temperature. Watching the movie of the solution, we see that initially the fluid is at rest. The source keeps heating the same volume of fluid whose temperature increases linearly at the beginning until its buoyancy is able to move it upwards. The hottest part of the fluid is therefore transported away from the solution and fluid taking its place is heated for only a short time before being moved out of the source region, therefore remaining cooler than the initial bubble. If  [2.x.422]  (in the program it is nonzero but very small) then the hottest part of the fluid should be advected along with the flow with its temperature constant. That's what we can see in the graphs with the smallest  [2.x.423] : Once the maximum temperature is reached, it hardly changes any more. On the other hand, the larger the artificial diffusion, the more the hot spot is diffused. Note that for this criterion, the time step size does not play a significant role. 

So to sum up, likely the best choice would appear to be  [2.x.424]  and  [2.x.425] . The curve is a bit wobbly, but overall pictures looks pretty reasonable with the exception of some over and undershoots close to the start time due to Gibb's phenomenon. 


[1.x.228][1.x.229] 


One can repeat the same sequence of experiments for higher order elements as well. Here are the graphs for bi-quadratic shape functions ( [2.x.426] ) for the temperature, while we retain the  [2.x.427]  stable Taylor-Hood element for the Stokes system: 

 [2.x.428]  

Again, small values of  [2.x.429]  lead to less diffusion but we have to choose the time step very small to keep things under control. Too large values of  [2.x.430]  make for more diffusion, but again require small time steps. The best value would appear to be  [2.x.431] , as for the  [2.x.432]  element, and then we have to choose  [2.x.433]  &mdash; exactly half the size for the  [2.x.434]  element, a fact that may not be surprising if we state the CFL condition as the requirement that the time step be small enough so that the distance transport advects in each time step is no longer than one [1.x.230] away (which for  [2.x.435]  elements is  [2.x.436] , but for  [2.x.437]  elements is  [2.x.438] ). It turns out that  [2.x.439]  needs to be slightly larger for obtaining stable results also late in the simulation at times larger than 60, so we actually choose it as  [2.x.440]  in the code. 


[1.x.231][1.x.232] 


One can repeat these experiments in 3d and find the optimal time step for each value of  [2.x.441]  and find the best value of  [2.x.442] . What one finds is that for the same  [2.x.443]  already used in 2d, the time steps needs to be a bit smaller, by around a factor of 1.2 or so. This is easily explained: the time step restriction is  [2.x.444]  where  [2.x.445]  is the [1.x.233] of the cell. However, what is really needed is the distance between mesh points, which is  [2.x.446] . So a more appropriate form would be  [2.x.447] . 

The second find is that one needs to choose  [2.x.448]  slightly bigger (about  [2.x.449]  or so). This then again reduces the time step we can take. 





[1.x.234][1.x.235] 


Concluding, from the simple computations above,  [2.x.450]  appears to be a good choice for the stabilization parameter in 2d, and  [2.x.451]  in 3d. In a dimension independent way, we can model this as  [2.x.452] . If one does longer computations (several thousand time steps) on finer meshes, one realizes that the time step size is not quite small enough and that for stability one will have to reduce the above values a bit more (by about a factor of  [2.x.453] ). 

As a consequence, a formula that reconciles 2d, 3d, and variable polynomial degree and takes all factors in account reads as follows: [1.x.236] 

In the first form (in the center of the equation),  [2.x.454]  is a universal constant,  [2.x.455]  is the factor that accounts for the difference between cell diameter and grid point separation,  [2.x.456]  accounts for the increase in  [2.x.457]  with space dimension,  [2.x.458]  accounts for the distance between grid points for higher order elements, and  [2.x.459]  for the local speed of transport relative to the cell size. This is the formula that we use in the program. 

As for the question of whether to use  [2.x.460]  or  [2.x.461]  elements for the temperature, the following considerations may be useful: First, solving the temperature equation is hardly a factor in the overall scheme since almost the entire compute time goes into solving the Stokes system in each time step. Higher order elements for the temperature equation are therefore not a significant drawback. On the other hand, if one compares the size of the over- and undershoots the solution produces due to the discontinuous source description, one notices that for the choice of  [2.x.462]  and  [2.x.463]  as above, the  [2.x.464]  solution dips down to around  [2.x.465] , whereas the  [2.x.466]  solution only goes to  [2.x.467]  (remember that the exact solution should never become negative at all. This means that the  [2.x.468]  solution is significantly more accurate; the program therefore uses these higher order elements, despite the penalty we pay in terms of smaller time steps. 


[1.x.237][1.x.238] 


There are various ways to extend the current program. Of particular interest is, of course, to make it faster and/or increase the resolution of the program, in particular in 3d. This is the topic of the  [2.x.469]  tutorial program which will implement strategies to solve this problem in %parallel on a cluster. It is also the basis of the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that can solve realistic problems and that constitutes the further development of  [2.x.470] . 

Another direction would be to make the fluid flow more realistic. The program was initially written to simulate various cases simulating the convection of material in the earth's mantle, i.e. the zone between the outer earth core and the solid earth crust: there, material is heated from below and cooled from above, leading to thermal convection. The physics of this fluid are much more complicated than shown in this program, however: The viscosity of mantle material is strongly dependent on the temperature, i.e.  [2.x.471] , with the dependency frequently modeled as a viscosity that is reduced exponentially with rising temperature. Secondly, much of the dynamics of the mantle is determined by chemical reactions, primarily phase changes of the various crystals that make up the mantle; the buoyancy term on the right hand side of the Stokes equations then depends not only on the temperature, but also on the chemical composition at a given location which is advected by the flow field but also changes as a function of pressure and temperature. We will investigate some of these effects in later tutorial programs as well. [1.x.239] [1.x.240]  [2.x.472]  

 [2.x.473] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45] 

 [2.x.4]  

[1.x.46][1.x.47][1.x.48][1.x.49][1.x.50] 


[1.x.51] [1.x.52][1.x.53] 


This program does pretty much exactly what  [2.x.5]  already does: it solves the Boussinesq equations that describe the motion of a fluid whose temperature is not in equilibrium. As such, all the equations we have described in  [2.x.6]  still hold: we solve the same general partial differential equation (with only minor modifications to adjust for more realism in the problem setting), using the same finite element scheme, the same time stepping algorithm, and more or less the same stabilization method for the temperature advection-diffusion equation. As a consequence, you may first want to understand that program &mdash; and its implementation &mdash; before you work on the current one. 

The difference between  [2.x.7]  and the current program is that here we want to do things in %parallel, using both the availability of many machines in a cluster (with parallelization based on MPI) as well as many processor cores within a single machine (with parallelization based on threads). This program's main job is therefore to introduce the changes that are necessary to utilize the availability of these %parallel compute resources. In this regard, it builds on the  [2.x.8]  program that first introduces the necessary classes for much of the %parallel functionality, and on  [2.x.9]  that shows how this is done for a vector-valued problem. 

In addition to these changes, we also use a slightly different preconditioner, and we will have to make a number of changes that have to do with the fact that we want to solve a [1.x.54] problem here, not a model problem. The latter, in particular, will require that we think about scaling issues as well as what all those parameters and coefficients in the equations under consideration actually mean. We will discuss first the issues that affect changes in the mathematical formulation and solver structure, then how to parallelize things, and finally the actual testcase we will consider. 


[1.x.55][1.x.56] 


In  [2.x.10] , we used the following Stokes model for the velocity and pressure field: [1.x.57] 

The right hand side of the first equation appears a wee bit unmotivated. Here's how things should really be. We need the external forces that act on the fluid, which we assume are given by gravity only. In the current case, we assume that the fluid does expand slightly for the purposes of this gravity force, but not enough that we need to modify the incompressibility condition (the second equation). What this means is that for the purpose of the right hand side, we can assume that  [2.x.11] . An assumption that may not be entirely justified is that we can assume that the changes of density as a function of temperature are small, leading to an expression of the form  [2.x.12] , i.e., the density equals  [2.x.13]  at reference temperature and decreases linearly as the temperature increases (as the material expands). The force balance equation then looks properly written like this: [1.x.58] 

Now note that the gravity force results from a gravity potential as  [2.x.14] , so that we can re-write this as follows: [1.x.59] 

The second term on the right is time independent, and so we could introduce a new "dynamic" pressure  [2.x.15]  with which the Stokes equations would read: [1.x.60] 

This is exactly the form we used in  [2.x.16] , and it was appropriate to do so because all changes in the fluid flow are only driven by the dynamic pressure that results from temperature differences. (In other words: Any contribution to the right hand side that results from taking the gradient of a scalar field have no effect on the velocity field.) 

On the other hand, we will here use the form of the Stokes equations that considers the total pressure instead: [1.x.61] 

There are several advantages to this: 

- This way we can plot the pressure in our program in such a way that it   actually shows the total pressure that includes the effects of   temperature differences as well as the static pressure of the   overlying rocks. Since the pressure does not appear any further in any   of the other equations, whether to use one or the other is more a   matter of taste than of correctness. The flow field is exactly the   same, but we get a pressure that we can now compare with values that   are given in geophysical books as those that hold at the bottom of the   earth mantle, for example. 

- If we wanted to make the model even more realistic, we would have to take   into account that many of the material parameters (e.g. the viscosity, the   density, etc) not only depend on the temperature but also the   [1.x.62] pressure. 

- The model above assumed a linear dependence  [2.x.17]  and assumed that  [2.x.18]  is small. In   practice, this may not be so. In fact, realistic models are   certainly not linear, and  [2.x.19]  may also not be small for at least   part of the temperature range because the density's behavior is   substantially dependent not only on thermal expansion but by phase   changes. 

- A final reason to do this is discussed in the results section and   concerns possible extensions to the model we use here. It has to do   with the fact that the temperature equation (see below) we use here does not   include a term that contains the pressure. It should, however:   rock, like gas, heats up as you compress it. Consequently,   material that rises up cools adiabatically, and cold material that   sinks down heats adiabatically. We discuss this further below. 

 [2.x.20]  There is, however, a downside to this procedure. In the earth, the dynamic pressure is several orders of magnitude smaller than the total pressure. If we use the equations above and solve all variables to, say, 4 digits of accuracy, then we may be able to get the velocity and the total pressure right, but we will have no accuracy at all if we compute the dynamic pressure by subtracting from the total pressure the static part  [2.x.21] . If, for example, the dynamic pressure is six orders of magnitude smaller than the static pressure, then we need to solve the overall pressure to at least seven digits of accuracy to get anything remotely accurate. That said, in practice this turns out not to be a limiting factor. 




[1.x.63][1.x.64] 


Remember that we want to solve the following set of equations: [1.x.65] 

augmented by appropriate boundary and initial conditions. As discussed in  [2.x.22] , we will solve this set of equations by solving for a Stokes problem first in each time step, and then moving the temperature equation forward by one time interval. 

The problem under consideration in this current section is with the Stokes problem: if we discretize it as usual, we get a linear system [1.x.66] 

which in this program we will solve with a FGMRES solver. This solver iterates until the residual of these linear equations is below a certain tolerance, i.e., until [1.x.67] This does not make any sense from the viewpoint of physical units: the quantities involved here have physical units so that the first part of the residual has units  [2.x.23]  (most easily established by considering the term  [2.x.24]  and considering that the pressure has units  [2.x.25]  and the integration yields a factor of  [2.x.26] ), whereas the second part of the residual has units  [2.x.27] . Taking the norm of this residual vector would yield a quantity with units  [2.x.28] . This, quite obviously, does not make sense, and we should not be surprised that doing so is eventually going to come back hurting us. 

So why is this an issue here, but not in  [2.x.29] ? The reason back there is that everything was nicely balanced: velocities were on the order of one, the pressure likewise, the viscosity was one, and the domain had a diameter of  [2.x.30] . As a result, while nonsensical, nothing bad happened. On the other hand, as we will explain below, things here will not be that simply scaled:  [2.x.31]  will be around  [2.x.32] , velocities on the order of  [2.x.33] , pressure around  [2.x.34] , and the diameter of the domain is  [2.x.35] . In other words, the order of magnitude for the first equation is going to be  [2.x.36] , whereas the second equation will be around  [2.x.37] . Well, so what this will lead to is this: if the solver wants to make the residual small, it will almost entirely focus on the first set of equations because they are so much bigger, and ignore the divergence equation that describes mass conservation. That's exactly what happens: unless we set the tolerance to extremely small values, the resulting flow field is definitely not divergence free. As an auxiliary problem, it turns out that it is difficult to find a tolerance that always works; in practice, one often ends up with a tolerance that requires 30 or 40 iterations for most time steps, and 10,000 for some others. 

So what's a numerical analyst to do in a case like this? The answer is to start at the root and first make sure that everything is mathematically consistent first. In our case, this means that if we want to solve the system of Stokes equations jointly, we have to scale them so that they all have the same physical dimensions. In our case, this means multiplying the second equation by something that has units  [2.x.38] ; one choice is to multiply with  [2.x.39]  where  [2.x.40]  is a typical lengthscale in our domain (which experiments show is best chosen to be the diameter of plumes &mdash; around 10 km &mdash; rather than the diameter of the domain). Using these %numbers for  [2.x.41]  and  [2.x.42] , this factor is around  [2.x.43] . So, we now get this for the Stokes system: [1.x.68] 

The trouble with this is that the result is not symmetric any more (we have  [2.x.44]  at the bottom left, but not its transpose operator at the top right). This, however, can be cured by introducing a scaled pressure  [2.x.45] , and we get the scaled equations [1.x.69] 

This is now symmetric. Obviously, we can easily recover the original pressure  [2.x.46]  from the scaled pressure  [2.x.47]  that we compute as a result of this procedure. 

In the program below, we will introduce a factor  [2.x.48]  that corresponds to  [2.x.49] , and we will use this factor in the assembly of the system matrix and preconditioner. Because it is annoying and error prone, we will recover the unscaled pressure immediately following the solution of the linear system, i.e., the solution vector's pressure component will immediately be unscaled to retrieve the physical pressure. Since the solver uses the fact that we can use a good initial guess by extrapolating the previous solutions, we also have to scale the pressure immediately [1.x.70] solving. 




[1.x.71][1.x.72] 


In this tutorial program, we apply a variant of the preconditioner used in  [2.x.50] . That preconditioner was built to operate on the system matrix  [2.x.51]  in block form such that the product matrix [1.x.73] 

is of a form that Krylov-based iterative solvers like GMRES can solve in a few iterations. We then replaced the exact inverse of  [2.x.52]  by the action of an AMG preconditioner  [2.x.53]  based on a vector Laplace matrix, approximated the Schur complement  [2.x.54]  by a mass matrix  [2.x.55]  on the pressure space and wrote an <tt>InverseMatrix</tt> class for implementing the action of  [2.x.56]  on vectors. In the InverseMatrix class, we used a CG solve with an incomplete Cholesky (IC) preconditioner for performing the inner solves. 

An observation one can make is that we use just the action of a preconditioner for approximating the velocity inverse  [2.x.57]  (and the outer GMRES iteration takes care of the approximate character of the inverse), whereas we use a more or less [1.x.74] inverse for  [2.x.58] , realized by a fully converged CG solve. This appears unbalanced, but there's system to this madness: almost all the effort goes into the upper left block to which we apply the AMG preconditioner, whereas even an exact inversion of the pressure mass matrix costs basically nothing. Consequently, if it helps us reduce the overall number of iterations somewhat, then this effort is well spent. 

That said, even though the solver worked well for  [2.x.59] , we have a problem here that is a bit more complicated (cells are deformed, the pressure varies by orders of magnitude, and we want to plan ahead for more complicated physics), and so we'll change a few things slightly: 

- For more complex problems, it turns out that using just a single AMG V-cycle   as preconditioner is not always sufficient. The outer solver converges just   fine most of the time in a reasonable number of iterations (say, less than   50) but there are the occasional time step where it suddenly takes 700 or   so. What exactly is going on there is hard to determine, but the problem can   be avoided by using a more accurate solver for the top left   block. Consequently, we'll want to use a CG iteration to invert the top left   block of the preconditioner matrix, and use the AMG as a preconditioner for   the CG solver. 

- The downside of this is that, of course, the Stokes preconditioner becomes   much more expensive (approximately 10 times more expensive than when we just   use a single V-cycle). Our strategy then is this: let's do up to 30 GMRES   iterations with just the V-cycle as a preconditioner and if that doesn't   yield convergence, then take the best approximation of the Stokes solution   obtained after this first round of iterations and use that as the starting   guess for iterations where we use the full inner solver with a rather   lenient tolerance as preconditioner. In all our experiments this leads to   convergence in only a few additional iterations. 

- One thing we need to pay attention to is that when using a CG with a lenient   tolerance in the preconditioner, then  [2.x.60]  is no longer a   linear function of  [2.x.61]  (it is, of course, if we have a very stringent   tolerance in our solver, or if we only apply a single V-cycle). This is a   problem since now our preconditioner is no longer a linear operator; in   other words, every time GMRES uses it the preconditioner looks   different. The standard GMRES solver can't deal with this, leading to slow   convergence or even breakdown, but the F-GMRES variant is designed to deal   with exactly this kind of situation and we consequently use it. 

- On the other hand, once we have settled on using F-GMRES we can relax the   tolerance used in inverting the preconditioner for  [2.x.62] . In  [2.x.63] , we ran a   preconditioned CG method on  [2.x.64]  until the residual had been reduced   by 7 orders of magnitude. Here, we can again be more lenient because we know   that the outer preconditioner doesn't suffer. 

- In  [2.x.65] , we used a left preconditioner in which we first invert the top   left block of the preconditioner matrix, then apply the bottom left   (divergence) one, and then invert the bottom right. In other words, the   application of the preconditioner acts as a lower left block triangular   matrix. Another option is to use a right preconditioner that here would be   upper right block triangulation, i.e., we first invert the bottom right   Schur complement, apply the top right (gradient) operator and then invert   the elliptic top left block. To a degree, which one to choose is a matter of   taste. That said, there is one significant advantage to a right   preconditioner in GMRES-type solvers: the residual with which we determine   whether we should stop the iteration is the true residual, not the norm of   the preconditioned equations. Consequently, it is much simpler to compare it   to the stopping criterion we typically use, namely the norm of the right   hand side vector. In writing this code we found that the scaling issues we   discussed above also made it difficult to determine suitable stopping   criteria for left-preconditioned linear systems, and consequently this   program uses a right preconditioner. 

- In  [2.x.66] , we used an IC (incomplete Cholesky) preconditioner for the   pressure mass matrix in the Schur complement preconditioner and for the   solution of the temperature system. Here, we could in principle do the same,   but we do choose an even simpler preconditioner, namely a Jacobi   preconditioner for both systems. This is because here we target at massively   %parallel computations, where the decompositions for IC/ILU would have to be   performed block-wise for the locally owned degrees of freedom on each   processor. This means, that the preconditioner gets more like a Jacobi   preconditioner anyway, so we rather start from that variant straight   away. Note that we only use the Jacobi preconditioners for CG solvers with   mass matrices, where they give optimal ([1.x.75]-independent) convergence   anyway, even though they usually require about twice as many iterations as   an IC preconditioner. 

As a final note, let us remark that in  [2.x.67]  we computed the Schur complement  [2.x.68]  by approximating  [2.x.69] . Now, however, we have re-scaled the  [2.x.70]  and  [2.x.71]  operators. So  [2.x.72]  should now approximate  [2.x.73] . We use the discrete form of the right hand side of this as our approximation  [2.x.74]  to  [2.x.75] . 


[1.x.76][1.x.77] 


Similarly to  [2.x.76] , we will use an artificial viscosity for stabilization based on a residual of the equation.  As a difference to  [2.x.77] , we will provide two slightly different definitions of the stabilization parameter. For  [2.x.78] , we use the same definition as in  [2.x.79] : [1.x.78] 

where we compute the viscosity from a residual  [2.x.80]  of the equation, limited by a diffusion proportional to the mesh size  [2.x.81]  in regions where the residual is large (around steep gradients). This definition has been shown to work well for the given case,  [2.x.82]  in  [2.x.83] , but it is usually less effective as the diffusion for  [2.x.84] . For that case, we choose a slightly more readable definition of the viscosity, [1.x.79] 

where the first term gives again the maximum dissipation (similarly to a first order upwind scheme), [1.x.80] 

and the entropy viscosity is defined as [1.x.81] 



This formula is described in the article [1.x.82] Compared to the case  [2.x.85] , the residual is computed from the temperature entropy,  [2.x.86]  with  [2.x.87]  an average temperature (we choose the mean between the maximum and minimum temperature in the computation), which gives the following formula [1.x.83] 

The denominator in the formula for  [2.x.88]  is computed as the global deviation of the entropy from the space-averaged entropy  [2.x.89] . As in  [2.x.90] , we evaluate the artificial viscosity from the temperature and velocity at two previous time levels, in order to avoid a nonlinearity in its definition. 

The above definitions of the viscosity are simple, but depend on two parameters, namely  [2.x.91]  and  [2.x.92] .  For the current program, we want to go about this issue a bit more systematically for both parameters in the case  [2.x.93] , using the same line of reasoning with which we chose two other parameters in our discretization,  [2.x.94]  and  [2.x.95] , in the results section of  [2.x.96] . In particular, remember that we would like to make the artificial viscosity as small as possible while keeping it as large as necessary. In the following, let us describe the general strategy one may follow. The computations shown here were done with an earlier version of the program and so the actual numerical values you get when running the program may no longer match those shown here; that said, the general approach remains valid and has been used to find the values of the parameters actually used in the program. 

To see what is happening, note that below we will impose boundary conditions for the temperature between 973 and 4273 Kelvin, and initial conditions are also chosen in this range; for these considerations, we run the program without %internal heat sources or sinks, and consequently the temperature should always be in this range, barring any %internal oscillations. If the minimal temperature drops below 973 Kelvin, then we need to add stabilization by either increasing  [2.x.97]  or decreasing  [2.x.98] . 

As we did in  [2.x.99] , we first determine an optimal value of  [2.x.100]  by using the "traditional" formula [1.x.84] 

which we know to be stable if only  [2.x.101]  is large enough. Doing a couple hundred time steps (on a coarser mesh than the one shown in the program, and with a different viscosity that affects transport velocities and therefore time step sizes) in 2d will produce the following graph: 

 [2.x.102]  

As can be seen, values  [2.x.103]  are too small whereas  [2.x.104]  appears to work, at least to the time horizon shown here. As a remark on the side, there are at least two questions one may wonder here: First, what happens at the time when the solution becomes unstable? Looking at the graphical output, we can see that with the unreasonably coarse mesh chosen for these experiments, around time  [2.x.105]  seconds the plumes of hot material that have been rising towards the cold outer boundary and have then spread sideways are starting to get close to each other, squeezing out the cold material in-between. This creates a layer of cells into which fluids flows from two opposite sides and flows out toward a third, apparently a scenario that then produce these instabilities without sufficient stabilization. Second: In  [2.x.106] , we used  [2.x.107] ; why does this not work here? The answer to this is not entirely clear -- stabilization parameters are certainly known to depend on things like the shape of cells, for which we had squares in  [2.x.108]  but have trapezoids in the current program. Whatever the exact cause, we at least have a value of  [2.x.109] , namely 0.052 for 2d, that works for the current program. A similar set of experiments can be made in 3d where we find that  [2.x.110]  is a good choice &mdash; neatly leading to the formula  [2.x.111] . 

With this value fixed, we can go back to the original formula for the viscosity  [2.x.112]  and play with the constant  [2.x.113] , making it as large as possible in order to make  [2.x.114]  as small as possible. This gives us a picture like this: 

 [2.x.115]  

Consequently,  [2.x.116]  would appear to be the right value here. While this graph has been obtained for an exponent  [2.x.117] , in the program we use  [2.x.118]  instead, and in that case one has to re-tune the parameter (and observe that  [2.x.119]  appears in the numerator and not in the denominator). It turns out that  [2.x.120]  works with  [2.x.121] . 


[1.x.85][1.x.86] 


The standard Taylor-Hood discretization for Stokes, using the  [2.x.122]  element, is globally conservative, i.e.  [2.x.123] . This can easily be seen: the weak form of the divergence equation reads  [2.x.124] . Because the pressure space does contain the function  [2.x.125] , we get 

[1.x.87] 

by the divergence theorem. This property is important: if we want to use the velocity field  [2.x.126]  to transport along other quantities (such as the temperature in the current equations, but it could also be concentrations of chemical substances or entirely artificial tracer quantities) then the conservation property guarantees that the amount of the quantity advected remains constant. 

That said, there are applications where this [1.x.88] property is not enough. Rather, we would like that it holds [1.x.89], on every cell. This can be achieved by using the space  [2.x.127]  for discretization, where we have replaced the [1.x.90] space of tensor product polynomials of degree  [2.x.128]  for the pressure by the [1.x.91] space of the complete polynomials of the same degree. (Note that tensor product polynomials in 2d contain the functions  [2.x.129] , whereas the complete polynomials only have the functions  [2.x.130] .) This space turns out to be stable for the Stokes equation. 

Because the space is discontinuous, we can now in particular choose the test function  [2.x.131] , i.e. the characteristic function of cell  [2.x.132] . We then get in a similar fashion as above 

[1.x.92] 

showing the conservation property for cell  [2.x.133] . This clearly holds for each cell individually. 

There are good reasons to use this discretization. As mentioned above, this element guarantees conservation of advected quantities on each cell individually. A second advantage is that the pressure mass matrix we use as a preconditioner in place of the Schur complement becomes block diagonal and consequently very easy to invert. However, there are also downsides. For one, there are now more pressure variables, increasing the overall size of the problem, although this doesn't seem to cause much harm in practice. More importantly, though, the fact that now the divergence integrated over each cell is zero when it wasn't before does not guarantee that the divergence is pointwise smaller. In fact, as one can easily verify, the  [2.x.134]  norm of the divergence is [1.x.93] for this than for the standard Taylor-Hood discretization. (However, both converge at the same rate to zero, since it is easy to see that  [2.x.135] .) It is therefore not a priori clear that the error is indeed smaller just because we now have more degrees of freedom. 

Given these considerations, it remains unclear which discretization one should prefer. Consequently, we leave that up to the user and make it a parameter in the input file which one to use. 


[1.x.94][1.x.95] 


In the program, we will use a spherical shell as domain. This means that the inner and outer boundary of the domain are no longer "straight" (by which we usually mean that they are bilinear surfaces that can be represented by the FlatManifold class). Rather, they are curved and it seems prudent to use a curved approximation in the program if we are already using higher order finite elements for the velocity. Consequently, we will introduce a member variable of type MappingQ that denotes such a mapping ( [2.x.136]  and  [2.x.137]  introduce such mappings for the first time) and that we will use in all computations on cells that are adjacent to the boundary. Since this only affects a relatively small fraction of cells, the additional effort is not very large and we will take the luxury of using a quartic mapping for these cells. 


[1.x.96][1.x.97] 


Running convection codes in 3d with significant Rayleigh numbers requires a lot of computations &mdash; in the case of whole earth simulations on the order of one or several hundred million unknowns. This can obviously not be done with a single machine any more (at least not in 2010 when we started writing this code). Consequently, we need to parallelize it. Parallelization of scientific codes across multiple machines in a cluster of computers is almost always done using the Message Passing Interface (MPI). This program is no exception to that, and it follows the general spirit of the  [2.x.138]  and  [2.x.139]  programs in this though in practice it borrows more from  [2.x.140]  in which we first introduced the classes and strategies we use when we want to [1.x.98] distribute all computations, and  [2.x.141]  that shows how to do that for  [2.x.142]  "vector-valued problems": including, for example, splitting the mesh up into a number of parts so that each processor only stores its own share plus some ghost cells, and using strategies where no processor potentially has enough memory to hold the entries of the combined solution vector locally. The goal is to run this code on hundreds or maybe even thousands of processors, at reasonable scalability. 

 [2.x.143]  Even though it has a larger number,  [2.x.144]  comes logically before the current program. The same is true for  [2.x.145] . You will probably want to look at these programs before you try to understand what we do here. 

MPI is a rather awkward interface to program with. It is a semi-object oriented set of functions, and while one uses it to send data around a network, one needs to explicitly describe the data types because the MPI functions insist on getting the address of the data as  [2.x.146]  objects rather than deducing the data type automatically through overloading or templates. We've already seen in  [2.x.147]  and  [2.x.148]  how to avoid almost all of MPI by putting all the communication necessary into either the deal.II library or, in those programs, into PETSc. We'll do something similar here: like in  [2.x.149]  and  [2.x.150] , deal.II and the underlying p4est library are responsible for all the communication necessary for distributing the mesh, and we will let the Trilinos library (along with the wrappers in namespace TrilinosWrappers) deal with parallelizing the linear algebra components. We have already used Trilinos in  [2.x.151] , and will do so again here, with the difference that we will use its %parallel capabilities. 

Trilinos consists of a significant number of packages, implementing basic %parallel linear algebra operations (the Epetra package), different solver and preconditioner packages, and on to things that are of less importance to deal.II (e.g., optimization, uncertainty quantification, etc). deal.II's Trilinos interfaces encapsulate many of the things Trilinos offers that are of relevance to PDE solvers, and provides wrapper classes (in namespace TrilinosWrappers) that make the Trilinos matrix, vector, solver and preconditioner classes look very much the same as deal.II's own implementations of this functionality. However, as opposed to deal.II's classes, they can be used in %parallel if we give them the necessary information. As a consequence, there are two Trilinos classes that we have to deal with directly (rather than through wrappers), both of which are part of Trilinos' Epetra library of basic linear algebra and tool classes:  [2.x.152]   [2.x.153]  The Epetra_Comm class is an abstraction of an MPI "communicator", i.e.,   it describes how many and which machines can communicate with each other.   Each distributed object, such as a sparse matrix or a vector for which we   may want to store parts on different machines, needs to have a communicator   object to know how many parts there are, where they can be found, and how   they can be accessed. 

  In this program, we only really use one communicator object -- based on the   MPI variable  [2.x.154]  -- that encompasses [1.x.99]   processes that work together. It would be perfectly legitimate to start a   process on  [2.x.155]  machines but only store vectors on a subset of these by   producing a communicator object that only encompasses this subset of   machines; there is really no compelling reason to do so here, however. 

 [2.x.156]  The IndexSet class is used to describe which elements of a vector or which   rows of a matrix should reside on the current machine that is part of a   communicator. To create such an object, you need to know (i) the total   number of elements or rows, (ii) the indices of the elements you want to   store locally. We will set up these  [2.x.157]  in the    [2.x.158]  function below and then hand   it to every %parallel object we create. 

  Unlike PETSc, Trilinos makes no assumption that the elements of a vector   need to be partitioned into contiguous chunks. At least in principle, we   could store all elements with even indices on one processor and all odd ones   on another. That's not very efficient, of course, but it's   possible. Furthermore, the elements of these partitionings do not   necessarily be mutually exclusive. This is important because when   postprocessing solutions, we need access to all locally relevant or at least   the locally active degrees of freedom (see the module on  [2.x.159]    for a definition, as well as the discussion in  [2.x.160] ). Which elements the   Trilinos vector considers as locally owned is not important to us then. All   we care about is that it stores those elements locally that we need.  [2.x.161]  

There are a number of other concepts relevant to distributing the mesh to a number of processors; you may want to take a look at the  [2.x.162]  module and  [2.x.163]  or  [2.x.164]  before trying to understand this program.  The rest of the program is almost completely agnostic about the fact that we don't store all objects completely locally. There will be a few points where we have to limit loops over all cells to those that are locally owned, or where we need to distinguish between vectors that store only locally owned elements and those that store everything that is locally relevant (see  [2.x.165]  "this glossary entry"), but by and large the amount of heavy lifting necessary to make the program run in %parallel is well hidden in the libraries upon which this program builds. In any case, we will comment on these locations as we get to them in the program code. 


[1.x.100][1.x.101] 


The second strategy to parallelize a program is to make use of the fact that most computers today have more than one processor that all have access to the same memory. In other words, in this model, we don't explicitly have to say which pieces of data reside where -- all of the data we need is directly accessible and all we have to do is split [1.x.102] this data between the available processors. We will then couple this with the MPI parallelization outlined above, i.e., we will have all the processors on a machine work together to, for example, assemble the local contributions to the global matrix for the cells that this machine actually "owns" but not for those cells that are owned by other machines. We will use this strategy for four kinds of operations we frequently do in this program: assembly of the Stokes and temperature matrices, assembly of the matrix that forms the Stokes preconditioner, and assembly of the right hand side of the temperature system. 

All of these operations essentially look as follows: we need to loop over all cells for which  [2.x.166]  equals the index our machine has within the communicator object used for all communication (i.e.,  [2.x.167] , as explained above). The test we are actually going to use for this, and which describes in a concise way why we test this condition, is  [2.x.168] . On each such cell we need to assemble the local contributions to the global matrix or vector, and then we have to copy each cell's contribution into the global matrix or vector. Note that the first part of this (the loop) defines a range of iterators on which something has to happen. The second part, assembly of local contributions is something that takes the majority of CPU time in this sequence of steps, and is a typical example of things that can be done in %parallel: each cell's contribution is entirely independent of all other cells' contributions. The third part, copying into the global matrix, must not happen in %parallel since we are modifying one object and so several threads can not at the same time read an existing matrix element, add their contribution, and write the sum back into memory without danger of producing a [1.x.103]. 

deal.II has a class that is made for exactly this workflow: WorkStream, first discussed in  [2.x.169]  and  [2.x.170] . Its use is also extensively documented in the module on  [2.x.171]  (in the section on  [2.x.172]  "the WorkStream class") and we won't repeat here the rationale and detailed instructions laid out there, though you will want to read through this module to understand the distinction between scratch space and per-cell data. Suffice it to mention that we need the following: 

- An iterator range for those cells on which we are supposed to work. This is   provided by the FilteredIterator class which acts just like every other cell   iterator in deal.II with the exception that it skips all cells that do not   satisfy a particular predicate (i.e., a criterion that evaluates to true or   false). In our case, the predicate is whether a cell is locally owned. 

- A function that does the work on each cell for each of the tasks identified   above, i.e., functions that assemble the local contributions to Stokes matrix   and preconditioner, temperature matrix, and temperature right hand   side. These are the    [2.x.173] ,    [2.x.174] ,    [2.x.175] , and    [2.x.176]  functions in   the code below. These four functions can all have several instances   running in %parallel at the same time. 

- %Functions that copy the result of the previous ones into the global object   and that run sequentially to avoid race conditions. These are the    [2.x.177] ,    [2.x.178] ,    [2.x.179] , and    [2.x.180]    functions. 

We will comment on a few more points in the actual code, but in general their structure should be clear from the discussion in  [2.x.181] . 

The underlying technology for WorkStream identifies "tasks" that need to be worked on (e.g. assembling local contributions on a cell) and schedules these tasks automatically to available processors. WorkStream creates these tasks automatically, by splitting the iterator range into suitable chunks. 

 [2.x.182]  Using multiple threads within each MPI process only makes sense if you have fewer MPI processes running on each node of your cluster than there are processor cores on this machine. Otherwise, MPI will already keep your processors busy and you won't get any additional speedup from using threads. For example, if your cluster nodes have 8 cores as they often have at the time of writing this, and if your batch scheduler puts 8 MPI processes on each node, then using threads doesn't make the program any faster. Consequently, you probably want to either configure your deal.II without threads, or set the number of threads in  [2.x.183]  to 1 (third argument), or "export DEAL_II_NUM_THREADS=1" before running. That said, at the time of writing this, we only use the WorkStream class for assembling (parts of) linear systems, while 75% or more of the run time of the program is spent in the linear solvers that are not parallelized &mdash; in other words, the best we could hope is to parallelize the remaining 25%. 


[1.x.104][1.x.105] 


The setup for this program is mildly reminiscent of the problem we wanted to solve in the first place (see the introduction of  [2.x.184] ): convection in the earth mantle. As a consequence, we choose the following data, all of which appears in the program in units of meters and seconds (the SI system) even if we list them here in other units. We do note, however, that these choices are essentially still only exemplary, and not meant to result in a completely realistic description of convection in the earth mantle: for that, more and more difficult physics would have to be implemented, and several other aspects are currently missing from this program as well. We will come back to this issue in the results section again, but state for now that providing a realistic description is a goal of the [1.x.106] code in development at the time of writing this. 

As a reminder, let us again state the equations we want to solve are these: [1.x.107] 

augmented by boundary and initial conditions. We then have to choose data for the following quantities:  [2.x.185]     [2.x.186] The domain is an annulus (in 2d) or a spherical shell (in 3d) with inner   and outer radii that match that of the earth: the total radius of the earth   is 6371km, with the mantle starting at a depth of around 35km (just under   the solid earth [1.x.108] composed of   [1.x.109] and [1.x.110]) to a depth of 2890km (where the   [1.x.111] starts). The radii are therefore  [2.x.187] . This domain is conveniently generated using the    [2.x.188]  function. 

   [2.x.189] At the interface between crust and mantle, the temperature is between   500 and 900 degrees Celsius, whereas at its bottom it is around 4000 degrees   Celsius (see, for example, [1.x.112]). In Kelvin, we therefore choose  [2.x.190] ,    [2.x.191]  as boundary conditions at the inner and outer edge. 

  In addition to this, we also have to specify some initial conditions for   the temperature field. The real temperature field of the earth is quite   complicated as a consequence of the convection that has been going on for   more than four billion years -- in fact, it is the properties of this   temperature distribution that we want to explore with programs like   this. As a consequence, we   don't really have anything useful to offer here, but we can hope that if we   start with something and let things run for a while that the exact initial   conditions don't matter that much any more &mdash; as is in fact suggested   by looking at the pictures shown in the [1.x.113]. The initial temperature field we use here is given in terms of   the radius by   [1.x.114] 

  where   [1.x.115] 

  This complicated function is essentially a perturbation of a linear profile   between the inner and outer temperatures. In 2d, the function    [2.x.192]  looks like this (I got the picture from   [1.x.116]): 

   [2.x.193]  

  The point of this profile is that if we had used  [2.x.194]  instead of  [2.x.195]  in   the definition of  [2.x.196]  then it would simply be a linear   interpolation.  [2.x.197]  has the same function values as  [2.x.198]  on the inner and   outer boundaries (zero and one, respectively), but it stretches the   temperature profile a bit depending on the angle and the  [2.x.199]  value in 3d,   producing an angle-dependent perturbation of the linearly interpolating   field. We will see in the results section that this is an   entirely unphysical temperature field (though it will make for   interesting images) as the equilibrium state for the temperature   will be an almost constant temperature with boundary layers at the   inner and outer boundary. 

   [2.x.200] The right hand side of the temperature equation contains the rate of   %internal heating  [2.x.201] . The earth does heat naturally through several mechanisms:   radioactive decay, chemical separation (heavier elements sink to the bottom,   lighter ones rise to the top; the countercurrents dissipate energy equal to   the loss of potential energy by this separation process); heat release   by crystallization of liquid metal as the solid inner core of the earth   grows; and heat dissipation from viscous friction as the fluid moves. 

  Chemical separation is difficult to model since it requires modeling mantle   material as multiple phases; it is also a relatively small   effect. Crystallization heat is even more difficult since it is confined to   areas where temperature and pressure allow for phase changes, i.e., a   discontinuous process. Given the difficulties in modeling these two   phenomena, we will neglect them. 

  The other two are readily handled and, given the way we scaled the   temperature equation, lead to the equation   [1.x.117]   where  [2.x.202]  is the radiogenic heating in  [2.x.203] , and the second   term in the enumerator is viscous friction heating.  [2.x.204]  is the density   and  [2.x.205]  is the specific heat. The literature provides the following   approximate values:  [2.x.206] .   The other parameters are discussed elsewhere in this section. 

  We neglect one internal heat source, namely adiabatic heating here,   which will lead to a surprising temperature field. This point is   commented on in detail in the results section below. 

   [2.x.207] For the velocity we choose as boundary conditions  [2.x.208]  at the   inner radius (i.e., the fluid sticks to the earth core) and    [2.x.209]  at the outer radius (i.e., the fluid flows   tangentially along the bottom of the earth crust). Neither of these is   physically overly correct: certainly, on both boundaries, fluids can flow   tangentially, but they will incur a shear stress through friction against   the medium at the other side of the interface (the metallic core and the   crust, respectively). Such a situation could be modeled by a Robin-type   boundary condition for the tangential velocity; in either case, the normal (vertical)   velocity would be zero, although even that is not entirely correct since   continental plates also have vertical motion (see, for example, the   phenomenon of [1.x.118]). But to already make things worse for the tangential velocity,   the medium on the other side is in motion as well, so the shear stress   would, in the simplest case, be proportional to the [1.x.119], leading to a boundary condition of the form   [1.x.120] 

  with a proportionality constant  [2.x.210] . Rather than going down this route,   however, we go with the choice of zero (stick) and tangential   flow boundary conditions. 

  As a side note of interest, we may also have chosen tangential flow   conditions on both inner and outer boundary. That has a significant   drawback, however: it leaves the velocity not uniquely defined. The reason   is that all velocity fields  [2.x.211]  that correspond to a solid   body rotation around the center of the domain satisfy  [2.x.212] , and    [2.x.213] . As a consequence, if  [2.x.214]    satisfies equations and boundary conditions, then so does  [2.x.215] . That's certainly not a good situation that we would like   to avoid. The traditional way to work around this is to pick an arbitrary   point on the boundary and call this your fixed point by choosing the   velocity to be zero in all components there. (In 3d one has to choose two   points.) Since this program isn't meant to be too realistic to begin with,   we avoid this complication by simply fixing the velocity along the entire   interior boundary. 

   [2.x.216] To first order, the gravity vector always points downward. The question for   a body as big as the earth is just: where is "up". The naive answer of course is   "radially inward, towards the center of the earth". So at the surface of the   earth, we have   [1.x.121]   where  [2.x.217]  happens to be the average gravity   acceleration at the earth surface. But in the earth interior, the question   becomes a bit more complicated: at the (bary-)center of the earth, for   example, you have matter pulling equally hard in all directions, and so    [2.x.218] . In between, the net force is described as follows: let us   define the [1.x.122] by   [1.x.123]   then  [2.x.219] . If we assume that   the density  [2.x.220]  is constant throughout the earth, we can produce an   analytical expression for the gravity vector (don't try to integrate above   equation somehow -- it leads to elliptic integrals; a simpler way is to   notice that  [2.x.221]  and solving this   partial differential equation in all of  [2.x.222]  exploiting the   radial symmetry):   [1.x.124]   The factor  [2.x.223]  is the unit vector pointing   radially inward. Of course, within this problem, we are only interested in   the branch that pertains to within the earth, i.e.,  [2.x.224] . We would therefore only consider the expression   [1.x.125]   where we can infer the last expression because we know Earth's gravity at   the surface (where  [2.x.225] ). 

  One can derive a more general expression by integrating the   differential equation for  [2.x.226]  in the case that the density   distribution is radially symmetric, i.e.,  [2.x.227] . In that case, one would get   [1.x.126] 


  There are two problems with this, however: (i) The Earth is not homogeneous,   i.e., the density  [2.x.228]  depends on  [2.x.229] ; in fact it is not even a   function that only depends on the radius  [2.x.230] . In reality, gravity therefore   does not always decrease as we get deeper: because the earth core is so much   denser than the mantle, gravity actually peaks at around  [2.x.231]  at the core mantle boundary (see [1.x.127]). (ii) The density, and by   consequence the gravity vector, is not even constant in time: after all, the   problem we want to solve is the time dependent upwelling of hot, less dense   material and the downwelling of cold dense material. This leads to a gravity   vector that varies with space and time, and does not always point straight   down. 

  In order to not make the situation more complicated than necessary, we could   use the approximation that at the inner boundary of the mantle,   gravity is  [2.x.232]  and at the outer   boundary it is  [2.x.233] , in each case   pointing radially inward, and that in between gravity varies   linearly with the radial distance from the earth center. That said, it isn't   that hard to actually be slightly more realistic and assume (as we do below)   that the earth mantle has constant density. In that case, the equation above   can be integrated and we get an expression for  [2.x.234]  where we   can fit constants to match the gravity at the top and bottom of the earth   mantle to obtain   [1.x.128] 

   [2.x.235] The density of the earth mantle varies spatially, but not by very   much.  [2.x.236]  is a relatively good average   value for the density at reference temperature  [2.x.237]  Kelvin. 

   [2.x.238] The thermal expansion coefficient  [2.x.239]  also varies with depth   (through its dependence on temperature and pressure). Close to the surface,   it appears to be on the order of  [2.x.240] ,   whereas at the core mantle boundary, it may be closer to  [2.x.241] . As a reasonable value, let us choose    [2.x.242] . The density as a function   of temperature is then    [2.x.243] . 

   [2.x.244] The second to last parameter we need to specify is the viscosity    [2.x.245] . This is a tough one, because rocks at the temperatures and pressure   typical for the earth mantle flow so slowly that the viscosity can not be   determined accurately in the laboratory. So how do we know about the   viscosity of the mantle? The most commonly used route is to consider that   during and after ice ages, ice shields form and disappear on time scales   that are shorter than the time scale of flow in the mantle. As a   consequence, continents slowly sink into the earth mantle under the added   weight of an ice shield, and they rise up again slowly after the ice shield   has disappeared again (this is called [1.x.129][1.x.130]). By measuring the speed of this rebound, we can infer the   viscosity of the material that flows into the area vacated under the   rebounding continental plates. 

  Using this technique, values around  [2.x.246]  have been found as the most   likely, though the error bar on this is at least one order of magnitude. 

  While we will use this value, we again have to caution that there are many   physical reasons to assume that this is not the correct value. First, it   should really be made dependent on temperature: hotter material is most   likely to be less viscous than colder material. In reality, however, the   situation is even more complex. Most rocks in the mantle undergo phase   changes as temperature and pressure change: depending on temperature and   pressure, different crystal configurations are thermodynamically favored   over others, even if the chemical composition of the mantle were   homogeneous. For example, the common mantle material MgSiO<sub>3</sub> exists   in its [1.x.131] throughout most of the mantle, but in the lower mantle the   same substance is stable only as [1.x.132]. Clearly,   to compute realistic viscosities, we would not only need to know the exact   chemical composition of the mantle and the viscosities of all materials, but   we would also have to compute the thermodynamically most stable   configurations for all materials at each quadrature point. This is at the   time of writing this program not a feasible suggestion. 

   [2.x.247] Our last material parameter is the thermal diffusivity  [2.x.248] , which   is defined as  [2.x.249]  where  [2.x.250]  is the thermal   conductivity,  [2.x.251]  the density, and  [2.x.252]  the specific heat. For   this, the literature indicates that it increases from around  [2.x.253]  in the   upper mantle to around  [2.x.254]  in the lower   mantle, though the exact value   is not really all that important: heat transport through convection is   several orders of magnitude more important than through thermal   conduction. It may be of interest to know that perovskite, the most abundant   material in the earth mantle, appears to become transparent at pressures   above around 120 GPa (see, for example, J. Badro et al., Science 305,   383-386 (2004)); in the lower mantle, it may therefore be that heat   transport through radiative transfer is more efficient than through thermal   conduction. 

  In view of these considerations, let us choose    [2.x.255]    for the purpose of this program.  [2.x.256]  

All of these pieces of equation data are defined in the program in the  [2.x.257]  namespace. When run, the program produces long-term maximal velocities around 10-40 centimeters per year (see the results section below), approximately the physically correct order of magnitude. We will set the end time to 1 billion years. 

 [2.x.258]  The choice of the constants and material parameters above follows in large part the comprehensive book "Mantle Convection in the Earth and Planets, Part 1" by G. Schubert and D. L. Turcotte and P. Olson (Cambridge, 2001). It contains extensive discussion of ways to make the program more realistic. 


[1.x.133][1.x.134] 


Compared to  [2.x.259] , this program has a number of noteworthy differences: 

- The  [2.x.260]  namespace is significantly larger, reflecting   the fact that we now have much more physics to deal with. That said, most of   this additional physical detail is rather self-contained in functions in   this one namespace, and does not proliferate throughout the rest of the   program. 

- Of more obvious visibility is the fact that we have put a good number of   parameters into an input file handled by the ParameterHandler class (see,   for example,  [2.x.261] , for ways to set up run-time parameter files with this   class). This often makes sense when one wants to avoid re-compiling the   program just because one wants to play with a single parameter (think, for   example, of parameter studies determining the best values of the   stabilization constants discussed above), in particular given that it takes   a nontrivial amount of time to re-compile programs of the current size. To   just give an overview of the kinds of parameters we have moved from fixed   values into the input file, here is a listing of a typical    [2.x.262]  file:   [1.x.135] 



- There are, obviously, a good number of changes that have to do with the fact   that we want to run our program on a possibly very large number of   machines. Although one may suspect that this requires us to completely   re-structure our code, that isn't in fact the case (although the classes   that implement much of this functionality in deal.II certainly look very   different from an implementation viewpoint, but this doesn't reflect in   their public interface). Rather, the changes are mostly subtle, and the   overall structure of the main class is pretty much unchanged. That said, the   devil is in the detail: getting %parallel computing right, without   deadlocks, ensuring that the right data is available at the right place   (see, for example, the discussion on fully distributed vectors vs. vectors   with ghost elements), and avoiding bottlenecks is difficult and discussions   on this topic will appear in a good number of places in this program. 


[1.x.136][1.x.137] 


This is a tutorial program. That means that at least most of its focus needs to lie on demonstrating ways of using deal.II and associated libraries, and not diluting this teaching lesson by focusing overly much on physical details. Despite the lengthy section above on the choice of physical parameters, the part of the program devoted to this is actually quite short and self contained. 

That said, both  [2.x.263]  and the current  [2.x.264]  have not come about by chance but are certainly meant as wayposts along the path to a more comprehensive program that will simulate convection in the earth mantle. We call this code [1.x.138] (short for [1.x.139]); its development is funded by the [1.x.140] initiative with support from the National Science Foundation. More information on [1.x.141] is available at its [1.x.142]. [1.x.143] [1.x.144] 


[1.x.145]  [1.x.146] 




The first task as usual is to include the functionality of these well-known deal.II library files and some C++ header files. 

[1.x.147] 



This is the only include file that is new: It introduces the  [2.x.265]  equivalent of the  [2.x.266]  class to take a solution from on mesh to the next one upon mesh refinement, but in the case of parallel distributed triangulations: 

[1.x.148] 



The following classes are used in parallel distributed computations and have all already been introduced in  [2.x.267] : 

[1.x.149] 



The next step is like in all previous tutorial programs: We put everything into a namespace of its own and then import the deal.II classes and functions into it: 

[1.x.150] 




[1.x.151]  [1.x.152] 




In the following namespace, we define the various pieces of equation data that describe the problem. This corresponds to the various aspects of making the problem at least slightly realistic and that were exhaustively discussed in the description of the testcase in the introduction.    


We start with a few coefficients that have constant values (the comment after the value indicates its physical units): 

[1.x.153] 



The next set of definitions are for functions that encode the density as a function of temperature, the gravity vector, and the initial values for the temperature. Again, all of these (along with the values they compute) are discussed in the introduction: 

[1.x.154] 



As mentioned in the introduction we need to rescale the pressure to avoid the relative ill-conditioning of the momentum and mass conservation equations. The scaling factor is  [2.x.268]  where  [2.x.269]  was a typical length scale. By experimenting it turns out that a good length scale is the diameter of plumes, which is around 10 km: 

[1.x.155] 



The final number in this namespace is a constant that denotes the number of seconds per (average, tropical) year. We use this only when generating screen output: internally, all computations of this program happen in SI units (kilogram, meter, seconds) but writing geological times in seconds yields numbers that one can't relate to reality, and so we convert to years using the factor defined here: 

[1.x.156] 




[1.x.157]  [1.x.158] 




This namespace implements the preconditioner. As discussed in the introduction, this preconditioner differs in a number of key portions from the one used in  [2.x.270] . Specifically, it is a right preconditioner, implementing the matrix 

[1.x.159] 

where the two inverse matrix operations are approximated by linear solvers or, if the right flag is given to the constructor of this class, by a single AMG V-cycle for the velocity block. The three code blocks of the  [2.x.271]  function implement the multiplications with the three blocks of this preconditioner matrix and should be self explanatory if you have read through  [2.x.272]  or the discussion of composing solvers in  [2.x.273] . 

[1.x.160] 




[1.x.161]  [1.x.162]    


As described in the introduction, we will use the WorkStream mechanism discussed in the  [2.x.274]  module to parallelize operations among the processors of a single machine. The WorkStream class requires that data is passed around in two kinds of data structures, one for scratch data and one to pass data from the assembly function to the function that copies local contributions into global objects.    


The following namespace (and the two sub-namespaces) contains a collection of data structures that serve this purpose, one pair for each of the four operations discussed in the introduction that we will want to parallelize. Each assembly routine gets two sets of data: a Scratch array that collects all the classes and arrays that are used for the calculation of the cell contribution, and a CopyData array that keeps local matrices and vectors which will be written into the global matrix. Whereas CopyData is a container for the final data that is written into the global matrices and vector (and, thus, absolutely necessary), the Scratch arrays are merely there for performance reasons &mdash; it would be much more expensive to set up a FEValues object on each cell, than creating it only once and updating some derivative data.    


 [2.x.275]  had four assembly routines: One for the preconditioner matrix of the Stokes system, one for the Stokes matrix and right hand side, one for the temperature matrices and one for the right hand side of the temperature equation. We here organize the scratch arrays and CopyData objects for each of those four assembly components using a  [2.x.276]  environment (since we consider these as temporary objects we pass around, rather than classes that implement functionality of their own, though this is a more subjective point of view to distinguish between  [2.x.277] es).    


Regarding the Scratch objects, each struct is equipped with a constructor that creates an  [2.x.278]  object using the  [2.x.279] , Quadrature,  [2.x.280]  (which describes the interpolation of curved boundaries), and  [2.x.281]  instances. Moreover, we manually implement a copy constructor (since the FEValues class is not copyable by itself), and provide some additional vector fields that are used to hold intermediate data during the computation of local contributions.    


Let us start with the scratch arrays and, specifically, the one used for assembly of the Stokes preconditioner: 

[1.x.163] 



The next one is the scratch object used for the assembly of the full Stokes system. Observe that we derive the StokesSystem scratch class from the StokesPreconditioner class above. We do this because all the objects that are necessary for the assembly of the preconditioner are also needed for the actual matrix system and right hand side, plus some extra data. This makes the program more compact. Note also that the assembly of the Stokes system and the temperature right hand side further down requires data from temperature and velocity, respectively, so we actually need two FEValues objects for those two cases. 

[1.x.164] 



After defining the objects used in the assembly of the Stokes system, we do the same for the assembly of the matrices necessary for the temperature system. The general structure is very similar: 

[1.x.165] 



The final scratch object is used in the assembly of the right hand side of the temperature system. This object is significantly larger than the ones above because a lot more quantities enter the computation of the right hand side of the temperature equation. In particular, the temperature values and gradients of the previous two time steps need to be evaluated at the quadrature points, as well as the velocities and the strain rates (i.e. the symmetric gradients of the velocity) that enter the right hand side as friction heating terms. Despite the number of terms, the following should be rather self explanatory: 

[1.x.166] 



The CopyData objects are even simpler than the Scratch objects as all they have to do is to store the results of local computations until they can be copied into the global matrix or vector objects. These structures therefore only need to provide a constructor, a copy operation, and some arrays for local matrix, local vectors and the relation between local and global degrees of freedom (a.k.a.  [2.x.282] ). Again, we have one such structure for each of the four operations we will parallelize using the WorkStream class: 

[1.x.167] 




[1.x.168]  [1.x.169]    


This is the declaration of the main class. It is very similar to  [2.x.283]  but there are a number differences we will comment on below.    


The top of the class is essentially the same as in  [2.x.284] , listing the public methods and a set of private functions that do the heavy lifting. Compared to  [2.x.285]  there are only two additions to this section: the function  [2.x.286]  that computes the maximum CFL number over all cells which we then compute the global time step from, and the function  [2.x.287]  that is used in the computation of the entropy stabilization. It is akin to the  [2.x.288]  we have used in  [2.x.289]  for this purpose, but works on the entropy instead of the temperature instead. 

[1.x.170] 



The first significant new component is the definition of a struct for the parameters according to the discussion in the introduction. This structure is initialized by reading from a parameter file during construction of this object. 

[1.x.171] 



The  [2.x.290]  (for [1.x.172]) object is used to simplify writing output: each MPI process can use this to generate output as usual, but since each of these processes will (hopefully) produce the same output it will just be replicated many times over; with the ConditionalOStream class, only the output generated by one MPI process will actually be printed to screen, whereas the output by all the other threads will simply be forgotten. 

[1.x.173] 



The following member variables will then again be similar to those in  [2.x.291]  (and to other tutorial programs). As mentioned in the introduction, we fully distribute computations, so we will have to use the  [2.x.292]  class (see  [2.x.293] ) but the remainder of these variables is rather standard with two exceptions: 

     




- The  [2.x.294]  variable is used to denote a higher-order polynomial mapping. As mentioned in the introduction, we use this mapping when forming integrals through quadrature for all cells that are adjacent to either the inner or outer boundaries of our domain where the boundary is curved. 

     




- In a bit of naming confusion, you will notice below that some of the variables from namespace TrilinosWrappers are taken from namespace  [2.x.295]  (such as the right hand side vectors) whereas others are not (such as the various matrices). This is due to legacy reasons. We will frequently have to query velocities and temperatures at arbitrary quadrature points; consequently, rather than importing ghost information of a vector whenever we need access to degrees of freedom that are relevant locally but owned by another processor, we solve linear systems in %parallel but then immediately initialize a vector including ghost entries of the solution for further processing. The various  [2.x.296]  vectors are therefore filled immediately after solving their respective linear system in %parallel and will always contain values for all  [2.x.297]  "locally relevant degrees of freedom"; the fully distributed vectors that we obtain from the solution process and that only ever contain the  [2.x.298]  "locally owned degrees of freedom" are destroyed immediately after the solution process and after we have copied the relevant values into the member variable vectors. 

[1.x.174] 



The next member variable,  [2.x.299]  is used to conveniently account for compute time spent in certain "sections" of the code that are repeatedly entered. For example, we will enter (and leave) sections for Stokes matrix assembly and would like to accumulate the run time spent in this section over all time steps. Every so many time steps as well as at the end of the program (through the destructor of the TimerOutput class) we will then produce a nice summary of the times spent in the different sections into which we categorize the run-time of this program. 

[1.x.175] 



After these member variables we have a number of auxiliary functions that have been broken out of the ones listed above. Specifically, there are first three functions that we call from  [2.x.300]  and then the ones that do the assembling of linear systems: 

[1.x.176] 



Following the  [2.x.301]  "task-based parallelization" paradigm, we split all the assembly routines into two parts: a first part that can do all the calculations on a certain cell without taking care of other threads, and a second part (which is writing the local data into the global matrices and vectors) which can be entered by only one thread at a time. In order to implement that, we provide functions for each of those two steps for all the four assembly routines that we use in this program. The following eight functions do exactly this: 

[1.x.177] 



Finally, we forward declare a member class that we will define later on and that will be used to compute a number of quantities from our solution vectors that we'd like to put into the output files for visualization. 

[1.x.178] 




[1.x.179]  [1.x.180] 





[1.x.181]  [1.x.182]    


Here comes the definition of the parameters for the Stokes problem. We allow to set the end time for the simulation, the level of refinements (both global and adaptive, which in the sum specify what maximum level the cells are allowed to have), and the interval between refinements in the time stepping.    


Then, we let the user specify constants for the stabilization parameters (as discussed in the introduction), the polynomial degree for the Stokes velocity space, whether to use the locally conservative discretization based on FE_DGP elements for the pressure or not (FE_Q elements for pressure), and the polynomial degree for the temperature interpolation.    


The constructor checks for a valid input file (if not, a file with default parameters for the quantities is written), and eventually parses the parameters. 

[1.x.183] 



Next we have a function that declares the parameters that we expect in the input file, together with their data types, default values and a description: 

[1.x.184] 



And then we need a function that reads the contents of the ParameterHandler object we get by reading the input file and puts the results into variables that store the values of the parameters we have previously declared: 

[1.x.185] 




[1.x.186]  [1.x.187]    


The constructor of the problem is very similar to the constructor in  [2.x.302] . What is different is the %parallel communication: Trilinos uses a message passing interface (MPI) for data distribution. When entering the BoussinesqFlowProblem class, we have to decide how the parallelization is to be done. We choose a rather simple strategy and let all processors that are running the program work together, specified by the communicator  [2.x.303] . Next, we create the output stream (as we already did in  [2.x.304] ) that only generates output on the first MPI process and is completely forgetful on all others. The implementation of this idea is to check the process number when  [2.x.305]  gets a true argument, and it uses the  [2.x.306]  stream for output. If we are one processor five, for instance, then we will give a  [2.x.307] , which means that the output of that processor will not be printed. With the exception of the mapping object (for which we use polynomials of degree 4) all but the final member variable are exactly the same as in  [2.x.308] .    


This final object, the TimerOutput object, is then told to restrict output to the  [2.x.309]  stream (processor 0), and then we specify that we want to get a summary table at the end of the program which shows us wallclock times (as opposed to CPU times). We will manually also request intermediate summaries every so many time steps in the  [2.x.310]  function below. 

[1.x.188] 




[1.x.189]  [1.x.190] 

[1.x.191]  [1.x.192] 




Except for two small details, the function to compute the global maximum of the velocity is the same as in  [2.x.311] . The first detail is actually common to all functions that implement loops over all cells in the triangulation: When operating in %parallel, each processor can only work on a chunk of cells since each processor only has a certain part of the entire triangulation. This chunk of cells that we want to work on is identified via a so-called  [2.x.312] , as we also did in  [2.x.313] . All we need to change is hence to perform the cell-related operations only on cells that are owned by the current process (as opposed to ghost or artificial cells), i.e. for which the subdomain id equals the number of the process ID. Since this is a commonly used operation, there is a shortcut for this operation: we can ask whether the cell is owned by the current processor using  [2.x.314] .    


The second difference is the way we calculate the maximum value. Before, we could simply have a  [2.x.315]  variable that we checked against on each quadrature point for each cell. Now, we have to be a bit more careful since each processor only operates on a subset of cells. What we do is to first let each processor calculate the maximum among its cells, and then do a global communication operation  [2.x.316]  that computes the maximum value among all the maximum values of the individual processors. MPI provides such a call, but it's even simpler to use the respective function in namespace  [2.x.317]  using the MPI communicator object since that will do the right thing even if we work without MPI and on a single machine only. The call to  [2.x.318]  needs two arguments, namely the local maximum (input) and the MPI communicator, which is MPI_COMM_WORLD in this example. 

[1.x.193] 




[1.x.194]  [1.x.195] 




The next function does something similar, but we now compute the CFL number, i.e., maximal velocity on a cell divided by the cell diameter. This number is necessary to determine the time step size, as we use a semi-explicit time stepping scheme for the temperature equation (see  [2.x.319]  for a discussion). We compute it in the same way as above: Compute the local maximum over all locally owned cells, then exchange it via MPI to find the global maximum. 

[1.x.196] 




[1.x.197]  [1.x.198] 




Next comes the computation of the global entropy variation  [2.x.320]  where the entropy  [2.x.321]  is defined as discussed in the introduction.  This is needed for the evaluation of the stabilization in the temperature equation as explained in the introduction. The entropy variation is actually only needed if we use  [2.x.322]  as a power in the residual computation. The infinity norm is computed by the maxima over quadrature points, as usual in discrete computations.    


In order to compute this quantity, we first have to find the space-average  [2.x.323]  and then evaluate the maximum. However, that means that we would need to perform two loops. We can avoid the overhead by noting that  [2.x.324] , i.e., the maximum out of the deviation from the average entropy in positive and negative directions. The four quantities we need for the latter formula (maximum entropy, minimum entropy, average entropy, area) can all be evaluated in the same loop over all cells, so we choose this simpler variant. 

[1.x.199] 



In the two functions above we computed the maximum of numbers that were all non-negative, so we knew that zero was certainly a lower bound. On the other hand, here we need to find the maximum deviation from the average value, i.e., we will need to know the maximal and minimal values of the entropy for which we don't a priori know the sign.      


To compute it, we can therefore start with the largest and smallest possible values we can store in a double precision number: The minimum is initialized with a bigger and the maximum with a smaller number than any one that is going to appear. We are then guaranteed that these numbers will be overwritten in the loop on the first cell or, if this processor does not own any cells, in the communication step at the latest. The following loop then computes the minimum and maximum local entropy as well as keeps track of the area/volume of the part of the domain we locally own and the integral over the entropy on it: 

[1.x.200] 



Now we only need to exchange data between processors: we need to sum the two integrals ( [2.x.325] ), and get the extrema for maximum and minimum. We could do this through four different data exchanges, but we can it with two:  [2.x.326]  also exists in a variant that takes an array of values that are all to be summed up. And we can also utilize the  [2.x.327]  function by realizing that forming the minimum over the minimal entropies equals forming the negative of the maximum over the negative of the minimal entropies; this maximum can then be combined with forming the maximum over the maximal entropies. 

[1.x.201] 



Having computed everything this way, we can then compute the average entropy and find the  [2.x.328]  norm by taking the larger of the deviation of the maximum or minimum from the average: 

[1.x.202] 




[1.x.203]  [1.x.204] 




The next function computes the minimal and maximal value of the extrapolated temperature over the entire domain. Again, this is only a slightly modified version of the respective function in  [2.x.329] . As in the function above, we collect local minima and maxima and then compute the global extrema using the same trick as above.    


As already discussed in  [2.x.330] , the function needs to distinguish between the first and all following time steps because it uses a higher order temperature extrapolation scheme when at least two previous time steps are available. 

[1.x.205] 




[1.x.206]  [1.x.207] 




The function that calculates the viscosity is purely local and so needs no communication at all. It is mostly the same as in  [2.x.331]  but with an updated formulation of the viscosity if  [2.x.332]  is chosen: 

[1.x.208] 




[1.x.209]  [1.x.210] 




The following three functions set up the Stokes matrix, the matrix used for the Stokes preconditioner, and the temperature matrix. The code is mostly the same as in  [2.x.333] , but it has been broken out into three functions of their own for simplicity.    


The main functional difference between the code here and that in  [2.x.334]  is that the matrices we want to set up are distributed across multiple processors. Since we still want to build up the sparsity pattern first for efficiency reasons, we could continue to build the [1.x.211] sparsity pattern as a BlockDynamicSparsityPattern, as we did in  [2.x.335] . However, that would be inefficient: every processor would build the same sparsity pattern, but only initialize a small part of the matrix using it. It also violates the principle that every processor should only work on those cells it owns (and, if necessary the layer of ghost cells around it).    


Rather, we use an object of type  [2.x.336]  which is (obviously) a wrapper around a sparsity pattern object provided by Trilinos. The advantage is that the Trilinos sparsity pattern class can communicate across multiple processors: if this processor fills in all the nonzero entries that result from the cells it owns, and every other processor does so as well, then at the end after some MPI communication initiated by the  [2.x.337]  call, we will have the globally assembled sparsity pattern available with which the global matrix can be initialized.    


There is one important aspect when initializing Trilinos sparsity patterns in parallel: In addition to specifying the locally owned rows and columns of the matrices via the  [2.x.338]  index set, we also supply information about all the rows we are possibly going to write into when assembling on a certain processor. The set of locally relevant rows contains all such rows (possibly also a few unnecessary ones, but it is difficult to find the exact row indices before actually getting indices on all cells and resolving constraints). This additional information allows to exactly determine the structure for the off-processor data found during assembly. While Trilinos matrices are able to collect this information on the fly as well (when initializing them from some other reinit method), it is less efficient and leads to problems when assembling matrices with multiple threads. In this program, we pessimistically assume that only one processor at a time can write into the matrix while assembly (whereas the computation is parallel), which is fine for Trilinos matrices. In practice, one can do better by hinting WorkStream at cells that do not share vertices, allowing for parallelism among those cells (see the graph coloring algorithms and WorkStream with colored iterators argument). However, that only works when only one MPI processor is present because Trilinos' internal data structures for accumulating off-processor data on the fly are not thread safe. With the initialization presented here, there is no such problem and one could safely introduce graph coloring for this algorithm.    


The only other change we need to make is to tell the  [2.x.339]  function that it is only supposed to work on a subset of cells, namely the ones whose  [2.x.340]  equals the number of the current processor, and to ignore all other cells.    


This strategy is replicated across all three of the following functions.    


Note that Trilinos matrices store the information contained in the sparsity patterns, so we can safely release the  [2.x.341]  variable once the matrix has been given the sparsity structure. 

[1.x.212] 



The remainder of the setup function (after splitting out the three functions above) mostly has to deal with the things we need to do for parallelization across processors. Because setting all of this up is a significant compute time expense of the program, we put everything we do here into a timer group so that we can get summary information about the fraction of time spent in this part of the program at its end.    


At the top as usual we enumerate degrees of freedom and sort them by component/block, followed by writing their numbers to the screen from processor zero. The  [2.x.342]  function, when applied to a  [2.x.343]  object, sorts degrees of freedom in such a way that all degrees of freedom associated with subdomain zero come before all those associated with subdomain one, etc. For the Stokes part, this entails, however, that velocities and pressures become intermixed, but this is trivially solved by sorting again by blocks; it is worth noting that this latter operation leaves the relative ordering of all velocities and pressures alone, i.e. within the velocity block we will still have all those associated with subdomain zero before all velocities associated with subdomain one, etc. This is important since we store each of the blocks of this matrix distributed across all processors and want this to be done in such a way that each processor stores that part of the matrix that is roughly equal to the degrees of freedom located on those cells that it will actually work on.    


When printing the numbers of degrees of freedom, note that these numbers are going to be large if we use many processors. Consequently, we let the stream put a comma separator in between every three digits. The state of the stream, using the locale, is saved from before to after this operation. While slightly opaque, the code works because the default locale (which we get using the constructor call  [2.x.344] ) implies printing numbers with a comma separator for every third digit (i.e., thousands, millions, billions).    


In this function as well as many below, we measure how much time we spend here and collect that in a section called "Setup dof systems" across function invocations. This is done using an  [2.x.345]  object that gets a timer going in the section with above name of the `computing_timer` object upon construction of the local variable; the timer is stopped again when the destructor of the `timing_section` variable is called.  This, of course, happens either at the end of the function, or if we leave the function through a `return` statement or when an exception is thrown somewhere -- in other words, whenever we leave this function in any way. The use of such "scope" objects therefore makes sure that we do not have to manually add code that tells the timer to stop at every location where this function may be left. 

[1.x.213] 



After this, we have to set up the various partitioners (of type  [2.x.346] , see the introduction) that describe which parts of each matrix or vector will be stored where, then call the functions that actually set up the matrices, and at the end also resize the various vectors we keep around in this program. 

[1.x.214] 



Following this, we can compute constraints for the solution vectors, including hanging node constraints and homogeneous and inhomogeneous boundary values for the Stokes and temperature fields. Note that as for everything else, the constraint objects can not hold [1.x.215] constraints on every processor. Rather, each processor needs to store only those that are actually necessary for correctness given that it only assembles linear systems on cells it owns. As discussed in the  [2.x.347]  "this paper", the set of constraints we need to know about is exactly the set of constraints on all locally relevant degrees of freedom, so this is what we use to initialize the constraint objects. 

[1.x.216] 



All this done, we can then initialize the various matrix and vector objects to their proper sizes. At the end, we also record that all matrices and preconditioners have to be re-computed at the beginning of the next time step. Note how we initialize the vectors for the Stokes and temperature right hand sides: These are writable vectors (last boolean argument set to  [2.x.348]  that have the correct one-to-one partitioning of locally owned elements but are still given the relevant partitioning for means of figuring out the vector entries that are going to be set right away. As for matrices, this allows for writing local contributions into the vector with multiple threads (always assuming that the same vector entry is not accessed by multiple threads at the same time). The other vectors only allow for read access of individual elements, including ghosts, but are not suitable for solvers. 

[1.x.217] 




[1.x.218]  [1.x.219]    


Following the discussion in the introduction and in the  [2.x.349]  module, we split the assembly functions into different parts:    


 [2.x.350]   [2.x.351]  The local calculations of matrices and right hand sides, given a certain cell as input (these functions are named  [2.x.352]  below). The resulting function is, in other words, essentially the body of the loop over all cells in  [2.x.353] . Note, however, that these functions store the result from the local calculations in variables of classes from the CopyData namespace.    


 [2.x.354] These objects are then given to the second step which writes the local data into the global data structures (these functions are named  [2.x.355]  below). These functions are pretty trivial.    


 [2.x.356] These two subfunctions are then used in the respective assembly routine (called  [2.x.357]  below), where a WorkStream object is set up and runs over all the cells that belong to the processor's subdomain.   [2.x.358]  





[1.x.220]  [1.x.221]    


Let us start with the functions that builds the Stokes preconditioner. The first two of these are pretty trivial, given the discussion above. Note in particular that the main point in using the scratch data object is that we want to avoid allocating any objects on the free space each time we visit a new cell. As a consequence, the assembly function below only has automatic local variables, and everything else is accessed through the scratch data object, which is allocated only once before we start the loop over all cells: 

[1.x.222] 



Now for the function that actually puts things together, using the WorkStream functions.   [2.x.359]  needs a start and end iterator to enumerate the cells it is supposed to work on. Typically, one would use  [2.x.360]  and  [2.x.361]  for that but here we actually only want the subset of cells that in fact are owned by the current processor. This is where the FilteredIterator class comes into play: you give it a range of cells and it provides an iterator that only iterates over that subset of cells that satisfy a certain predicate (a predicate is a function of one argument that either returns true or false). The predicate we use here is  [2.x.362]  i.e., it returns true exactly if the cell is owned by the current processor. The resulting iterator range is then exactly what we need.    


With this obstacle out of the way, we call the  [2.x.363]  function with this set of cells, scratch and copy objects, and with pointers to two functions: the local assembly and copy-local-to-global function. These functions need to have very specific signatures: three arguments in the first and one argument in the latter case (see the documentation of the  [2.x.364]  function for the meaning of these arguments). Note how we use a lambda functions to create a function object that satisfies this requirement. It uses function arguments for the local assembly function that specify cell, scratch data, and copy data, as well as function argument for the copy function that expects the data to be written into the global matrix (also see the discussion in  [2.x.365] 's  [2.x.366]  function). On the other hand, the implicit zeroth argument of member functions (namely the  [2.x.367]  pointer of the object on which that member function is to operate on) is [1.x.223] to the  [2.x.368]  pointer of the current function and is captured. The  [2.x.369]  function, as a consequence, does not need to know anything about the object these functions work on.    


When the WorkStream is executed, it will create several local assembly routines of the first kind for several cells and let some available processors work on them. The function that needs to be synchronized, i.e., the write operation into the global matrix, however, is executed by only one thread at a time in the prescribed order. Of course, this only holds for the parallelization on a single MPI process. Different MPI processes will have their own WorkStream objects and do that work completely independently (and in different memory spaces). In a distributed calculation, some data will accumulate at degrees of freedom that are not owned by the respective processor. It would be inefficient to send data around every time we encounter such a dof. What happens instead is that the Trilinos sparse matrix will keep that data and send it to the owner at the end of assembly, by calling the  [2.x.370]  command. 

[1.x.224] 



The final function in this block initiates assembly of the Stokes preconditioner matrix and then in fact builds the Stokes preconditioner. It is mostly the same as in the serial case. The only difference to  [2.x.371]  is that we use a Jacobi preconditioner for the pressure mass matrix instead of IC, as discussed in the introduction. 

[1.x.225] 




[1.x.226]  [1.x.227] 




The next three functions implement the assembly of the Stokes system, again split up into a part performing local calculations, one for writing the local data into the global matrix and vector, and one for actually running the loop over all cells with the help of the WorkStream class. Note that the assembly of the Stokes matrix needs only to be done in case we have changed the mesh. Otherwise, just the (temperature-dependent) right hand side needs to be calculated here. Since we are working with distributed matrices and vectors, we have to call the respective  [2.x.372]  functions in the end of the assembly in order to send non-local data to the owner process. 

[1.x.228] 




[1.x.229]  [1.x.230] 




The task to be performed by the next three functions is to calculate a mass matrix and a Laplace matrix on the temperature system. These will be combined in order to yield the semi-implicit time stepping matrix that consists of the mass matrix plus a time  [2.x.373] dependent weight factor times the Laplace matrix. This function is again essentially the body of the loop over all cells from  [2.x.374] .    


The two following functions perform similar services as the ones above. 

[1.x.231] 




[1.x.232]  [1.x.233] 




This is the last assembly function. It calculates the right hand side of the temperature system, which includes the convection and the stabilization terms. It includes a lot of evaluations of old solutions at the quadrature points (which are necessary for calculating the artificial viscosity of stabilization), but is otherwise similar to the other assembly functions. Notice, once again, how we resolve the dilemma of having inhomogeneous boundary conditions, by just making a right hand side at this point (compare the comments for the  [2.x.375]  function above): We create some matrix columns with exactly the values that would be entered for the temperature stiffness matrix, in case we have inhomogeneously constrained dofs. That will account for the correct balance of the right hand side vector with the matrix system of temperature. 

[1.x.234] 



In the function that runs the WorkStream for actually calculating the right hand side, we also generate the final matrix. As mentioned above, it is a sum of the mass matrix and the Laplace matrix, times some time  [2.x.376] dependent weight. This weight is specified by the BDF-2 time integration scheme, see the introduction in  [2.x.377] . What is new in this tutorial program (in addition to the use of MPI parallelization and the WorkStream class), is that we now precompute the temperature preconditioner as well. The reason is that the setup of the Jacobi preconditioner takes a noticeable time compared to the solver because we usually only need between 10 and 20 iterations for solving the temperature system (this might sound strange, as Jacobi really only consists of a diagonal, but in Trilinos it is derived from more general framework for point relaxation preconditioners which is a bit inefficient). Hence, it is more efficient to precompute the preconditioner, even though the matrix entries may slightly change because the time step might change. This is not too big a problem because we remesh every few time steps (and regenerate the preconditioner then). 

[1.x.235] 



The next part is computing the right hand side vectors.  To do so, we first compute the average temperature  [2.x.378]  that we use for evaluating the artificial viscosity stabilization through the residual  [2.x.379] . We do this by defining the midpoint between maximum and minimum temperature as average temperature in the definition of the entropy viscosity. An alternative would be to use the integral average, but the results are not very sensitive to this choice. The rest then only requires calling  [2.x.380]  again, binding the arguments to the  [2.x.381]  function that are the same in every call to the correct values: 

[1.x.236] 




[1.x.237]  [1.x.238] 




This function solves the linear systems in each time step of the Boussinesq problem. First, we work on the Stokes system and then on the temperature system. In essence, it does the same things as the respective function in  [2.x.382] . However, there are a few changes here.    


The first change is related to the way we store our solution: we keep the vectors with locally owned degrees of freedom plus ghost nodes on each MPI node. When we enter a solver which is supposed to perform matrix-vector products with a distributed matrix, this is not the appropriate form, though. There, we will want to have the solution vector to be distributed in the same way as the matrix, i.e. without any ghosts. So what we do first is to generate a distributed vector called  [2.x.383]  and put only the locally owned dofs into that, which is neatly done by the  [2.x.384]  of the Trilinos vector.    


Next, we scale the pressure solution (or rather, the initial guess) for the solver so that it matches with the length scales in the matrices, as discussed in the introduction. We also immediately scale the pressure solution back to the correct units after the solution is completed.  We also need to set the pressure values at hanging nodes to zero. This we also did in  [2.x.385]  in order not to disturb the Schur complement by some vector entries that actually are irrelevant during the solve stage. As a difference to  [2.x.386] , here we do it only for the locally owned pressure dofs. After solving for the Stokes solution, each processor copies the distributed solution back into the solution vector that also includes ghost elements.    


The third and most obvious change is that we have two variants for the Stokes solver: A fast solver that sometimes breaks down, and a robust solver that is slower. This is what we already discussed in the introduction. Here is how we realize it: First, we perform 30 iterations with the fast solver based on the simple preconditioner based on the AMG V-cycle instead of an approximate solve (this is indicated by the  [2.x.387]  argument to the  [2.x.388]  object). If we converge, everything is fine. If we do not converge, the solver control object will throw an exception  [2.x.389]  Usually, this would abort the program because we don't catch them in our usual  [2.x.390]  functions. This is certainly not what we want to happen here. Rather, we want to switch to the strong solver and continue the solution process with whatever vector we got so far. Hence, we catch the exception with the C++ try/catch mechanism. We then simply go through the same solver sequence again in the  [2.x.391]  clause, this time passing the  [2.x.392]  flag to the preconditioner for the strong solver, signaling an approximate CG solve. 

[1.x.239] 



Now let's turn to the temperature part: First, we compute the time step size. We found that we need smaller time steps for 3D than for 2D for the shell geometry. This is because the cells are more distorted in that case (it is the smallest edge length that determines the CFL number). Instead of computing the time step from maximum velocity and minimal mesh size as in  [2.x.393] , we compute local CFL numbers, i.e., on each cell we compute the maximum velocity times the mesh size, and compute the maximum of them. Hence, we need to choose the factor in front of the time step slightly smaller.      


After temperature right hand side assembly, we solve the linear system for temperature (with fully distributed vectors without any ghosts), apply constraints and copy the vector back to one with ghosts.      


In the end, we extract the temperature range similarly to  [2.x.394]  to produce some output (for example in order to help us choose the stabilization constants, as discussed in the introduction). The only difference is that we need to exchange maxima over all processors. 

[1.x.240] 




[1.x.241]  [1.x.242] 




Next comes the function that generates the output. The quantities to output could be introduced manually like we did in  [2.x.395] . An alternative is to hand this task over to a class PostProcessor that inherits from the class DataPostprocessor, which can be attached to DataOut. This allows us to output derived quantities from the solution, like the friction heating included in this example. It overloads the virtual function  [2.x.396]  which is then internally called from  [2.x.397]  We have to give it values of the numerical solution, its derivatives, normals to the cell, the actual evaluation points and any additional quantities. This follows the same procedure as discussed in  [2.x.398]  and other programs. 

[1.x.243] 



Here we define the names for the variables we want to output. These are the actual solution values for velocity, pressure, and temperature, as well as the friction heating and to each cell the number of the processor that owns it. This allows us to visualize the partitioning of the domain among the processors. Except for the velocity, which is vector-valued, all other quantities are scalar. 

[1.x.244] 



Now we implement the function that computes the derived quantities. As we also did for the output, we rescale the velocity from its SI units to something more readable, namely cm/year. Next, the pressure is scaled to be between 0 and the maximum pressure. This makes it more easily comparable -- in essence making all pressure variables positive or zero. Temperature is taken as is, and the friction heating is computed as  [2.x.399] .    


The quantities we output here are more for illustration, rather than for actual scientific value. We come back to this briefly in the results section of this program and explain what one may in fact be interested in. 

[1.x.245] 



The  [2.x.400]  function has a similar task to the one in  [2.x.401] . However, here we are going to demonstrate a different technique on how to merge output from different DoFHandler objects. The way we're going to achieve this recombination is to create a joint DoFHandler that collects both components, the Stokes solution and the temperature solution. This can be nicely done by combining the finite elements from the two systems to form one FESystem, and let this collective system define a new DoFHandler object. To be sure that everything was done correctly, we perform a sanity check that ensures that we got all the dofs from both Stokes and temperature even in the combined system. We then combine the data vectors. Unfortunately, there is no straight-forward relation that tells us how to sort Stokes and temperature vector into the joint vector. The way we can get around this trouble is to rely on the information collected in the FESystem. For each dof on a cell, the joint finite element knows to which equation component (velocity component, pressure, or temperature) it belongs – that's the information we need! So we step through all cells (with iterators into all three DoFHandlers moving in sync), and for each joint cell dof, we read out that component using the  [2.x.402]  function (see there for a description of what the various parts of its return value contain). We also need to keep track whether we're on a Stokes dof or a temperature dof, which is contained in joint_fe.system_to_base_index(i).first.first. Eventually, the dof_indices data structures on either of the three systems tell us how the relation between global vector and local dofs looks like on the present cell, which concludes this tedious work. We make sure that each processor only works on the subdomain it owns locally (and not on ghost or artificial cells) when building the joint solution vector. The same will then have to be done in  [2.x.403]  but that function does so automatically.    


What we end up with is a set of patches that we can write using the functions in DataOutBase in a variety of output formats. Here, we then have to pay attention that what each processor writes is really only its own part of the domain, i.e. we will want to write each processor's contribution into a separate file. This we do by adding an additional number to the filename when we write the solution. This is not really new, we did it similarly in  [2.x.404] . Note that we write in the compressed format  [2.x.405]  instead of plain vtk files, which saves quite some storage.    


All the rest of the work is done in the PostProcessor class. 

[1.x.246] 




[1.x.247]  [1.x.248] 




This function isn't really new either. Since the  [2.x.406]  function that we call in the middle has its own timer section, we split timing this function into two sections. It will also allow us to easily identify which of the two is more expensive.    


One thing of note, however, is that we only want to compute error indicators on the locally owned subdomain. In order to achieve this, we pass one additional argument to the  [2.x.407]  function. Note that the vector for error estimates is resized to the number of active cells present on the current process, which is less than the total number of active cells on all processors (but more than the number of locally owned active cells); each processor only has a few coarse cells around the locally owned ones, as also explained in  [2.x.408] .    


The local error estimates are then handed to a %parallel version of GridRefinement (in namespace  [2.x.409]  see also  [2.x.410] ) which looks at the errors and finds the cells that need refinement by comparing the error values across processors. As in  [2.x.411] , we want to limit the maximum grid level. So in case some cells have been marked that are already at the finest level, we simply clear the refine flags. 

[1.x.249] 



With all flags marked as necessary, we can then tell the  [2.x.412]  objects to get ready to transfer data from one mesh to the next, which they will do when notified by Triangulation as part of the  [2.x.413]  call. The syntax is similar to the non-%parallel solution transfer (with the exception that here a pointer to the vector entries is enough). The remainder of the function further down below is then concerned with setting up the data structures again after mesh refinement and restoring the solution vectors on the new mesh. 

[1.x.250] 



enforce constraints to make the interpolated solution conforming on the new mesh: 

[1.x.251] 



enforce constraints to make the interpolated solution conforming on the new mesh: 

[1.x.252] 




[1.x.253]  [1.x.254] 




This is the final and controlling function in this class. It, in fact, runs the entire rest of the program and is, once more, very similar to  [2.x.414] . The only substantial difference is that we use a different mesh now (a  [2.x.415]  instead of a simple cube geometry). 

[1.x.255] 



 [2.x.416]  supports parallel vector classes with most standard finite elements via deal.II's own native MatrixFree framework: since we use standard Lagrange elements of moderate order this function works well here. 

[1.x.256] 



Having so computed the current temperature field, let us set the member variable that holds the temperature nodes. Strictly speaking, we really only need to set  [2.x.417]  since the first thing we will do is to compute the Stokes solution that only requires the previous time step's temperature field. That said, nothing good can come from not initializing the other vectors as well (especially since it's a relatively cheap operation and we only have to do it once at the beginning of the program) if we ever want to extend our numerical method or physical model, and so we initialize  [2.x.418]  and  [2.x.419]  as well. The assignment makes sure that the vectors on the left hand side (which where initialized to contain ghost elements as well) also get the correct ghost elements. In other words, the assignment here requires communication between processors: 

[1.x.257] 



In order to speed up linear solvers, we extrapolate the solutions from the old time levels to the new one. This gives a very good initial guess, cutting the number of iterations needed in solvers by more than one half. We do not need to extrapolate in the last iteration, so if we reached the final time, we stop here.          


As the last thing during a time step (before actually bumping up the number of the time step), we check whether the current time step number is divisible by 100, and if so we let the computing timer print a summary of CPU times spent so far. 

[1.x.258] 



Trilinos sadd does not like ghost vectors even as input. Copy into distributed vectors for now: 

[1.x.259] 



If we are generating graphical output, do so also for the last time step unless we had just done so before we left the do-while loop 

[1.x.260] 




[1.x.261]  [1.x.262] 




The main function is short as usual and very similar to the one in  [2.x.420] . Since we use a parameter file which is specified as an argument in the command line, we have to read it in here and pass it on to the Parameters class for parsing. If no filename is given in the command line, we simply use the  [2.x.421]  file which is distributed together with the program. 




Because 3d computations are simply very slow unless you throw a lot of processors at them, the program defaults to 2d. You can get the 3d version by changing the constant dimension below to 3. 

[1.x.263] 

[1.x.264][1.x.265] 


When run, the program simulates convection in 3d in much the same way as  [2.x.422]  did, though with an entirely different testcase. 


[1.x.266][1.x.267] 


Before we go to this testcase, however, let us show a few results from a slightly earlier version of this program that was solving exactly the testcase we used in  [2.x.423] , just that we now solve it in parallel and with much higher resolution. We show these results mainly for comparison. 

Here are two images that show this higher resolution if we choose a 3d computation in  [2.x.424]  and if we set  [2.x.425]  and  [2.x.426] . At the time steps shown, the meshes had around 72,000 and 236,000 cells, for a total of 2,680,000 and 8,250,000 degrees of freedom, respectively, more than an order of magnitude more than we had available in  [2.x.427] : 

 [2.x.428]  

The computation was done on a subset of 50 processors of the Brazos cluster at Texas A&amp;M University. 


[1.x.268][1.x.269] 


Next, we will run  [2.x.429]  with the parameter file in the directory with one change: we increase the final time to 1e9. Here we are using 16 processors. The command to launch is (note that  [2.x.430] .prm is the default): 

<code> <pre> \ [2.x.431]  mpirun -np 16 ./ [2.x.432]  Number of active cells: 12,288 (on 6 levels) Number of degrees of freedom: 186,624 (99,840+36,864+49,920) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 41 iterations.    Maximal velocity: 60.4935 cm/year    Time step: 18166.9 years    17 CG iterations for temperature    Temperature range: 973 4273.16 

Number of active cells: 15,921 (on 7 levels) Number of degrees of freedom: 252,723 (136,640+47,763+68,320) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 50 iterations.    Maximal velocity: 60.3223 cm/year    Time step: 10557.6 years    19 CG iterations for temperature    Temperature range: 973 4273.16 

Number of active cells: 19,926 (on 8 levels) Number of degrees of freedom: 321,246 (174,312+59,778+87,156) 

Timestep 0:  t=0 years 

   Rebuilding Stokes preconditioner...    Solving Stokes system... 50 iterations.    Maximal velocity: 57.8396 cm/year    Time step: 5453.78 years    18 CG iterations for temperature    Temperature range: 973 4273.16 

Timestep 1:  t=5453.78 years 

   Solving Stokes system... 49 iterations.    Maximal velocity: 59.0231 cm/year    Time step: 5345.86 years    18 CG iterations for temperature    Temperature range: 973 4273.16 

Timestep 2:  t=10799.6 years 

   Solving Stokes system... 24 iterations.    Maximal velocity: 60.2139 cm/year    Time step: 5241.51 years    17 CG iterations for temperature    Temperature range: 973 4273.16 

[...] 

Timestep 100:  t=272151 years 

   Solving Stokes system... 21 iterations.    Maximal velocity: 161.546 cm/year    Time step: 1672.96 years    17 CG iterations for temperature    Temperature range: 973 4282.57 

Number of active cells: 56,085 (on 8 levels) Number of degrees of freedom: 903,408 (490,102+168,255+245,051) 




+---------------------------------------------+------------+------------+ | Total wallclock time elapsed since start    |       115s |            | |                                             |            |            | | Section                         | no. calls |  wall time | % of total | +---------------------------------+-----------+------------+------------+ | Assemble Stokes system          |       103 |      2.82s |       2.5% | | Assemble temperature matrices   |        12 |     0.452s |      0.39% | | Assemble temperature rhs        |       103 |      11.5s |        10% | | Build Stokes preconditioner     |        12 |      2.09s |       1.8% | | Solve Stokes system             |       103 |      90.4s |        79% | | Solve temperature system        |       103 |      1.53s |       1.3% | | Postprocessing                  |         3 |     0.532s |      0.46% | | Refine mesh structure, part 1   |        12 |      0.93s |      0.81% | | Refine mesh structure, part 2   |        12 |     0.384s |      0.33% | | Setup dof systems               |        13 |      2.96s |       2.6% | +---------------------------------+-----------+------------+------------+ 

[...] 

+---------------------------------------------+------------+------------+ | Total wallclock time elapsed since start    |  9.14e+04s |            | |                                             |            |            | | Section                         | no. calls |  wall time | % of total | +---------------------------------+-----------+------------+------------+ | Assemble Stokes system          |     47045 |  2.05e+03s |       2.2% | | Assemble temperature matrices   |      4707 |       310s |      0.34% | | Assemble temperature rhs        |     47045 |   8.7e+03s |       9.5% | | Build Stokes preconditioner     |      4707 |  1.48e+03s |       1.6% | | Solve Stokes system             |     47045 |  7.34e+04s |        80% | | Solve temperature system        |     47045 |  1.46e+03s |       1.6% | | Postprocessing                  |      1883 |       222s |      0.24% | | Refine mesh structure, part 1   |      4706 |       641s |       0.7% | | Refine mesh structure, part 2   |      4706 |       259s |      0.28% | | Setup dof systems               |      4707 |  1.86e+03s |         2% | +---------------------------------+-----------+------------+------------+ </pre> </code> 

The simulation terminates when the time reaches the 1 billion years selected in the input file.  You can extrapolate from this how long a simulation would take for a different final time (the time step size ultimately settles on somewhere around 20,000 years, so computing for two billion years will take 100,000 time steps, give or take 20%).  As can be seen here, we spend most of the compute time in assembling linear systems and &mdash; above all &mdash; in solving Stokes systems. 


To demonstrate the output we show the output from every 1250th time step here:  [2.x.433]  

The last two images show the grid as well as the partitioning of the mesh for the same computation with 16 subdomains and 16 processors. The full dynamics of this simulation are really only visible by looking at an animation, for example the one [1.x.270]. This image is well worth watching due to its artistic quality and entrancing depiction of the evolution of the magma plumes. 

If you watch the movie, you'll see that the convection pattern goes through several stages: First, it gets rid of the instable temperature layering with the hot material overlain by the dense cold material. After this great driver is removed and we have a sort of stable situation, a few blobs start to separate from the hot boundary layer at the inner ring and rise up, with a few cold fingers also dropping down from the outer boundary layer. During this phase, the solution remains mostly symmetric, reflecting the 12-fold symmetry of the original mesh. In a final phase, the fluid enters vigorous chaotic stirring in which all symmetries are lost. This is a pattern that then continues to dominate flow. 

These different phases can also be identified if we look at the maximal velocity as a function of time in the simulation: 

 [2.x.434]  

Here, the velocity (shown in centimeters per year) becomes very large, to the order of several meters per year) at the beginning when the temperature layering is instable. It then calms down to relatively small values before picking up again in the chaotic stirring regime. There, it remains in the range of 10-40 centimeters per year, quite within the physically expected region. 


[1.x.271][1.x.272] 


3d computations are very expensive computationally. Furthermore, as seen above, interesting behavior only starts after quite a long time requiring more CPU hours than is available on a typical cluster. Consequently, rather than showing a complete simulation here, let us simply show a couple of pictures we have obtained using the successor to this program, called [1.x.273] (short for [1.x.274]), that is being developed independently of deal.II and that already incorporates some of the extensions discussed below. The following two pictures show isocontours of the temperature and the partition of the domain (along with the mesh) onto 512 processors: 

 [2.x.435]  


[1.x.275] [1.x.276][1.x.277] 


There are many directions in which this program could be extended. As mentioned at the end of the introduction, most of these are under active development in the [1.x.278] (short for [1.x.279]) code at the time this tutorial program is being finished. Specifically, the following are certainly topics that one should address to make the program more useful: 

 [2.x.436]     [2.x.437]  [1.x.280]   The temperature field we get in our simulations after a while   is mostly constant with boundary layers at the inner and outer   boundary, and streamers of cold and hot material mixing   everything. Yet, this doesn't match our expectation that things   closer to the earth core should be hotter than closer to the   surface. The reason is that the energy equation we have used does   not include a term that describes adiabatic cooling and heating:   rock, like gas, heats up as you compress it. Consequently, material   that rises up cools adiabatically, and cold material that sinks down   heats adiabatically. The correct temperature equation would   therefore look somewhat like this:   [1.x.281] 

  or, expanding the advected derivative  [2.x.438] :   [1.x.282] 

  In other words, as pressure increases in a rock volume   ( [2.x.439] ) we get an additional heat source, and vice   versa. 

  The time derivative of the pressure is a bit awkward to   implement. If necessary, one could approximate using the fact   outlined in the introduction that the pressure can be decomposed   into a dynamic component due to temperature differences and the   resulting flow, and a static component that results solely from the   static pressure of the overlying rock. Since the latter is much   bigger, one may approximate  [2.x.440] , and consequently    [2.x.441] .   In other words, if the fluid is moving in the direction of gravity   (downward) it will be compressed and because in that case  [2.x.442]  we get a positive heat source. Conversely, the   fluid will cool down if it moves against the direction of gravity. 

 [2.x.443]  [1.x.283]   As already hinted at in the temperature model above,   mantle rocks are not incompressible. Rather, given the enormous pressures in   the earth mantle (at the core-mantle boundary, the pressure is approximately   140 GPa, equivalent to 1,400,000 times atmospheric pressure), rock actually   does compress to something around 1.5 times the density it would have   at surface pressure. Modeling this presents any number of   difficulties. Primarily, the mass conservation equation is no longer    [2.x.444]  but should read    [2.x.445]  where the density  [2.x.446]  is now no longer   spatially constant but depends on temperature and pressure. A consequence is   that the model is now no longer linear; a linearized version of the Stokes   equation is also no longer symmetric requiring us to rethink preconditioners   and, possibly, even the discretization. We won't go into detail here as to   how this can be resolved. 

 [2.x.447]  [1.x.284] As already hinted at in various places,   material parameters such as the density, the viscosity, and the various   thermal parameters are not constant throughout the earth mantle. Rather,   they nonlinearly depend on the pressure and temperature, and in the case of   the viscosity on the strain rate  [2.x.448] . For complicated   models, the only way to solve such models accurately may be to actually   iterate this dependence out in each time step, rather than simply freezing   coefficients at values extrapolated from the previous time step(s). 

 [2.x.449]  [1.x.285] Running this program in 2d on a number of   processors allows solving realistic models in a day or two. However, in 3d,   compute times are so large that one runs into two typical problems: (i) On   most compute clusters, the queuing system limits run times for individual   jobs are to 2 or 3 days; (ii) losing the results of a computation due to   hardware failures, misconfigurations, or power outages is a shame when   running on hundreds of processors for a couple of days. Both of these   problems can be addressed by periodically saving the state of the program   and, if necessary, restarting the program at this point. This technique is   commonly called [1.x.286] and it requires that the entire   state of the program is written to a permanent storage location (e.g. a hard   drive). Given the complexity of the data structures of this program, this is   not entirely trivial (it may also involve writing gigabytes or more of   data), but it can be made easier by realizing that one can save the state   between two time steps where it essentially only consists of the mesh and   solution vectors; during restart one would then first re-enumerate degrees   of freedom in the same way as done before and then re-assemble   matrices. Nevertheless, given the distributed nature of the data structures   involved here, saving and restoring the state of a program is not   trivial. An additional complexity is introduced by the fact that one may   want to change the number of processors between runs, for example because   one may wish to continue computing on a mesh that is finer than the one used   to precompute a starting temperature field at an intermediate time. 

 [2.x.450]  [1.x.287] The point of computations like this is   not simply to solve the equations. Rather, it is typically the exploration   of different physical models and their comparison with things that we can   measure at the earth surface, in order to find which models are realistic   and which are contradicted by reality. To this end, we need to compute   quantities from our solution vectors that are related to what we can   observe. Among these are, for example, heatfluxes at the surface of the   earth, as well as seismic velocities throughout the mantle as these affect   earthquake waves that are recorded by seismographs. 

 [2.x.451]  [1.x.288] As can be seen above for the 3d case, the mesh in 3d is primarily refined along the inner boundary. This is because the boundary layer there is stronger than any other transition in the domain, leading us to refine there almost exclusively and basically not at all following the plumes. One certainly needs better refinement criteria to track the parts of the solution we are really interested in better than the criterion used here, namely the KellyErrorEstimator applied to the temperature, is able to.  [2.x.452]  


There are many other ways to extend the current program. However, rather than discussing them here, let us point to the much larger open source code ASPECT (see https://aspect.geodynamics.org/ ) that constitutes the further development of  [2.x.453]  and that already includes many such possible extensions. [1.x.289] [1.x.290]  [2.x.454]  

 [2.x.455] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45] 

 [2.x.3]  

[1.x.46] 

 [2.x.4]  The program uses the [1.x.47] linear solvers (these can be found in Trilinos in the Aztec/Amesos packages) and an automatic differentiation package, Sacado, also part of Trilinos. deal.II must be configured to use Trilinos. Refer to the [1.x.48] file for instructions how to do this. 

 [2.x.5]  While this program demonstrates the use of automatic differentiation well, it does not express the state of the art in Euler equation solvers. There are much faster and more accurate method for this equation, and you should take a look at  [2.x.6]  and  [2.x.7]  to see how this equation can be solved more efficiently. 




[1.x.49][1.x.50] [1.x.51] 


[1.x.52][1.x.53] 


The equations that describe the movement of a compressible, inviscid gas (the so-called Euler equations of gas dynamics) are a basic system of conservation laws. In spatial dimension  [2.x.8]  they read [1.x.54] with the solution  [2.x.9]  consisting of  [2.x.10]  the fluid density,  [2.x.11]  the flow velocity (and thus  [2.x.12]  being the linear momentum density), and  [2.x.13]  the energy density of the gas. We interpret the equations above as  [2.x.14] ,  [2.x.15] . 

For the Euler equations, the flux matrix  [2.x.16]  (or system of flux functions) is defined as (shown here for the case  [2.x.17] ) [1.x.55] 

and we will choose as particular right hand side forcing only the effects of gravity, described by [1.x.56] 

where  [2.x.18]  denotes the gravity vector. With this, the entire system of equations reads: [1.x.57] 

These equations describe, respectively, the conservation of momentum, mass, and energy. The system is closed by a relation that defines the pressure:  [2.x.19] . For the constituents of air (mainly nitrogen and oxygen) and other diatomic gases, the ratio of specific heats is  [2.x.20] . 

This problem obviously falls into the class of vector-valued problems. A general overview of how to deal with these problems in deal.II can be found in the  [2.x.21]  module. 

[1.x.58][1.x.59] 


Discretization happens in the usual way, taking into account that this is a hyperbolic problem in the same style as the simple one discussed in  [2.x.22] : We choose a finite element space  [2.x.23] , and integrate our conservation law against our (vector-valued) test function  [2.x.24] .  We then integrate by parts and approximate the boundary flux with a [1.x.60] flux  [2.x.25] , [1.x.61] 

where a superscript  [2.x.26]  denotes the interior trace of a function, and  [2.x.27]  represents the outer trace. The diffusion term  [2.x.28]  is introduced strictly for stability,  where  [2.x.29]  is the mesh size and  [2.x.30]  is a parameter prescribing how  much diffusion to add. 

On the boundary, we have to say what the outer trace  [2.x.31]  is. Depending on the boundary condition, we prescribe either of the following:  [2.x.32]   [2.x.33]  Inflow boundary:  [2.x.34]  is prescribed to be the desired value.  [2.x.35]  Supersonic outflow boundary:  [2.x.36]   [2.x.37]  Subsonic outflow boundary:  [2.x.38]  except that the energy variable is modified to support a prescribed pressure  [2.x.39] , i.e.  [2.x.40]   [2.x.41]  Reflective boundary: we set  [2.x.42]  so that  [2.x.43]  and  [2.x.44] .  [2.x.45]  

More information on these issues can be found, for example, in Ralf Hartmann's PhD thesis ("Adaptive Finite Element Methods for the Compressible Euler Equations", PhD thesis, University of Heidelberg, 2002). 

We use a time stepping scheme to substitute the time derivative in the above equations. For simplicity, we define  [2.x.46]  as the spatial residual at time step  [2.x.47]  : 

[1.x.62] 



At each time step, our full discretization is thus that the residual applied to any test function  [2.x.48]  equals zero: [1.x.63] 

where  [2.x.49]  and  [2.x.50] . Choosing  [2.x.51]  results in the explicit (forward) Euler scheme,  [2.x.52]  in the stable implicit (backward) Euler scheme, and  [2.x.53]  in the Crank-Nicolson scheme. 

In the implementation below, we choose the Lax-Friedrichs flux for the function  [2.x.54] , i.e.   [2.x.55] , where  [2.x.56]  is either a fixed number specified in the input file, or where  [2.x.57]  is a mesh dependent value. In the latter case, it is chosen as  [2.x.58]  with  [2.x.59]  the diameter of the face to which the flux is applied, and  [2.x.60]  the current time step. 

With these choices, equating the residual to zero results in a nonlinear system of equations  [2.x.61] . We solve this nonlinear system by a Newton iteration (in the same way as explained in  [2.x.62] ), i.e. by iterating [1.x.64] 

until  [2.x.63]  (the residual) is sufficiently small. By testing with the nodal basis of a finite element space instead of all  [2.x.64] , we arrive at a linear system for  [2.x.65] : [1.x.65] 

This linear system is, in general, neither symmetric nor has any particular definiteness properties. We will either use a direct solver or Trilinos' GMRES implementation to solve it. As will become apparent from the [1.x.66], this fully implicit iteration converges very rapidly (typically in 3 steps) and with the quadratic convergence order expected from a Newton method. 


[1.x.67][1.x.68] 


Since computing the Jacobian matrix  [2.x.66]  is a terrible beast, we use an automatic differentiation package, Sacado, to do this.  Sacado is a package within the [1.x.69] framework and offers a C++ template class  [2.x.67]  ( [2.x.68]  standing for "forward automatic differentiation") that supports basic arithmetic operators and functions such as  [2.x.69]  etc. In order to use this feature, one declares a collection of variables of this type and then denotes some of this collection as degrees of freedom, the rest of the variables being functions of the independent variables.  These variables are used in an algorithm, and as the variables are used, their sensitivities with respect to the degrees of freedom are continuously updated. 

One can imagine that for the full Jacobian matrix as a whole, this could be prohibitively expensive: the number of independent variables are the  [2.x.70] , the dependent variables the elements of the vector  [2.x.71] . Both of these vectors can easily have tens of thousands of elements or more.  However, it is important to note that not all elements of  [2.x.72]  depend on all elements of  [2.x.73] : in fact, an entry in  [2.x.74]  only depends on an element of  [2.x.75]  if the two corresponding shape functions overlap and couple in the weak form. 

Specifically, it is wise to define a minimum set of independent AD variables that the residual on the current cell may possibly depend on: on every element, we define those variables as independent that correspond to the degrees of freedom defined on this cell (or, if we have to compute jump terms between cells, that correspond to degrees of freedom defined on either of the two adjacent cells), and the dependent variables are the elements of the local residual vector. Not doing this, i.e. defining [1.x.70] elements of  [2.x.76]  as independent, will result a very expensive computation of a lot of zeros: the elements of the local residual vector are independent of almost all elements of the solution vector, and consequently their derivatives are zero; however, trying to compute these zeros can easily take 90% or more of the compute time of the entire program, as shown in an experiment inadvertently made by a student a few years after this program was first written. 


Coming back to the question of computing the Jacobian automatically: The author has used this approach side by side with a hand coded Jacobian for the incompressible Navier-Stokes problem and found the Sacado approach to be just as fast as using a hand coded Jacobian, but infinitely simpler and less error prone: Since using the auto-differentiation requires only that one code the residual  [2.x.77] , ensuring code correctness and maintaining code becomes tremendously more simple -- the Jacobian matrix  [2.x.78]  is computed by essentially the same code that also computes the residual  [2.x.79] . 

All this said, here's a very simple example showing how Sacado can be used: 

[1.x.71] 



The output are the derivatives  [2.x.80]  of  [2.x.81]  at  [2.x.82] . 

It should be noted that Sacado provides more auto-differentiation capabilities than the small subset used in this program.  However, understanding the example above is enough to understand the use of Sacado in this Euler flow program. 

[1.x.72][1.x.73] 

The program uses either the Aztec iterative solvers, or the Amesos sparse direct solver, both provided by the Trilinos package.  This package is inherently designed to be used in a parallel program, however, it may be used in serial just as easily, as is done here.  The Epetra package is the basic vector/matrix library upon which the solvers are built.  This very powerful package can be used to describe the parallel distribution of a vector, and to define sparse matrices that operate on these vectors.  Please view the commented code for more details on how these solvers are used within the example. 

[1.x.74][1.x.75] 

The example uses an ad hoc refinement indicator that shows some usefulness in shock-type problems, and in the downhill flow example included.  We refine according to the squared gradient of the density. Hanging nodes are handled by computing the numerical flux across cells that are of differing refinement levels, rather than using the AffineConstraints class as in all other tutorial programs so far.  In this way, the example combines the continuous and DG methodologies. It also simplifies the generation of the Jacobian because we do not have to track constrained degrees of freedom through the automatic differentiation used to compute it. 

 [2.x.83]  Whereas this program was written in 2008, we were unaware of any publication that would actually have used this approach. However, a more recent paper by A. Dedner, R. Kl&ouml;fkorn, and M. Kr&auml;nkel ("Continuous Finite-Elements on Non-Conforming Grids Using Discontinuous Galerkin Stabilization", Proceedings of Finite Volumes for Complex Applications VII - Methods and Theoretical Aspects, Springer, 2014) comes close. 

Further, we enforce a maximum number of refinement levels to keep refinement under check.  It is the author's experience that for adaptivity for a time dependent problem, refinement can easily lead the simulation to a screeching halt, because of time step restrictions if the mesh becomes too fine in any part of the domain, if care is not taken.  The amount of refinement is limited in the example by letting the user specify the maximum level of refinement that will be present anywhere in the mesh.  In this way, refinement tends not to slow the simulation to a halt.  This, of course, is purely a heuristic strategy, and if the author's advisor heard about it, the author would likely be exiled forever from the finite  element error estimation community. 

[1.x.76][1.x.77] 


We use an input file deck to drive the simulation.  In this way, we can alter the boundary conditions and other important properties of the simulation without having to recompile.  For more information on the format, look at the [1.x.78], where we describe an example input file in more detail. 

In previous example programs, we have usually hard-coded the initial and boundary conditions. In this program, we instead use the expression parser class FunctionParser so that we can specify a generic expression in the input file and have it parsed at run time &mdash; this way, we can change initial conditions without the need to recompile the program. Consequently, no classes named InitialConditions or BoundaryConditions will be declared in the program below. 


[1.x.79][1.x.80] 


The implementation of this program is split into three essential parts:  [2.x.84]     [2.x.85] The  [2.x.86]  class that encapsulates everything that   completely describes the specifics of the Euler equations. This includes the   flux matrix  [2.x.87] , the numerical flux  [2.x.88] , the right hand side  [2.x.89] ,   boundary conditions, refinement indicators, postprocessing the output, and   similar things that require knowledge of the meaning of the individual   components of the solution vectors and the equations. 

   [2.x.90] A namespace that deals with everything that has to do with run-time   parameters. 

   [2.x.91] The  [2.x.92]  class that deals with time stepping,   outer nonlinear and inner linear solves, assembling the linear systems, and   the top-level logic that drives all this.  [2.x.93]  

The reason for this approach is that it separates the various concerns in a program: the  [2.x.94]  is written in such a way that it would be relatively straightforward to adapt it to a different set of equations: One would simply re-implement the members of the  [2.x.95]  class for some other hyperbolic equation, or augment the existing equations by additional ones (for example by advecting additional variables, or by adding chemistry, etc). Such modifications, however, would not affect the time stepping, or the nonlinear solvers if correctly done, and consequently nothing in the  [2.x.96]  would have to be modified. 

Similarly, if we wanted to improve on the linear or nonlinear solvers, or on the time stepping scheme (as hinted at the end of the [1.x.81]), then this would not require changes in the  [2.x.97]  at all. [1.x.82] [1.x.83] 


[1.x.84]  [1.x.85] 




First a standard set of deal.II includes. Nothing special to comment on here: 

[1.x.86] 



Then, as mentioned in the introduction, we use various Trilinos packages as linear solvers as well as for automatic differentiation. These are in the following include files. 




Since deal.II provides interfaces to the basic Trilinos matrices, preconditioners and solvers, we include them similarly as deal.II linear algebra structures. 

[1.x.87] 



Sacado is the automatic differentiation package within Trilinos, which is used to find the Jacobian for a fully implicit Newton iteration: 

[1.x.88] 



And this again is C++: 

[1.x.89] 



To end this section, introduce everything in the dealii library into the namespace into which the contents of this program will go: 

[1.x.90] 




[1.x.91]  [1.x.92] 




Here we define the flux function for this particular system of conservation laws, as well as pretty much everything else that's specific to the Euler equations for gas dynamics, for reasons discussed in the introduction. We group all this into a structure that defines everything that has to do with the flux. All members of this structure are static, i.e. the structure has no actual state specified by instance member variables. The better way to do this, rather than a structure with all static members would be to use a namespace -- but namespaces can't be templatized and we want some of the member variables of the structure to depend on the space dimension, which we in our usual way introduce using a template parameter. 

[1.x.93] 




[1.x.94]  [1.x.95] 




First a few variables that describe the various components of our solution vector in a generic way. This includes the number of components in the system (Euler's equations have one entry for momenta in each spatial direction, plus the energy and density components, for a total of  [2.x.98]  components), as well as functions that describe the index within the solution vector of the first momentum component, the density component, and the energy density component. Note that all these %numbers depend on the space dimension; defining them in a generic way (rather than by implicit convention) makes our code more flexible and makes it easier to later extend it, for example by adding more components to the equations. 

[1.x.96] 



When generating graphical output way down in this program, we need to specify the names of the solution variables as well as how the various components group into vector and scalar fields. We could describe this there, but in order to keep things that have to do with the Euler equation localized here and the rest of the program as generic as possible, we provide this sort of information in the following two functions: 

[1.x.97] 




[1.x.98]  [1.x.99] 




Next, we define the gas constant. We will set it to 1.4 in its definition immediately following the declaration of this class (unlike integer variables, like the ones above, static const floating point member variables cannot be initialized within the class declaration in C++). This value of 1.4 is representative of a gas that consists of molecules composed of two atoms, such as air which consists up to small traces almost entirely of  [2.x.99]  and  [2.x.100] . 

[1.x.100] 



In the following, we will need to compute the kinetic energy and the pressure from a vector of conserved variables. This we can do based on the energy density and the kinetic energy  [2.x.101]  (note that the independent variables contain the momentum components  [2.x.102] , not the velocities  [2.x.103] ). 

[1.x.101] 




[1.x.102]  [1.x.103] 




We define the flux function  [2.x.104]  as one large matrix.  Each row of this matrix represents a scalar conservation law for the component in that row.  The exact form of this matrix is given in the introduction. Note that we know the size of the matrix: it has as many rows as the system has components, and  [2.x.105]  columns; rather than using a FullMatrix object for such a matrix (which has a variable number of rows and columns and must therefore allocate memory on the heap each time such a matrix is created), we use a rectangular array of numbers right away.      


We templatize the numerical type of the flux function so that we may use the automatic differentiation type here.  Similarly, we will call the function with different input vector data types, so we templatize on it as well: 

[1.x.104] 



First compute the pressure that appears in the flux matrix, and then compute the first  [2.x.106]  columns of the matrix that correspond to the momentum terms: 

[1.x.105] 



Then the terms for the density (i.e. mass conservation), and, lastly, conservation of energy: 

[1.x.106] 




[1.x.107]  [1.x.108] 




On the boundaries of the domain and across hanging nodes we use a numerical flux function to enforce boundary conditions.  This routine is the basic Lax-Friedrich's flux with a stabilization parameter  [2.x.107] . It's form has also been given already in the introduction: 

[1.x.109] 




[1.x.110]  [1.x.111] 




In the same way as describing the flux function  [2.x.108] , we also need to have a way to describe the right hand side forcing term. As mentioned in the introduction, we consider only gravity here, which leads to the specific form  [2.x.109] , shown here for the 3d case. More specifically, we will consider only  [2.x.110]  in 3d, or  [2.x.111]  in 2d. This naturally leads to the following function: 

[1.x.112] 




[1.x.113]  [1.x.114] 




Another thing we have to deal with is boundary conditions. To this end, let us first define the kinds of boundary conditions we currently know how to deal with: 

[1.x.115] 



The next part is to actually decide what to do at each kind of boundary. To this end, remember from the introduction that boundary conditions are specified by choosing a value  [2.x.112]  on the outside of a boundary given an inhomogeneity  [2.x.113]  and possibly the solution's value  [2.x.114]  on the inside. Both are then passed to the numerical flux  [2.x.115]  to define boundary contributions to the bilinear form.      


Boundary conditions can in some cases be specified for each component of the solution vector independently. For example, if component  [2.x.116]  is marked for inflow, then  [2.x.117] . If it is an outflow, then  [2.x.118] . These two simple cases are handled first in the function below.      


There is a little snag that makes this function unpleasant from a C++ language viewpoint: The output vector  [2.x.119]  will of course be modified, so it shouldn't be a  [2.x.120]  argument. Yet it is in the implementation below, and needs to be in order to allow the code to compile. The reason is that we call this function at a place where  [2.x.121]  is of type  [2.x.122] , this being 2d table with indices representing the quadrature point and the vector component, respectively. We call this function with  [2.x.123]  as last argument; subscripting a 2d table yields a temporary accessor object representing a 1d vector, just what we want here. The problem is that a temporary accessor object can't be bound to a non-const reference argument of a function, as we would like here, according to the C++ 1998 and 2003 standards (something that will be fixed with the next standard in the form of rvalue references).  We get away with making the output argument here a constant because it is the [1.x.116] object that's constant, not the table it points to: that one can still be written to. The hack is unpleasant nevertheless because it restricts the kind of data types that may be used as template argument to this function: a regular vector isn't going to do because that one can not be written to when marked  [2.x.124] . With no good solution around at the moment, we'll go with the pragmatic, even if not pretty, solution shown here: 

[1.x.117] 



Prescribed pressure boundary conditions are a bit more complicated by the fact that even though the pressure is prescribed, we really are setting the energy component here, which will depend on velocity and pressure. So even though this seems like a Dirichlet type boundary condition, we get sensitivities of energy to velocity and density (unless these are also prescribed): 

[1.x.118] 



We prescribe the velocity (we are dealing with a particular component here so that the average of the velocities is orthogonal to the surface normal.  This creates sensitivities of across the velocity components. 

[1.x.119] 




[1.x.120]  [1.x.121] 




In this class, we also want to specify how to refine the mesh. The class  [2.x.125]  that will use all the information we provide here in the  [2.x.126]  class is pretty agnostic about the particular conservation law it solves: as doesn't even really care how many components a solution vector has. Consequently, it can't know what a reasonable refinement indicator would be. On the other hand, here we do, or at least we can come up with a reasonable choice: we simply look at the gradient of the density, and compute  [2.x.127] , where  [2.x.128]  is the center of cell  [2.x.129] .      


There are certainly a number of equally reasonable refinement indicators, but this one does, and it is easy to compute: 

[1.x.122] 




[1.x.123]  [1.x.124] 




Finally, we declare a class that implements a postprocessing of data components. The problem this class solves is that the variables in the formulation of the Euler equations we use are in conservative rather than physical form: they are momentum densities  [2.x.130] , density  [2.x.131] , and energy density  [2.x.132] . What we would like to also put into our output file are velocities  [2.x.133]  and pressure  [2.x.134] .      


In addition, we would like to add the possibility to generate schlieren plots. Schlieren plots are a way to visualize shocks and other sharp interfaces. The word "schlieren" is a German word that may be translated as "striae" -- it may be simpler to explain it by an example, however: schlieren is what you see when you, for example, pour highly concentrated alcohol, or a transparent saline solution, into water; the two have the same color, but they have different refractive indices and so before they are fully mixed light goes through the mixture along bent rays that lead to brightness variations if you look at it. That's "schlieren". A similar effect happens in compressible flow because the refractive index depends on the pressure (and therefore the density) of the gas.      


The origin of the word refers to two-dimensional projections of a three-dimensional volume (we see a 2d picture of the 3d fluid). In computational fluid dynamics, we can get an idea of this effect by considering what causes it: density variations. Schlieren plots are therefore produced by plotting  [2.x.135] ; obviously,  [2.x.136]  is large in shocks and at other highly dynamic places. If so desired by the user (by specifying this in the input file), we would like to generate these schlieren plots in addition to the other derived quantities listed above.      


The implementation of the algorithms to compute derived quantities from the ones that solve our problem, and to output them into data file, rests on the DataPostprocessor class. It has extensive documentation, and other uses of the class can also be found in  [2.x.137] . We therefore refrain from extensive comments. 

[1.x.125] 



This is the only function worth commenting on. When generating graphical output, the DataOut and related classes will call this function on each cell, with access to values, gradients, Hessians, and normal vectors (in case we're working on faces) at each quadrature point. Note that the data at each quadrature point is itself vector-valued, namely the conserved variables. What we're going to do here is to compute the quantities we're interested in at each quadrature point. Note that for this we can ignore the Hessians ("inputs.solution_hessians") and normal vectors ("inputs.normals"). 

[1.x.126] 



At the beginning of the function, let us make sure that all variables have the correct sizes, so that we can access individual vector elements without having to wonder whether we might read or write invalid elements; we also check that the  [2.x.138]  vector only contains data if we really need it (the system knows about this because we say so in the  [2.x.139]  function below). For the inner vectors, we check that at least the first element of the outer vector has the correct inner size: 

[1.x.127] 



Then loop over all quadrature points and do our work there. The code should be pretty self-explanatory. The order of output variables is first  [2.x.140]  velocities, then the pressure, and if so desired the schlieren plot. Note that we try to be generic about the order of variables in the input vector, using the  [2.x.141]  and  [2.x.142]  information: 

[1.x.128] 




[1.x.129]  [1.x.130] 




Our next job is to define a few classes that will contain run-time parameters (for example solver tolerances, number of iterations, stabilization parameter, and the like). One could do this in the main class, but we separate it from that one to make the program more modular and easier to read: Everything that has to do with run-time parameters will be in the following namespace, whereas the program logic is in the main class.    


We will split the run-time parameters into a few separate structures, which we will all put into a namespace  [2.x.143] . Of these classes, there are a few that group the parameters for individual groups, such as for solvers, mesh refinement, or output. Each of these classes have functions  [2.x.144]  and  [2.x.145]  that declare parameter subsections and entries in a ParameterHandler object, and retrieve actual parameter values from such an object, respectively. These classes declare all their parameters in subsections of the ParameterHandler.    


The final class of the following namespace combines all the previous classes by deriving from them and taking care of a few more entries at the top level of the input file, as well as a few odd other entries in subsections that are too short to warrant a structure by themselves.    


It is worth pointing out one thing here: None of the classes below have a constructor that would initialize the various member variables. This isn't a problem, however, since we will read all variables declared in these classes from the input file (or indirectly: a ParameterHandler object will read it from there, and we will get the values from this object), and they will be initialized this way. In case a certain variable is not specified at all in the input file, this isn't a problem either: The ParameterHandler class will in this case simply take the default value that was specified when declaring an entry in the  [2.x.146]  functions of the classes below. 

[1.x.131] 




[1.x.132]  [1.x.133]      


The first of these classes deals with parameters for the linear inner solver. It offers parameters that indicate which solver to use (GMRES as a solver for general non-symmetric indefinite systems, or a sparse direct solver), the amount of output to be produced, as well as various parameters that tweak the thresholded incomplete LU decomposition (ILUT) that we use as a preconditioner for GMRES.      


In particular, the ILUT takes the following parameters: 

- ilut_fill: the number of extra entries to add when forming the ILU decomposition 

- ilut_atol, ilut_rtol: When forming the preconditioner, for certain problems bad conditioning (or just bad luck) can cause the preconditioner to be very poorly conditioned.  Hence it can help to add diagonal perturbations to the original matrix and form the preconditioner for this slightly better matrix.  ATOL is an absolute perturbation that is added to the diagonal before forming the prec, and RTOL is a scaling factor  [2.x.147] . 

- ilut_drop: The ILUT will drop any values that have magnitude less than this value.  This is a way to manage the amount of memory used by this preconditioner.      


The meaning of each parameter is also briefly described in the third argument of the  [2.x.148]  call in  [2.x.149] . 

[1.x.134] 




[1.x.135]  [1.x.136]      


Similarly, here are a few parameters that determine how the mesh is to be refined (and if it is to be refined at all). For what exactly the shock parameters do, see the mesh refinement functions further down. 

[1.x.137] 




[1.x.138]  [1.x.139]      


Next a section on flux modifications to make it more stable. In particular, two options are offered to stabilize the Lax-Friedrichs flux: either choose  [2.x.150]  where  [2.x.151]  is either a fixed number specified in the input file, or where  [2.x.152]  is a mesh dependent value. In the latter case, it is chosen as  [2.x.153]  with  [2.x.154]  the diameter of the face to which the flux is applied, and  [2.x.155]  the current time step. 

[1.x.140] 




[1.x.141]  [1.x.142]      


Then a section on output parameters. We offer to produce Schlieren plots (the squared gradient of the density, a tool to visualize shock fronts), and a time interval between graphical output in case we don't want an output file every time step. 

[1.x.143] 




[1.x.144]  [1.x.145]      


Finally the class that brings it all together. It declares a number of parameters itself, mostly ones at the top level of the parameter file as well as several in section too small to warrant their own classes. It also contains everything that is actually space dimension dependent, like initial or boundary conditions.      


Since this class is derived from all the ones above, the  [2.x.156]  functions call the respective functions of the base classes as well.      


Note that this class also handles the declaration of initial and boundary conditions specified in the input file. To this end, in both cases, there are entries like "w_0 value" which represent an expression in terms of  [2.x.157]  that describe the initial or boundary condition as a formula that will later be parsed by the FunctionParser class. Similar expressions exist for "w_1", "w_2", etc, denoting the  [2.x.158]  conserved variables of the Euler system. Similarly, we allow up to  [2.x.159]  boundary indicators to be used in the input file, and each of these boundary indicators can be associated with an inflow, outflow, or pressure boundary condition, with homogeneous boundary conditions being specified for each component and each boundary indicator separately.      


The data structure used to store the boundary indicators is a bit complicated. It is an array of  [2.x.160]  elements indicating the range of boundary indicators that will be accepted. For each entry in this array, we store a pair of data in the  [2.x.161]  structure: first, an array of size  [2.x.162]  that for each component of the solution vector indicates whether it is an inflow, outflow, or other kind of boundary, and second a FunctionParser object that describes all components of the solution vector for this boundary id at once.      


The  [2.x.163]  structure requires a constructor since we need to tell the function parser object at construction time how many vector components it is to describe. This initialization can therefore not wait till we actually set the formulas the FunctionParser object represents later in  [2.x.164]       


For the same reason of having to tell Function objects their vector size at construction time, we have to have a constructor of the  [2.x.165]  class that at least initializes the other FunctionParser object, i.e. the one describing initial conditions. 

[1.x.146] 




[1.x.147]  [1.x.148] 




Here finally comes the class that actually does something with all the Euler equation and parameter specifics we've defined above. The public interface is pretty much the same as always (the constructor now takes the name of a file from which to read parameters, which is passed on the command line). The private function interface is also pretty similar to the usual arrangement, with the  [2.x.166]  function split into three parts: one that contains the main loop over all cells and that then calls the other two for integrals over cells and faces, respectively. 

[1.x.149] 



The first few member variables are also rather standard. Note that we define a mapping object to be used throughout the program when assembling terms (we will hand it to every FEValues and FEFaceValues object); the mapping we use is just the standard  [2.x.167]  mapping -- nothing fancy, in other words -- but declaring one here and using it throughout the program will make it simpler later on to change it if that should become necessary. This is, in fact, rather pertinent: it is known that for transsonic simulations with the Euler equations, computations do not converge even as  [2.x.168]  if the boundary approximation is not of sufficiently high order. 

[1.x.150] 



Next come a number of data vectors that correspond to the solution of the previous time step ( [2.x.169] ), the best guess of the current solution ( [2.x.170] ; we say [1.x.151] because the Newton iteration to compute it may not have converged yet, whereas  [2.x.171]  refers to the fully converged final result of the previous time step), and a predictor for the solution at the next time step, computed by extrapolating the current and previous solution one time step into the future: 

[1.x.152] 



This final set of member variables (except for the object holding all run-time parameters at the very bottom and a screen output stream that only prints something if verbose output has been requested) deals with the interface we have in this program to the Trilinos library that provides us with linear solvers. Similarly to including PETSc matrices in  [2.x.172]  and  [2.x.173] , all we need to do is to create a Trilinos sparse matrix instead of the standard deal.II class. The system matrix is used for the Jacobian in each Newton step. Since we do not intend to run this program in parallel (which wouldn't be too hard with Trilinos data structures, though), we don't have to think about anything else like distributing the degrees of freedom. 

[1.x.153] 




[1.x.154]  [1.x.155]    


There is nothing much to say about the constructor. Essentially, it reads the input file and fills the parameter object with the parsed values: 

[1.x.156] 




[1.x.157]  [1.x.158]    


The following (easy) function is called each time the mesh is changed. All it does is to resize the Trilinos matrix according to a sparsity pattern that we generate as in all the previous tutorial programs. 

[1.x.159] 




[1.x.160]  [1.x.161]    


This and the following two functions are the meat of this program: They assemble the linear system that results from applying Newton's method to the nonlinear system of conservation equations.    


This first function puts all of the assembly pieces together in a routine that dispatches the correct piece for each cell/face.  The actual implementation of the assembly on these objects is done in the following functions.    


At the top of the function we do the usual housekeeping: allocate FEValues, FEFaceValues, and FESubfaceValues objects necessary to do the integrations on cells, faces, and subfaces (in case of adjoining cells on different refinement levels). Note that we don't need all information (like values, gradients, or real locations of quadrature points) for all of these objects, so we only let the FEValues classes whatever is actually necessary by specifying the minimal set of UpdateFlags. For example, when using a FEFaceValues object for the neighboring cell we only need the shape values: Given a specific face, the quadrature points and  [2.x.174]  values are the same as for the current cells, and the normal vectors are known to be the negative of the normal vectors of the current cell. 

[1.x.162] 



Then loop over all cells, initialize the FEValues object for the current cell and call the function that assembles the problem on this cell. 

[1.x.163] 



Then loop over all the faces of this cell.  If a face is part of the external boundary, then assemble boundary conditions there (the fifth argument to  [2.x.175]  indicates whether we are working on an external or internal face; if it is an external face, the fourth argument denoting the degrees of freedom indices of the neighbor is ignored, so we pass an empty vector): 

[1.x.164] 



The alternative is that we are dealing with an internal face. There are two cases that we need to distinguish: that this is a normal face between two cells at the same refinement level, and that it is a face between two cells of the different refinement levels.            


In the first case, there is nothing we need to do: we are using a continuous finite element, and face terms do not appear in the bilinear form in this case. The second case usually does not lead to face terms either if we enforce hanging node constraints strongly (as in all previous tutorial programs so far whenever we used continuous finite elements -- this enforcement is done by the AffineConstraints class together with  [2.x.176]  In the current program, however, we opt to enforce continuity weakly at faces between cells of different refinement level, for two reasons: (i) because we can, and more importantly (ii) because we would have to thread the automatic differentiation we use to compute the elements of the Newton matrix from the residual through the operations of the AffineConstraints class. This would be possible, but is not trivial, and so we choose this alternative approach.            


What needs to be decided is which side of an interface between two cells of different refinement level we are sitting on.            


Let's take the case where the neighbor is more refined first. We then have to loop over the children of the face of the current cell and integrate on each of them. We sprinkle a couple of assertions into the code to ensure that our reasoning trying to figure out which of the neighbor's children's faces coincides with a given subface of the current cell's faces is correct -- a bit of defensive programming never hurts.            


We then call the function that integrates over faces; since this is an internal face, the fifth argument is false, and the sixth one is ignored so we pass an invalid value again: 

[1.x.165] 



The other possibility we have to care for is if the neighbor is coarser than the current cell (in particular, because of the usual restriction of only one hanging node per face, the neighbor must be exactly one level coarser than the current cell, something that we check with an assertion). Again, we then integrate over this interface: 

[1.x.166] 




[1.x.167]  [1.x.168]    


This function assembles the cell term by computing the cell part of the residual, adding its negative to the right hand side vector, and adding its derivative with respect to the local variables to the Jacobian (i.e. the Newton matrix). Recall that the cell contributions to the residual read  [2.x.177]   [2.x.178]   [2.x.179]  where  [2.x.180]   [2.x.181]   [2.x.182]  for both  [2.x.183]  and  [2.x.184]  ,  [2.x.185]  is the  [2.x.186] th vector valued test function. Furthermore, the scalar product  [2.x.187]  is understood as  [2.x.188]  where  [2.x.189]  is the  [2.x.190] th component of the  [2.x.191] th test function.    


   


At the top of this function, we do the usual housekeeping in terms of allocating some local variables that we will need later. In particular, we will allocate variables that will hold the values of the current solution  [2.x.192]  after the  [2.x.193] th Newton iteration (variable  [2.x.194] ) and the previous time step's solution  [2.x.195]  (variable  [2.x.196] ).    


In addition to these, we need the gradients of the current variables.  It is a bit of a shame that we have to compute these; we almost don't.  The nice thing about a simple conservation law is that the flux doesn't generally involve any gradients.  We do need these, however, for the diffusion stabilization.    


The actual format in which we store these variables requires some explanation. First, we need values at each quadrature point for each of the  [2.x.197]  components of the solution vector. This makes for a two-dimensional table for which we use deal.II's Table class (this is more efficient than  [2.x.198]  because it only needs to allocate memory once, rather than once for each element of the outer vector). Similarly, the gradient is a three-dimensional table, which the Table class also supports.    


Secondly, we want to use automatic differentiation. To this end, we use the  [2.x.199]  template for everything that is computed from the variables with respect to which we would like to compute derivatives. This includes the current solution and gradient at the quadrature points (which are linear combinations of the degrees of freedom) as well as everything that is computed from them such as the residual, but not the previous time step's solution. These variables are all found in the first part of the function, along with a variable that we will use to store the derivatives of a single component of the residual: 

[1.x.169] 



Next, we have to define the independent variables that we will try to determine by solving a Newton step. These independent variables are the values of the local degrees of freedom which we extract here: 

[1.x.170] 



The next step incorporates all the magic: we declare a subset of the autodifferentiation variables as independent degrees of freedom, whereas all the other ones remain dependent functions. These are precisely the local degrees of freedom just extracted. All calculations that reference them (either directly or indirectly) will accumulate sensitivities with respect to these variables.      


In order to mark the variables as independent, the following does the trick, marking  [2.x.200]  as the  [2.x.201] th independent variable out of a total of  [2.x.202] : 

[1.x.171] 



After all these declarations, let us actually compute something. First, the values of  [2.x.203]  and  [2.x.204] , which we can compute from the local DoF values by using the formula  [2.x.205] , where  [2.x.206]  is the  [2.x.207] th entry of the (local part of the) solution vector, and  [2.x.208]  the value of the  [2.x.209] th vector-valued shape function evaluated at quadrature point  [2.x.210] . The gradient can be computed in a similar way.      


Ideally, we could compute this information using a call into something like  [2.x.211]  and  [2.x.212]  but since (i) we would have to extend the FEValues class for this, and (ii) we don't want to make the entire  [2.x.213]  vector fad types, only the local cell variables, we explicitly code the loop above. Before this, we add another loop that initializes all the fad variables to zero: 

[1.x.172] 



Next, in order to compute the cell contributions, we need to evaluate  [2.x.214] ,  [2.x.215]  and  [2.x.216] ,  [2.x.217]  at all quadrature points. To store these, we also need to allocate a bit of memory. Note that we compute the flux matrices and right hand sides in terms of autodifferentiation variables, so that the Jacobian contributions can later easily be computed from it: 







[1.x.173] 



We now have all of the pieces in place, so perform the assembly.  We have an outer loop through the components of the system, and an inner loop over the quadrature points, where we accumulate contributions to the  [2.x.218] th residual  [2.x.219] . The general formula for this residual is given in the introduction and at the top of this function. We can, however, simplify it a bit taking into account that the  [2.x.220] th (vector-valued) test function  [2.x.221]  has in reality only a single nonzero component (more on this topic can be found in the  [2.x.222]  module). It will be represented by the variable  [2.x.223]  below. With this, the residual term can be re-written as [1.x.174] 

where integrals are understood to be evaluated through summation over quadrature points.      


We initially sum all contributions of the residual in the positive sense, so that we don't need to negative the Jacobian entries.  Then, when we sum into the  [2.x.224]  vector, we negate this residual. 

[1.x.175] 



The residual for each row (i) will be accumulating into this fad variable.  At the end of the assembly for this row, we will query for the sensitivities to this variable and add them into the Jacobian. 







[1.x.176] 



At the end of the loop, we have to add the sensitivities to the matrix and subtract the residual from the right hand side. Trilinos FAD data type gives us access to the derivatives using  [2.x.225] , so we store the data in a temporary array. This information about the whole row of local dofs is then added to the Trilinos matrix at once (which supports the data types we have chosen). 

[1.x.177] 




[1.x.178]  [1.x.179]    


Here, we do essentially the same as in the previous function. At the top, we introduce the independent variables. Because the current function is also used if we are working on an internal face between two cells, the independent variables are not only the degrees of freedom on the current cell but in the case of an interior face also the ones on the neighbor. 

[1.x.180] 



Next, we need to define the values of the conservative variables  [2.x.226]  on this side of the face ( [2.x.227] ) and on the opposite side ( [2.x.228] ), for both  [2.x.229]  and   [2.x.230] . The "this side" values can be computed in exactly the same way as in the previous function, but note that the  [2.x.231]  variable now is of type FEFaceValues or FESubfaceValues: 

[1.x.181] 



Computing "opposite side" is a bit more complicated. If this is an internal face, we can compute it as above by simply using the independent variables from the neighbor: 

[1.x.182] 



On the other hand, if this is an external boundary face, then the values of  [2.x.232]  will be either functions of  [2.x.233] , or they will be prescribed, depending on the kind of boundary condition imposed here.      


To start the evaluation, let us ensure that the boundary id specified for this boundary is one for which we actually have data in the parameters object. Next, we evaluate the function object for the inhomogeneity.  This is a bit tricky: a given boundary might have both prescribed and implicit values.  If a particular component is not prescribed, the values evaluate to zero and are ignored below.      


The rest is done by a function that actually knows the specifics of Euler equation boundary conditions. Note that since we are using fad variables here, sensitivities will be updated appropriately, a process that would otherwise be tremendously complicated. 

[1.x.183] 



Here we assume that boundary type, boundary normal vector and boundary data values maintain the same during time advancing. 

[1.x.184] 



Now that we have  [2.x.234]  and  [2.x.235] , we can go about computing the numerical flux function  [2.x.236]  for each quadrature point. Before calling the function that does so, we also need to determine the Lax-Friedrich's stability parameter: 







[1.x.185] 



Now assemble the face term in exactly the same way as for the cell contributions in the previous function. The only difference is that if this is an internal face, we also have to take into account the sensitivities of the residual contributions to the degrees of freedom on the neighboring cell: 

[1.x.186] 




[1.x.187]  [1.x.188]    


Here, we actually solve the linear system, using either of Trilinos' Aztec or Amesos linear solvers. The result of the computation will be written into the argument vector passed to this function. The result is a pair of number of iterations and the final linear residual. 







[1.x.189] 



If the parameter file specified that a direct solver shall be used, then we'll get here. The process is straightforward, since deal.II provides a wrapper class to the Amesos direct solver within Trilinos. All we have to do is to create a solver control object (which is just a dummy object here, since we won't perform any iterations), and then create the direct solver object. When actually doing the solve, note that we don't pass a preconditioner. That wouldn't make much sense for a direct solver anyway.  At the end we return the solver control statistics &mdash; which will tell that no iterations have been performed and that the final linear residual is zero, absent any better information that may be provided here: 

[1.x.190] 



Likewise, if we are to use an iterative solver, we use Aztec's GMRES solver. We could use the Trilinos wrapper classes for iterative solvers and preconditioners here as well, but we choose to use an Aztec solver directly. For the given problem, Aztec's internal preconditioner implementations are superior over the ones deal.II has wrapper classes to, so we use ILU-T preconditioning within the AztecOO solver and set a bunch of options that can be changed from the parameter file.          


There are two more practicalities: Since we have built our right hand side and solution vector as deal.II Vector objects (as opposed to the matrix, which is a Trilinos object), we must hand the solvers Trilinos Epetra vectors.  Luckily, they support the concept of a 'view', so we just send in a pointer to our deal.II vectors. We have to provide an Epetra_Map for the vector that sets the parallel distribution, which is just a dummy object in serial. The easiest way is to ask the matrix for its map, and we're going to be ready for matrix-vector products with it.          


Secondly, the Aztec solver wants us to pass a Trilinos Epetra_CrsMatrix in, not the deal.II wrapper class itself. So we access to the actual Trilinos matrix in the Trilinos wrapper class by the command trilinos_matrix(). Trilinos wants the matrix to be non-constant, so we have to manually remove the constantness using a const_cast. 

[1.x.191] 




[1.x.192]  [1.x.193] 




This function is real simple: We don't pretend that we know here what a good refinement indicator would be. Rather, we assume that the  [2.x.237]  class would know about this, and so we simply defer to the respective function we've implemented there: 

[1.x.194] 




[1.x.195]  [1.x.196] 




Here, we use the refinement indicators computed before and refine the mesh. At the beginning, we loop over all cells and mark those that we think should be refined: 

[1.x.197] 



Then we need to transfer the various solution vectors from the old to the new grid while we do the refinement. The SolutionTransfer class is our friend here; it has a fairly extensive documentation, including examples, so we won't comment much on the following code. The last three lines simply re-set the sizes of some other vectors to the now correct size: 

[1.x.198] 




[1.x.199]  [1.x.200] 




This function now is rather straightforward. All the magic, including transforming data from conservative variables to physical ones has been abstracted and moved into the EulerEquations class so that it can be replaced in case we want to solve some other hyperbolic conservation law.    


Note that the number of the output file is determined by keeping a counter in the form of a static variable that is set to zero the first time we come to this function and is incremented by one at the end of each invocation. 

[1.x.201] 




[1.x.202]  [1.x.203] 




This function contains the top-level logic of this program: initialization, the time loop, and the inner Newton iteration.    


At the beginning, we read the mesh file specified by the parameter file, setup the DoFHandler and various vectors, and then interpolate the given initial conditions on this mesh. We then perform a number of mesh refinements, based on the initial conditions, to obtain a mesh that is already well adapted to the starting solution. At the end of this process, we output the initial solution. 

[1.x.204] 



Size all of the fields. 

[1.x.205] 



We then enter into the main time stepping loop. At the top we simply output some status information so one can keep track of where a computation is, as well as the header for a table that indicates progress of the nonlinear inner iteration: 

[1.x.206] 



Then comes the inner Newton iteration to solve the nonlinear problem in each time step. The way it works is to reset matrix and right hand side to zero, then assemble the linear system. If the norm of the right hand side is small enough, then we declare that the Newton iteration has converged. Otherwise, we solve the linear system, update the current solution with the Newton increment, and output convergence information. At the end, we check that the number of Newton iterations is not beyond a limit of 10 -- if it is, it appears likely that iterations are diverging and further iterations would do no good. If that happens, we throw an exception that will be caught in  [2.x.238]  with status information being displayed before the program aborts.          


Note that the way we write the AssertThrow macro below is by and large equivalent to writing something like <code>if (!(nonlin_iter @<= 10)) throw ExcMessage ("No convergence in nonlinear solver");</code>. The only significant difference is that AssertThrow also makes sure that the exception being thrown carries with it information about the location (file name and line number) where it was generated. This is not overly critical here, because there is only a single place where this sort of exception can happen; however, it is generally a very useful tool when one wants to find out where an error occurred. 

[1.x.207] 



We only get to this point if the Newton iteration has converged, so do various post convergence tasks here:          


First, we update the time and produce graphical output if so desired. Then we update a predictor for the solution at the next time step by approximating  [2.x.239]  to try and make adaptivity work better.  The idea is to try and refine ahead of a front, rather than stepping into a coarse set of elements and smearing the old_solution.  This simple time extrapolator does the job. With this, we then refine the mesh if so desired by the user, and finally continue on with the next time step: 

[1.x.208] 




[1.x.209]  [1.x.210] 




The following ``main'' function is similar to previous examples and need not to be commented on. Note that the program aborts if no input file name is given on the command line. 

[1.x.211] 

[1.x.212] [1.x.213][1.x.214] 


We run the problem with the mesh  [2.x.240]  (this file is in the same directory as the source code for this program) and the following input deck (available as  [2.x.241]  in the same directory): 

[1.x.215] 



When we run the program, we get the following kind of output: 

[1.x.216] 



This output reports the progress of the Newton iterations and the time stepping. Note that our implementation of the Newton iteration indeed shows the expected quadratic convergence order: the norm of the nonlinear residual in each step is roughly the norm of the previous step squared. This leads to the very rapid convergence we can see here. This holds until times up to  [2.x.242]  at which time the nonlinear iteration reports a lack of convergence: 

[1.x.217] 



We may find out the cause and possible remedies by looking at the animation of the solution. 

The result of running these computations is a bunch of output files that we can pass to our visualization program of choice. When we collate them into a movie, the results of last several time steps looks like this: 

 [2.x.243]  

As we see, when the heavy mass of fluid hits the left bottom corner, some oscillation occurs and lead to the divergence of the iteration. A lazy solution to this issue is add more viscosity. If we set the diffusion power  [2.x.244]  instead of  [2.x.245] , the simulation would be able to survive this crisis. Then, the result looks like this: 


 [2.x.246]  

The heavy mass of fluid is drawn down the slope by gravity, where it collides with the ski lodge and is flung into the air!  Hopefully everyone escapes! And also, we can see the boundary between heavy mass and light mass blur quickly due to the artificial viscosity. 

We can also visualize the evolution of the adaptively refined grid: 

 [2.x.247]  

The adaptivity follows and precedes the flow pattern, based on the heuristic refinement scheme discussed above. 





[1.x.218] [1.x.219][1.x.220] 


[1.x.221][1.x.222] 


The numerical scheme we have chosen is not particularly stable when the artificial viscosity is small while is too diffusive when the artificial viscosity is large. Furthermore, it is known there are more advanced techniques to stabilize the solution, for example streamline diffusion, least-squares stabilization terms, entropy viscosity. 




[1.x.223][1.x.224] 


While the Newton method as a nonlinear solver appears to work very well if the time step is small enough, the linear solver can be improved. For example, in the current scheme whenever we use an iterative solver, an ILU is computed anew for each Newton step; likewise, for the direct solver, an LU decomposition of the Newton matrix is computed in each step. This is obviously wasteful: from one Newton step to another, and probably also between time steps, the Newton matrix does not radically change: an ILU or a sparse LU decomposition for one Newton step is probably still a very good preconditioner for the next Newton or time step. Avoiding the recomputation would therefore be a good way to reduce the amount of compute time. 

One could drive this a step further: since close to convergence the Newton matrix changes only a little bit, one may be able to define a quasi-Newton scheme where we only re-compute the residual (i.e. the right hand side vector) in each Newton iteration, and re-use the Newton matrix. The resulting scheme will likely not be of quadratic convergence order, and we have to expect to do a few more nonlinear iterations; however, given that we don't have to spend the time to build the Newton matrix each time, the resulting scheme may well be faster. 


[1.x.225][1.x.226] 


The residual calculated in  [2.x.248]  function reads     [2.x.249]  This means that we calculate the spatial residual twice at one Newton iteration step: once respect to the current solution  [2.x.250]  and once more respect to the last time step solution  [2.x.251]  which remains the same during all Newton iterations through one timestep. Cache up the explicit part of residual   [2.x.252]  during Newton iteration will save lots of labor. 


[1.x.227][1.x.228] 


Finally, as a direction beyond the immediate solution of the Euler equations, this program tries very hard to separate the implementation of everything that is specific to the Euler equations into one class (the  [2.x.253]  class), and everything that is specific to assembling the matrices and vectors, nonlinear and linear solvers, and the general top-level logic into another (the  [2.x.254]  class). 

By replacing the definitions of flux matrices and numerical fluxes in this class, as well as the various other parts defined there, it should be possible to apply the  [2.x.255]  class to other hyperbolic conservation laws as well. [1.x.229] [1.x.230]  [2.x.256]  

 [2.x.257] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25] 

 [2.x.3]  

[1.x.26] 

 [2.x.4]  

[1.x.27] 

[1.x.28][1.x.29] 


[1.x.30][1.x.31] 

The incompressible motion of an inviscid fluid past a body (for example air past an airplane wing, or air or water past a propeller) is usually modeled by the Euler equations of fluid dynamics: 

[1.x.32] where the fluid density  [2.x.5]  and the acceleration  [2.x.6]  due to external forces are given and the velocity  [2.x.7]  and the pressure  [2.x.8]  are the unknowns. Here  [2.x.9]  is a closed bounded region representing the body around which the fluid moves. 

The above equations can be derived from Navier-Stokes equations assuming that the effects due to viscosity are negligible compared to those due to the pressure gradient, inertial forces and the external forces. This is the opposite case of the Stokes equations discussed in  [2.x.10]  which are the limit case of dominant viscosity, i.e. where the velocity is so small that inertia forces can be neglected. On the other hand, owing to the assumed incompressibility, the equations are not suited for very high speed gas flows where compressibility and the equation of state of the gas have to be taken into account, leading to the Euler equations of gas dynamics, a hyperbolic system. 

For the purpose of this tutorial program, we will consider only stationary flow without external forces: [1.x.33] 


Uniqueness of the solution of the Euler equations is ensured by adding the boundary conditions [1.x.34] 

which is to say that the body is at rest in our coordinate systems and is not permeable, and that the fluid has (constant) velocity  [2.x.11]  at infinity. An alternative viewpoint is that our coordinate system moves along with the body whereas the background fluid is at rest at infinity. Notice that we define the normal  [2.x.12]  as the [1.x.35] normal to the domain  [2.x.13] , which is the opposite of the outer normal to the integration domain. 

For both stationary and non stationary flow, the solution process starts by solving for the velocity in the second equation and substituting in the first equation in order to find the pressure. The solution of the stationary Euler equations is typically performed in order to understand the behavior of the given (possibly complex) geometry when a prescribed motion is enforced on the system. 

The first step in this process is to change the frame of reference from a coordinate system moving along with the body to one in which the body moves through a fluid that is at rest at infinity. This can be expressed by introducing a new velocity  [2.x.14]  for which we find that the same equations hold (because  [2.x.15] ) and we have boundary conditions [1.x.36] 

If we assume that the fluid is irrotational, i.e.,  [2.x.16]  in  [2.x.17] , we can represent the velocity, and consequently also the perturbation velocity, as the gradient of a scalar function: [1.x.37] and so the second part of Euler equations above can be rewritten as the homogeneous Laplace equation for the unknown  [2.x.18] : [1.x.38] while the momentum equation reduces to Bernoulli's equation that expresses the pressure  [2.x.19]  as a function of the potential  [2.x.20] : [1.x.39] 

So we can solve the problem by solving the Laplace equation for the potential.  We recall that the following functions, called fundamental solutions of the Laplace equation, 

[1.x.40] 

satisfy in a distributional sense the equation: 

[1.x.41] 

where the derivative is done in the variable  [2.x.21] . By using the usual Green identities, our problem can be written on the boundary  [2.x.22]  only. We recall the general definition of the second Green %identity: 

[1.x.42] 

where  [2.x.23]  is the normal to the surface of  [2.x.24]  pointing outwards from the domain of integration  [2.x.25] . 

In our case the domain of integration is the domain  [2.x.26] , whose boundary is  [2.x.27] , where the "boundary" at infinity is defined as 

[1.x.43] 

In our program the normals are defined as [1.x.44] to the domain  [2.x.28] , that is, they are in fact [1.x.45] to the integration domain, and some care is required in defining the various integrals with the correct signs for the normals, i.e. replacing  [2.x.29]  by  [2.x.30] . 

If we substitute  [2.x.31]  and  [2.x.32]  in the Green %identity with the solution  [2.x.33]  and with the fundamental solution of the Laplace equation respectively, as long as  [2.x.34]  is chosen in the region  [2.x.35] , we obtain: [1.x.46] 

where the normals are now pointing [1.x.47] the domain of integration. 

Notice that in the above equation, we also have the integrals on the portion of the boundary at  [2.x.36] . Using the boundary conditions of our problem, we have that  [2.x.37]  is zero at infinity (which simplifies the integral on  [2.x.38]  on the right hand side). 

The integral on  [2.x.39]  that appears on the left hand side can be treated by observing that  [2.x.40]  implies that  [2.x.41]  at infinity is necessarily constant. We define its value to be  [2.x.42] .  It is an easy exercise to prove that 

[1.x.48] 

Using this result, we can reduce the above equation only on the boundary  [2.x.43]  using the so-called Single and Double Layer Potential operators: 

[1.x.49] 

(The name of these operators comes from the fact that they describe the electric potential in  [2.x.44]  due to a single thin sheet of charges along a surface, and due to a double sheet of charges and anti-charges along the surface, respectively.) 

In our case, we know the Neumann values of  [2.x.45]  on the boundary:  [2.x.46] . Consequently, [1.x.50] If we take the limit for  [2.x.47]  tending to  [2.x.48]  of the above equation, using well known properties of the single and double layer operators, we obtain an equation for  [2.x.49]  just on the boundary  [2.x.50]  of  [2.x.51] : 

[1.x.51] 

which is the Boundary Integral Equation (BIE) we were looking for, where the quantity  [2.x.52]  is the fraction of angle or solid angle by which the point  [2.x.53]  sees the domain of integration  [2.x.54] . 

In particular, at points  [2.x.55]  where the boundary  [2.x.56]  is differentiable (i.e. smooth) we have  [2.x.57] , but the value may be smaller or larger at points where the boundary has a corner or an edge. 

Substituting the single and double layer operators we get: [1.x.52] for two dimensional flows and [1.x.53] for three dimensional flows, where the normal derivatives of the fundamental solutions have been written in a form that makes computation easier. In either case,  [2.x.58]  is the solution of an integral equation posed entirely on the boundary since both  [2.x.59] . 

Notice that the fraction of angle (in 2d) or solid angle (in 3d)  [2.x.60]  by which the point  [2.x.61]  sees the domain  [2.x.62]  can be defined using the double layer potential itself: [1.x.54] 

The reason why this is possible can be understood if we consider the fact that the solution of a pure Neumann problem is known up to an arbitrary constant  [2.x.63] , which means that, if we set the Neumann data to be zero, then any constant  [2.x.64]  will be a solution. Inserting the constant solution and the Neumann boundary condition in the boundary integral equation, we have 

[1.x.55] 

The integral on  [2.x.65]  is unity, see above, so division by the constant  [2.x.66]  gives us the explicit expression above for  [2.x.67] . 

While this example program is really only focused on the solution of the boundary integral equation, in a realistic setup one would still need to solve for the velocities. To this end, note that we have just computed  [2.x.68]  for all  [2.x.69] . In the next step, we can compute (analytically, if we want) the solution  [2.x.70]  in all of  [2.x.71] . To this end, recall that we had [1.x.56] where now we have everything that is on the right hand side ( [2.x.72]  and  [2.x.73]  are integrals we can evaluate, the normal velocity on the boundary is given, and  [2.x.74]  on the boundary we have just computed). Finally, we can then recover the velocity as  [2.x.75] . 

Notice that the evaluation of the above formula for  [2.x.76]  should yield zero as a result, since the integration of the Dirac delta  [2.x.77]  in the domain  [2.x.78]  is always zero by definition. 

As a final test, let us verify that this velocity indeed satisfies the momentum balance equation for a stationary flow field, i.e., whether  [2.x.79]  where  [2.x.80]  for some (unknown) pressure  [2.x.81]  and a given constant  [2.x.82] . In other words, we would like to verify that Bernoulli's law as stated above indeed holds. To show this, we use that the left hand side of this equation equates to 

[1.x.57] 

where we have used that  [2.x.83]  is constant. We would like to write this expression as the gradient of something (remember that  [2.x.84]  is a constant). The next step is more convenient if we consider the components of the equation individually (summation over indices that appear twice is implied): 

[1.x.58] 

because  [2.x.85]  and  [2.x.86] . Next, 

[1.x.59] 

Again, the last term disappears because  [2.x.87]  is constant and we can merge the first and third term into one: 

[1.x.60] 



We now only need to massage that last term a bit more. Using the product rule, we get 

[1.x.61] 

The first of these terms is zero (because, again, the summation over  [2.x.88]  gives  [2.x.89] , which is zero). The last term can be written as  [2.x.90]  which is in the desired gradient form. As a consequence, we can now finally state that 

[1.x.62] 

or in vector form: [1.x.63] or in other words: [1.x.64] Because the pressure is only determined up to a constant (it appears only with a gradient in the equations), an equally valid definition is [1.x.65] This is exactly Bernoulli's law mentioned above. 


[1.x.66][1.x.67] 


Numerical approximations of Boundary Integral Equations (BIE) are commonly referred to as the boundary element method or panel method (the latter expression being used mostly in the computational fluid dynamics community). The goal of the following test problem is to solve the integral formulation of the Laplace equation with Neumann boundary conditions, using a circle and a sphere respectively in two and three space dimensions, illustrating along the way the features that allow one to treat boundary element problems almost as easily as finite element problems using the deal.II library. 

To this end, let  [2.x.91]  be a subdivision of the manifold  [2.x.92]  into  [2.x.93]  line segments if  [2.x.94] , or  [2.x.95]  quadrilaterals if  [2.x.96] . We will call each individual segment or quadrilateral an [1.x.68] or [1.x.69], independently of the dimension  [2.x.97]  of the surrounding space  [2.x.98] . We define the finite dimensional space  [2.x.99]  as [1.x.70] with basis functions  [2.x.100]  for which we will use the usual FE_Q finite element, with the catch that this time it is defined on a manifold of codimension one (which we do by using the second template argument that is usually defaulted to equal the first; here, we will create objects  [2.x.101]  dimensional cells in a  [2.x.102]  dimensional space). An element  [2.x.103]  of  [2.x.104]  is uniquely identified by the vector  [2.x.105]  of its coefficients  [2.x.106] , that is: [1.x.71] where summation  is implied over repeated indexes. Note that we could use discontinuous elements here &mdash; in fact, there is no real reason to use continuous ones since the integral formulation does not imply any derivatives on our trial functions so continuity is unnecessary, and often in the literature only piecewise constant elements are used. 

[1.x.72][1.x.73] 


By far, the most common approximation of boundary integral equations is by use of the collocation based boundary element method. 

This method requires the evaluation of the boundary integral equation at a number of collocation points which is equal to the number of unknowns of the system. The choice of these points is a delicate matter, that requires a careful study. Assume that these points are known for the moment, and call them  [2.x.107]  with  [2.x.108] . 

The problem then becomes: Given the datum  [2.x.109] , find a function  [2.x.110]  in  [2.x.111]  such that the following  [2.x.112]  equations are satisfied: 

[1.x.74] 

where the quantity  [2.x.113]  is the fraction of (solid) angle by which the point  [2.x.114]  sees the domain  [2.x.115] , as explained above, and we set  [2.x.116]  to be zero.  If the support points  [2.x.117]  are chosen appropriately, then the problem can be written as the following linear system: 

[1.x.75] 

where 

[1.x.76] 

From a linear algebra point of view, the best possible choice of the collocation points is the one that renders the matrix  [2.x.118]  the most diagonally dominant. A natural choice is then to select the  [2.x.119]  collocation points to be the support points of the nodal basis functions  [2.x.120] . In that case,  [2.x.121] , and as a consequence the matrix  [2.x.122]  is diagonal with entries [1.x.77] where we have used that  [2.x.123]  for the usual Lagrange elements. With this choice of collocation points, the computation of the entries of the matrices  [2.x.124] ,  [2.x.125]  and of the right hand side  [2.x.126]  requires the evaluation of singular integrals on the elements  [2.x.127]  of the triangulation  [2.x.128] . As usual in these cases, all integrations are performed on a reference simple domain, i.e., we assume that each element  [2.x.129]  of  [2.x.130]  can be expressed as a linear (in two dimensions) or bi-linear (in three dimensions) transformation of the reference boundary element  [2.x.131] , and we perform the integrations after a change of variables from the real element  [2.x.132]  to the reference element  [2.x.133] . 

[1.x.78][1.x.79] 


In two dimensions it is not necessary to compute the diagonal elements  [2.x.134]  of the system matrix, since, even if the denominator goes to zero when  [2.x.135] , the numerator is always zero because  [2.x.136]  and  [2.x.137]  are orthogonal (on our polygonal approximation of the boundary of  [2.x.138] ), and the only singular integral arises in the computation of  [2.x.139]  on the i-th element of  [2.x.140] : [1.x.80] 

This can be easily treated by the QGaussLogR quadrature formula. 

Similarly, it is possible to use the QGaussOneOverR quadrature formula to perform the singular integrations in three dimensions. The interested reader will find detailed explanations on how these quadrature rules work in their documentation. 

The resulting matrix  [2.x.141]  is full. Depending on its size, it might be convenient to use a direct solver or an iterative one. For the purpose of this example code, we chose to use only an iterative solver, without providing any preconditioner. 

If this were a production code rather than a demonstration of principles, there are techniques that are available to not store full matrices but instead store only those entries that are large and/or relevant. In the literature on boundary element methods, a plethora of methods is available that allows to determine which elements are important and which are not, leading to a significantly sparser representation of these matrices that also facilitates rapid evaluations of the scalar product between vectors and matrices. This not being the goal of this program, we leave this for more sophisticated implementations. 


[1.x.81][1.x.82] 


The implementation is rather straight forward. The main point that hasn't been used in any of the previous tutorial programs is that most classes in deal.II are not only templated on the dimension, but in fact on the dimension of the manifold on which we pose the differential equation as well as the dimension of the space into which this manifold is embedded. By default, the second template argument equals the first, meaning for example that we want to solve on a two-dimensional region of two-dimensional space. The triangulation class to use in this case would be  [2.x.142] , which is an equivalent way of writing  [2.x.143] . 

However, this doesn't have to be so: in the current example, we will for example want to solve on the surface of a sphere, which is a two-dimensional manifold embedded in a three-dimensional space. Consequently, the right class will be  [2.x.144] , and correspondingly we will use  [2.x.145]  as the DoF handler class and  [2.x.146]  for finite elements. 

Some further details on what one can do with things that live on curved manifolds can be found in the report [1.x.83][1.x.84]. In addition, the  [2.x.147]  tutorial program extends what we show here to cases where the equation posed on the manifold is not an integral operator but in fact involves derivatives. 


[1.x.85][1.x.86] 


The testcase we will be solving is for a circular (in 2d) or spherical (in 3d) obstacle. Meshes for these geometries will be read in from files in the current directory and an object of type SphericalManifold will then be attached to the triangulation to allow mesh refinement that respects the continuous geometry behind the discrete initial mesh. 

For a sphere of radius  [2.x.148]  translating at a velocity of  [2.x.149]  in the  [2.x.150]  direction, the potential reads 

[1.x.87] 

see, e.g. J. N. Newman, [1.x.88], 1977, pp. 127. For unit speed and radius, and restricting  [2.x.151]  to lie on the surface of the sphere,  [2.x.152] . In the test problem, the flow is  [2.x.153] , so the appropriate exact solution on the surface of the sphere is the superposition of the above solution with the analogous solution along the  [2.x.154]  and  [2.x.155]  axes, or  [2.x.156] . [1.x.89] [1.x.90] 


[1.x.91]  [1.x.92] 




The program starts with including a bunch of include files that we will use in the various parts of the program. Most of them have been discussed in previous tutorials already: 

[1.x.93] 



And here are a few C++ standard header files that we will need: 

[1.x.94] 



The last part of this preamble is to import everything in the dealii namespace into the one into which everything in this program will go: 

[1.x.95] 




[1.x.96]  [1.x.97] 




First, let us define a bit of the boundary integral equation machinery. 




The following two functions are the actual calculations of the single and double layer potential kernels, that is  [2.x.157]  and  [2.x.158] . They are well defined only if the vector  [2.x.159]  is different from zero. 

[1.x.98] 




[1.x.99]  [1.x.100] 




The structure of a boundary element method code is very similar to the structure of a finite element code, and so the member functions of this class are like those of most of the other tutorial programs. In particular, by now you should be familiar with reading parameters from an external file, and with the splitting of the different tasks into different modules. The same applies to boundary element methods, and we won't comment too much on them, except on the differences. 

[1.x.101] 



The only really different function that we find here is the assembly routine. We wrote this function in the most possible general way, in order to allow for easy generalization to higher order methods and to different fundamental solutions (e.g., Stokes or Maxwell).      


The most noticeable difference is the fact that the final matrix is full, and that we have a nested loop inside the usual loop on cells that visits all support points of the degrees of freedom.  Moreover, when the support point lies inside the cell which we are visiting, then the integral we perform becomes singular.      


The practical consequence is that we have two sets of quadrature formulas, finite element values and temporary storage, one for standard integration and one for the singular integration, which are used where necessary. 

[1.x.102] 



There are two options for the solution of this problem. The first is to use a direct solver, and the second is to use an iterative solver. We opt for the second option.      


The matrix that we assemble is not symmetric, and we opt to use the GMRES method; however the construction of an efficient preconditioner for boundary element methods is not a trivial issue. Here we use a non preconditioned GMRES solver. The options for the iterative solver, such as the tolerance, the maximum number of iterations, are selected through the parameter file. 

[1.x.103] 



Once we obtained the solution, we compute the  [2.x.160]  error of the computed potential as well as the  [2.x.161]  error of the approximation of the solid angle. The mesh we are using is an approximation of a smooth curve, therefore the computed diagonal matrix of fraction of angles or solid angles  [2.x.162]  should be constantly equal to  [2.x.163] . In this routine we output the error on the potential and the error in the approximation of the computed angle. Notice that the latter error is actually not the error in the computation of the angle, but a measure of how well we are approximating the sphere and the circle.      


Experimenting a little with the computation of the angles gives very accurate results for simpler geometries. To verify this you can comment out, in the read_domain() method, the tria.set_manifold(1, manifold) line, and check the alpha that is generated by the program. By removing this call, whenever the mesh is refined new nodes will be placed along the straight lines that made up the coarse mesh, rather than be pulled onto the surface that we really want to approximate. In the three dimensional case, the coarse grid of the sphere is obtained starting from a cube, and the obtained values of alphas are exactly  [2.x.164]  on the nodes of the faces,  [2.x.165]  on the nodes of the edges and  [2.x.166]  on the 8 nodes of the vertices. 

[1.x.104] 



Once we obtained a solution on the codimension one domain, we want to interpolate it to the rest of the space. This is done by performing again the convolution of the solution with the kernel in the compute_exterior_solution() function.      


We would like to plot the velocity variable which is the gradient of the potential solution. The potential solution is only known on the boundary, but we use the convolution with the fundamental solution to interpolate it on a standard dim dimensional continuous finite element space. The plot of the gradient of the extrapolated solution will give us the velocity we want.      


In addition to the solution on the exterior domain, we also output the solution on the domain's boundary in the output_results() function, of course. 

[1.x.105] 



To allow for dimension independent programming, we specialize this single function to extract the singular quadrature formula needed to integrate the singular kernels in the interior of the cells. 

[1.x.106] 



The usual deal.II classes can be used for boundary element methods by specifying the "codimension" of the problem. This is done by setting the optional second template arguments to Triangulation, FiniteElement and DoFHandler to the dimension of the embedding space. In our case we generate either 1 or 2 dimensional meshes embedded in 2 or 3 dimensional spaces.      


The optional argument by default is equal to the first argument, and produces the usual finite element classes that we saw in all previous examples.      


The class is constructed in a way to allow for arbitrary order of approximation of both the domain (through high order mapping) and the finite element space. The order of the finite element space and of the mapping can be selected in the constructor of the class. 







[1.x.107] 



In BEM methods, the matrix that is generated is dense. Depending on the size of the problem, the final system might be solved by direct LU decomposition, or by iterative methods. In this example we use an unpreconditioned GMRES method. Building a preconditioner for BEM method is non trivial, and we don't treat this subject here. 







[1.x.108] 



The next two variables will denote the solution  [2.x.167]  as well as a vector that will hold the values of  [2.x.168]  (the fraction of  [2.x.169]  visible from a point  [2.x.170] ) at the support points of our shape functions. 







[1.x.109] 



The convergence table is used to output errors in the exact solution and in the computed alphas. 







[1.x.110] 



The following variables are the ones that we fill through a parameter file.  The new objects that we use in this example are the  [2.x.171]  object and the QuadratureSelector object.      


The  [2.x.172]  class allows us to easily and quickly define new function objects via parameter files, with custom definitions which can be very complex (see the documentation of that class for all the available options).      


We will allocate the quadrature object using the QuadratureSelector class that allows us to generate quadrature formulas based on an identifying string and on the possible degree of the formula itself. We used this to allow custom selection of the quadrature formulas for the standard integration, and to define the order of the singular quadrature rule.      


We also define a couple of parameters which are used in case we wanted to extend the solution to the entire domain. 







[1.x.111] 




[1.x.112]  [1.x.113] 




The constructor initializes the various object in much the same way as done in the finite element programs such as  [2.x.173]  or  [2.x.174] . The only new ingredient here is the ParsedFunction object, which needs, at construction time, the specification of the number of components.    


For the exact solution the number of vector components is one, and no action is required since one is the default value for a ParsedFunction object. The wind, however, requires dim components to be specified. Notice that when declaring entries in a parameter file for the expression of the  [2.x.175]  we need to specify the number of components explicitly, since the function  [2.x.176]  is static, and has no knowledge of the number of components. 

[1.x.114] 



For both two and three dimensions, we set the default input data to be such that the solution is  [2.x.177]  or  [2.x.178] . The actually computed solution will have value zero at infinity. In this case, this coincide with the exact solution, and no additional corrections are needed, but you should be aware of the fact that we arbitrarily set  [2.x.179] , and the exact solution we pass to the program needs to have the same value at infinity for the error to be computed correctly.      


The use of the  [2.x.180]  object is pretty straight forward. The  [2.x.181]  function takes an additional integer argument that specifies the number of components of the given function. Its default value is one. When the corresponding  [2.x.182]  method is called, the calling object has to have the same number of components defined here, otherwise an exception is thrown.      


When declaring entries, we declare both 2 and three dimensional functions. However only the dim-dimensional one is ultimately parsed. This allows us to have only one parameter file for both 2 and 3 dimensional problems.      


Notice that from a mathematical point of view, the wind function on the boundary should satisfy the condition  [2.x.183] , for the problem to have a solution. If this condition is not satisfied, then no solution can be found, and the solver will not converge. 

[1.x.115] 



In the solver section, we set all SolverControl parameters. The object will then be fed to the GMRES solver in the solve_system() function. 

[1.x.116] 



After declaring all these parameters to the ParameterHandler object, let's read an input file that will give the parameters their values. We then proceed to extract these values from the ParameterHandler object: 

[1.x.117] 



Finally, here's another example of how to use parameter files in dimension independent programming.  If we wanted to switch off one of the two simulations, we could do this by setting the corresponding "Run 2d simulation" or "Run 3d simulation" flag to false: 

[1.x.118] 




[1.x.119]  [1.x.120] 




A boundary element method triangulation is basically the same as a (dim-1) dimensional triangulation, with the difference that the vertices belong to a (dim) dimensional space.    


Some of the mesh formats supported in deal.II use by default three dimensional points to describe meshes. These are the formats which are compatible with the boundary element method capabilities of deal.II. In particular we can use either UCD or GMSH formats. In both cases, we have to be particularly careful with the orientation of the mesh, because, unlike in the standard finite element case, no reordering or compatibility check is performed here.  All meshes are considered as oriented, because they are embedded in a higher dimensional space. (See the documentation of the GridIn and of the Triangulation for further details on orientation of cells in a triangulation.) In our case, the normals to the mesh are external to both the circle in 2d or the sphere in 3d.    


The other detail that is required for appropriate refinement of the boundary element mesh is an accurate description of the manifold that the mesh approximates. We already saw this several times for the boundary of standard finite element meshes (for example in  [2.x.184]  and  [2.x.185] ), and here the principle and usage is the same, except that the SphericalManifold class takes an additional template parameter that specifies the embedding space dimension. 







[1.x.121] 



The call to  [2.x.186]  copies the manifold (via  [2.x.187]  so we do not need to worry about invalid pointers to  [2.x.188] : 

[1.x.122] 




[1.x.123]  [1.x.124] 




This function globally refines the mesh, distributes degrees of freedom, and resizes matrices and vectors. 







[1.x.125] 




[1.x.126]  [1.x.127] 




The following is the main function of this program, assembling the matrix that corresponds to the boundary integral equation. 

[1.x.128] 



First we initialize an FEValues object with the quadrature formula for the integration of the kernel in non singular cells. This quadrature is selected with the parameter file, and needs to be quite precise, since the functions we are integrating are not polynomial functions. 

[1.x.129] 



Unlike in finite element methods, if we use a collocation boundary element method, then in each assembly loop we only assemble the information that refers to the coupling between one degree of freedom (the degree associated with support point  [2.x.189] ) and the current cell. This is done using a vector of fe.dofs_per_cell elements, which will then be distributed to the matrix in the global row  [2.x.190] . The following object will hold this information: 

[1.x.130] 



The index  [2.x.191]  runs on the collocation points, which are the support points of the  [2.x.192] th basis function, while  [2.x.193]  runs on inner integration points. 




We construct a vector of support points which will be used in the local integrations: 

[1.x.131] 



After doing so, we can start the integration loop over all cells, where we first initialize the FEValues object and get the values of  [2.x.194]  at the quadrature points (this vector field should be constant, but it doesn't hurt to be more general): 

[1.x.132] 



We then form the integral over the current cell for all degrees of freedom (note that this includes degrees of freedom not located on the current cell, a deviation from the usual finite element integrals). The integral that we need to perform is singular if one of the local degrees of freedom is the same as the support point  [2.x.195] . A the beginning of the loop we therefore check whether this is the case, and we store which one is the singular index: 

[1.x.133] 



We then perform the integral. If the index  [2.x.196]  is not one of the local degrees of freedom, we simply have to add the single layer terms to the right hand side, and the double layer terms to the matrix: 

[1.x.134] 



Now we treat the more delicate case. If we are here, this means that the cell that runs on the  [2.x.197]  index contains support_point[i]. In this case both the single and the double layer potential are singular, and they require special treatment.                  


Whenever the integration is performed with the singularity inside the given cell, then a special quadrature formula is used that allows one to integrate arbitrary functions against a singular weight on the reference cell.                  


The correct quadrature formula is selected by the get_singular_quadrature function, which is explained in detail below. 

[1.x.135] 



Finally, we need to add the contributions of the current cell to the global matrix. 

[1.x.136] 



The second part of the integral operator is the term  [2.x.198] . Since we use a collocation scheme,  [2.x.199]  and the corresponding matrix is a diagonal one with entries equal to  [2.x.200] . 




One quick way to compute this diagonal matrix of the solid angles, is to use the Neumann matrix itself. It is enough to multiply the matrix with a vector of elements all equal to -1, to get the diagonal matrix of the alpha angles, or solid angles (see the formula in the introduction for this). The result is then added back onto the system matrix object to yield the final form of the matrix: 

[1.x.137] 




[1.x.138]  [1.x.139] 




The next function simply solves the linear system. 

[1.x.140] 




[1.x.141]  [1.x.142] 




The computation of the errors is exactly the same in all other example programs, and we won't comment too much. Notice how the same methods that are used in the finite element methods can be used here. 

[1.x.143] 



The error in the alpha vector can be computed directly using the  [2.x.201]  function, since on each node, the value should be  [2.x.202] . All errors are then output and appended to our ConvergenceTable object for later computation of convergence rates: 

[1.x.144] 



Singular integration requires a careful selection of the quadrature rules. In particular the deal.II library provides quadrature rules which are tailored for logarithmic singularities (QGaussLog, QGaussLogR), as well as for 1/R singularities (QGaussOneOverR).    


Singular integration is typically obtained by constructing weighted quadrature formulas with singular weights, so that it is possible to write    


[1.x.145]    


where  [2.x.203]  is a given singularity, and the weights and quadrature points  [2.x.204]  are carefully selected to make the formula above an equality for a certain class of functions  [2.x.205] .    


In all the finite element examples we have seen so far, the weight of the quadrature itself (namely, the function  [2.x.206] ), was always constantly equal to 1.  For singular integration, we have two choices: we can use the definition above, factoring out the singularity from the integrand (i.e., integrating  [2.x.207]  with the special quadrature rule), or we can ask the quadrature rule to "normalize" the weights  [2.x.208]  with  [2.x.209] :    


[1.x.146]    


We use this second option, through the  [2.x.210]  parameter of both QGaussLogR and QGaussOneOverR.    


These integrals are somewhat delicate, especially in two dimensions, due to the transformation from the real to the reference cell, where the variable of integration is scaled with the determinant of the transformation.    


In two dimensions this process does not result only in a factor appearing as a constant factor on the entire integral, but also on an additional integral altogether that needs to be evaluated:    


[1.x.147]    


This process is taken care of by the constructor of the QGaussLogR class, which adds additional quadrature points and weights to take into consideration also the second part of the integral.    


A similar reasoning should be done in the three dimensional case, since the singular quadrature is tailored on the inverse of the radius  [2.x.211]  in the reference cell, while our singular function lives in real space, however in the three dimensional case everything is simpler because the singularity scales linearly with the determinant of the transformation. This allows us to build the singular two dimensional quadrature rules only once and, reuse them over all cells.    


In the one dimensional singular integration this is not possible, since we need to know the scaling parameter for the quadrature, which is not known a priori. Here, the quadrature rule itself depends also on the size of the current cell. For this reason, it is necessary to create a new quadrature for each singular integration.    


The different quadrature rules are built inside the get_singular_quadrature, which is specialized for dim=2 and dim=3, and they are retrieved inside the assemble_system function. The index given as an argument is the index of the unit support point where the singularity is located. 







[1.x.148] 




[1.x.149]  [1.x.150] 




We'd like to also know something about the value of the potential  [2.x.212]  in the exterior domain: after all our motivation to consider the boundary integral problem was that we wanted to know the velocity in the exterior domain!    


To this end, let us assume here that the boundary element domain is contained in the box  [2.x.213] , and we extrapolate the actual solution inside this box using the convolution with the fundamental solution. The formula for this is given in the introduction.    


The reconstruction of the solution in the entire space is done on a continuous finite element grid of dimension dim. These are the usual ones, and we don't comment any further on them. At the end of the function, we output this exterior solution in, again, much the usual way. 

[1.x.151] 




[1.x.152]  [1.x.153] 




Outputting the results of our computations is a rather mechanical tasks. All the components of this function have been discussed before. 

[1.x.154] 




[1.x.155]  [1.x.156] 




This is the main function. It should be self explanatory in its briefness: 

[1.x.157] 




[1.x.158]  [1.x.159] 




This is the main function of this program. It is exactly like all previous tutorial programs: 

[1.x.160] 

[1.x.161][1.x.162] 


We ran the program using the following  [2.x.214]  file (which can also be found in the directory in which all the other source files are): 

[1.x.163] 



When we run the program, the following is printed on screen: 

[1.x.164] 



As we can see from the convergence table in 2d, if we choose quadrature formulas which are accurate enough, then the error we obtain for  [2.x.215]  should be exactly the inverse of the number of elements. The approximation of the circle with N segments of equal size generates a regular polygon with N faces, whose angles are exactly  [2.x.216] , therefore the error we commit should be exactly  [2.x.217] . In fact this is a very good indicator that we are performing the singular integrals in an appropriate manner. 

The error in the approximation of the potential  [2.x.218]  is largely due to approximation of the domain. A much better approximation could be obtained by using higher order mappings. 

If we modify the main() function, setting fe_degree and mapping_degree to two, and raise the order of the quadrature formulas  in the parameter file, we obtain the following convergence table for the two dimensional simulation 

[1.x.165] 



and 

[1.x.166] 



for the three dimensional case. As we can see, convergence results are much better with higher order mapping, mainly due to a better resolution of the curved geometry. Notice that, given the same number of degrees of freedom, for example in step 3 of the Q1 case and step 2 of Q2 case in the three dimensional simulation, the error is roughly three orders of magnitude lower. 

The result of running these computations is a bunch of output files that we can pass to our visualization program of choice. The output files are of two kind: the potential on the boundary element surface, and the potential extended to the outer and inner domain. The combination of the two for the two dimensional case looks like 

 [2.x.219]  

while in three dimensions we show first the potential on the surface, together with a contour plot, 

 [2.x.220]  

and then the external contour plot of the potential, with opacity set to 25%: 

 [2.x.221]  


[1.x.167] [1.x.168][1.x.169] 


This is the first tutorial program that considers solving equations defined on surfaces embedded in higher dimensional spaces. But the equation discussed here was relatively simple because it only involved an integral operator, not derivatives which are more difficult to define on the surface. The  [2.x.222]  tutorial program considers such problems and provides the necessary tools. 

From a practical perspective, the Boundary Element Method (BEM) used here suffers from two bottlenecks. The first is that assembling the matrix has a cost that is *quadratic* in the number of unknowns, that is  [2.x.223]  where  [2.x.224]  is the total number of unknowns. This can be seen by looking at the `assemble_system()` function, which has this structure: 

[1.x.170] 

Here, the first loop walks over all cells (one factor of  [2.x.225] ) whereas the inner loop contributes another factor of  [2.x.226] . 

This has to be contrasted with the finite element method for *local* differential operators: There, we loop over all cells (one factor of  [2.x.227] ) and on each cell do an amount of work that is independent of how many cells or unknowns there are. This clearly presents a bottleneck. 

The second bottleneck is that the system matrix is dense (i.e., is of type FullMatrix) because every degree of freedom couples with every other degree of freedom. As pointed out above, just *computing* this matrix with its  [2.x.228]  nonzero entries necessarily requires at least  [2.x.229]  operations, but it's worth pointing out that it also costs this many operations to just do one matrix-vector product. If the GMRES method used to solve the linear system requires a number of iterations that grows with the size of the problem, as is typically the case, then solving the linear system will require a number of operations that grows even faster than just  [2.x.230] . 

"Real" boundary element methods address these issues by strategies that determine which entries of the matrix will be small and can consequently be neglected (at the cost of introducing an additional error, of course). This is possible by recognizing that the matrix entries decay with the (physical) distance between the locations where degrees of freedom  [2.x.231]  and  [2.x.232]  are defined. This can be exploited in methods such as the Fast Multipole Method (FMM) that control which matrix entries must be stored and computed to achieve a certain accuracy, and -- if done right -- result in methods in which both assembly and solution of the linear system requires less than  [2.x.233]  operations. 

Implementing these methods clearly presents opportunities to extend the current program. [1.x.171] [1.x.172]  [2.x.234]  

 [2.x.235] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28] 

 [2.x.3]  

[1.x.29] 

[1.x.30] [1.x.31][1.x.32] 


[1.x.33] [1.x.34][1.x.35] 

The purpose of this program is to show how to effectively solve the incompressible time-dependent Navier-Stokes equations. These equations describe the flow of a viscous incompressible fluid and read 

[1.x.36] 

where  [2.x.4]  represents the velocity of the flow and  [2.x.5]  the pressure. This system of equations is supplemented by the initial condition [1.x.37] with  [2.x.6]  sufficiently smooth and solenoidal, and suitable boundary conditions. For instance, an admissible boundary condition, is [1.x.38] It is possible to prescribe other boundary conditions as well. In the test case that we solve here the boundary is partitioned into two disjoint subsets  [2.x.7]  and we have [1.x.39] and [1.x.40] where  [2.x.8]  is the outer unit normal. The boundary conditions on  [2.x.9]  are often used to model outflow conditions. 

In previous tutorial programs (see for instance  [2.x.10]  and  [2.x.11] ) we have seen how to solve the time-independent Stokes equations using a Schur complement approach. For the time-dependent case, after time discretization, we would arrive at a system like 

[1.x.41] 

where  [2.x.12]  is the time-step. Although the structure of this system is similar to the Stokes system and thus it could be solved using a Schur complement approach, it turns out that the condition number of the Schur complement is proportional to  [2.x.13] . This makes the system very difficult to solve, and means that for the Navier-Stokes equations, this is not a useful avenue to the solution. 

[1.x.42] [1.x.43][1.x.44] 


Rather, we need to come up with a different approach to solve the time-dependent Navier-Stokes equations. The difficulty in their solution comes from the fact that the velocity and the pressure are coupled through the constraint [1.x.45] for which the pressure is the Lagrange multiplier. Projection methods aim at decoupling this constraint from the diffusion (Laplace) operator. 

Let us shortly describe how the projection methods look like in a semi-discrete setting. The objective is to obtain a sequence of velocities  [2.x.14]  and pressures  [2.x.15] . We will also obtain a sequence  [2.x.16]  of auxiliary variables. Suppose that from the initial conditions, and an application of a first order method we have found  [2.x.17]  and  [2.x.18] . Then the projection method consists of the following steps:  [2.x.19]     [2.x.20]  [1.x.46]: Extrapolation. Define:   [1.x.47]    [2.x.21]  [1.x.48]: Diffusion step. We find  [2.x.22]  that solves the single   linear equation   [1.x.49] 

   [2.x.23]  [1.x.50]: Projection. Find  [2.x.24]  that solves   [1.x.51]    [2.x.25]  [1.x.52]: Pressure correction. Here we have two options:      [2.x.26]         [2.x.27]  [1.x.53]. The pressure is updated by:       [1.x.54]        [2.x.28]  [1.x.55]. In this case       [1.x.56]      [2.x.29]   [2.x.30]  

Without going into details, let us remark a few things about the projection methods that we have just described:  [2.x.31]     [2.x.32]  The advection term  [2.x.33]  is replaced by its [1.x.57]   [1.x.58]   This is consistent with the continuous equation (because  [2.x.34] ,   though this is not true pointwise for the discrete solution) and it is needed to   guarantee unconditional stability of the   time-stepping scheme. Moreover, to linearize the term we use the second order extrapolation  [2.x.35]  of    [2.x.36] .    [2.x.37]  The projection step is a realization of the Helmholtz decomposition   [1.x.59]   where   [1.x.60]   and   [1.x.61]   Indeed, if we use this decomposition on  [2.x.38]  we obtain   [1.x.62]   with  [2.x.39] . Taking the divergence of this equation we arrive at the projection equation.    [2.x.40]  The more accurate of the two variants outlined above is the rotational   one. However, the program below implements both variants. Moreover, in the author's experience,   the standard form is the one that should be used if, for instance, the viscosity  [2.x.41]  is variable.  [2.x.42]  


<p> The standard incremental scheme and the rotational incremental scheme were first considered by van Kan in  [2.x.43]     [2.x.44]  J. van Kan, "A second-order accurate pressure-correction scheme for viscous incompressible flow",        SIAM Journal on Scientific and Statistical Computing, vol. 7, no. 3, pp. 870–891, 1986  [2.x.45]  and is analyzed by Guermond in  [2.x.46]     [2.x.47]  J.-L. Guermond, "Un résultat de convergence d’ordre deux en temps pour                         l’approximation des équations de Navier–Stokes par une technique de projection incrémentale",        ESAIM: Mathematical Modelling and Numerical Analysis, vol. 33, no. 1, pp. 169–189, 1999  [2.x.48]  for the case  [2.x.49] . It turns out that this technique suffers from unphysical boundary conditions for the kinematic pressure that lead to reduced rates of convergence. To prevent this, Timmermans et al. proposed in  [2.x.50]     [2.x.51]  L. Timmermans, P. Minev, and F. Van De Vosse,        "An approximate projection scheme for incompressible flow using spectral elements",        International Journal for Numerical Methods in Fluids, vol. 22, no. 7, pp. 673–688, 1996  [2.x.52]  the rotational pressure-correction projection method that uses a divergence correction for the kinematic pressure. A thorough analysis for scheme has first been performed in  [2.x.53]     [2.x.54]  J.-L. Guermond and J. Shen, "On the error estimates for the rotational pressure-correction projection methods",        Mathematics of Computation, vol. 73, no. 248, pp. 1719–1737, 2004  [2.x.55]  for the Stokes problem. </p> 

[1.x.63] [1.x.64][1.x.65] 

To obtain a fully discrete setting of the method we, as always, need a variational formulation. There is one subtle issue here given the nature of the boundary conditions. When we multiply the equation by a suitable test function one of the term that arises is [1.x.66] If we, say, had Dirichlet boundary conditions on the whole boundary then after integration by parts we would obtain [1.x.67] One of the advantages of this formulation is that it fully decouples the components of the velocity. Moreover, they all share the same system matrix. This can be exploited in the program. 

However, given the nonstandard boundary conditions, to be able to take them into account we need to use the following %identity [1.x.68] so that when we integrate by parts and take into account the boundary conditions we obtain [1.x.69] which is the form that we would have to use. Notice that this couples the components of the velocity. Moreover, to enforce the boundary condition on the pressure, we need to rewrite [1.x.70] where the boundary integral in  [2.x.56]  equals zero given the boundary conditions for the velocity, and the one in  [2.x.57]  given the boundary conditions for the pressure. 

In the simplified case where the boundary  [2.x.58]  is %parallel to a coordinate axis, which holds for the testcase that we carry out below, it can actually be shown that [1.x.71] This issue is not very often addressed in the literature. For more information the reader can consult, for instance,  [2.x.59]     [2.x.60]  J.-L. GUERMOND, L. QUARTAPELLE, On the approximation of the unsteady Navier-Stokes equations by   finite element projection methods, Numer. Math., 80  (1998) 207-238    [2.x.61]  J.-L. GUERMOND, P. MINEV, J. SHEN, Error analysis of pressure-correction schemes for the   Navier-Stokes equations with open boundary conditions, SIAM J. Numer. Anal., 43  1 (2005) 239--258.  [2.x.62]  




[1.x.72] [1.x.73][1.x.74] 


Our implementation of the projection methods follows [1.x.75] the description given above. We must note, however, that as opposed to most other problems that have several solution components, we do not use vector-valued finite elements. Instead, we use separate finite elements the components of the velocity and the pressure, respectively, and use different  [2.x.63] 's for those as well. The main reason for doing this is that, as we see from the description of the scheme, the  [2.x.64]  components of the velocity and the pressure are decoupled. As a consequence, the equations for all the velocity components look all the same, have the same system matrix, and can be solved in %parallel. Obviously, this approach has also its disadvantages. For instance, we need to keep several  [2.x.65] s and iterators synchronized when assembling matrices and right hand sides; obtaining quantities that are inherent to vector-valued functions (e.g. divergences) becomes a little awkward, and others. 

[1.x.76] [1.x.77][1.x.78] 


The testcase that we use for this program consists of the flow around a square obstacle. The geometry is as follows: 

 [2.x.66]  

with  [2.x.67] , making the geometry slightly non-symmetric. 

We impose no-slip boundary conditions on both the top and bottom walls and the obstacle. On the left side we have the inflow boundary condition [1.x.79] with  [2.x.68] , i.e. the inflow boundary conditions correspond to Poiseuille flow for this configuration. Finally, on the right vertical wall we impose the condition that the vertical component of the velocity and the pressure should both be zero. The final time  [2.x.69] . [1.x.80] [1.x.81] 


[1.x.82]  [1.x.83] 




We start by including all the necessary deal.II header files and some C++ related ones. Each one of them has been discussed in previous tutorial programs, so we will not get into details here. 

[1.x.84] 



Finally this is as in all previous programs: 

[1.x.85] 




[1.x.86]  [1.x.87]    


Since our method has several parameters that can be fine-tuned we put them into an external file, so that they can be determined at run-time.    


This includes, in particular, the formulation of the equation for the auxiliary variable  [2.x.70] , for which we declare an  [2.x.71] . Next, we declare a class that is going to read and store all the parameters that our program needs to run. 

[1.x.88] 



In the constructor of this class we declare all the parameters. The details of how this works have been discussed elsewhere, for example in  [2.x.72] . 

[1.x.89] 




[1.x.90]  [1.x.91] 




In the next namespace, we declare the initial and boundary conditions: 

[1.x.92] 



As we have chosen a completely decoupled formulation, we will not take advantage of deal.II's capabilities to handle vector valued problems. We do, however, want to use an interface for the equation data that is somehow dimension independent. To be able to do that, our functions should be able to know on which spatial component we are currently working, and we should be able to have a common interface to do that. The following class is an attempt in that direction. 

[1.x.93] 



With this class defined, we declare classes that describe the boundary conditions for velocity and pressure: 

[1.x.94] 




[1.x.95]  [1.x.96] 




Now for the main class of the program. It implements the various versions of the projection method for Navier-Stokes equations. The names for all the methods and member variables should be self-explanatory, taking into account the implementation details given in the introduction. 

[1.x.97] 



The next few structures and functions are for doing various things in parallel. They follow the scheme laid out in  [2.x.73] , using the WorkStream class. As explained there, this requires us to declare two structures for each of the assemblers, a per-task data and a scratch data structure. These are then handed over to functions that assemble local contributions and that copy these local contributions to the global objects.      


One of the things that are specific to this program is that we don't just have a single DoFHandler object that represents both the velocities and the pressure, but we use individual DoFHandler objects for these two kinds of variables. We pay for this optimization when we want to assemble terms that involve both variables, such as the divergence of the velocity and the gradient of the pressure, times the respective test functions. When doing so, we can't just anymore use a single FEValues object, but rather we need two, and they need to be initialized with cell iterators that point to the same cell in the triangulation but different DoFHandlers.      


To do this in practice, we declare a "synchronous" iterator -- an object that internally consists of several (in our case two) iterators, and each time the synchronous iteration is moved forward one step, each of the iterators stored internally is moved forward one step as well, thereby always staying in sync. As it so happens, there is a deal.II class that facilitates this sort of thing. (What is important here is to know that two DoFHandler objects built on the same triangulation will walk over the cells of the triangulation in the same order.) 

[1.x.98] 



The same general layout also applies to the following classes and functions implementing the assembly of the advection term: 

[1.x.99] 



The final few functions implement the diffusion solve as well as postprocessing the output, including computing the curl of the velocity: 

[1.x.100] 




[1.x.101]  [1.x.102] 




In the constructor, we just read all the data from the  [2.x.74]  object that is passed as an argument, verify that the data we read is reasonable and, finally, create the triangulation and load the initial data. 

[1.x.103] 




[1.x.104]  [1.x.105] 




The method that creates the triangulation and refines it the needed number of times. After creating the triangulation, it creates the mesh dependent data, i.e. it distributes degrees of freedom and renumbers them, and initializes the matrices and vectors that we will use. 

[1.x.106] 




[1.x.107]  [1.x.108] 




This method creates the constant matrices and loads the initial data 

[1.x.109] 




[1.x.110]  [1.x.111] 




In this set of methods we initialize the sparsity patterns, the constraints (if any) and assemble the matrices that do not depend on the timestep  [2.x.75] . Note that for the Laplace and mass matrices, we can use functions in the library that do this. Because the expensive operations of this function -- creating the two matrices -- are entirely independent, we could in principle mark them as tasks that can be worked on in %parallel using the  [2.x.76]  functions. We won't do that here since these functions internally already are parallelized, and in particular because the current function is only called once per program run and so does not incur a cost in each time step. The necessary modifications would be quite straightforward, however. 

[1.x.112] 



The initialization of the matrices that act on the pressure space is similar to the ones that act on the velocity space. 

[1.x.113] 



For the gradient operator, we start by initializing the sparsity pattern and compressing it. It is important to notice here that the gradient operator acts from the pressure space into the velocity space, so we have to deal with two different finite element spaces. To keep the loops synchronized, we use the alias that we have defined before, namely  [2.x.77] . 

[1.x.114] 




[1.x.115]  [1.x.116] 




This is the time marching function, which starting at  [2.x.78]  advances in time using the projection method with time step  [2.x.79]  until  [2.x.80] .    


Its second parameter,  [2.x.81]  indicates whether the function should output information what it is doing at any given moment: for example, it will say whether we are working on the diffusion, projection substep; updating preconditioners etc. Rather than implementing this output using code like  [2.x.82]  we use the ConditionalOStream class to do that for us. That class takes an output stream and a condition that indicates whether the things you pass to it should be passed through to the given output stream, or should just be ignored. This way, above code simply becomes  [2.x.83]  and does the right thing in either case. 

[1.x.119] 




[1.x.120]  [1.x.121] 




The implementation of a diffusion step. Note that the expensive operation is the diffusion solve at the end of the function, which we have to do once for each velocity component. To accelerate things a bit, we allow to do this in %parallel, using the  [2.x.84]  function which makes sure that the  [2.x.85]  solves are all taken care of and are scheduled to available processors: if your machine has more than one processor core and no other parts of this program are using resources currently, then the diffusion solves will run in %parallel. On the other hand, if your system has only one processor core then running things in %parallel would be inefficient (since it leads, for example, to cache congestion) and things will be executed sequentially. 

[1.x.122] 




[1.x.123]  [1.x.124] 




The following few functions deal with assembling the advection terms, which is the part of the system matrix for the diffusion step that changes at every time step. As mentioned above, we will run the assembly loop over all cells in %parallel, using the WorkStream class and other facilities as described in the documentation module on  [2.x.86] . 

[1.x.125] 




[1.x.126]  [1.x.127] 




This implements the projection step: 

[1.x.128] 




[1.x.129]  [1.x.130] 




This is the pressure update step of the projection method. It implements the standard formulation of the method, that is [1.x.131] or the rotational form, which is [1.x.132] 

[1.x.133] 




[1.x.134]  [1.x.135] 




This method plots the current solution. The main difficulty is that we want to create a single output file that contains the data for all velocity components, the pressure, and also the vorticity of the flow. On the other hand, velocities and the pressure live on separate DoFHandler objects, and so can't be written to the same file using a single DataOut object. As a consequence, we have to work a bit harder to get the various pieces of data into a single DoFHandler object, and then use that to drive graphical output.    


We will not elaborate on this process here, but rather refer to  [2.x.87] , where a similar procedure is used (and is documented) to create a joint DoFHandler object for all variables.    


Let us also note that we here compute the vorticity as a scalar quantity in a separate function, using the  [2.x.88]  projection of the quantity  [2.x.89]  onto the finite element space used for the components of the velocity. In principle, however, we could also have computed as a pointwise quantity from the velocity, and do so through the DataPostprocessor mechanism discussed in  [2.x.90]  and  [2.x.91] . 

[1.x.136] 



Following is the helper function that computes the vorticity by projecting the term  [2.x.92]  onto the finite element space used for the components of the velocity. The function is only called whenever we generate graphical output, so not very often, and as a consequence we didn't bother parallelizing it using the WorkStream concept as we do for the other assembly functions. That should not be overly complicated, however, if needed. Moreover, the implementation that we have here only works for 2d, so we bail if that is not the case. 

[1.x.137] 




[1.x.138]  [1.x.139] 




The main function looks very much like in all the other tutorial programs, so there is little to comment on here: 

[1.x.140] 

[1.x.141] [1.x.142][1.x.143] 


[1.x.144] [1.x.145][1.x.146] 


We run the code with the following  [2.x.93] , which can be found in the same directory as the source: 

[1.x.147] 



Since the  [2.x.94] , we do not get any kind of output besides the number of the time step the program is currently working on. If we were to set it to  [2.x.95]  we would get information on what the program is doing and how many steps each iterative process had to make to converge, etc. 

Let us plot the obtained results for  [2.x.96]  (i.e. time steps 200, 1000, 2400, 4000, and 5000), where in the left column we show the vorticity and in the right the velocity field: 

 [2.x.97]  

The images show nicely the development and extension of a vortex chain behind the obstacles, with the sign of the vorticity indicating whether this is a left or right turning vortex. 


[1.x.148] [1.x.149][1.x.150] 


We can change the Reynolds number,  [2.x.98] , in the parameter file to a value of  [2.x.99] . Doing so, and reducing the time step somewhat as well, yields the following images at times  [2.x.100] : 

 [2.x.101]  

For this larger Reynolds number, we observe unphysical oscillations, especially for the vorticity. The discretization scheme has now difficulties in correctly resolving the flow, which should still be laminar and well-organized. These phenomena are typical of discretization schemes that lack robustness in under-resolved scenarios, where under-resolved means that the Reynolds number computed with the mesh size instead of the physical dimensions of the geometry is large. We look at a zoom at the region behind the obstacle, and the mesh size we have there: 


 [2.x.102]  

We can easily test our hypothesis by re-running the simulation with one more mesh refinement set in the parameter file: 

 [2.x.103]  

Indeed, the vorticity field now looks much smoother. While we can expect that further refining the mesh will suppress the remaining oscillations as well, one should take measures to obtain a robust scheme in the limit of coarse resolutions, as described below. 


[1.x.151] [1.x.152][1.x.153] 


This program can be extended in the following directions:  [2.x.104]     [2.x.105]  Adaptive mesh refinement: As we have seen, we computed everything on a single fixed mesh.   Using adaptive mesh refinement can lead to increased accuracy while not significantly increasing the   computational time. 

   [2.x.106]  Adaptive time-stepping: Although there apparently is currently no theory about   projection methods with variable time step,   practice shows that they perform very well. 

   [2.x.107]  High Reynolds %numbers: As we can see from the results, increasing the Reynolds number changes significantly   the behavior of the discretization scheme. Using well-known stabilization techniques we could be able to   compute the flow in this, or many other problems, when the Reynolds number is very large and where computational   costs demand spatial resolutions for which the flow is only marginally resolved, especially for 3D turbulent   flows. 

   [2.x.108]  Variable density incompressible flows: There are projection-like methods for the case of incompressible   flows with variable density. Such flows play a role if fluids of different   density mix, for example fresh water and salt water, or alcohol and water. 

   [2.x.109]  Compressible Navier-Stokes equations: These equations are relevant for   cases where   velocities are high enough so that the fluid becomes compressible, but not   fast enough that we get into a regime where viscosity becomes negligible   and the Navier-Stokes equations need to be replaced by the hyperbolic Euler   equations of gas dynamics. Compressibility starts to become a factor if the   velocity becomes greater than about one third of the speed of sound, so it   is not a factor for almost all terrestrial vehicles. On the other hand,   commercial jetliners fly at about 85 per cent of the speed of sound, and   flow over the wings becomes significantly supersonic, a regime in which the   compressible Navier-Stokes equations are not applicable any more   either. There are significant applications for the range in between,   however, such as for small aircraft or the fast trains in many European and   East Asian countries.  [2.x.110]  [1.x.154] [1.x.155]  [2.x.111]  

 [2.x.112] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19] 

 [2.x.3]  

[1.x.20] 

[1.x.21] [1.x.22][1.x.23] 


The problem we want to solve in this example is an eigenspectrum problem. Eigenvalue problems appear in a wide context of problems, for example in the computation of electromagnetic standing waves in cavities, vibration modes of drum membranes, or oscillations of lakes and estuaries. One of the most enigmatic applications is probably the computation of stationary or quasi-static wave functions in quantum mechanics. The latter application is what we would like to investigate here, though the general techniques outlined in this program are of course equally applicable to the other applications above. 

Eigenspectrum problems have the general form 

[1.x.24] 

where the Dirichlet boundary condition on  [2.x.4]  could also be replaced by Neumann or Robin conditions;  [2.x.5]  is an operator that generally also contains differential operators. 

Under suitable conditions, the above equations have a set of solutions  [2.x.6] ,  [2.x.7] , where  [2.x.8]  can be a finite or infinite set (and in the latter case it may be a discrete or sometimes at least in part a continuous set). In either case, let us note that there is no longer just a single solution, but a set of solutions (the various eigenfunctions and corresponding eigenvalues) that we want to compute. The problem of numerically finding all eigenvalues (eigenfunctions) of such eigenvalue problems is a formidable challenge. In fact, if the set  [2.x.9]  is infinite, the challenge is of course intractable.  Most of the time however we are really only interested in a small subset of these values (functions); and fortunately, the interface to the SLEPc library that we will use for this tutorial program allows us to select which portion of the eigenspectrum and how many solutions we want to solve for. 

In this program, the eigenspectrum solvers we use are classes provided by deal.II that wrap around the linear algebra implementation of the [1.x.25] library; SLEPc itself builds on the [1.x.26] library for linear algebra contents. 

[1.x.27] [1.x.28][1.x.29] 


The basic equation of stationary quantum mechanics is the Schrödinger equation which models the motion of particles in an external potential  [2.x.10] . The particle is described by a wave function  [2.x.11]  that satisfies a relation of the (nondimensionalized) form 

[1.x.30] 

As a consequence, this particle can only exist in a certain number of eigenstates that correspond to the energy eigenvalues  [2.x.12]  admitted as solutions of this equation. The orthodox (Copenhagen) interpretation of quantum mechanics posits that, if a particle has energy  [2.x.13]  then the probability of finding it at location  [2.x.14]  is proportional to  [2.x.15]  where  [2.x.16]  is the eigenfunction that corresponds to this eigenvalue. 

In order to numerically find solutions to this equation, i.e. a set of pairs of eigenvalues/eigenfunctions, we use the usual finite element approach of multiplying the equation from the left with test functions, integrating by parts, and searching for solutions in finite dimensional spaces by approximating  [2.x.17] , where  [2.x.18]  is a vector of expansion coefficients. We then immediately arrive at the following equation that discretizes the continuous eigenvalue problem: [1.x.31] In matrix and vector notation, this equation then reads: [1.x.32] where  [2.x.19]  is the stiffness matrix arising from the differential operator  [2.x.20] , and  [2.x.21]  is the mass matrix. The solution to the eigenvalue problem is an eigenspectrum  [2.x.22] , with associated eigenfunctions  [2.x.23] . 


[1.x.33][1.x.34] 


In this program, we use Dirichlet boundary conditions for the wave function  [2.x.24] . What this means, from the perspective of a finite element code, is that only the interior degrees of freedom are real degrees of [1.x.35]: the ones on the boundary are not free but are forced to have a zero value, after all. On the other hand, the finite element method gains much of its power and simplicity from the fact that we just do the same thing on every cell, without having to think too much about where a cell is, whether it bounds on a less refined cell and consequently has a hanging node, or is adjacent to the boundary. All such checks would make the assembly of finite element linear systems unbearably difficult to write and even more so to read. 

Consequently, of course, when you distribute degrees of freedom with your DoFHandler object, you don't care whether some of the degrees of freedom you enumerate are at a Dirichlet boundary. They all get numbers. We just have to take care of these degrees of freedom at a later time when we apply boundary values. There are two basic ways of doing this (either using  [2.x.25]  [1.x.36] assembling the linear system, or using  [2.x.26]  [1.x.37] assembly; see the  [2.x.27]  "constraints module" for more information), but both result in the same: a linear system that has a total number of rows equal to the number of [1.x.38] degrees of freedom, including those that lie on the boundary. However, degrees of freedom that are constrained by Dirichlet conditions are separated from the rest of the linear system by zeroing out the corresponding row and column, putting a single positive entry on the diagonal, and the corresponding Dirichlet value on the right hand side. 

If you assume for a moment that we had renumbered degrees of freedom in such a way that all of those on the Dirichlet boundary come last, then the linear system we would get when solving a regular PDE with a right hand side would look like this: 

[1.x.39] 

Here, subscripts  [2.x.28]  and  [2.x.29]  correspond to interior and boundary degrees of freedom, respectively. The interior degrees of freedom satisfy the linear system  [2.x.30]  which yields the correct solution in the interior, and boundary values are determined by  [2.x.31]  where  [2.x.32]  is a diagonal matrix that results from the process of eliminating boundary degrees of freedom, and  [2.x.33]  is chosen in such a way that  [2.x.34]  has the correct boundary values for every boundary degree of freedom  [2.x.35] . (For the curious, the entries of the matrix  [2.x.36]  result from adding modified local contributions to the global matrix where for the local matrices the diagonal elements, if non-zero, are set to their absolute value; otherwise, they are set to the average of absolute values of the diagonal. This process guarantees that the entries of  [2.x.37]  are positive and of a size comparable to the rest of the diagonal entries, ensuring that the resulting matrix does not incur unreasonable losses of accuracy due to roundoff involving matrix entries of drastically different size. The actual values that end up on the diagonal are difficult to predict and you should treat them as arbitrary and unpredictable, but positive.) 

For "regular" linear systems, this all leads to the correct solution. On the other hand, for eigenvalue problems, this is not so trivial. There, eliminating boundary values affects both matrices  [2.x.38]  and  [2.x.39]  that we will solve with in the current tutorial program. After elimination of boundary values, we then receive an eigenvalue problem that can be partitioned like this: 

[1.x.40] 

This form makes it clear that there are two sets of eigenvalues: the ones we care about, and spurious eigenvalues from the separated problem [1.x.41] These eigenvalues are spurious since they result from an eigenvalue system that operates only on boundary nodes -- nodes that are not real degrees of [1.x.42]. Of course, since the two matrices  [2.x.40]  are diagonal, we can exactly quantify these spurious eigenvalues: they are  [2.x.41]  (where the indices  [2.x.42]  corresponds exactly to the degrees of freedom that are constrained by Dirichlet boundary values). 

So how does one deal with them? The fist part is to recognize when our eigenvalue solver finds one of them. To this end, the program computes and prints an interval within which these eigenvalues lie, by computing the minimum and maximum of the expression  [2.x.43]  over all constrained degrees of freedom. In the program below, this already suffices: we find that this interval lies outside the set of smallest eigenvalues and corresponding eigenfunctions we are interested in and compute, so there is nothing we need to do here. 

On the other hand, it may happen that we find that one of the eigenvalues we compute in this program happens to be in this interval, and in that case we would not know immediately whether it is a spurious or a true eigenvalue. In that case, one could simply scale the diagonal elements of either matrix after computing the two matrices, thus shifting them away from the frequency of interest in the eigen-spectrum. This can be done by using the following code, making sure that all spurious eigenvalues are exactly equal to  [2.x.44] : 

[1.x.43] 

However, this strategy is not pursued here as the spurious eigenvalues we get from our program as-is happen to be greater than the lowest five that we will calculate and are interested in. 


[1.x.44][1.x.45] 


The program below is essentially just a slightly modified version of  [2.x.45] . The things that are different are the following: 

 [2.x.46]  

 [2.x.47] The main class (named  [2.x.48] ) now no longer has a single solution vector, but a whole set of vectors for the various eigenfunctions we want to compute. Moreover, the  [2.x.49]  function, which has the top-level control over everything here, initializes and finalizes the interface to SLEPc and PETSc simultaneously via  [2.x.50]  and  [2.x.51] . [2.x.52]  

 [2.x.53] We use PETSc matrices and vectors as in  [2.x.54]  and  [2.x.55]  since that is what the SLEPc eigenvalue solvers require. [2.x.56]  

 [2.x.57] The function  [2.x.58]  is entirely different from anything seen so far in the tutorial, as it does not just solve a linear system but actually solves the eigenvalue problem. It is built on the SLEPc library, and more immediately on the deal.II SLEPc wrappers in the class  [2.x.59]  

 [2.x.60] We use the ParameterHandler class to describe a few input parameters, such as the exact form of the potential  [2.x.61] , the number of global refinement steps of the mesh, or the number of eigenvalues we want to solve for. We could go much further with this but stop at making only a few of the things that one could select at run time actual input file parameters. In order to see what could be done in this regard, take a look at  [2.x.62]  " [2.x.63] " and  [2.x.64] . [2.x.65]  

 [2.x.66] We use the FunctionParser class to make the potential  [2.x.67]  a run-time parameter that can be specified in the input file as a formula. [2.x.68]  

 [2.x.69]  

The rest of the program follows in a pretty straightforward way from  [2.x.70] . [1.x.46] [1.x.47] 


[1.x.48]  [1.x.49] 




As mentioned in the introduction, this program is essentially only a slightly revised version of  [2.x.71] . As a consequence, most of the following include files are as used there, or at least as used already in previous tutorial programs: 

[1.x.50] 



IndexSet is used to set the size of each  [2.x.72]  

[1.x.51] 



PETSc appears here because SLEPc depends on this library: 

[1.x.52] 



And then we need to actually import the interfaces for solvers that SLEPc provides: 

[1.x.53] 



We also need some standard C++: 

[1.x.54] 



Finally, as in previous programs, we import all the deal.II class and function names into the namespace into which everything in this program will go: 

[1.x.55] 




[1.x.56]  [1.x.57] 




Following is the class declaration for the main class template. It looks pretty much exactly like what has already been shown in  [2.x.73] : 

[1.x.58] 



With these exceptions: For our eigenvalue problem, we need both a stiffness matrix for the left hand side as well as a mass matrix for the right hand side. We also need not just one solution function, but a whole set of these for the eigenfunctions we want to compute, along with the corresponding eigenvalues: 

[1.x.59] 



And then we need an object that will store several run-time parameters that we will specify in an input file: 

[1.x.60] 



Finally, we will have an object that contains "constraints" on our degrees of freedom. This could include hanging node constraints if we had adaptively refined meshes (which we don't have in the current program). Here, we will store the constraints for boundary nodes  [2.x.74] . 

[1.x.61] 




[1.x.62]  [1.x.63] 





[1.x.64]  [1.x.65] 




First up, the constructor. The main new part is handling the run-time input parameters. We need to declare their existence first, and then read their values from the input file whose name is specified as an argument to this function: 

[1.x.66] 



TODO investigate why the minimum number of refinement steps required to obtain the correct eigenvalue degeneracies is 6 

[1.x.67] 




[1.x.68]  [1.x.69] 




The next function creates a mesh on the domain  [2.x.75] , refines it as many times as the input file calls for, and then attaches a DoFHandler to it and initializes the matrices and vectors to their correct sizes. We also build the constraints that correspond to the boundary values  [2.x.76] .    


For the matrices, we use the PETSc wrappers. These have the ability to allocate memory as necessary as non-zero entries are added. This seems inefficient: we could as well first compute the sparsity pattern, initialize the matrices with it, and as we then insert entries we can be sure that we do not need to re-allocate memory and free the one used previously. One way to do that would be to use code like this:  [2.x.77]  instead of the two  [2.x.78]  calls for the stiffness and mass matrices below.    


This doesn't quite work, unfortunately. The code above may lead to a few entries in the non-zero pattern to which we only ever write zero entries; most notably, this holds true for off-diagonal entries for those rows and columns that belong to boundary nodes. This shouldn't be a problem, but for whatever reason, PETSc's ILU preconditioner, which we use to solve linear systems in the eigenvalue solver, doesn't like these extra entries and aborts with an error message.    


In the absence of any obvious way to avoid this, we simply settle for the second best option, which is have PETSc allocate memory as necessary. That said, since this is not a time critical part, this whole affair is of no further importance. 

[1.x.71] 



The next step is to take care of the eigenspectrum. In this case, the outputs are eigenvalues and eigenfunctions, so we set the size of the list of eigenfunctions and eigenvalues to be as large as we asked for in the input file. When using a  [2.x.79]  the Vector is initialized using an IndexSet. IndexSet is used not only to resize the  [2.x.80]  but it also associates an index in the  [2.x.81]  with a degree of freedom (see  [2.x.82]  for a more detailed explanation). The function complete_index_set() creates an IndexSet where every valid index is part of the set. Note that this program can only be run sequentially and will throw an exception if used in parallel. 

[1.x.72] 




[1.x.73]  [1.x.74] 




Here, we assemble the global stiffness and mass matrices from local contributions  [2.x.83]  and  [2.x.84]  respectively. This function should be immediately familiar if you've seen previous tutorial programs. The only thing new would be setting up an object that described the potential  [2.x.85]  using the expression that we got from the input file. We then need to evaluate this object at the quadrature points on each cell. If you've seen how to evaluate function objects (see, for example the coefficient in  [2.x.86] ), the code here will also look rather familiar. 

[1.x.75] 



Now that we have the local matrix contributions, we transfer them into the global objects and take care of zero boundary constraints: 

[1.x.76] 



At the end of the function, we tell PETSc that the matrices have now been fully assembled and that the sparse matrix representation can now be compressed as no more entries will be added: 

[1.x.77] 



Before leaving the function, we calculate spurious eigenvalues, introduced to the system by zero Dirichlet constraints. As discussed in the introduction, the use of Dirichlet boundary conditions coupled with the fact that the degrees of freedom located at the boundary of the domain remain part of the linear system we solve, introduces a number of spurious eigenvalues. Below, we output the interval within which they all lie to ensure that we can ignore them should they show up in our computations. 

[1.x.78] 




[1.x.79]  [1.x.80] 




This is the key new functionality of the program. Now that the system is set up, here is a good time to actually solve the problem: As with other examples this is done using a "solve" routine. Essentially, it works as in other programs: you set up a SolverControl object that describes the accuracy to which we want to solve the linear systems, and then we select the kind of solver we want. Here we choose the Krylov-Schur solver of SLEPc, a pretty fast and robust choice for this kind of problem: 

[1.x.81] 



We start here, as we normally do, by assigning convergence control we want: 

[1.x.82] 



Before we actually solve for the eigenfunctions and -values, we have to also select which set of eigenvalues to solve for. Lets select those eigenvalues and corresponding eigenfunctions with the smallest real part (in fact, the problem we solve here is symmetric and so the eigenvalues are purely real). After that, we can actually let SLEPc do its work: 

[1.x.83] 



The output of the call above is a set of vectors and values. In eigenvalue problems, the eigenfunctions are only determined up to a constant that can be fixed pretty arbitrarily. Knowing nothing about the origin of the eigenvalue problem, SLEPc has no other choice than to normalize the eigenvectors to one in the  [2.x.87]  (vector) norm. Unfortunately this norm has little to do with any norm we may be interested from a eigenfunction perspective: the  [2.x.88]  norm, or maybe the  [2.x.89]  norm.      


Let us choose the latter and rescale eigenfunctions so that they have  [2.x.90]  instead of  [2.x.91]  (where  [2.x.92]  is the  [2.x.93] th eigen[1.x.84] and  [2.x.94]  the corresponding vector of nodal values). For the  [2.x.95]  elements chosen here, we know that the maximum of the function  [2.x.96]  is attained at one of the nodes, so  [2.x.97] , making the normalization in the  [2.x.98]  norm trivial. Note that this doesn't work as easily if we had chosen  [2.x.99]  elements with  [2.x.100] : there, the maximum of a function does not necessarily have to be attained at a node, and so  [2.x.101]  (although the equality is usually nearly true). 

[1.x.85] 



Finally return the number of iterations it took to converge: 

[1.x.86] 




[1.x.87]  [1.x.88] 




This is the last significant function of this program. It uses the DataOut class to generate graphical output from the eigenfunctions for later visualization. It works as in many of the other tutorial programs.    


The whole collection of functions is then output as a single VTK file. 

[1.x.89] 



The only thing worth discussing may be that because the potential is specified as a function expression in the input file, it would be nice to also have it as a graphical representation along with the eigenfunctions. The process to achieve this is relatively straightforward: we build an object that represents  [2.x.102]  and then we interpolate this continuous function onto the finite element space. The result we also attach to the DataOut object for visualization. 

[1.x.90] 




[1.x.91]  [1.x.92] 




This is the function which has the top-level control over everything. It is almost exactly the same as in  [2.x.103] : 

[1.x.93] 




[1.x.94]  [1.x.95] 

[1.x.96] 



This program can only be run in serial. Otherwise, throw an exception. 

[1.x.97] 



All the while, we are watching out if any exceptions should have been generated. If that is so, we panic... 

[1.x.98] 



If no exceptions are thrown, then we tell the program to stop monkeying around and exit nicely: 

[1.x.99] 

[1.x.100][1.x.101] 


[1.x.102][1.x.103] 


The problem's input is parameterized by an input file  [2.x.104]  which could, for example, contain the following text: 

[1.x.104] 



Here, the potential is zero inside the domain, and we know that the eigenvalues are given by  [2.x.105]  where  [2.x.106] . Eigenfunctions are sines and cosines with  [2.x.107]  and  [2.x.108]  periods in  [2.x.109]  and  [2.x.110]  directions. This matches the output our program generates: 

[1.x.105] These eigenvalues are exactly the ones that correspond to pairs  [2.x.111] ,  [2.x.112]  and  [2.x.113] ,  [2.x.114] , and  [2.x.115] . A visualization of the corresponding eigenfunctions would look like this: 

 [2.x.116]  

[1.x.106][1.x.107] 


It is always worth playing a few games in the playground! So here goes with a few suggestions: 

 [2.x.117]  

 [2.x.118]  The potential used above (called the [1.x.108] because it is a flat potential surrounded by infinitely high walls) is interesting because it allows for analytically known solutions. Apart from that, it is rather boring, however. That said, it is trivial to play around with the potential by just setting it to something different in the input file. For example, let us assume that we wanted to work with the following potential in 2d: [1.x.109] In other words, the potential is -100 in two sectors of a circle of radius 0.75, -5 in the other two sectors, and zero outside the circle. We can achieve this by using the following in the input file: 

[1.x.110] 

If in addition we also increase the mesh refinement by one level, we get the following results: 

[1.x.111] 



The output file also contains an interpolated version of the potential, which looks like this (note that as expected the lowest few eigenmodes have probability densities  [2.x.119]  that are significant only where the potential is the lowest, i.e. in the top right and bottom left sector of inner circle of the potential): 

 [2.x.120]  

The first five eigenfunctions are now like this: 

 [2.x.121]  

 [2.x.122]  In our derivation of the problem we have assumed that the particle is confined to a domain  [2.x.123]  and that at the boundary of this domain its probability  [2.x.124]  of being is zero. This is equivalent to solving the eigenvalue problem on all of  [2.x.125]  and assuming that the energy potential is finite only inside a region  [2.x.126]  and infinite outside. It is relatively easy to show that  [2.x.127]  at all locations  [2.x.128]  where  [2.x.129] . So the question is what happens if our potential is not of this form, i.e. there is no bounded domain outside of which the potential is infinite? In that case, it may be worth to just consider a very large domain at the boundary of which  [2.x.130]  is at least very large, if not infinite. Play around with a few cases like this and explore how the spectrum and eigenfunctions change as we make the computational region larger and larger. 

 [2.x.131]  What happens if we investigate the simple harmonic oscillator problem  [2.x.132] ? This potential is exactly of the form discussed in the previous paragraph and has hyper spherical symmetry. One may want to use a large spherical domain with a large outer radius, to approximate the whole-space problem (say, by invoking  [2.x.133]  

 [2.x.134]  The plots above show the wave function  [2.x.135] , but the physical quantity of interest is actually the probability density  [2.x.136]  for the particle to be at location  [2.x.137] . Some visualization programs can compute derived quantities from the data in an input file, but we can also do so right away when creating the output file. The facility to do that is the DataPostprocessor class that can be used in conjunction with the DataOut class. Examples of how this can be done can be found in  [2.x.138]  and  [2.x.139] . 

 [2.x.140]  What happens if the particle in the box has %internal degrees of freedom? For example, if the particle were a spin- [2.x.141]  particle? In that case, we may want to start solving a vector-valued problem instead. 

 [2.x.142]  Our implementation of the deal.II library here uses the PETScWrappers and SLEPcWrappers and is suitable for running on serial machine architecture. However, for larger grids and with a larger number of degrees-of-freedom, we may want to run our application on parallel architectures. A parallel implementation of the above code can be particularly useful here since the generalized eigenspectrum problem is somewhat more expensive to solve than the standard problems considered in most of the earlier tutorials. Fortunately, modifying the above program to be MPI compliant is a relatively straightforward procedure. A sketch of how this can be done can be found in  [2.x.143]  " [2.x.144] ". 

 [2.x.145]  Finally, there are alternatives to using the SLEPc eigenvalue solvers. deal.II has interfaces to one of them, ARPACK (see [1.x.112] for setup instructions), implemented in the ArpackSolver class. Here is a short and quick overview of what one would need to change to use it, provided you have a working installation of ARPACK and deal.II has been configured properly for it (see the deal.II [1.x.113] file): 

First, in order to use the ARPACK interfaces, we can go back to using standard deal.II matrices and vectors, so we start by replacing the PETSc and SLEPc headers 

[1.x.114] 

with these: 

[1.x.115] 

ARPACK allows complex eigenvalues, so we will also need 

[1.x.116] 



Secondly, we switch back to the deal.II matrix and vector definitions in the main class: 

[1.x.117] 

and initialize them as usual in  [2.x.146] : 

[1.x.118] 



For solving the eigenvalue problem with ARPACK, we finally need to modify  [2.x.147] : 

[1.x.119] 

Note how we have used an exact decomposition (using SparseDirectUMFPACK) as a preconditioner to ARPACK.  [2.x.148]  [1.x.120] [1.x.121]  [2.x.149]  

 [2.x.150] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31] 

 [2.x.4]  

[1.x.32][1.x.33] 

[1.x.34] [1.x.35][1.x.36] 


This example shows how to implement a matrix-free method, that is, a method that does not explicitly store the matrix elements, for a second-order Poisson equation with variable coefficients on a hypercube. The linear system will be solved with a multigrid method and uses large-scale parallelism with MPI. 

The major motivation for matrix-free methods is the fact that on today's processors access to main memory (i.e., for objects that do not fit in the caches) has become the bottleneck in many solvers for partial differential equations: To perform a matrix-vector product based on matrices, modern CPUs spend far more time waiting for data to arrive from memory than on actually doing the floating point multiplications and additions. Thus, if we could substitute looking up matrix elements in memory by re-computing them &mdash; or rather, the operator represented by these entries &mdash;, we may win in terms of overall run-time even if this requires a significant number of additional floating point operations. That said, to realize this with a trivial implementation is not enough and one needs to really look at the details to gain in performance. This tutorial program and the papers referenced above show how one can implement such a scheme and demonstrates the speedup that can be obtained. 


[1.x.37][1.x.38] 


In this example, we consider the Poisson problem [1.x.39] where  [2.x.5]  is a variable coefficient. Below, we explain how to implement a matrix-vector product for this problem without explicitly forming the matrix. The construction can, of course, be done in a similar way for other equations as well. 

We choose as domain  [2.x.6]  and  [2.x.7] . Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution. 


[1.x.40][1.x.41] 


In order to find out how we can write a code that performs a matrix-vector product, but does not need to store the matrix elements, let us start at looking how a finite element matrix [1.x.42] is assembled: [1.x.43] 

In this formula, the matrix [1.x.44]<sub>cell,loc-glob</sub> is a rectangular matrix that defines the index mapping from local degrees of freedom in the current cell to the global degrees of freedom. The information from which this operator can be built is usually encoded in the  [2.x.8]  variable and is used in the assembly calls filling matrices in deal.II. Here, [1.x.45]<sub>cell</sub> denotes the cell matrix associated with [1.x.46]. 

If we are to perform a matrix-vector product, we can hence use that [1.x.47] 

where [1.x.48]<sub>cell</sub> are the values of [1.x.49] at the degrees of freedom of the respective cell, and [1.x.50]<sub>cell</sub>=[1.x.51]<sub>cell</sub>[1.x.52]<sub>cell</sub> correspondingly for the result. A naive attempt to implement the local action of the Laplacian would hence be to use the following code: 

[1.x.53] 



Here we neglected boundary conditions as well as any hanging nodes we may have, though neither would be very difficult to include using the AffineConstraints class. Note how we first generate the local matrix in the usual way as a sum over all quadrature points for each local matrix entry. To form the actual product as expressed in the above formula, we extract the values of  [2.x.9]  of the cell-related degrees of freedom (the action of [1.x.54]<sub>cell,loc-glob</sub>), multiply by the local matrix (the action of [1.x.55]<sub>cell</sub>), and finally add the result to the destination vector  [2.x.10]  (the action of [1.x.56]<sub>cell,loc-glob</sub><sup>T</sup>, added over all the elements). It is not more difficult than that, in principle. 

While this code is completely correct, it is very slow. For every cell, we generate a local matrix, which takes three nested loops with loop length equal to the number of local degrees of freedom to compute. The multiplication itself is then done by two nested loops, which means that it is much cheaper. 

One way to improve this is to realize that conceptually the local matrix can be thought of as the product of three matrices, [1.x.57] 

where for the example of the Laplace operator the ([1.x.58]*dim+[1.x.59])-th element of [1.x.60]<sub>cell</sub> is given by  [2.x.11] . This matrix consists of  [2.x.12]  rows and  [2.x.13]  columns. The matrix [1.x.61]<sub>cell</sub> is diagonal and contains the values  [2.x.14]  (or, rather, @p dim copies of each of these values). This kind of representation of finite element matrices can often be found in the engineering literature. 

When the cell matrix is applied to a vector, [1.x.62] 

one would then not form the matrix-matrix products, but rather multiply one matrix at a time with a vector from right to left so that only three successive matrix-vector products are formed. This approach removes the three nested loops in the calculation of the local matrix, which reduces the complexity of the work on one cell from something like  [2.x.15]  to  [2.x.16] . An interpretation of this algorithm is that we first transform the vector of values on the local DoFs to a vector of gradients on the quadrature points. In the second loop, we multiply these gradients by the integration weight and the coefficient. The third loop applies the second gradient (in transposed form), so that we get back to a vector of (Laplacian) values on the cell dofs. 

The bottleneck in the above code is the operations done by the call to  [2.x.17]  for every  [2.x.18] , which take about as much time as the other steps together (at least if the mesh is unstructured; deal.II can recognize that the gradients are often unchanged on structured meshes). That is certainly not ideal and we would like to do better than this. What the reinit function does is to calculate the gradient in real space by transforming the gradient on the reference cell using the Jacobian of the transformation from real to reference cell. This is done for each basis function on the cell, for each quadrature point. The Jacobian does not depend on the basis function, but it is different on different quadrature points in general. If you only build the matrix once as we've done in all previous tutorial programs, there is nothing to be optimized since  [2.x.19]  needs to be called on every cell. In this process, the transformation is applied while computing the local matrix elements. 

In a matrix-free implementation, however, we will compute those integrals very often because iterative solvers will apply the matrix many times during the solution process. Therefore, we need to think about whether we may be able to cache some data that gets reused in the operator applications, i.e., integral computations. On the other hand, we realize that we must not cache too much data since otherwise we get back to the situation where memory access becomes the dominating factor. Therefore, we will not store the transformed gradients in the matrix [1.x.63], as they would in general be different for each basis function and each quadrature point on every element for curved meshes. 

The trick is to factor out the Jacobian transformation and first apply the gradient on the reference cell only. This operation interpolates the vector of values on the local dofs to a vector of (unit-coordinate) gradients on the quadrature points. There, we first apply the Jacobian that we factored out from the gradient, then apply the weights of the quadrature, and finally apply the transposed Jacobian for preparing the third loop which tests by the gradients on the unit cell and sums over quadrature points. 

Let us again write this in terms of matrices. Let the matrix [1.x.64]<sub>cell</sub> denote the cell-related gradient matrix, with each row containing the values on the quadrature points. It is constructed by a matrix-matrix product as [1.x.65] where [1.x.66]<sub>ref_cell</sub> denotes the gradient on the reference cell and [1.x.67]<sup>-T</sup><sub>cell</sub> denotes the inverse transpose Jacobian of the transformation from unit to real cell (in the language of transformations, the operation represented by [1.x.68]<sup>-T</sup><sub>cell</sub> represents a covariant transformation). [1.x.69]<sup>-T</sup><sub>cell</sub> is block-diagonal, and the blocks size is equal to the dimension of the problem. Each diagonal block is the Jacobian transformation that goes from the reference cell to the real cell. 

Putting things together, we find that [1.x.70] 

so we calculate the product (starting the local product from the right) [1.x.71] 



[1.x.72] 



Note how we create an additional FEValues object for the reference cell gradients and how we initialize it to the reference cell. The actual derivative data is then applied by the inverse, transposed Jacobians (deal.II calls the Jacobian matrix from real to unit cell inverse_jacobian, as the forward transformation is from unit to real cell). The factor  [2.x.20]  is block-diagonal over quadrature. In this form, one realizes that variable coefficients (possibly expressed through a tensor) and general grid topologies with Jacobian transformations have a similar effect on the coefficient transforming the unit-cell derivatives. 

At this point, one might wonder why we store the matrix  [2.x.21]  and the coefficient separately, rather than only the complete factor  [2.x.22] . The latter would use less memory because the tensor is symmetric with six independent values in 3D, whereas for the former we would need nine entries for the inverse transposed Jacobian, one for the quadrature weight and Jacobian determinant, and one for the coefficient, totaling to 11 doubles. The reason is that the former approach allows for implementing generic differential operators through a common framework of cached data, whereas the latter specifically stores the coefficient for the Laplacian. In case applications demand for it, this specialization could pay off and would be worthwhile to consider. Note that the implementation in deal.II is smart enough to detect Cartesian or affine geometries where the Jacobian is constant throughout the cell and needs not be stored for every cell (and indeed often is the same over different cells as well). 

The final optimization that is most crucial from an operation count point of view is to make use of the tensor product structure in the basis functions. This is possible because we have factored out the gradient from the reference cell operation described by [1.x.73]<sub>ref_cell</sub>, i.e., an interpolation operation over the completely regular data fields of the reference cell. We illustrate the process of complexity reduction in two space dimensions, but the same technique can be used in higher dimensions. On the reference cell, the basis functions are of the tensor product form  [2.x.23] . The part of the matrix [1.x.74]<sub>ref_cell</sub> that computes the first component has the form  [2.x.24] , where [1.x.75]<sub>grad,x</sub> and [1.x.76]<sub>val,y</sub> contain the evaluation of all the 1D basis functions on all the 1D quadrature points. Forming a matrix [1.x.77] with [1.x.78] containing the coefficient belonging to basis function  [2.x.25] , we get  [2.x.26] . This reduces the complexity for computing this product from  [2.x.27]  to  [2.x.28] , where [1.x.79]-1 is the degree of the finite element (i.e., equivalently, [1.x.80] is the number of shape functions in each coordinate direction), or  [2.x.29]  to  [2.x.30]  in general. The reason why we look at the complexity in terms of the polynomial degree is since we want to be able to go to high degrees and possibly increase the polynomial degree [1.x.81] instead of the grid resolution. Good algorithms for moderate degrees like the ones used here are linear in the polynomial degree independent on the dimension, as opposed to matrix-based schemes or naive evaluation through FEValues. The techniques used in the implementations of deal.II have been established in the spectral element community since the 1980s. 

Implementing a matrix-free and cell-based finite element operator requires a somewhat different program design as compared to the usual matrix assembly codes shown in previous tutorial programs. The data structures for doing this are the MatrixFree class that collects all data and issues a (parallel) loop over all cells and the FEEvaluation class that evaluates finite element basis functions by making use of the tensor product structure. 

The implementation of the matrix-free matrix-vector product shown in this tutorial is slower than a matrix-vector product using a sparse matrix for linear elements, but faster for all higher order elements thanks to the reduced complexity due to the tensor product structure and due to less memory transfer during computations. The impact of reduced memory transfer is particularly beneficial when working on a multicore processor where several processing units share access to memory. In that case, an algorithm which is computation bound will show almost perfect parallel speedup (apart from possible changes of the processor's clock frequency through turbo modes depending on how many cores are active), whereas an algorithm that is bound by memory transfer might not achieve similar speedup (even when the work is perfectly parallel and one could expect perfect scaling like in sparse matrix-vector products). An additional gain with this implementation is that we do not have to build the sparse matrix itself, which can also be quite expensive depending on the underlying differential equation. Moreover, the above framework is simple to generalize to nonlinear operations, as we demonstrate in  [2.x.31] . 


[1.x.82][1.x.83] 


Above, we have gone to significant lengths to implement a matrix-vector product that does not actually store the matrix elements. In many user codes, however, one wants more than just doing a few matrix-vector products &mdash; one wants to do as few of these operations as possible when solving linear systems. In theory, we could use the CG method without preconditioning; however, that would not be very efficient for the Laplacian. Rather, preconditioners are used for increasing the speed of convergence. Unfortunately, most of the more frequently used preconditioners such as SSOR, ILU or algebraic multigrid (AMG) cannot be used here because their implementation requires knowledge of the elements of the system matrix. 

One solution is to use geometric multigrid methods as shown in  [2.x.32] . They are known to be very fast, and they are suitable for our purpose since all ingredients, including the transfer between different grid levels, can be expressed in terms of matrix-vector products related to a collection of cells. All one needs to do is to find a smoother that is based on matrix-vector products rather than all the matrix entries. One such candidate would be a damped Jacobi iteration that requires access to the matrix diagonal, but it is often not sufficiently good in damping all high-frequency errors. The properties of the Jacobi method can be improved by iterating it a few times with the so-called Chebyshev iteration. The Chebyshev iteration is described by a polynomial expression of the matrix-vector product where the coefficients can be chosen to achieve certain properties, in this case to smooth the high-frequency components of the error which are associated to the eigenvalues of the Jacobi-preconditioned matrix. At degree zero, the Jacobi method with optimal damping parameter is retrieved, whereas higher order corrections are used to improve the smoothing properties. The effectiveness of Chebyshev smoothing in multigrid has been demonstrated, e.g., in the article [1.x.84][1.x.85]. This publication also identifies one more advantage of Chebyshev smoothers that we exploit here, namely that they are easy to parallelize, whereas SOR/Gauss&ndash;Seidel smoothing relies on substitutions, for which a naive parallelization works on diagonal sub-blocks of the matrix, thereby decreases efficiency (for more detail see e.g. Y. Saad, Iterative Methods for Sparse Linear Systems, SIAM, 2nd edition, 2003, chapters 11 & 12). 

The implementation into the multigrid framework is then straightforward. The multigrid implementation in this program is similar to  [2.x.33]  and includes adaptivity. 


[1.x.86][1.x.87] 


The computational kernels for evaluation in FEEvaluation are written in a way to optimally use computational resources. To achieve this, they do not operate on double data types, but something we call VectorizedArray (check e.g. the return type of  [2.x.34]  which is VectorizedArray for a scalar element and a Tensor of VectorizedArray for a vector finite element). VectorizedArray is a short array of doubles or float whose length depends on the particular computer system in use. For example, systems based on x86-64 support the streaming SIMD extensions (SSE), where the processor's vector units can process two doubles (or four single-precision floats) by one CPU instruction. Newer processors (from about 2012 and onwards) support the so-called advanced vector extensions (AVX) with 256 bit operands, which can use four doubles and eight floats, respectively. Vectorization is a single-instruction/multiple-data (SIMD) concept, that is, one CPU instruction is used to process multiple data values at once. Often, finite element programs do not use vectorization explicitly as the benefits of this concept are only in arithmetic intensive operations. The bulk of typical finite element workloads are memory bandwidth limited (operations on sparse matrices and vectors) where the additional computational power is useless. 

Behind the scenes, optimized BLAS packages might heavily rely on vectorization, though. Also, optimizing compilers might automatically transform loops involving standard code into more efficient vectorized form (deal.II uses OpenMP SIMD pragmas inside the regular loops of vector updates). However, the data flow must be very regular in order for compilers to produce efficient code. For example, already the automatic vectorization of the prototype operation that benefits from vectorization, matrix-matrix products, fails on most compilers (as of writing this tutorial in early 2012 and updating in late 2016, neither gcc nor the Intel compiler manage to produce useful vectorized code for the  [2.x.35]  function, and not even on the simpler case where the matrix bounds are compile-time constants instead of run-time constants as in  [2.x.36]  The main reason for this is that the information to be processed at the innermost loop (that is where vectorization is applied) is not necessarily a multiple of the vector length, leaving parts of the resources unused. Moreover, the data that can potentially be processed together might not be laid out in a contiguous way in memory or not with the necessary alignment to address boundaries that are needed by the processor. Or the compiler might not be able to prove that data arrays do not overlap when loading several elements at once. 

In the matrix-free implementation in deal.II, we have therefore chosen to apply vectorization at the level which is most appropriate for finite element computations: The cell-wise computations are typically exactly the same for all cells (except for indices in the indirect addressing used while reading from and writing to vectors), and hence SIMD can be used to process several cells at once. In all what follows, you can think of a VectorizedArray to hold data from several cells. Remember that it is not related to the spatial dimension and the number of elements e.g. in a Tensor or Point. 

Note that vectorization depends on the CPU the code is running on and for which the code is compiled. In order to generate the fastest kernels of FEEvaluation for your computer, you should compile deal.II with the so-called [1.x.88] processor variant. When using the gcc compiler, it can be enabled by setting the variable <tt>CMAKE_CXX_FLAGS</tt> to <tt>"-march=native"</tt> in the cmake build settings (on the command line, specify <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>, see the deal.II README for more information). Similar options exist for other compilers. We output the current vectorization length in the run() function of this example. 


[1.x.89][1.x.90] 


As mentioned above, all components in the matrix-free framework can easily be parallelized with MPI using domain decomposition. Thanks to the easy access to large-scale parallel meshes through p4est (see  [2.x.37]  for details) in deal.II, and the fact that cell-based loops with matrix-free evaluation [1.x.91] need a decomposition of the mesh into chunks of roughly equal size on each processor, there is relatively little to do to write a parallel program working with distributed memory. While other tutorial programs using MPI have relied on either PETSc or Trilinos, this program uses deal.II's own parallel vector facilities. 

The deal.II parallel vector class,  [2.x.38]  holds the processor-local part of the solution as well as data fields for ghosted DoFs, i.e. DoFs that are owned by a remote processor but accessed by cells that are owned by the present processor. In the  [2.x.39]  "glossary" these degrees of freedom are referred to as locally active degrees of freedom. The function  [2.x.40]  provides a method that sets this design. Note that hanging nodes can relate to additional ghosted degrees of freedom that must be included in the distributed vector but are not part of the locally active DoFs in the sense of the  [2.x.41]  "glossary". Moreover, the distributed vector holds the MPI metadata for DoFs that are owned locally but needed by other processors. A benefit of the design of this vector class is the way ghosted entries are accessed. In the storage scheme of the vector, the data array extends beyond the processor-local part of the solution with further vector entries available for the ghosted degrees of freedom. This gives a contiguous index range for all locally active degrees of freedom. (Note that the index range depends on the exact configuration of the mesh.) Since matrix-free operations can be thought of doing linear algebra that is performance critical, and performance-critical code cannot waste time on doing MPI-global to MPI-local index translations, the availability of an index spaces local to one MPI rank is fundamental. The way things are accessed here is a direct array access. This is provided through  [2.x.42]  but it is actually rarely needed because all of this happens internally in FEEvaluation. 

The design of  [2.x.43]  is similar to the  [2.x.44]  and  [2.x.45]  data types we have used in  [2.x.46]  and  [2.x.47]  before, but since we do not need any other parallel functionality of these libraries, we use the  [2.x.48]  class of deal.II instead of linking in another large library in this tutorial program. Also note that the PETSc and Trilinos vectors do not provide the fine-grained control over ghost entries with direct array access because they abstract away the necessary implementation details. [1.x.92] [1.x.93] 

First include the necessary files from the deal.II library. 

[1.x.94] 



This includes the data structures for the efficient implementation of matrix-free methods or more generic finite element operators with the class MatrixFree. 

[1.x.95] 



To be efficient, the operations performed in the matrix-free implementation require knowledge of loop lengths at compile time, which are given by the degree of the finite element. Hence, we collect the values of the two template parameters that can be changed at one place in the code. Of course, one could make the degree of the finite element a run-time parameter by compiling the computational kernels for all degrees that are likely (say, between 1 and 6) and selecting the appropriate kernel at run time. Here, we simply choose second order  [2.x.49]  elements and choose dimension 3 as standard. 

[1.x.96] 




[1.x.97]  [1.x.98] 




We define a variable coefficient function for the Poisson problem. It is similar to the function in  [2.x.50]  but we use the form  [2.x.51]  instead of a discontinuous one. It is merely to demonstrate the possibilities of this implementation, rather than making much sense physically. We define the coefficient in the same way as functions in earlier tutorial programs. There is one new function, namely a  [2.x.52]  method with template argument  [2.x.53]  

[1.x.99] 



This is the new function mentioned above: Evaluate the coefficient for abstract type  [2.x.54]  It might be just a usual double, but it can also be a somewhat more complicated type that we call VectorizedArray. This data type is essentially a short array of doubles as discussed in the introduction that holds data from several cells. For example, we evaluate the coefficient shown here not on a simple point as usually done, but we hand it a Point<dim,VectorizedArray<double> > point, which is actually a collection of four points in the case of AVX. Do not confuse the entries in VectorizedArray with the different coordinates of the point. Indeed, the data is laid out such that  [2.x.55]  returns a VectorizedArray, which in turn contains the x-coordinate for the first point and the second point. You may access the coordinates individually using e.g.  [2.x.56] , j=0,1,2,3, but it is recommended to define operations on a VectorizedArray as much as possible in order to make use of vectorized operations.    


In the function implementation, we assume that the number type overloads basic arithmetic operations, so we just write the code as usual. The base class function  [2.x.57]  is then computed from the templated function with double type, in order to avoid duplicating code. 

[1.x.100] 




[1.x.101]  [1.x.102] 




The following class, called  [2.x.58] , implements the differential operator. For all practical purposes, it is a matrix, i.e., you can ask it for its size (member functions  [2.x.59] ) and you can apply it to a vector (the  [2.x.60]  function). The difference to a real matrix of course lies in the fact that this class does not actually store the [1.x.103] of the matrix, but only knows how to compute the action of the operator when applied to a vector.    


The infrastructure describing the matrix size, the initialization from a MatrixFree object, and the various interfaces to matrix-vector products through vmult() and Tvmult() methods, is provided by the class  [2.x.61]  from which this class derives. The LaplaceOperator class defined here only has to provide a few interfaces, namely the actual action of the operator through the apply_add() method that gets used in the vmult() functions, and a method to compute the diagonal entries of the underlying matrix. We need the diagonal for the definition of the multigrid smoother. Since we consider a problem with variable coefficient, we further implement a method that can fill the coefficient values.    


Note that the file  [2.x.62]  already contains an implementation of the Laplacian through the class  [2.x.63]  For educational purposes, the operator is re-implemented in this tutorial program, explaining the ingredients and concepts used there.    


This program makes use of the data cache for finite element operator application that is integrated in deal.II. This data cache class is called MatrixFree. It contains mapping information (Jacobians) and index relations between local and global degrees of freedom. It also contains constraints like the ones from hanging nodes or Dirichlet boundary conditions. Moreover, it can issue a loop over all cells in %parallel, making sure that only cells are worked on that do not share any degree of freedom (this makes the loop thread-safe when writing into destination vectors). This is a more advanced strategy compared to the WorkStream class described in the  [2.x.64]  module. Of course, to not destroy thread-safety, we have to be careful when writing into class-global structures.    


The class implementing the Laplace operator has three template arguments, one for the dimension (as many deal.II classes carry), one for the degree of the finite element (which we need to enable efficient computations through the FEEvaluation class), and one for the underlying scalar type. We want to use  [2.x.65]  numbers (i.e., double precision, 64-bit floating point) for the final matrix, but floats (single precision, 32-bit floating point numbers) for the multigrid level matrices (as that is only a preconditioner, and floats can be processed twice as fast). The class FEEvaluation also takes a template argument for the number of quadrature points in one dimension. In the code below, we hard-code it to  [2.x.66] . If we wanted to change it independently of the polynomial degree, we would need to add a template parameter as is done in the  [2.x.67]  class.    


As a sidenote, if we implemented several different operations on the same grid and degrees of freedom (like a mass matrix and a Laplace matrix), we would define two classes like the current one for each of the operators (derived from the  [2.x.68]  class), and let both of them refer to the same MatrixFree data cache from the general problem class. The interface through  [2.x.69]  requires us to only provide a minimal set of functions. This concept allows for writing complex application codes with many matrix-free operations.    




 [2.x.70]  Storing values of type  [2.x.71]  requires care: Here, we use the deal.II table class which is prepared to hold the data with correct alignment. However, storing e.g. an  [2.x.72]  is not possible with vectorization: A certain alignment of the data with the memory address boundaries is required (essentially, a VectorizedArray that is 32 bytes long in case of AVX needs to start at a memory address that is divisible by 32). The table class (as well as the AlignedVector class it is based on) makes sure that this alignment is respected, whereas  [2.x.73]  does not in general, which may lead to segmentation faults at strange places for some systems or suboptimal performance for other systems. 

[1.x.104] 



This is the constructor of the  [2.x.74]  class. All it does is to call the default constructor of the base class  [2.x.75]  which in turn is based on the Subscriptor class that asserts that this class is not accessed after going out of scope e.g. in a preconditioner. 

[1.x.105] 




[1.x.106]  [1.x.107] 




To initialize the coefficient, we directly give it the Coefficient class defined above and then select the method  [2.x.76]  with vectorized number (which the compiler can deduce from the point data type). The use of the FEEvaluation class (and its template arguments) will be explained below. 

[1.x.108] 




[1.x.109]  [1.x.110] 




Here comes the main function of this class, the evaluation of the matrix-vector product (or, in general, a finite element operator evaluation). This is done in a function that takes exactly four arguments, the MatrixFree object, the destination and source vectors, and a range of cells that are to be worked on. The method  [2.x.77]  in the MatrixFree class will internally call this function with some range of cells that is obtained by checking which cells are possible to work on simultaneously so that write operations do not cause any race condition. Note that the cell range used in the loop is not directly the number of (active) cells in the current mesh, but rather a collection of batches of cells.  In other word, "cell" may be the wrong term to begin with, since FEEvaluation groups data from several cells together. This means that in the loop over quadrature points we are actually seeing a group of quadrature points of several cells as one block. This is done to enable a higher degree of vectorization.  The number of such "cells" or "cell batches" is stored in MatrixFree and can be queried through  [2.x.78]  Compared to the deal.II cell iterators, in this class all cells are laid out in a plain array with no direct knowledge of level or neighborship relations, which makes it possible to index the cells by unsigned integers.    


The implementation of the Laplace operator is quite simple: First, we need to create an object FEEvaluation that contains the computational kernels and has data fields to store temporary results (e.g. gradients evaluated on all quadrature points on a collection of a few cells). Note that temporary results do not use a lot of memory, and since we specify template arguments with the element order, the data is stored on the stack (without expensive memory allocation). Usually, one only needs to set two template arguments, the dimension as a first argument and the degree of the finite element as the second argument (this is equal to the number of degrees of freedom per dimension minus one for FE_Q elements). However, here we also want to be able to use float numbers for the multigrid preconditioner, which is the last (fifth) template argument. Therefore, we cannot rely on the default template arguments and must also fill the third and fourth field, consequently. The third argument specifies the number of quadrature points per direction and has a default value equal to the degree of the element plus one. The fourth argument sets the number of components (one can also evaluate vector-valued functions in systems of PDEs, but the default is a scalar element), and finally the last argument sets the number type.    


Next, we loop over the given cell range and then we continue with the actual implementation:  [2.x.79]   [2.x.80] Tell the FEEvaluation object the (macro) cell we want to work on.   [2.x.81] Read in the values of the source vectors ( [2.x.82]  including the resolution of constraints. This stores  [2.x.83]  as described in the introduction.   [2.x.84] Compute the unit-cell gradient (the evaluation of finite element functions). Since FEEvaluation can combine value computations with gradient computations, it uses a unified interface to all kinds of derivatives of order between zero and two. We only want gradients, no values and no second derivatives, so we set the function arguments to true in the gradient slot (second slot), and to false in the values slot (first slot). There is also a third slot for the Hessian which is false by default, so it needs not be given. Note that the FEEvaluation class internally evaluates shape functions in an efficient way where one dimension is worked on at a time (using the tensor product form of shape functions and quadrature points as mentioned in the introduction). This gives complexity equal to  [2.x.85]  for polynomial degree  [2.x.86]  in  [2.x.87]  dimensions, compared to the naive approach with loops over all local degrees of freedom and quadrature points that is used in FEValues and costs  [2.x.88] .   [2.x.89] Next comes the application of the Jacobian transformation, the multiplication by the variable coefficient and the quadrature weight. FEEvaluation has an access function  [2.x.90]  that applies the Jacobian and returns the gradient in real space. Then, we just need to multiply by the (scalar) coefficient, and let the function  [2.x.91]  apply the second Jacobian (for the test function) and the quadrature weight and Jacobian determinant (JxW). Note that the submitted gradient is stored in the same data field as where it is read from in  [2.x.92]  Therefore, you need to make sure to not read from the same quadrature point again after having called  [2.x.93]  on that particular quadrature point. In general, it is a good idea to copy the result of  [2.x.94]  when it is used more often than once.   [2.x.95] Next follows the summation over quadrature points for all test functions that corresponds to the actual integration step. For the Laplace operator, we just multiply by the gradient, so we call the integrate function with the respective argument set. If you have an equation where you test by both the values of the test functions and the gradients, both template arguments need to be set to true. Calling first the integrate function for values and then gradients in a separate call leads to wrong results, since the second call will internally overwrite the results from the first call. Note that there is no function argument for the second derivative for integrate step.   [2.x.96] Eventually, the local contributions in the vector  [2.x.97]  as mentioned in the introduction need to be added into the result vector (and constraints are applied). This is done with a call to  [2.x.98]  the same name as the corresponding function in the AffineConstraints (only that we now store the local vector in the FEEvaluation object, as are the indices between local and global degrees of freedom).   [2.x.99]  

[1.x.111] 



This function implements the loop over all cells for the  [2.x.100]  interface. This is done with the  [2.x.101]  of the MatrixFree class, which takes the operator() of this class with arguments MatrixFree, OutVector, InVector, cell_range. When working with MPI parallelization (but no threading) as is done in this tutorial program, the cell loop corresponds to the following three lines of code:    


 [2.x.102]     


Here, the two calls update_ghost_values() and compress() perform the data exchange on processor boundaries for MPI, once for the source vector where we need to read from entries owned by remote processors, and once for the destination vector where we have accumulated parts of the residuals that need to be added to the respective entry of the owner processor. However,  [2.x.103]  does not only abstract away those two calls, but also performs some additional optimizations. On the one hand, it will split the update_ghost_values() and compress() calls in a way to allow for overlapping communication and computation. The local_apply function is then called with three cell ranges representing partitions of the cell range from 0 to  [2.x.104]  On the other hand, cell_loop also supports thread parallelism in which case the cell ranges are split into smaller chunks and scheduled in an advanced way that avoids access to the same vector entry by several threads. That feature is explained in  [2.x.105] .    


Note that after the cell loop, the constrained degrees of freedom need to be touched once more for sensible vmult() operators: Since the assembly loop automatically resolves constraints (just as the  [2.x.106]  call does), it does not compute any contribution for constrained degrees of freedom, leaving the respective entries zero. This would represent a matrix that had empty rows and columns for constrained degrees of freedom. However, iterative solvers like CG only work for non-singular matrices. The easiest way to do that is to set the sub-block of the matrix that corresponds to constrained DoFs to an identity matrix, in which case application of the matrix would simply copy the elements of the right hand side vector into the left hand side. Fortunately, the vmult() implementations  [2.x.107]  do this automatically for us outside the apply_add() function, so we do not need to take further action here.    


When using the combination of MatrixFree and FEEvaluation in parallel with MPI, there is one aspect to be careful about &mdash; the indexing used for accessing the vector. For performance reasons, MatrixFree and FEEvaluation are designed to access vectors in MPI-local index space also when working with multiple processors. Working in local index space means that no index translation needs to be performed at the place the vector access happens, apart from the unavoidable indirect addressing. However, local index spaces are ambiguous: While it is standard convention to access the locally owned range of a vector with indices between 0 and the local size, the numbering is not so clear for the ghosted entries and somewhat arbitrary. For the matrix-vector product, only the indices appearing on locally owned cells (plus those referenced via hanging node constraints) are necessary. However, in deal.II we often set all the degrees of freedom on ghosted elements as ghosted vector entries, called the  [2.x.108]  "locally relevant DoFs described in the glossary". In that case, the MPI-local index of a ghosted vector entry can in general be different in the two possible ghost sets, despite referring to the same global index. To avoid problems, FEEvaluation checks that the partitioning of the vector used for the matrix-vector product does indeed match with the partitioning of the indices in MatrixFree by a check called  [2.x.109]  To facilitate things, the  [2.x.110]  class includes a mechanism to fit the ghost set to the correct layout. This happens in the ghost region of the vector, so keep in mind that the ghost region might be modified in both the destination and source vector after a call to a vmult() method. This is legitimate because the ghost region of a distributed deal.II vector is a mutable section and filled on demand. Vectors used in matrix-vector products must not be ghosted upon entry of vmult() functions, so no information gets lost. 

[1.x.113] 



The following function implements the computation of the diagonal of the operator. Computing matrix entries of a matrix-free operator evaluation turns out to be more complicated than evaluating the operator. Fundamentally, we could obtain a matrix representation of the operator by applying the operator on [1.x.114] unit vectors. Of course, that would be very inefficient since we would need to perform [1.x.115] operator evaluations to retrieve the whole matrix. Furthermore, this approach would completely ignore the matrix sparsity. On an individual cell, however, this is the way to go and actually not that inefficient as there usually is a coupling between all degrees of freedom inside the cell.    


We first initialize the diagonal vector to the correct parallel layout. This vector is encapsulated in a member called inverse_diagonal_entries of type DiagonalMatrix in the base class  [2.x.111]  This member is a shared pointer that we first need to initialize and then get the vector representing the diagonal entries in the matrix. As to the actual diagonal computation, we again use the cell_loop infrastructure of MatrixFree to invoke a local worker routine called local_compute_diagonal(). Since we will only write into a vector but not have any source vector, we put a dummy argument of type <tt>unsigned int</tt> in place of the source vector to confirm with the cell_loop interface. After the loop, we need to set the vector entries subject to Dirichlet boundary conditions to one (either those on the boundary described by the AffineConstraints object inside MatrixFree or the indices at the interface between different grid levels in adaptive multigrid). This is done through the function  [2.x.112]  and matches with the setting in the matrix-vector product provided by the Base operator. Finally, we need to invert the diagonal entries which is the form required by the Chebyshev smoother based on the Jacobi iteration. In the loop, we assert that all entries are non-zero, because they should either have obtained a positive contribution from integrals or be constrained and treated by  [2.x.113]  following cell_loop. 

[1.x.116] 



In the local compute loop, we compute the diagonal by a loop over all columns in the local matrix and putting the entry 1 in the [1.x.117]th slot and a zero entry in all other slots, i.e., we apply the cell-wise differential operator on one unit vector at a time. The inner part invoking  [2.x.114]  the loop over quadrature points, and  [2.x.115]  is exactly the same as in the local_apply function. Afterwards, we pick out the [1.x.118]th entry of the local result and put it to a temporary storage (as we overwrite all entries in the array behind  [2.x.116]  with the next loop iteration). Finally, the temporary storage is written to the destination vector. Note how we use  [2.x.117]  and  [2.x.118]  to read and write to the data field that FEEvaluation uses for the integration on the one hand and writes into the global vector on the other hand.    


Given that we are only interested in the matrix diagonal, we simply throw away all other entries of the local matrix that have been computed along the way. While it might seem wasteful to compute the complete cell matrix and then throw away everything but the diagonal, the integration are so efficient that the computation does not take too much time. Note that the complexity of operator evaluation per element is  [2.x.119]  for polynomial degree  [2.x.120] , so computing the whole matrix costs us  [2.x.121]  operations, not too far away from  [2.x.122]  complexity for computing the diagonal with FEValues. Since FEEvaluation is also considerably faster due to vectorization and other optimizations, the diagonal computation with this function is actually the fastest (simple) variant. (It would be possible to compute the diagonal with sum factorization techniques in  [2.x.123]  operations involving specifically adapted kernels&mdash;but since such kernels are only useful in that particular context and the diagonal computation is typically not on the critical path, they have not been implemented in deal.II.)    


Note that the code that calls distribute_local_to_global on the vector to accumulate the diagonal entries into the global matrix has some limitations. For operators with hanging node constraints that distribute an integral contribution of a constrained DoF to several other entries inside the distribute_local_to_global call, the vector interface used here does not exactly compute the diagonal entries, but lumps some contributions located on the diagonal of the local matrix that would end up in a off-diagonal position of the global matrix to the diagonal. The result is correct up to discretization accuracy as explained in [1.x.119], but not mathematically equal. In this tutorial program, no harm can happen because the diagonal is only used for the multigrid level matrices where no hanging node constraints appear. 

[1.x.120] 




[1.x.121]  [1.x.122] 




This class is based on the one in  [2.x.124] . However, we replaced the SparseMatrix<double> class by our matrix-free implementation, which means that we can also skip the sparsity patterns. Notice that we define the LaplaceOperator class with the degree of finite element as template argument (the value is defined at the top of the file), and that we use float numbers for the multigrid level matrices.    


The class also has a member variable to keep track of all the detailed timings for setting up the entire chain of data before we actually go about solving the problem. In addition, there is an output stream (that is disabled by default) that can be used to output details for the individual setup operations instead of the summary only that is printed out by default.    


Since this program is designed to be used with MPI, we also provide the usual  [2.x.125]  output stream that only prints the information of the processor with MPI rank 0. The grid used for this programs can either be a distributed triangulation based on p4est (in case deal.II is configured to use p4est), otherwise it is a serial grid that only runs without MPI. 

[1.x.123] 



When we initialize the finite element, we of course have to use the degree specified at the top of the file as well (otherwise, an exception will be thrown at some point, since the computational kernel defined in the templated LaplaceOperator class and the information from the finite element read out by MatrixFree will not match). The constructor of the triangulation needs to set an additional flag that tells the grid to conform to the 2:1 cell balance over vertices, which is needed for the convergence of the geometric multigrid routines. For the distributed grid, we also need to specifically enable the multigrid hierarchy. 

[1.x.124] 



The LaplaceProblem class holds an additional output stream that collects detailed timings about the setup phase. This stream, called time_details, is disabled by default through the  [2.x.126]  argument specified here. For detailed timings, removing the  [2.x.127]  argument prints all the details. 

[1.x.125] 




[1.x.126]  [1.x.127] 




The setup stage is in analogy to  [2.x.128]  with relevant changes due to the LaplaceOperator class. The first thing to do is to set up the DoFHandler, including the degrees of freedom for the multigrid levels, and to initialize constraints from hanging nodes and homogeneous Dirichlet conditions. Since we intend to use this programs in %parallel with MPI, we need to make sure that the constraints get to know the locally relevant degrees of freedom, otherwise the storage would explode when using more than a few hundred millions of degrees of freedom, see  [2.x.129] . 




Once we have created the multigrid dof_handler and the constraints, we can call the reinit function for the global matrix operator as well as each level of the multigrid scheme. The main action is to set up the  [2.x.130]  instance for the problem. The base class of the  [2.x.131]  class,  [2.x.132]  is initialized with a shared pointer to MatrixFree object. This way, we can simply create it here and then pass it on to the system matrix and level matrices, respectively. For setting up MatrixFree, we need to activate the update flag in the AdditionalData field of MatrixFree that enables the storage of quadrature point coordinates in real space (by default, it only caches data for gradients (inverse transposed Jacobians) and JxW values). Note that if we call the reinit function without specifying the level (i.e., giving  [2.x.133] ), MatrixFree constructs a loop over the active cells. In this tutorial, we do not use threads in addition to MPI, which is why we explicitly disable it by setting the  [2.x.134]  to  [2.x.135]  Finally, the coefficient is evaluated and vectors are initialized as explained above. 

[1.x.128] 



Next, initialize the matrices for the multigrid method on all the levels. The data structure MGConstrainedDoFs keeps information about the indices subject to boundary conditions as well as the indices on edges between different refinement levels as described in the  [2.x.136]  tutorial program. We then go through the levels of the mesh and construct the constraints and matrices on each level. These follow closely the construction of the system matrix on the original mesh, except the slight difference in naming when accessing information on the levels rather than the active cells. 

[1.x.129] 




[1.x.130]  [1.x.131] 




The assemble function is very simple since all we have to do is to assemble the right hand side. Thanks to FEEvaluation and all the data cached in the MatrixFree class, which we query from  [2.x.137]  this can be done in a few lines. Since this call is not wrapped into a  [2.x.138]  (which would be an alternative), we must not forget to call compress() at the end of the assembly to send all the contributions of the right hand side to the owner of the respective degree of freedom. 

[1.x.132] 




[1.x.133]  [1.x.134] 




The solution process is similar as in  [2.x.139] . We start with the setup of the transfer. For  [2.x.140]  there is a very fast transfer class called MGTransferMatrixFree that does the interpolation between the grid levels with the same fast sum factorization kernels that get also used in FEEvaluation. 

[1.x.135] 



As a smoother, this tutorial program uses a Chebyshev iteration instead of SOR in  [2.x.141] . (SOR would be very difficult to implement because we do not have the matrix elements available explicitly, and it is difficult to make it work efficiently in %parallel.)  The smoother is initialized with our level matrices and the mandatory additional data for the Chebyshev smoother. We use a relatively high degree here (5), since matrix-vector products are comparably cheap. We choose to smooth out a range of  [2.x.142]  in the smoother where  [2.x.143]  is an estimate of the largest eigenvalue (the factor 1.2 is applied inside PreconditionChebyshev). In order to compute that eigenvalue, the Chebyshev initialization performs a few steps of a CG algorithm without preconditioner. Since the highest eigenvalue is usually the easiest one to find and a rough estimate is enough, we choose 10 iterations. Finally, we also set the inner preconditioner type in the Chebyshev method which is a Jacobi iteration. This is represented by the DiagonalMatrix class that gets the inverse diagonal entry provided by our LaplaceOperator class.      


On level zero, we initialize the smoother differently because we want to use the Chebyshev iteration as a solver. PreconditionChebyshev allows the user to switch to solver mode where the number of iterations is internally chosen to the correct value. In the additional data object, this setting is activated by choosing the polynomial degree to  [2.x.144]  The algorithm will then attack all eigenvalues between the smallest and largest one in the coarse level matrix. The number of steps in the Chebyshev smoother are chosen such that the Chebyshev convergence estimates guarantee to reduce the residual by the number specified in the variable @p smoothing_range. Note that for solving,  [2.x.145]  is a relative tolerance and chosen smaller than one, in this case, we select three orders of magnitude, whereas it is a number larger than 1 when only selected eigenvalues are smoothed.      


From a computational point of view, the Chebyshev iteration is a very attractive coarse grid solver as long as the coarse size is moderate. This is because the Chebyshev method performs only matrix-vector products and vector updates, which typically parallelize better to the largest cluster size with more than a few tens of thousands of cores than inner product involved in other iterative methods. The former involves only local communication between neighbors in the (coarse) mesh, whereas the latter requires global communication over all processors. 

[1.x.136] 



The next step is to set up the interface matrices that are needed for the case with hanging nodes. The adaptive multigrid realization in deal.II implements an approach called local smoothing. This means that the smoothing on the finest level only covers the local part of the mesh defined by the fixed (finest) grid level and ignores parts of the computational domain where the terminal cells are coarser than this level. As the method progresses to coarser levels, more and more of the global mesh will be covered. At some coarser level, the whole mesh will be covered. Since all level matrices in the multigrid method cover a single level in the mesh, no hanging nodes appear on the level matrices. At the interface between multigrid levels, homogeneous Dirichlet boundary conditions are set while smoothing. When the residual is transferred to the next coarser level, however, the coupling over the multigrid interface needs to be taken into account. This is done by the so-called interface (or edge) matrices that compute the part of the residual that is missed by the level matrix with homogeneous Dirichlet conditions. We refer to the  [2.x.146]  "Multigrid paper by Janssen and Kanschat" for more details.      


For the implementation of those interface matrices, there is already a pre-defined class  [2.x.147]  that wraps the routines  [2.x.148]  and  [2.x.149]  in a new class with @p vmult() and  [2.x.150]  operations (that were originally written for matrices, hence expecting those names). Note that vmult_interface_down is used during the restriction phase of the multigrid V-cycle, whereas vmult_interface_up is used during the prolongation phase.      


Once the interface matrix is created, we set up the remaining Multigrid preconditioner infrastructure in complete analogy to  [2.x.151]  to obtain a  [2.x.152]  object that can be applied to a matrix. 

[1.x.137] 



The setup of the multigrid routines is quite easy and one cannot see any difference in the solve process as compared to  [2.x.153] . All the magic is hidden behind the implementation of the  [2.x.154]  operation. Note that we print out the solve time and the accumulated setup time through standard out, i.e., in any case, whereas detailed times for the setup operations are only printed in case the flag for detail_times in the constructor is changed. 







[1.x.138] 




[1.x.139]  [1.x.140] 




Here is the data output, which is a simplified version of  [2.x.155] . We use the standard VTU (= compressed VTK) output for each grid produced in the refinement process. In addition, we use a compression algorithm that is optimized for speed rather than disk usage. The default setting (which optimizes for disk usage) makes saving the output take about 4 times as long as running the linear solver, while setting  [2.x.156]  to  [2.x.157]  lowers this to only one fourth the time of the linear solve.    


We disable the output when the mesh gets too large. A variant of this program has been run on hundreds of thousands MPI ranks with as many as 100 billion grid cells, which is not directly accessible to classical visualization tools. 

[1.x.141] 




[1.x.142]  [1.x.143] 




The function that runs the program is very similar to the one in  [2.x.158] . We do few refinement steps in 3D compared to 2D, but that's it.    


Before we run the program, we output some information about the detected vectorization level as discussed in the introduction. 

[1.x.144] 




[1.x.145]  [1.x.146] 




Apart from the fact that we set up the MPI framework according to  [2.x.159] , there are no surprises in the main function. 

[1.x.147] 

[1.x.148][1.x.149] 


[1.x.150][1.x.151] 


Since this example solves the same problem as  [2.x.160]  (except for a different coefficient), there is little to say about the solution. We show a picture anyway, illustrating the size of the solution through both isocontours and volume rendering: 

 [2.x.161]  

Of more interest is to evaluate some aspects of the multigrid solver. When we run this program in 2D for quadratic ( [2.x.162] ) elements, we get the following output (when run on one core in release mode): 

[1.x.152] 



As in  [2.x.163] , we see that the number of CG iterations remains constant with increasing number of degrees of freedom. A constant number of iterations (together with optimal computational properties) means that the computing time approximately quadruples as the problem size quadruples from one cycle to the next. The code is also very efficient in terms of storage. Around 2-4 million degrees of freedom fit into 1 GB of memory, see also the MPI results below. An interesting fact is that solving one linear system is cheaper than the setup, despite not building a matrix (approximately half of which is spent in the  [2.x.164]  and  [2.x.165]  calls). This shows the high efficiency of this approach, but also that the deal.II data structures are quite expensive to set up and the setup cost must be amortized over several system solves. 

Not much changes if we run the program in three spatial dimensions. Since we use uniform mesh refinement, we get eight times as many elements and approximately eight times as many degrees of freedom with each cycle: 

[1.x.153] 



Since it is so easy, we look at what happens if we increase the polynomial degree. When selecting the degree as four in 3D, i.e., on  [2.x.166]  elements, by changing the line <code>const unsigned int degree_finite_element=4;</code> at the top of the program, we get the following program output: 

[1.x.154] 



Since  [2.x.167]  elements on a certain mesh correspond to  [2.x.168]  elements on half the mesh size, we can compare the run time at cycle 4 with fourth degree polynomials with cycle 5 using quadratic polynomials, both at 2.1 million degrees of freedom. The surprising effect is that the solver for  [2.x.169]  element is actually slightly faster than for the quadratic case, despite using one more linear iteration. The effect that higher-degree polynomials are similarly fast or even faster than lower degree ones is one of the main strengths of matrix-free operator evaluation through sum factorization, see the [1.x.155]. This is fundamentally different to matrix-based methods that get more expensive per unknown as the polynomial degree increases and the coupling gets denser. 

In addition, also the setup gets a bit cheaper for higher order, which is because fewer elements need to be set up. 

Finally, let us look at the timings with degree 8, which corresponds to another round of mesh refinement in the lower order methods: 

[1.x.156] 



Here, the initialization seems considerably slower than before, which is mainly due to the computation of the diagonal of the matrix, which actually computes a 729 x 729 matrix on each cell and throws away everything but the diagonal. The solver times, however, are again very close to the quartic case, showing that the linear increase with the polynomial degree that is theoretically expected is almost completely offset by better computational characteristics and the fact that higher order methods have a smaller share of degrees of freedom living on several cells that add to the evaluation complexity. 

[1.x.157][1.x.158] 


In order to understand the capabilities of the matrix-free implementation, we compare the performance of the 3d example above with a sparse matrix implementation based on  [2.x.170]  by measuring both the computation times for the initialization of the problem (distribute DoFs, setup and assemble matrices, setup multigrid structures) and the actual solution for the matrix-free variant and the variant based on sparse matrices. We base the preconditioner on float numbers and the actual matrix and vectors on double numbers, as shown above. Tests are run on an Intel Core i7-5500U notebook processor (two cores and [1.x.159] support, i.e., four operations on doubles can be done with one CPU instruction, which is heavily used in FEEvaluation), optimized mode, and two MPI ranks. 

 [2.x.171]  

The table clearly shows that the matrix-free implementation is more than twice as fast for the solver, and more than six times as fast when it comes to initialization costs. As the problem size is made a factor 8 larger, we note that the times usually go up by a factor eight, too (as the solver iterations are constant at six). The main deviation is in the sparse matrix between 5k and 36k degrees of freedom, where the time increases by a factor 12. This is the threshold where the (L3) cache in the processor can no longer hold all data necessary for the matrix-vector products and all matrix elements must be fetched from main memory. 

Of course, this picture does not necessarily translate to all cases, as there are problems where knowledge of matrix entries enables much better solvers (as happens when the coefficient is varying more strongly than in the above example). Moreover, it also depends on the computer system. The present system has good memory performance, so sparse matrices perform comparably well. Nonetheless, the matrix-free implementation gives a nice speedup already for the [1.x.160]<sub>2</sub> elements used in this example. This becomes particularly apparent for time-dependent or nonlinear problems where sparse matrices would need to be reassembled over and over again, which becomes much easier with this class. And of course, thanks to the better complexity of the products, the method gains increasingly larger advantages when the order of the elements increases (the matrix-free implementation has costs 4[1.x.161]<sup>2</sup>[1.x.162] per degree of freedom, compared to 2[1.x.163] for the sparse matrix, so it will win anyway for order 4 and higher in 3d). 

[1.x.164][1.x.165] 


As explained in the introduction and the in-code comments, this program can be run in parallel with MPI. It turns out that geometric multigrid schemes work really well and can scale to very large machines. To the authors' knowledge, the geometric multigrid results shown here are the largest computations done with deal.II as of late 2016, run on up to 147,456 cores of the [1.x.166]. The ingredients for scalability beyond 1000 cores are that no data structure that depends on the global problem size is held in its entirety on a single processor and that the communication is not too frequent in order not to run into latency issues of the network.  For PDEs solved with iterative solvers, the communication latency is often the limiting factor, rather than the throughput of the network. For the example of the SuperMUC system, the point-to-point latency between two processors is between 1e-6 and 1e-5 seconds, depending on the proximity in the MPI network. The matrix-vector products with  [2.x.172]  from this class involves several point-to-point communication steps, interleaved with computations on each core. The resulting latency of a matrix-vector product is around 1e-4 seconds. Global communication, for example an  [2.x.173]  operation that accumulates the sum of a single number per rank over all ranks in the MPI network, has a latency of 1e-4 seconds. The multigrid V-cycle used in this program is also a form of global communication. Think about the coarse grid solve that happens on a single processor: It accumulates the contributions from all processors before it starts. When completed, the coarse grid solution is transferred to finer levels, where more and more processors help in smoothing until the fine grid. Essentially, this is a tree-like pattern over the processors in the network and controlled by the mesh. As opposed to the  [2.x.174]  operations where the tree in the reduction is optimized to the actual links in the MPI network, the multigrid V-cycle does this according to the partitioning of the mesh. Thus, we cannot expect the same optimality. Furthermore, the multigrid cycle is not simply a walk up and down the refinement tree, but also communication on each level when doing the smoothing. In other words, the global communication in multigrid is more challenging and related to the mesh that provides less optimization opportunities. The measured latency of the V-cycle is between 6e-3 and 2e-2 seconds, i.e., the same as 60 to 200 MPI_Allreduce operations. 

The following figure shows a scaling experiments on  [2.x.175]  elements. Along the lines, the problem size is held constant as the number of cores is increasing. When doubling the number of cores, one expects a halving of the computational time, indicated by the dotted gray lines. The results show that the implementation shows almost ideal behavior until an absolute time of around 0.1 seconds is reached. The solver tolerances have been set such that the solver performs five iterations. This way of plotting data is the [1.x.167] of the algorithm. As we go to very large core counts, the curves flatten out a bit earlier, which is because of the communication network in SuperMUC where communication between processors farther away is slightly slower. 

 [2.x.176]  

In addition, the plot also contains results for [1.x.168] that lists how the algorithm behaves as both the number of processor cores and elements is increased at the same pace. In this situation, we expect that the compute time remains constant. Algorithmically, the number of CG iterations is constant at 5, so we are good from that end. The lines in the plot are arranged such that the top left point in each data series represents the same size per processor, namely 131,072 elements (or approximately 3.5 million degrees of freedom per core). The gray lines indicating ideal strong scaling are by the same factor of 8 apart. The results show again that the scaling is almost ideal. The parallel efficiency when going from 288 cores to 147,456 cores is at around 75% for a local problem size of 750,000 degrees of freedom per core which takes 1.0s on 288 cores, 1.03s on 2304 cores, 1.19s on 18k cores, and 1.35s on 147k cores. The algorithms also reach a very high utilization of the processor. The largest computation on 147k cores reaches around 1.7 PFLOPs/s on SuperMUC out of an arithmetic peak of 3.2 PFLOPs/s. For an iterative PDE solver, this is a very high number and significantly more is often only reached for dense linear algebra. Sparse linear algebra is limited to a tenth of this value. 

As mentioned in the introduction, the matrix-free method reduces the memory consumption of the data structures. Besides the higher performance due to less memory transfer, the algorithms also allow for very large problems to fit into memory. The figure below shows the computational time as we increase the problem size until an upper limit where the computation exhausts memory. We do this for 1k cores, 8k cores, and 65k cores and see that the problem size can be varied over almost two orders of magnitude with ideal scaling. The largest computation shown in this picture involves 292 billion ( [2.x.177] ) degrees of freedom. On a DG computation of 147k cores, the above algorithms were also run involving up to 549 billion (2^39) DoFs. 

 [2.x.178]  

Finally, we note that while performing the tests on the large-scale system shown above, improvements of the multigrid algorithms in deal.II have been developed. The original version contained the sub-optimal code based on MGSmootherPrecondition where some MPI_Allreduce commands (checking whether all vector entries are zero) were done on each smoothing operation on each level, which only became apparent on 65k cores and more. However, the following picture shows that the improvement already pay off on a smaller scale, here shown on computations on up to 14,336 cores for  [2.x.179]  elements: 

 [2.x.180]  


[1.x.169][1.x.170] 


As explained in the code, the algorithm presented here is prepared to run on adaptively refined meshes. If only part of the mesh is refined, the multigrid cycle will run with local smoothing and impose Dirichlet conditions along the interfaces which differ in refinement level for smoothing through the  [2.x.181]  class. Due to the way the degrees of freedom are distributed over levels, relating the owner of the level cells to the owner of the first descendant active cell, there can be an imbalance between different processors in MPI, which limits scalability by a factor of around two to five. 

[1.x.171][1.x.172] 


[1.x.173][1.x.174] 


As mentioned above the code is ready for locally adaptive h-refinement. For the Poisson equation one can employ the Kelly error indicator, implemented in the KellyErrorEstimator class. However one needs to be careful with the ghost indices of parallel vectors. In order to evaluate the jump terms in the error indicator, each MPI process needs to know locally relevant DoFs. However  [2.x.182]  function initializes the vector only with some locally relevant DoFs. The ghost indices made available in the vector are a tight set of only those indices that are touched in the cell integrals (including constraint resolution). This choice has performance reasons, because sending all locally relevant degrees of freedom would be too expensive compared to the matrix-vector product. Consequently the solution vector as-is is not suitable for the KellyErrorEstimator class. The trick is to change the ghost part of the partition, for example using a temporary vector and  [2.x.183]  as shown below. 

[1.x.175] 



[1.x.176][1.x.177] 


This program is parallelized with MPI only. As an alternative, the MatrixFree loop can also be issued in hybrid mode, for example by using MPI parallelizing over the nodes of a cluster and with threads through Intel TBB within the shared memory region of one node. To use this, one would need to both set the number of threads in the MPI_InitFinalize data structure in the main function, and set the  [2.x.184]  to partition_color to actually do the loop in parallel. This use case is discussed in  [2.x.185] . 

[1.x.178][1.x.179] 


The presented program assumes homogeneous Dirichlet boundary conditions. When going to non-homogeneous conditions, the situation is a bit more intricate. To understand how to implement such a setting, let us first recall how these arise in the mathematical formulation and how they are implemented in a matrix-based variant. In essence, an inhomogeneous Dirichlet condition sets some of the nodal values in the solution to given values rather than determining them through the variational principles, [1.x.180] 

where  [2.x.186]  denotes the nodal values of the solution and  [2.x.187]  denotes the set of all nodes. The set  [2.x.188]  is the subset of the nodes that are subject to Dirichlet boundary conditions where the solution is forced to equal  [2.x.189]  as the interpolation of boundary values on the Dirichlet-constrained node points  [2.x.190] . We then insert this solution representation into the weak form, e.g. the Laplacian shown above, and move the known quantities to the right hand side: [1.x.181] 

In this formula, the equations are tested for all basis functions  [2.x.191]  with  [2.x.192]  that are not related to the nodes constrained by Dirichlet conditions. 

In the implementation in deal.II, the integrals  [2.x.193]  on the right hand side are already contained in the local matrix contributions we assemble on each cell. When using  [2.x.194]  as first described in the  [2.x.195]  and  [2.x.196]  tutorial programs, we can account for the contribution of inhomogeneous constraints [1.x.182] by multiplying the columns [1.x.183] and rows [1.x.184] of the local matrix according to the integrals  [2.x.197]  by the inhomogeneities and subtracting the resulting from the position [1.x.185] in the global right-hand-side vector, see also the  [2.x.198]  module. In essence, we use some of the integrals that get eliminated from the left hand side of the equation to finalize the right hand side contribution. Similar mathematics are also involved when first writing all entries into a left hand side matrix and then eliminating matrix rows and columns by  [2.x.199]  

In principle, the components that belong to the constrained degrees of freedom could be eliminated from the linear system because they do not carry any information. In practice, in deal.II we always keep the size of the linear system the same to avoid handling two different numbering systems and avoid confusion about the two different index sets. In order to ensure that the linear system does not get singular when not adding anything to constrained rows, we then add dummy entries to the matrix diagonal that are otherwise unrelated to the real entries. 

In a matrix-free method, we need to take a different approach, since the @p LaplaceOperator class represents the matrix-vector product of a [1.x.186] operator (the left-hand side of the last formula).  It does not matter whether the AffineConstraints object passed to the  [2.x.200]  contains inhomogeneous constraints or not, the  [2.x.201]  call will only resolve the homogeneous part of the constraints as long as it represents a [1.x.187] operator. 

In our matrix-free code, the right hand side computation where the contribution of inhomogeneous conditions ends up is completely decoupled from the matrix operator and handled by a different function above. Thus, we need to explicitly generate the data that enters the right hand side rather than using a byproduct of the matrix assembly. Since we already know how to apply the operator on a vector, we could try to use those facilities for a vector where we only set the Dirichlet values: 

[1.x.188] 

or, equivalently, if we already had filled the inhomogeneous constraints into an AffineConstraints object, 

[1.x.189] 



We could then pass the vector  [2.x.202]  to the @p  [2.x.203]  function and add the new contribution to the @p system_rhs vector that gets filled in the  [2.x.204]  function. However, this idea does not work because the  [2.x.205]  call used inside the vmult() functions assumes homogeneous values on all constraints (otherwise the operator would not be a linear operator but an affine one). To also retrieve the values of the inhomogeneities, we could select one of two following strategies. 

[1.x.190][1.x.191] 


The class FEEvaluation has a facility that addresses precisely this requirement: For non-homogeneous Dirichlet values, we do want to skip the implicit imposition of homogeneous (Dirichlet) constraints upon reading the data from the vector  [2.x.206]  For example, we could extend the @p  [2.x.207]  function to deal with inhomogeneous Dirichlet values as follows, assuming the Dirichlet values have been interpolated into the object  [2.x.208]  

[1.x.192] 



In this code, we replaced the  [2.x.209]  function for the tentative solution vector by  [2.x.210]  that ignores all constraints. Due to this setup, we must make sure that other constraints, e.g. by hanging nodes, are correctly distributed to the input vector already as they are not resolved as in  [2.x.211]  Inside the loop, we then evaluate the Laplacian and repeat the second derivative call with  [2.x.212]  from the  [2.x.213]  class, but with the sign switched since we wanted to subtract the contribution of Dirichlet conditions on the right hand side vector according to the formula above. When we invoke the  [2.x.214]  call, we then set both arguments regarding the value slot and first derivative slot to true to account for both terms added in the loop over quadrature points. Once the right hand side is assembled, we then go on to solving the linear system for the homogeneous problem, say involving a variable  [2.x.215]  After solving, we can add  [2.x.216]  to the  [2.x.217]  vector that contains the final (inhomogeneous) solution. 

Note that the negative sign for the Laplacian alongside with a positive sign for the forcing that we needed to build the right hand side is a more general concept: We have implemented nothing else than Newton's method for nonlinear equations, but applied to a linear system. We have used an initial guess for the variable  [2.x.218]  in terms of the Dirichlet boundary conditions and computed a residual  [2.x.219] . The linear system was then solved as  [2.x.220]  and we finally computed  [2.x.221] . For a linear system, we obviously reach the exact solution after a single iteration. If we wanted to extend the code to a nonlinear problem, we would rename the  [2.x.222]  function into a more descriptive name like @p assemble_residual() that computes the (weak) form of the residual, whereas the  [2.x.223]  function would get the linearization of the residual with respect to the solution variable. 

[1.x.193][1.x.194] 


A second alternative to get the right hand side that re-uses the @p  [2.x.224]  function is to instead add a second LaplaceOperator that skips Dirichlet constraints. To do this, we initialize a second MatrixFree object which does not have any boundary value constraints. This  [2.x.225]  object is then passed to a  [2.x.226]  class instance @p inhomogeneous_operator that is only used to create the right hand side: 

[1.x.195] 



A more sophisticated implementation of this technique could reuse the original MatrixFree object. This can be done by initializing the MatrixFree object with multiple blocks, where each block corresponds to a different AffineConstraints object. Doing this would require making substantial modifications to the LaplaceOperator class, but the  [2.x.227]  class that comes with the library can do this. See the discussion on blocks in  [2.x.228]  for more information on how to set up blocks. [1.x.196] [1.x.197]  [2.x.229]  

 [2.x.230] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19] 

 [2.x.3]  

[1.x.20] 

[1.x.21] 

[1.x.22][1.x.23] 


In this example, we show how to solve a partial differential equation (PDE) on a codimension one surface  [2.x.4]  made of quadrilaterals, i.e. on a surface in 3d or a line in 2d. We focus on the following elliptic second order PDE 

[1.x.24] 

which generalized the Laplace equation we have previously solved in several of the early tutorial programs. Our implementation is based on  [2.x.5] .  [2.x.6]  also solves problems on lower dimensional surfaces; however, there we only consider integral equations that do not involve derivatives on the solution variable, while here we actually have to investigate what it means to take derivatives of a function only defined on a (possibly curved) surface. 

In order to define the above operator, we start by introducing some notations. Let  [2.x.7]  be a parameterization of a surface  [2.x.8]  from a reference element  [2.x.9] , i.e. each point  [2.x.10]  induces a point  [2.x.11] . Then let [1.x.25] denotes the corresponding first fundamental form, where  [2.x.12]  is the derivative (Jacobian) of the mapping. In the following,  [2.x.13]  will be either the entire surface  [2.x.14]  or, more convenient for the finite element method, any face  [2.x.15] , where  [2.x.16]  is a partition (triangulation) of  [2.x.17]  constituted of quadrilaterals. We are now in position to define the tangential gradient of a function  [2.x.18]  by [1.x.26] The surface Laplacian (also called the Laplace-Beltrami operator) is then defined as   [2.x.19] . Note that an alternate way to compute the surface gradient on smooth surfaces  [2.x.20]  is [1.x.27] where  [2.x.21]  is a "smooth" extension of  [2.x.22]  in a tubular neighborhood of  [2.x.23]  and  [2.x.24]  is the normal of  [2.x.25] . Since  [2.x.26] , we deduce [1.x.28] Worth mentioning, the term  [2.x.27]  appearing in the above expression is the total curvature of the surface (sum of principal curvatures). 

As usual, we are only interested in weak solutions for which we can use  [2.x.28]  finite elements (rather than requiring  [2.x.29]  continuity as for strong solutions). We therefore resort to the weak formulation [1.x.29] and take advantage of the partition  [2.x.30]  to further write [1.x.30] Moreover, each integral in the above expression is computed in the reference element  [2.x.31]  so that 

[1.x.31] 

and [1.x.32] Finally, we use a quadrature formula defined by points  [2.x.32]  and weights  [2.x.33]  to evaluate the above integrals and obtain [1.x.33] and [1.x.34] 


Fortunately, deal.II has already all the tools to compute the above expressions. In fact, they barely differ from the ways in which we solve the usual Laplacian, only requiring the surface coordinate mapping to be provided in the constructor of the FEValues class. This surface description given, in the codimension one surface case, the two routines  [2.x.34]  and  [2.x.35]  return 

[1.x.35] 

This provides exactly the terms we need for our computations. 

On a more general note, details for the finite element approximation on surfaces can be found for instance in [Dziuk, in Partial differential equations and calculus of variations 1357, Lecture Notes in Math., 1988], [Demlow, SIAM J. Numer. Anal.  47(2), 2009] and [Bonito, Nochetto, and Pauletti, SIAM J. Numer. Anal. 48(5), 2010]. 




[1.x.36][1.x.37] 


In general when you want to test numerically the accuracy and/or order of convergence of an algorithm you need to provide an exact solution. The usual trick is to pick a function that we want to be the solution, then apply the differential operator to it that defines a forcing term for the right hand side. This is what we do in this example. In the current case, the form of the domain is obviously also essential. 

We produce one test case for a 2d problem and another one for 3d: 

 [2.x.36]   [2.x.37]    In 2d, let's choose as domain a half circle. On this domain, we choose the   function  [2.x.38]  as the solution. To compute the right hand   side, we have to compute the surface Laplacian of the   solution function. There are (at least) two ways to do that. The first one   is to project away the normal derivative as described above using the natural extension of  [2.x.39]  (still denoted by  [2.x.40] ) over  [2.x.41] , i.e. to compute   [1.x.38]   where  [2.x.42]  is the total curvature of  [2.x.43] .   Since we are on the unit circle,  [2.x.44]  and  [2.x.45]  so that   [1.x.39] 

  A somewhat simpler way, at least for the current case of a curve in   two-dimensional space, is to note that we can map the interval  [2.x.46]  onto the domain  [2.x.47]  using the transformation    [2.x.48] .   At position  [2.x.49] , the value of the solution is then    [2.x.50] .   Taking into account that the transformation is length preserving, i.e. a   segment of length  [2.x.51]  is mapped onto a piece of curve of exactly the same   length, the tangential Laplacian then satisfies   [1.x.40] 

  which is of course the same result as we had above.  [2.x.52]   [2.x.53]    In 3d, the domain is again half of the surface of the unit ball, i.e. a half   sphere or dome. We choose  [2.x.54]  as   the solution. We can compute the right hand side of the   equation,  [2.x.55] , in the same way as the method above (with  [2.x.56] ), yielding an   awkward and lengthy expression. You can find the full expression in the   source code.  [2.x.57]   [2.x.58]  

In the program, we will also compute the  [2.x.59]  seminorm error of the solution. Since the solution function and its numerical approximation are only defined on the manifold, the obvious definition of this error functional is  [2.x.60] . This requires us to provide the [1.x.41] gradient  [2.x.61]  to the function  [2.x.62]  (first introduced in  [2.x.63] ), which we will do by implementing the function  [2.x.64]  in the program below. 


[1.x.42][1.x.43] 


If you've read through  [2.x.65]  and understand the discussion above of how solution and right hand side correspond to each other, you will be immediately familiar with this program as well. In fact, there are only two things that are of significance: 

- The way we generate the mesh that triangulates the computational domain. 

- The way we use Mapping objects to describe that the domain on which we solve   the partial differential equation is not planar but in fact curved. 

Mapping objects were already introduced in  [2.x.66]  and  [2.x.67]  and as explained there, there is usually not a whole lot you have to know about how they work as long as you have a working description of how the boundary looks. In essence, we will simply declare an appropriate object of type MappingQ that will automatically obtain the boundary description from the Triangulation. The mapping object will then be passed to the appropriate functions, and we will get a boundary description for half circles or half spheres that is predefined in the library. 

The rest of the program follows closely  [2.x.68]  and, as far as computing the error,  [2.x.69] . Some aspects of this program, in particular the use of two template arguments on the classes Triangulation, DoFHandler, and similar, are already described in detail in  [2.x.70] ; you may wish to read through this tutorial program as well. [1.x.44] [1.x.45] 


[1.x.46]  [1.x.47] 




If you've read through  [2.x.71]  and  [2.x.72] , you will recognize that we have used all of the following include files there already. Consequently, we will not explain their meaning here again. 

[1.x.48] 




[1.x.49]  [1.x.50] 




This class is almost exactly similar to the  [2.x.73]  class in  [2.x.74] . 




The essential differences are these: 

   




- The template parameter now denotes the dimensionality of the embedding space, which is no longer the same as the dimensionality of the domain and the triangulation on which we compute. We indicate this by calling the parameter  [2.x.75]  and introducing a constant  [2.x.76]  equal to the dimensionality of the domain -- here equal to  [2.x.77] . 

- All member variables that have geometric aspects now need to know about both their own dimensionality as well as that of the embedding space. Consequently, we need to specify both of their template parameters one for the dimension of the mesh  [2.x.78]  and the other for the dimension of the embedding space,  [2.x.79]  This is exactly what we did in  [2.x.80] , take a look there for a deeper explanation. 

- We need an object that describes which kind of mapping to use from the reference cell to the cells that the triangulation is composed of. The classes derived from the Mapping base class do exactly this. Throughout most of deal.II, if you don't do anything at all, the library assumes that you want an object of kind MappingQ1 that uses a (bi-, tri-)linear mapping. In many cases, this is quite sufficient, which is why the use of these objects is mostly optional: for example, if you have a polygonal two-dimensional domain in two-dimensional space, a bilinear mapping of the reference cell to the cells of the triangulation yields an exact representation of the domain. If you have a curved domain, one may want to use a higher order mapping for those cells that lie at the boundary of the domain -- this is what we did in  [2.x.81] , for example. However, here we have a curved domain, not just a curved boundary, and while we can approximate it with bilinearly mapped cells, it is really only prudent to use a higher order mapping for all cells. Consequently, this class has a member variable of type MappingQ; we will choose the polynomial degree of the mapping equal to the polynomial degree of the finite element used in the computations to ensure optimal approximation, though this iso-parametricity is not required. 

[1.x.51] 




[1.x.52]  [1.x.53] 




Next, let us define the classes that describe the exact solution and the right hand sides of the problem. This is in analogy to  [2.x.82]  and  [2.x.83]  where we also defined such objects. Given the discussion in the introduction, the actual formulas should be self-explanatory. A point of interest may be how we define the value and gradient functions for the 2d and 3d cases separately, using explicit specializations of the general template. An alternative to doing it this way might have been to define the general template and have a  [2.x.84]  statement (or a sequence of  [2.x.85] s) for each possible value of the spatial dimension. 

[1.x.54] 




[1.x.55]  [1.x.56] 




The rest of the program is actually quite unspectacular if you know  [2.x.86] . Our first step is to define the constructor, setting the polynomial degree of the finite element and mapping, and associating the DoF handler to the triangulation: 

[1.x.57] 




[1.x.58]  [1.x.59] 




The next step is to create the mesh, distribute degrees of freedom, and set up the various variables that describe the linear system. All of these steps are standard with the exception of how to create a mesh that describes a surface. We could generate a mesh for the domain we are interested in, generate a triangulation using a mesh generator, and read it in using the GridIn class. Or, as we do here, we generate the mesh using the facilities in the GridGenerator namespace.    


In particular, what we're going to do is this (enclosed between the set of braces below): we generate a  [2.x.87]  dimensional mesh for the half disk (in 2d) or half ball (in 3d), using the  [2.x.88]  function. This function sets the boundary indicators of all faces on the outside of the boundary to zero for the ones located on the perimeter of the disk/ball, and one on the straight part that splits the full disk/ball into two halves. The next step is the main point: The  [2.x.89]  function creates a mesh that consists of those cells that are the faces of the previous mesh, i.e. it describes the [1.x.60] cells of the original (volume) mesh. However, we do not want all faces: only those on the perimeter of the disk or ball which carry boundary indicator zero; we can select these cells using a set of boundary indicators that we pass to  [2.x.90]     


There is one point that needs to be mentioned. In order to refine a surface mesh appropriately if the manifold is curved (similarly to refining the faces of cells that are adjacent to a curved boundary), the triangulation has to have an object attached to it that describes where new vertices should be located. If you don't attach such a boundary object, they will be located halfway between existing vertices; this is appropriate if you have a domain with straight boundaries (e.g. a polygon) but not when, as here, the manifold has curvature. So for things to work properly, we need to attach a manifold object to our (surface) triangulation, in much the same way as we've already done in 1d for the boundary. We create such an object and attach it to the triangulation.    


The final step in creating the mesh is to refine it a number of times. The rest of the function is the same as in previous tutorial programs. 

[1.x.61] 




[1.x.62]  [1.x.63] 




The following is the central function of this program, assembling the matrix that corresponds to the surface Laplacian (Laplace-Beltrami operator). Maybe surprisingly, it actually looks exactly the same as for the regular Laplace operator discussed in, for example,  [2.x.91] . The key is that the  [2.x.92]  function does the magic: It returns the surface gradient  [2.x.93]  of the  [2.x.94] th shape function at the  [2.x.95] th quadrature point. The rest then does not need any changes either: 

[1.x.64] 




[1.x.65]  [1.x.66] 




The next function is the one that solves the linear system. Here, too, no changes are necessary: 

[1.x.67] 




[1.x.68]  [1.x.69] 




This is the function that generates graphical output from the solution. Most of it is boilerplate code, but there are two points worth pointing out: 

   




- The  [2.x.96]  function can take two kinds of vectors: Either vectors that have one value per degree of freedom defined by the DoFHandler object previously attached via  [2.x.97]  and vectors that have one value for each cell of the triangulation, for example to output estimated errors for each cell. Typically, the DataOut class knows to tell these two kinds of vectors apart: there are almost always more degrees of freedom than cells, so we can differentiate by the two kinds looking at the length of a vector. We could do the same here, but only because we got lucky: we use a half sphere. If we had used the whole sphere as domain and  [2.x.98]  elements, we would have the same number of cells as vertices and consequently the two kinds of vectors would have the same number of elements. To avoid the resulting confusion, we have to tell the  [2.x.99]  function which kind of vector we have: DoF data. This is what the third argument to the function does. 

- The  [2.x.100]  function can generate output that subdivides each cell so that visualization programs can resolve curved manifolds or higher polynomial degree shape functions better. We here subdivide each element in each coordinate direction as many times as the polynomial degree of the finite element in use. 

[1.x.70] 




[1.x.71]  [1.x.72] 




This is the last piece of functionality: we want to compute the error in the numerical solution. It is a verbatim copy of the code previously shown and discussed in  [2.x.101] . As mentioned in the introduction, the  [2.x.102]  class provides the (tangential) gradient of the solution. To avoid evaluating the error only a superconvergence points, we choose a quadrature rule of sufficiently high order. 

[1.x.73] 




[1.x.74]  [1.x.75] 




The last function provides the top-level logic. Its contents are self-explanatory: 

[1.x.76] 




[1.x.77]  [1.x.78] 




The remainder of the program is taken up by the  [2.x.103]  function. It follows exactly the general layout first introduced in  [2.x.104]  and used in all following tutorial programs: 

[1.x.79] 

[1.x.80][1.x.81] 


When you run the program, the following output should be printed on screen: 

[1.x.82] 




By playing around with the number of global refinements in the  [2.x.105]  function you increase or decrease mesh refinement. For example, doing one more refinement and only running the 3d surface problem yields the following output: 

[1.x.83] 



This is what we expect: make the mesh size smaller by a factor of two and the error goes down by a factor of four (remember that we use bi-quadratic elements). The full sequence of errors from one to five refinements looks like this, neatly following the theoretically predicted pattern: 

[1.x.84] 



Finally, the program produces graphical output that we can visualize. Here is a plot of the results: 

 [2.x.106]  

The program also works for 1d curves in 2d, not just 2d surfaces in 3d. You can test this by changing the template argument in  [2.x.107]  like so: 

[1.x.85] 

The domain is a curve in 2d, and we can visualize the solution by using the third dimension (and color) to denote the value of the function  [2.x.108] . This then looks like so (the white curve is the domain, the colored curve is the solution extruded into the third dimension, clearly showing the change in sign as the curve moves from one quadrant of the domain into the adjacent one): 

 [2.x.109]  


[1.x.86] [1.x.87][1.x.88] 


Computing on surfaces only becomes interesting if the surface is more interesting than just a half sphere. To achieve this, deal.II can read meshes that describe surfaces through the usual GridIn class. Or, in case you have an analytic description, a simple mesh can sometimes be stretched and bent into a shape we are interested in. 

Let us consider a relatively simple example: we take the half sphere we used before, we stretch it by a factor of 10 in the z-direction, and then we jumble the x- and y-coordinates a bit. Let's show the computational domain and the solution first before we go into details of the implementation below: 

 [2.x.110]  

 [2.x.111]  

The way to produce such a mesh is by using the  [2.x.112]  function. It needs a way to transform each individual mesh point to a different position. Let us here use the following, rather simple function (remember: stretch in one direction, jumble in the other two): 

[1.x.89] 



If we followed the  [2.x.113]  function, we would extract the half spherical surface mesh as before, warp it into the shape we want, and refine as often as necessary. This is not quite as simple as we'd like here, though: refining requires that we have an appropriate manifold object attached to the triangulation that describes where new vertices of the mesh should be located upon refinement. I'm sure it's possible to describe this manifold in a not-too-complicated way by simply undoing the transformation above (yielding the spherical surface again), finding the location of a new point on the sphere, and then re-warping the result. But I'm a lazy person, and since doing this is not really the point here, let's just make our lives a bit easier: we'll extract the half sphere, refine it as often as necessary, get rid of the object that describes the manifold since we now no longer need it, and then finally warp the mesh. With the function above, this would look as follows: 

[1.x.90] 



Note that the only essential addition is the line marked with asterisks. It is worth pointing out one other thing here, though: because we detach the manifold description from the surface mesh, whenever we use a mapping object in the rest of the program, it has no curves boundary description to go on any more. Rather, it will have to use the implicit, FlatManifold class that is used on all parts of the domain not explicitly assigned a different manifold object. Consequently, whether we use MappingQ(2), MappingQ(15) or MappingQ1, each cell of our mesh will be mapped using a bilinear approximation. 

All these drawbacks aside, the resulting pictures are still pretty. The only other differences to what's in  [2.x.114]  is that we changed the right hand side to  [2.x.115]  and the boundary values (through the  [2.x.116]  class) to  [2.x.117] . Of course, we now no longer know the exact solution, so the computation of the error at the end of  [2.x.118]  will yield a meaningless number. [1.x.91] [1.x.92]  [2.x.119]  

 [2.x.120] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] b. 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9] 

[1.x.10] 

In this program, we use the interior penalty method and Nitsche's weak boundary conditions to solve Poisson's equation. We use multigrid methods on locally refined meshes, which are generated using a bulk criterion and a standard error estimator based on cell and face residuals. All operators are implemented using the MeshWorker interface. 

Like in  [2.x.3] , the discretization relies on finite element spaces, which are polynomial inside the mesh cells  [2.x.4] , but have no continuity between cells. Since such functions have two values on each interior face  [2.x.5] , one from each side, we define mean value and jump operators as follows: let [1.x.11]<sub>1</sub> and [1.x.12]<sub>2</sub> be the two cells sharing a face, and let the traces of functions [1.x.13] and the outer normal vectors [1.x.14][1.x.15] be labeled accordingly. Then, on the face, we let [1.x.16] 

Note, that if such an expression contains a normal vector, the averaging operator turns into a jump. The interior penalty method for the problem [1.x.17] becomes [1.x.18] 



Here,  [2.x.6]  is the penalty parameter, which is chosen as follows: for a face [1.x.19] of a cell [1.x.20], compute the value [1.x.21] where [1.x.22] is the polynomial degree of the finite element functions and  [2.x.7]  and  [2.x.8]  denote the  [2.x.9]  and  [2.x.10]  dimensional Hausdorff measure of the corresponding object. If the face is at the boundary, choose  [2.x.11] . For an interior face, we take the average of the two values at this face. 

In our finite element program, we distinguish three different integrals, corresponding to the sums over cells, interior faces and boundary faces above. Since the  [2.x.12]  organizes the sums for us, we only need to implement the integrals over each mesh element. The class MatrixIntegrator below has these three functions for the left hand side of the formula, the class RHSIntegrator for the right. 

As we will see below, even the error estimate is of the same structure, since it can be written as 

[1.x.23] 



Thus, the functions for assembling matrices, right hand side and error estimates below exhibit that these loops are all generic and can be programmed in the same way. 

This program is related to  [2.x.13] b, in that it uses MeshWorker and discontinuous Galerkin methods. While there, we solved an advection problem, here it is a diffusion problem. Here, we also use multigrid preconditioning and a theoretically justified error estimator, see Karakashian and Pascal (2003). The multilevel scheme was discussed in detail in Kanschat (2004). The adaptive iteration and its convergence have been discussed (for triangular meshes) in Hoppe, Kanschat, and Warburton (2009). [1.x.24] [1.x.25] 

The include files for the linear algebra: A regular SparseMatrix, which in turn will include the necessary files for SparsityPattern and Vector classes. 

[1.x.26] 



Include files for setting up the mesh 

[1.x.27] 



Include files for FiniteElement classes and DoFHandler. 

[1.x.28] 



The include files for using the MeshWorker framework 

[1.x.29] 



The include file for local integrators associated with the Laplacian 

[1.x.30] 



Support for multigrid methods 

[1.x.31] 



Finally, we take our exact solution from the library as well as quadrature and additional tools. 

[1.x.32] 



All classes of the deal.II library are in the namespace dealii. In order to save typing, we tell the compiler to search names in there as well. 

[1.x.33] 



This is the function we use to set the boundary values and also the exact solution we compare to. 

[1.x.34] 




[1.x.35]  [1.x.36] 




MeshWorker separates local integration from the loops over cells and faces. Thus, we have to write local integration classes for generating matrices, the right hand side and the error estimator. 




All these classes have the same three functions for integrating over cells, boundary faces and interior faces, respectively. All the information needed for the local integration is provided by  [2.x.14]  Note that the signature of the functions cannot be changed, because it is expected by  [2.x.15]  




The first class defining local integrators is responsible for computing cell and face matrices. It is used to assemble the global matrix as well as the level matrices. 

[1.x.37] 



On each cell, we integrate the Dirichlet form. We use the library of ready made integrals in LocalIntegrators to avoid writing these loops ourselves. Similarly, we implement Nitsche boundary conditions and the interior penalty fluxes between cells.    


The boundary and flux terms need a penalty parameter, which should be adjusted to the cell size and the polynomial degree. A safe choice of this parameter for constant coefficients can be found in  [2.x.16]  and we use this below. 

[1.x.38] 



Interior faces use the interior penalty method 

[1.x.39] 



The second local integrator builds the right hand side. In our example, the right hand side function is zero, such that only the boundary condition is set here in weak form. 

[1.x.40] 



The third local integrator is responsible for the contributions to the error estimate. This is the standard energy estimator due to Karakashian and Pascal (2003). 

[1.x.41] 



The cell contribution is the Laplacian of the discrete solution, since the right hand side is zero. 

[1.x.42] 



At the boundary, we use simply a weighted form of the boundary residual, namely the norm of the difference between the finite element solution and the correct boundary condition. 

[1.x.43] 



Finally, on interior faces, the estimator consists of the jumps of the solution and its normal derivative, weighted appropriately. 

[1.x.44] 



Finally we have an integrator for the error. Since the energy norm for discontinuous Galerkin problems not only involves the difference of the gradient inside the cells, but also the jump terms across faces and at the boundary, we cannot just use  [2.x.17]  Instead, we use the MeshWorker interface to compute the error ourselves. 




There are several different ways to define this energy norm, but all of them are equivalent to each other uniformly with mesh size (some not uniformly with polynomial degree). Here, we choose [1.x.45] 







[1.x.46] 



Here we have the integration on cells. There is currently no good interface in MeshWorker that would allow us to access values of regular functions in the quadrature points. Thus, we have to create the vectors for the exact function's values and gradients inside the cell integrator. After that, everything is as before and we just add up the squares of the differences. 




Additionally to computing the error in the energy norm, we use the capability of the mesh worker to compute two functionals at the same time and compute the [1.x.47]-error in the same loop. Obviously, this one does not have any jump terms and only appears in the integration on cells. 

[1.x.48] 




[1.x.49]  [1.x.50] 




This class does the main job, like in previous examples. For a description of the functions declared here, please refer to the implementation below. 

[1.x.51] 



The member objects related to the discretization are here. 

[1.x.52] 



Then, we have the matrices and vectors related to the global discrete system. 

[1.x.53] 



Finally, we have a group of sparsity patterns and sparse matrices related to the multilevel preconditioner.  First, we have a level matrix and its sparsity pattern. 

[1.x.54] 



When we perform multigrid with local smoothing on locally refined meshes, additional matrices are required; see Kanschat (2004). Here is the sparsity pattern for these edge matrices. We only need one, because the pattern of the up matrix is the transpose of that of the down matrix. Actually, we do not care too much about these details, since the MeshWorker is filling these matrices. 

[1.x.55] 



The flux matrix at the refinement edge, coupling fine level degrees of freedom to coarse level. 

[1.x.56] 



The transpose of the flux matrix at the refinement edge, coupling coarse level degrees of freedom to fine level. 

[1.x.57] 



The constructor simply sets up the coarse grid and the DoFHandler. The FiniteElement is provided as a parameter to allow flexibility. 

[1.x.58] 



In this function, we set up the dimension of the linear system and the sparsity patterns for the global matrix as well as the level matrices. 

[1.x.59] 



First, we use the finite element to distribute degrees of freedom over the mesh and number them. 

[1.x.60] 



Then, we already know the size of the vectors representing finite element functions. 

[1.x.61] 



Next, we set up the sparsity pattern for the global matrix. Since we do not know the row sizes in advance, we first fill a temporary DynamicSparsityPattern object and copy it to the regular SparsityPattern once it is complete. 

[1.x.62] 



The global system is set up, now we attend to the level matrices. We resize all matrix objects to hold one matrix per level. 

[1.x.63] 



It is important to update the sparsity patterns after <tt>clear()</tt> was called for the level matrices, since the matrices lock the sparsity pattern through the SmartPointer and Subscriptor mechanism. 

[1.x.64] 



Now all objects are prepared to hold one sparsity pattern or matrix per level. What's left is setting up the sparsity patterns on each level. 

[1.x.65] 



These are roughly the same lines as above for the global matrix, now for each level. 

[1.x.66] 



Additionally, we need to initialize the transfer matrices at the refinement edge between levels. They are stored at the index referring to the finer of the two indices, thus there is no such object on level 0. 

[1.x.67] 



In this function, we assemble the global system matrix, where by global we indicate that this is the matrix of the discrete system we solve and it is covering the whole mesh. 

[1.x.68] 



First, we need t set up the object providing the values we integrate. This object contains all FEValues and FEFaceValues objects needed and also maintains them automatically such that they always point to the current cell. To this end, we need to tell it first, where and what to compute. Since we are not doing anything fancy, we can rely on their standard choice for quadrature rules.      


Since their default update flags are minimal, we add what we need additionally, namely the values and gradients of shape functions on all objects (cells, boundary and interior faces). Afterwards, we are ready to initialize the container, which will create all necessary FEValuesBase objects for integration. 

[1.x.69] 



This is the object into which we integrate local data. It is filled by the local integration routines in MatrixIntegrator and then used by the assembler to distribute the information into the global matrix. 

[1.x.70] 



Furthermore, we need an object that assembles the local matrix into the global matrix. These assembler objects have all the knowledge of the structures of the target object, in this case a SparseMatrix, possible constraints and the mesh structure. 

[1.x.71] 



Now comes the part we coded ourselves, the local integrator. This is the only part which is problem dependent. 

[1.x.72] 



Now, we throw everything into a  [2.x.18]  which here traverses all active cells of the mesh, computes cell and face matrices and assembles them into the global matrix. We use the variable <tt>dof_handler</tt> here in order to use the global numbering of degrees of freedom. 

[1.x.73] 



Now, we do the same for the level matrices. Not too surprisingly, this function looks like a twin of the previous one. Indeed, there are only two minor differences. 

[1.x.74] 



Obviously, the assembler needs to be replaced by one filling level matrices. Note that it automatically fills the edge matrices as well. 

[1.x.75] 



Here is the other difference to the previous function: we run over all cells, not only the active ones. And we use functions ending on  [2.x.19]  since we need the degrees of freedom on each level, not the global numbering. 

[1.x.76] 



Here we have another clone of the assemble function. The difference to assembling the system matrix consists in that we assemble a vector here. 

[1.x.77] 



Since this assembler allows us to fill several vectors, the interface is a little more complicated as above. The pointers to the vectors have to be stored in an AnyData object. While this seems to cause two extra lines of code here, it actually comes handy in more complex applications. 

[1.x.78] 



Now that we have coded all functions building the discrete linear system, it is about time that we actually solve it. 

[1.x.79] 



The solver of choice is conjugate gradient. 

[1.x.80] 



Now we are setting up the components of the multilevel preconditioner. First, we need transfer between grid levels. The object we are using here generates sparse matrices for these transfers. 

[1.x.81] 



Then, we need an exact solver for the matrix on the coarsest level. 

[1.x.82] 



While transfer and coarse grid solver are pretty much generic, more flexibility is offered for the smoother. First, we choose Gauss-Seidel as our smoothing method. 

[1.x.83] 



Do two smoothing steps on each level. 

[1.x.84] 



Since the SOR method is not symmetric, but we use conjugate gradient iteration below, here is a trick to make the multilevel preconditioner a symmetric operator even for nonsymmetric smoothers. 

[1.x.85] 



The smoother class optionally implements the variable V-cycle, which we do not want here. 

[1.x.86] 



Finally, we must wrap our matrices in an object having the required multiplication functions. 

[1.x.87] 



Now, we are ready to set up the V-cycle operator and the multilevel preconditioner. 

[1.x.88] 



Let us not forget the edge matrices needed because of the adaptive refinement. 

[1.x.89] 



After all preparations, wrap the Multigrid object into another object, which can be used as a regular preconditioner, 

[1.x.90] 



and use it to solve the system. 

[1.x.91] 



Another clone of the assemble function. The big difference to the previous ones is here that we also have an input vector. 

[1.x.92] 



The results of the estimator are stored in a vector with one entry per cell. Since cells in deal.II are not numbered, we have to create our own numbering in order to use this vector. For the assembler used below the information in which component of a vector the result is stored is transmitted by the user_index variable for each cell. We need to set this numbering up here.      


On the other hand, somebody might have used the user indices already. So, let's be good citizens and save them before tampering with them. 

[1.x.93] 



This starts like before, 

[1.x.94] 



but now we need to notify the info box of the finite element function we want to evaluate in the quadrature points. First, we create an AnyData object with this vector, which is the solution we just computed. 

[1.x.95] 



Then, we tell the  [2.x.20]  for cells, that we need the second derivatives of this solution (to compute the Laplacian). Therefore, the Boolean arguments selecting function values and first derivatives a false, only the last one selecting second derivatives is true. 

[1.x.96] 



On interior and boundary faces, we need the function values and the first derivatives, but not second derivatives. 

[1.x.97] 



And we continue as before, with the exception that the default update flags are already adjusted to the values and derivatives we requested above. 

[1.x.98] 



The assembler stores one number per cell, but else this is the same as in the computation of the right hand side. 

[1.x.99] 



Right before we return the result of the error estimate, we restore the old user indices. 

[1.x.100] 



Here we compare our finite element solution with the (known) exact solution and compute the mean quadratic error of the gradient and the function itself. This function is a clone of the estimation function right above. 




Since we compute the error in the energy and the [1.x.101]-norm, respectively, our block vector needs two blocks here. 

[1.x.102] 



Create graphical output. We produce the filename by collating the name from its various components, including the refinement cycle that we output with two digits. 

[1.x.103] 



And finally the adaptive loop, more or less like in previous examples. 

[1.x.104] 

[1.x.105][1.x.106] 


[1.x.107][1.x.108] 

First, the program produces the usual logfile here stored in <tt>deallog</tt>. It reads (with omission of intermediate steps) 

[1.x.109] 



This log for instance shows that the number of conjugate gradient iteration steps is constant at approximately 15. 

[1.x.110][1.x.111] 


 [2.x.21]  Using the perl script <tt>postprocess.pl</tt>, we extract relevant data into <tt>output.dat</tt>, which can be used to plot graphs with <tt>gnuplot</tt>. The graph above for instance was produced using the gnuplot script <tt>plot_errors.gpl</tt> via 

[1.x.112] 



Reference data can be found in <tt>output.reference.dat</tt>. [1.x.113] [1.x.114]  [2.x.22]  

 [2.x.23] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18] 

 [2.x.3]  

[1.x.19] 




 [2.x.4]  As a prerequisite of this program, you need to have both PETSc and the p4est library installed. The installation of deal.II together with these two additional libraries is described in the [1.x.20] file. Note also that to work properly, this program needs access to the Hypre preconditioner package implementing algebraic multigrid; it can be installed as part of PETSc but has to be explicitly enabled during PETSc configuration; see the page linked to from the installation instructions for PETSc. 


[1.x.21] [1.x.22][1.x.23] 


 [2.x.5]  

Given today's computers, most finite element computations can be done on a single machine. The majority of previous tutorial programs therefore shows only this, possibly splitting up work among a number of processors that, however, can all access the same, shared memory space. That said, there are problems that are simply too big for a single machine and in that case the problem has to be split up in a suitable way among multiple machines each of which contributes its part to the whole. A simple way to do that was shown in  [2.x.6]  and  [2.x.7] , where we show how a program can use [1.x.24] to parallelize assembling the linear system, storing it, solving it, and computing error estimators. All of these operations scale relatively trivially (for a definition of what it means for an operation to "scale", see  [2.x.8]  "this glossary entry"), but there was one significant drawback: for this to be moderately simple to implement, each MPI processor had to keep its own copy of the entire Triangulation and DoFHandler objects. Consequently, while we can suspect (with good reasons) that the operations listed above can scale to thousands of computers and problem sizes of billions of cells and billions of degrees of freedom, building the one big mesh for the entire problem these thousands of computers are solving on every last processor is clearly not going to scale: it is going to take forever, and maybe more importantly no single machine will have enough memory to store a mesh that has a billion cells (at least not at the time of writing this). In reality, programs like  [2.x.9]  and  [2.x.10]  can therefore not be run on more than maybe 100 or 200 processors and even there storing the Triangulation and DoFHandler objects consumes the vast majority of memory on each machine. 

Consequently, we need to approach the problem differently: to scale to very large problems each processor can only store its own little piece of the Triangulation and DoFHandler objects. deal.II implements such a scheme in the  [2.x.11]  namespace and the classes therein. It builds on an external library, [1.x.25] (a play on the expression [1.x.26] that describes the parallel storage of a hierarchically constructed mesh as a forest of quad- or oct-trees). You need to [1.x.27] but apart from that all of its workings are hidden under the surface of deal.II. 

In essence, what the  [2.x.12]  class and code inside the DoFHandler class do is to split the global mesh so that every processor only stores a small bit it "owns" along with one layer of "ghost" cells that surround the ones it owns. What happens in the rest of the domain on which we want to solve the partial differential equation is unknown to each processor and can only be inferred through communication with other machines if such information is needed. This implies that we also have to think about problems in a different way than we did in, for example,  [2.x.13]  and  [2.x.14] : no processor can have the entire solution vector for postprocessing, for example, and every part of a program has to be parallelized because no processor has all the information necessary for sequential operations. 

A general overview of how this parallelization happens is described in the  [2.x.15]  documentation module. You should read it for a top-level overview before reading through the source code of this program. A concise discussion of many terms we will use in the program is also provided in the  [2.x.16]  "Distributed Computing paper". It is probably worthwhile reading it for background information on how things work internally in this program. 


[1.x.28][1.x.29] 


This program essentially re-solves what we already do in  [2.x.17] , i.e. it solves the Laplace equation 

[1.x.30] 

The difference of course is now that we want to do so on a mesh that may have a billion cells, with a billion or so degrees of freedom. There is no doubt that doing so is completely silly for such a simple problem, but the point of a tutorial program is, after all, not to do something useful but to show how useful programs can be implemented using deal.II. Be that as it may, to make things at least a tiny bit interesting, we choose the right hand side as a discontinuous function, 

[1.x.31] 

so that the solution has a singularity along the sinusoidal line snaking its way through the domain. As a consequence, mesh refinement will be concentrated along this line. You can see this in the mesh picture shown below in the results section. 

Rather than continuing here and giving a long introduction, let us go straight to the program code. If you have read through  [2.x.18]  and the  [2.x.19]  documentation module, most of things that are going to happen should be familiar to you already. In fact, comparing the two programs you will notice that the additional effort necessary to make things work in %parallel is almost insignificant: the two programs have about the same number of lines of code (though  [2.x.20]  spends more space on dealing with coefficients and output). In either case, the comments below will only be on the things that set  [2.x.21]  apart from  [2.x.22]  and that aren't already covered in the  [2.x.23]  documentation module. 




 [2.x.24]  This program will be able to compute on as many processors as you want to throw at it, and for as large a problem as you have the memory and patience to solve. However, there [1.x.32] a limit: the number of unknowns can not exceed the largest number that can be stored with an object of type  [2.x.25]  By default, this is an alias for <code>unsigned int</code>, which on most machines today is a 32-bit integer, limiting you to some 4 billion (in reality, since this program uses PETSc, you will be limited to half that as PETSc uses signed integers). However, this can be changed during configuration to use 64-bit integers, see the ReadMe file. This will give problem sizes you are unlikely to exceed anytime soon. [1.x.33] [1.x.34] 


[1.x.35]  [1.x.36] 




Most of the include files we need for this program have already been discussed in previous programs. In particular, all of the following should already be familiar friends: 

[1.x.37] 



This program can use either PETSc or Trilinos for its parallel algebra needs. By default, if deal.II has been configured with PETSc, it will use PETSc. Otherwise, the following few lines will check that deal.II has been configured with Trilinos and take that. 




But there may be cases where you want to use Trilinos, even though deal.II has *also* been configured with PETSc, for example to compare the performance of these two libraries. To do this, add the following \#define to the source code:  [2.x.26]  




Using this logic, the following lines will then import either the PETSc or Trilinos wrappers into the namespace `LA` (for "linear algebra). In the former case, we are also defining the macro `USE_PETSC_LA` so that we can detect if we are using PETSc (see solve() for an example where this is necessary). 

[1.x.39] 



The following, however, will be new or be used in new roles. Let's walk through them. The first of these will provide the tools of the  [2.x.27]  namespace that we will use to query things like the number of processors associated with the current MPI universe, or the number within this universe the processor this job runs on is: 

[1.x.40] 



The next one provides a class, ConditionOStream that allows us to write code that would output things to a stream (such as  [2.x.28]  on every processor but throws the text away on all but one of them. We could achieve the same by simply putting an  [2.x.29]  statement in front of each place where we may generate output, but this doesn't make the code any prettier. In addition, the condition whether this processor should or should not produce output to the screen is the same every time -- and consequently it should be simple enough to put it into the statements that generate output itself. 

[1.x.41] 



After these preliminaries, here is where it becomes more interesting. As mentioned in the  [2.x.30]  module, one of the fundamental truths of solving problems on large numbers of processors is that there is no way for any processor to store everything (e.g. information about all cells in the mesh, all degrees of freedom, or the values of all elements of the solution vector). Rather, every processor will [1.x.42] a few of each of these and, if necessary, may [1.x.43] about a few more, for example the ones that are located on cells adjacent to the ones this processor owns itself. We typically call the latter [1.x.44], [1.x.45] or [1.x.46]. The point of this discussion here is that we need to have a way to indicate which elements a particular processor owns or need to know of. This is the realm of the IndexSet class: if there are a total of  [2.x.31]  cells, degrees of freedom, or vector elements, associated with (non-negative) integral indices  [2.x.32] , then both the set of elements the current processor owns as well as the (possibly larger) set of indices it needs to know about are subsets of the set  [2.x.33] . IndexSet is a class that stores subsets of this set in an efficient format: 

[1.x.47] 



The next header file is necessary for a single function,  [2.x.34]  The role of this function will be explained below. 

[1.x.48] 



The final two, new header files provide the class  [2.x.35]  that provides meshes distributed across a potentially very large number of processors, while the second provides the namespace  [2.x.36]  that offers functions that can adaptively refine such distributed meshes: 

[1.x.49] 




[1.x.50]  [1.x.51] 




Next let's declare the main class of this program. Its structure is almost exactly that of the  [2.x.37]  tutorial program. The only significant differences are: 

- The  [2.x.38]  variable that describes the set of processors we want this code to run on. In practice, this will be MPI_COMM_WORLD, i.e. all processors the batch scheduling system has assigned to this particular job. 

- The presence of the  [2.x.39]  variable of type ConditionOStream. 

- The obvious use of  [2.x.40]  instead of Triangulation. 

- The presence of two IndexSet objects that denote which sets of degrees of freedom (and associated elements of solution and right hand side vectors) we own on the current processor and which we need (as ghost elements) for the algorithms in this program to work. 

- The fact that all matrices and vectors are now distributed. We use either the PETSc or Trilinos wrapper classes so that we can use one of the sophisticated preconditioners offered by Hypre (with PETSc) or ML (with Trilinos). Note that as part of this class, we store a solution vector that does not only contain the degrees of freedom the current processor owns, but also (as ghost elements) all those vector elements that correspond to "locally relevant" degrees of freedom (i.e. all those that live on locally owned cells or the layer of ghost cells that surround it). 

[1.x.52] 




[1.x.53]  [1.x.54] 





[1.x.55]  [1.x.56] 




Constructors and destructors are rather trivial. In addition to what we do in  [2.x.41] , we set the set of processors we want to work on to all machines available (MPI_COMM_WORLD); ask the triangulation to ensure that the mesh remains smooth and free to refined islands, for example; and initialize the  [2.x.42]  variable to only allow processor zero to output anything. The final piece is to initialize a timer that we use to determine how much compute time the different parts of the program take: 

[1.x.57] 




[1.x.58]  [1.x.59] 




The following function is, arguably, the most interesting one in the entire program since it goes to the heart of what distinguishes %parallel  [2.x.43]  from sequential  [2.x.44] .    


At the top we do what we always do: tell the DoFHandler object to distribute degrees of freedom. Since the triangulation we use here is distributed, the DoFHandler object is smart enough to recognize that on each processor it can only distribute degrees of freedom on cells it owns; this is followed by an exchange step in which processors tell each other about degrees of freedom on ghost cell. The result is a DoFHandler that knows about the degrees of freedom on locally owned cells and ghost cells (i.e. cells adjacent to locally owned cells) but nothing about cells that are further away, consistent with the basic philosophy of distributed computing that no processor can know everything. 

[1.x.60] 



The next two lines extract some information we will need later on, namely two index sets that provide information about which degrees of freedom are owned by the current processor (this information will be used to initialize solution and right hand side vectors, and the system matrix, indicating which elements to store on the current processor and which to expect to be stored somewhere else); and an index set that indicates which degrees of freedom are locally relevant (i.e. live on cells that the current processor owns or on the layer of ghost cells around the locally owned cells; we need all of these degrees of freedom, for example, to estimate the error on the local cells). 

[1.x.61] 



Next, let us initialize the solution and right hand side vectors. As mentioned above, the solution vector we seek does not only store elements we own, but also ghost entries; on the other hand, the right hand side vector only needs to have the entries the current processor owns since all we will ever do is write into it, never read from it on locally owned cells (of course the linear solvers will read from it, but they do not care about the geometric location of degrees of freedom). 

[1.x.62] 



The next step is to compute hanging node and boundary value constraints, which we combine into a single object storing all constraints.      


As with all other things in %parallel, the mantra must be that no processor can store all information about the entire universe. As a consequence, we need to tell the AffineConstraints object for which degrees of freedom it can store constraints and for which it may not expect any information to store. In our case, as explained in the  [2.x.45]  module, the degrees of freedom we need to care about on each processor are the locally relevant ones, so we pass this to the  [2.x.46]  function. As a side note, if you forget to pass this argument, the AffineConstraints class will allocate an array with length equal to the largest DoF index it has seen so far. For processors with high MPI process number, this may be very large -- maybe on the order of billions. The program would then allocate more memory than for likely all other operations combined for this single array. 

[1.x.63] 



The last part of this function deals with initializing the matrix with accompanying sparsity pattern. As in previous tutorial programs, we use the DynamicSparsityPattern as an intermediate with which we then initialize the system matrix. To do so we have to tell the sparsity pattern its size but as above there is no way the resulting object will be able to store even a single pointer for each global degree of freedom; the best we can hope for is that it stores information about each locally relevant degree of freedom, i.e. all those that we may ever touch in the process of assembling the matrix (the  [2.x.47]  "distributed computing paper" has a long discussion why one really needs the locally relevant, and not the small set of locally active degrees of freedom in this context).      


So we tell the sparsity pattern its size and what DoFs to store anything for and then ask  [2.x.48]  to fill it (this function ignores all cells that are not locally owned, mimicking what we will do below in the assembly process). After this, we call a function that exchanges entries in these sparsity pattern between processors so that in the end each processor really knows about all the entries that will exist in that part of the finite element matrix that it will own. The final step is to initialize the matrix with the sparsity pattern. 

[1.x.64] 




[1.x.65]  [1.x.66] 




The function that then assembles the linear system is comparatively boring, being almost exactly what we've seen before. The points to watch out for are: 

- Assembly must only loop over locally owned cells. There are multiple ways to test that; for example, we could compare a cell's subdomain_id against information from the triangulation as in <code>cell->subdomain_id() == triangulation.locally_owned_subdomain()</code>, or skip all cells for which the condition <code>cell->is_ghost() || cell->is_artificial()</code> is true. The simplest way, however, is to simply ask the cell whether it is owned by the local processor. 

- Copying local contributions into the global matrix must include distributing constraints and boundary values. In other words, we cannot (as we did in  [2.x.49] ) first copy every local contribution into the global matrix and only in a later step take care of hanging node constraints and boundary values. The reason is, as discussed in  [2.x.50] , that the parallel vector classes do not provide access to arbitrary elements of the matrix once they have been assembled into it -- in parts because they may simply no longer reside on the current processor but have instead been shipped to a different machine. 

- The way we compute the right hand side (given the formula stated in the introduction) may not be the most elegant but will do for a program whose focus lies somewhere entirely different. 

[1.x.67] 



Notice that the assembling above is just a local operation. So, to form the "global" linear system, a synchronization between all processors is needed. This could be done by invoking the function compress(). See  [2.x.51]  "Compressing distributed objects" for more information on what is compress() designed to do. 

[1.x.68] 




[1.x.69]  [1.x.70] 




Even though solving linear systems on potentially tens of thousands of processors is by far not a trivial job, the function that does this is -- at least at the outside -- relatively simple. Most of the parts you've seen before. There are really only two things worth mentioning: 

- Solvers and preconditioners are built on the deal.II wrappers of PETSc and Trilinos functionality. It is relatively well known that the primary bottleneck of massively %parallel linear solvers is not actually the communication between processors, but the fact that it is difficult to produce preconditioners that scale well to large numbers of processors. Over the second half of the first decade of the 21st century, it has become clear that algebraic multigrid (AMG) methods turn out to be extremely efficient in this context, and we will use one of them -- either the BoomerAMG implementation of the Hypre package that can be interfaced to through PETSc, or a preconditioner provided by ML, which is part of Trilinos -- for the current program. The rest of the solver itself is boilerplate and has been shown before. Since the linear system is symmetric and positive definite, we can use the CG method as the outer solver. 

- Ultimately, we want a vector that stores not only the elements of the solution for degrees of freedom the current processor owns, but also all other locally relevant degrees of freedom. On the other hand, the solver itself needs a vector that is uniquely split between processors, without any overlap. We therefore create a vector at the beginning of this function that has these properties, use it to solve the linear system, and only assign it to the vector we want at the very end. This last step ensures that all ghost elements are also copied as necessary. 

[1.x.71] 




[1.x.72]  [1.x.73] 




The function that estimates the error and refines the grid is again almost exactly like the one in  [2.x.52] . The only difference is that the function that flags cells to be refined is now in namespace  [2.x.53]  -- a namespace that has functions that can communicate between all involved processors and determine global thresholds to use in deciding which cells to refine and which to coarsen.    


Note that we didn't have to do anything special about the KellyErrorEstimator class: we just give it a vector with as many elements as the local triangulation has cells (locally owned cells, ghost cells, and artificial ones), but it only fills those entries that correspond to cells that are locally owned. 

[1.x.74] 




[1.x.75]  [1.x.76] 




Compared to the corresponding function in  [2.x.54] , the one here is a tad more complicated. There are two reasons: the first one is that we do not just want to output the solution but also for each cell which processor owns it (i.e. which "subdomain" it is in). Secondly, as discussed at length in  [2.x.55]  and  [2.x.56] , generating graphical data can be a bottleneck in parallelizing. In  [2.x.57] , we have moved this step out of the actual computation but shifted it into a separate program that later combined the output from various processors into a single file. But this doesn't scale: if the number of processors is large, this may mean that the step of combining data on a single processor later becomes the longest running part of the program, or it may produce a file that's so large that it can't be visualized any more. We here follow a more sensible approach, namely creating individual files for each MPI process and leaving it to the visualization program to make sense of that.    


To start, the top of the function looks like it usually does. In addition to attaching the solution vector (the one that has entries for all locally relevant, not only the locally owned, elements), we attach a data vector that stores, for each cell, the subdomain the cell belongs to. This is slightly tricky, because of course not every processor knows about every cell. The vector we attach therefore has an entry for every cell that the current processor has in its mesh (locally owned ones, ghost cells, and artificial cells), but the DataOut class will ignore all entries that correspond to cells that are not owned by the current processor. As a consequence, it doesn't actually matter what values we write into these vector entries: we simply fill the entire vector with the number of the current MPI process (i.e. the subdomain_id of the current process); this correctly sets the values we care for, i.e. the entries that correspond to locally owned cells, while providing the wrong value for all other elements -- but these are then ignored anyway. 

[1.x.77] 



The next step is to write this data to disk. We write up to 8 VTU files in parallel with the help of MPI-IO. Additionally a PVTU record is generated, which groups the written VTU files. 

[1.x.78] 




[1.x.79]  [1.x.80] 




The function that controls the overall behavior of the program is again like the one in  [2.x.58] . The minor difference are the use of  [2.x.59]  for output to the console (see also  [2.x.60] ) and that we only generate graphical output if at most 32 processors are involved. Without this limit, it would be just too easy for people carelessly running this program without reading it first to bring down the cluster interconnect and fill any file system available :-)    


A functional difference to  [2.x.61]  is the use of a square domain and that we start with a slightly finer mesh (5 global refinement cycles) -- there just isn't much of a point showing a massively %parallel program starting on 4 cells (although admittedly the point is only slightly stronger starting on 1024). 

[1.x.81] 




[1.x.82]  [1.x.83] 




The final function,  [2.x.62] , again has the same structure as in all other programs, in particular  [2.x.63] . Like the other programs that use MPI, we have to initialize and finalize MPI, which is done using the helper object  [2.x.64]  The constructor of that class also initializes libraries that depend on MPI, such as p4est, PETSc, SLEPc, and Zoltan (though the last two are not used in this tutorial). The order here is important: we cannot use any of these libraries until they are initialized, so it does not make sense to do anything before creating an instance of  [2.x.65]  




After the solver finishes, the LaplaceProblem destructor will run followed by  [2.x.66]  This order is also important:  [2.x.67]  calls  [2.x.68]  (and finalization functions for other libraries), which will delete any in-use PETSc objects. This must be done after we destruct the Laplace solver to avoid double deletion errors. Fortunately, due to the order of destructor call rules of C++, we do not need to worry about any of this: everything happens in the correct order (i.e., the reverse of the order of construction). The last function called by  [2.x.69]  is  [2.x.70] : i.e., once this object is destructed the program should exit since MPI will no longer be available. 

[1.x.84] 

[1.x.85][1.x.86] 


When you run the program, on a single processor or with your local MPI installation on a few, you should get output like this: 

[1.x.87] 



The exact numbers differ, depending on how many processors we use; this is due to the fact that the preconditioner depends on the partitioning of the problem, the solution then differs in the last few digits, and consequently the mesh refinement differs slightly. The primary thing to notice here, though, is that the number of iterations does not increase with the size of the problem. This guarantees that we can efficiently solve even the largest problems. 

When run on a sufficiently large number of machines (say a few thousand), this program can relatively easily solve problems with well over one billion unknowns in less than a minute. On the other hand, such big problems can no longer be visualized, so we also ran the program on only 16 processors. Here are a mesh, along with its partitioning onto the 16 processors, and the corresponding solution: 

 [2.x.71]  

The mesh on the left has a mere 7,069 cells. This is of course a problem we would easily have been able to solve already on a single processor using  [2.x.72] , but the point of the program was to show how to write a program that scales to many more machines. For example, here are two graphs that show how the run time of a large number of parts of the program scales on problems with around 52 and 375 million degrees of freedom if we take more and more processors (these and the next couple of graphs are taken from an earlier version of the  [2.x.73]  "Distributed Computing paper"; updated graphs showing data of runs on even larger numbers of processors, and a lot more interpretation can be found in the final version of the paper): 

 [2.x.74]  

As can clearly be seen, the program scales nicely to very large numbers of processors. (For a discussion of what we consider "scalable" programs, see  [2.x.75]  "this glossary entry".) The curves, in particular the linear solver, become a bit wobbly at the right end of the graphs since each processor has too little to do to offset the cost of communication (the part of the whole problem each processor has to solve in the above two examples is only 13,000 and 90,000 degrees of freedom when 4,096 processors are used; a good rule of thumb is that parallel programs work well if each processor has at least 100,000 unknowns). 

While the strong scaling graphs above show that we can solve a problem of fixed size faster and faster if we take more and more processors, the more interesting question may be how big problems can become so that they can still be solved within a reasonable time on a machine of a particular size. We show this in the following two graphs for 256 and 4096 processors: 

 [2.x.76]  

What these graphs show is that all parts of the program scale linearly with the number of degrees of freedom. This time, lines are wobbly at the left as the size of local problems is too small. For more discussions of these results we refer to the  [2.x.77]  "Distributed Computing paper". 

So how large are the largest problems one can solve? At the time of writing this problem, the limiting factor is that the program uses the BoomerAMG algebraic multigrid method from the [1.x.88] as a preconditioner, which unfortunately uses signed 32-bit integers to index the elements of a %distributed matrix. This limits the size of problems to  [2.x.78]  degrees of freedom. From the graphs above it is obvious that the scalability would extend beyond this number, and one could expect that given more than the 4,096 machines shown above would also further reduce the compute time. That said, one can certainly expect that this limit will eventually be lifted by the hypre developers. 

On the other hand, this does not mean that deal.II cannot solve bigger problems. Indeed,  [2.x.79]  shows how one can solve problems that are not just a little, but very substantially larger than anything we have shown here. 




[1.x.89] [1.x.90][1.x.91] 


In a sense, this program is the ultimate solver for the Laplace equation: it can essentially solve the equation to whatever accuracy you want, if only you have enough processors available. Since the Laplace equation by itself is not terribly interesting at this level of accuracy, the more interesting possibilities for extension therefore concern not so much this program but what comes beyond it. For example, several of the other programs in this tutorial have significant run times, especially in 3d. It would therefore be interesting to use the techniques explained here to extend other programs to support parallel distributed computations. We have done this for  [2.x.80]  in the  [2.x.81]  tutorial program, but the same would apply to, for example,  [2.x.82]  and  [2.x.83]  for hyperbolic time dependent problems,  [2.x.84]  for gas dynamics, or  [2.x.85]  for the Navier-Stokes equations. 

Maybe equally interesting is the problem of postprocessing. As mentioned above, we only show pictures of the solution and the mesh for 16 processors because 4,096 processors solving 1 billion unknowns would produce graphical output on the order of several 10 gigabyte. Currently, no program is able to visualize this amount of data in any reasonable way unless it also runs on at least several hundred processors. There are, however, approaches where visualization programs directly communicate with solvers on each processor with each visualization process rendering the part of the scene computed by the solver on this processor. Implementing such an interface would allow to quickly visualize things that are otherwise not amenable to graphical display. [1.x.92] [1.x.93]  [2.x.86]  

 [2.x.87] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27] 

 [2.x.3]  

[1.x.28] 


[1.x.29] [1.x.30][1.x.31] 


This example is based on the Laplace equation in 2d and deals with the question what happens if a membrane is deflected by some external force but is also constrained by an obstacle. In other words, think of a elastic membrane clamped at the boundary to a rectangular frame (we choose  [2.x.4] ) and that sags through due to gravity acting on it. What happens now if there is an obstacle under the membrane that prevents it from reaching its equilibrium position if gravity was the only existing force? In the current example program, we will consider that under the membrane is a stair step obstacle against which gravity pushes the membrane. 

This problem is typically called the "obstacle problem" (see also [1.x.32]), and it results in a variational inequality, rather than a variational equation when put into the weak form. We will below derive it from the classical formulation, but before we go on to discuss the mathematics let us show how the solution of the problem we will consider in this tutorial program looks to gain some intuition of what we should expect: 

 [2.x.5]  

Here, at the left, we see the displacement of the membrane. The shape of the obstacle underneath is clearly visible. On the right, we overlay which parts of the membrane are in contact with the obstacle. We will later call this set of points the "active set" to indicate that an inequality constraint is active there. 


[1.x.33][1.x.34] 


The classical formulation of the problem possesses the following form: 

[1.x.35] 

with  [2.x.6] .   [2.x.7]  is a scalar valued function that denotes the vertical displacement of the membrane. The first equation is called equilibrium condition with a force of areal density  [2.x.8] . Here, we will consider this force to be gravity. The second one is known as Hooke's Law that says that the stresses  [2.x.9]  are proportional to the gradient of the displacements  [2.x.10]  (the proportionality constant, often denoted by  [2.x.11] , has been set to one here, without loss of generality; if it is constant, it can be put into the right hand side function). At the boundary we have zero Dirichlet conditions. Obviously, the first two equations can be combined to yield  [2.x.12] . 

Intuitively, gravity acts downward and so  [2.x.13]  is a negative function (we choose  [2.x.14]  in this program). The first condition then means that the total force acting on the membrane is gravity plus something positive: namely the upward force that the obstacle exerts on the membrane at those places where the two of them are in contact. How big is this additional force? We don't know yet (and neither do we know "where" it actually acts) but it must be so that the membrane doesn't penetrate the obstacle. 

The fourth equality above together with the last inequality forms the obstacle condition which has to hold at every point of the whole domain. The latter of these two means that the membrane must be above the obstacle  [2.x.15]  everywhere. The second to last equation, often called the "complementarity condition" says that where the membrane is not in contact with the obstacle (i.e., those  [2.x.16]  where  [2.x.17] ), then  [2.x.18]  at these locations; in other words, no additional forces act there, as expected. On the other hand, where  [2.x.19]  we can have  [2.x.20] , i.e., there can be additional forces (though there don't have to be: it is possible for the membrane to just touch, not press against, the obstacle). 


[1.x.36][1.x.37] 


An obvious way to obtain the variational formulation of the obstacle problem is to consider the total potential energy: [1.x.38] 

We have to find a solution  [2.x.21]  of the following minimization problem: [1.x.39] 

with the convex set of admissible displacements: [1.x.40] 

This set takes care of the third and fifth conditions above (the boundary values and the complementarity condition). 

Consider now the minimizer  [2.x.22]  of  [2.x.23]  and any other function  [2.x.24] . Then the function [1.x.41] 

takes its minimum at  [2.x.25]  (because  [2.x.26]  is a minimizer of the energy functional  [2.x.27] ), so that  [2.x.28]  for any choice of  [2.x.29] . Note that  [2.x.30]  because of the convexity of  [2.x.31] . If we compute  [2.x.32]  it yields the variational formulation we are searching for: 

[1.x.42] [1.x.43] 



This is the typical form of variational inequalities, where not just  [2.x.33]  appears in the bilinear form but in fact  [2.x.34] . The reason is this: if  [2.x.35]  is not constrained, then we can find test functions  [2.x.36]  in  [2.x.37]  so that  [2.x.38]  can have any sign. By choosing test functions  [2.x.39]  so that  [2.x.40]  it follows that the inequality can only hold for both  [2.x.41]  and  [2.x.42]  if the two sides are in fact equal, i.e., we obtain a variational equality. 

On the other hand, if  [2.x.43]  then  [2.x.44]  only allows test functions  [2.x.45]  so that in fact  [2.x.46] . This means that we can't test the equation with both  [2.x.47]  and  [2.x.48]  as above, and so we can no longer conclude that the two sides are in fact equal. Thus, this mimics the way we have discussed the complementarity condition above. 




[1.x.44][1.x.45] 


The variational inequality above is awkward to work with. We would therefore like to reformulate it as an equivalent saddle point problem. We introduce a Lagrange multiplier  [2.x.49]  and the convex cone  [2.x.50] ,  [2.x.51]  dual space of  [2.x.52] ,  [2.x.53]  of Lagrange multipliers, where  [2.x.54]  denotes the duality pairing between  [2.x.55]  and  [2.x.56] . Intuitively,  [2.x.57]  is the cone of all "non-positive functions", except that  [2.x.58]  and so contains other objects besides regular functions as well. This yields: 

[1.x.46] 

[1.x.47] 

[1.x.48] 

[1.x.49] 

In other words, we can consider  [2.x.59]  as the negative of the additional, positive force that the obstacle exerts on the membrane. The inequality in the second line of the statement above only appears to have the wrong sign because we have  [2.x.60]  at points where  [2.x.61] , given the definition of  [2.x.62] . 

The existence and uniqueness of  [2.x.63]  of this saddle point problem has been stated in Glowinski, Lions and Tr&eacute;moli&egrave;res: Numerical Analysis of Variational Inequalities, North-Holland, 1981. 




[1.x.50][1.x.51] 


There are different methods to solve the variational inequality. As one possibility you can understand the saddle point problem as a convex quadratic program (QP) with inequality constraints. 

To get there, let us assume that we discretize both  [2.x.64]  and  [2.x.65]  with the same finite element space, for example the usual  [2.x.66]  spaces. We would then get the equations [1.x.52] 

where  [2.x.67]  is the mass matrix on the chosen finite element space and the indices  [2.x.68]  above are for all degrees of freedom in the set  [2.x.69]  of degrees of freedom located in the interior of the domain (we have Dirichlet conditions on the perimeter). However, we can make our life simpler if we use a particular quadrature rule when assembling all terms that yield this mass matrix, namely a quadrature formula where quadrature points are only located at the interpolation points at which shape functions are defined; since all but one shape function are zero at these locations, we get a diagonal mass matrix with 

[1.x.53] 

To define  [2.x.70]  we use the same technique as for  [2.x.71] . In other words, we define 

[1.x.54] 

where  [2.x.72]  is a suitable approximation of  [2.x.73] . The integral in the definition of  [2.x.74]  and  [2.x.75]  are then approximated by the trapezoidal rule. With this, the equations above can be restated as [1.x.55] 



Now we define for each degree of freedom  [2.x.76]  the function [1.x.56] 

with some  [2.x.77] . (In this program we choose  [2.x.78] . It is a kind of a penalty parameter which depends on the problem itself and needs to be chosen large enough; for example there is no convergence for  [2.x.79]  using the current program if we use 7 global refinements.) 

After some head-scratching one can then convince oneself that the inequalities above can equivalently be rewritten as [1.x.57] 

The primal-dual active set strategy we will use here is an iterative scheme which is based on this condition to predict the next active and inactive sets  [2.x.80]  and  [2.x.81]  (that is, those complementary sets of indices  [2.x.82]  for which  [2.x.83]  is either equal to or not equal to the value of the obstacle  [2.x.84] ). For a more in depth treatment of this approach, see Hintermueller, Ito, Kunisch: The primal-dual active set strategy as a semismooth newton method, SIAM J. OPTIM., 2003, Vol. 13, No. 3, pp. 865-888. 

[1.x.58][1.x.59] 


The algorithm for the primal-dual active set method works as follows (NOTE:  [2.x.85] ): 

1. Initialize  [2.x.86]  and  [2.x.87] , such that   [2.x.88]  and   [2.x.89]  and set  [2.x.90] . 2. Find the primal-dual pair  [2.x.91]  that satisfies  [1.x.60] 

 Note that the second and third conditions imply that exactly  [2.x.92]  unknowns  are fixed, with the first condition yielding the remaining  [2.x.93]  equations  necessary to determine both  [2.x.94]  and  [2.x.95] . 3. Define the new active and inactive sets by  [1.x.61] 

4. If  [2.x.96]  (and then, obviously, also   [2.x.97] ) then stop, else set  [2.x.98]  and go to step  (2). 

The method is called "primal-dual" because it uses both primal (the displacement  [2.x.99] ) as well as dual variables (the Lagrange multiplier  [2.x.100] ) to determine the next active set. 

At the end of this section, let us add two observations. First, for any primal-dual pair  [2.x.101]  that satisfies these condition, we can distinguish the following cases: 

1.  [2.x.102]  (i active):    [2.x.103]    Then either  [2.x.104]  and  [2.x.105]  (penetration) or  [2.x.106]  and  [2.x.107]  (pressing load). 2.  [2.x.108]  (i inactive):    [2.x.109]    Then either  [2.x.110]  and  [2.x.111]  (no contact) or  [2.x.112]  and  [2.x.113]  (unpressing load). 

Second, the method above appears intuitively correct and useful but a bit ad hoc. However, it can be derived in a concisely in the following way. To this end, note that we'd like to solve the nonlinear system [1.x.62] 

We can iteratively solve this by always linearizing around the previous iterate (i.e., applying a Newton method), but for this we need to linearize the function  [2.x.114]  that is not differentiable. That said, it is slantly differentiable, and in fact we have [1.x.63] 

[1.x.64] 

This suggest a semismooth Newton step of the form [1.x.65] 

where we have split matrices  [2.x.115]  as well as vectors in the natural way into rows and columns whose indices belong to either the active set  [2.x.116]  or the inactive set  [2.x.117] . 

Rather than solving for updates  [2.x.118] , we can also solve for the variables we are interested in right away by setting  [2.x.119]  and  [2.x.120]  and bringing all known terms to the right hand side. This yields [1.x.66] 

These are the equations outlined above in the description of the basic algorithm. 

We could even drive this a bit further. It's easy to see that we can eliminate the third row and the third column because it implies  [2.x.121] : [1.x.67] 

This shows that one in fact only needs to solve for the Lagrange multipliers located on the active set. By considering the second row one would then recover the full Lagrange multiplier vector through [1.x.68] 

Because of the third row and the fact that  [2.x.122]  is a diagonal matrix we are able to calculate  [2.x.123]  directly. We can therefore also write the linear system as follows: [1.x.69] 

Fortunately, this form is easy to arrive at: we simply build the usual Laplace linear system [1.x.70] 

and then let the AffineConstraints class eliminate all constrained degrees of freedom, namely  [2.x.124] , in the same way as if the dofs in  [2.x.125]  were Dirichlet data. The result linear system (the second to last one above) is symmetric and positive definite and we solve it with a CG-method and the AMG preconditioner from Trilinos. 


[1.x.71][1.x.72] 


This tutorial is quite similar to  [2.x.126] . The general structure of the program follows  [2.x.127]  with minor differences: 

- We need two new methods,  [2.x.128]  and    [2.x.129] . 

- We need new member variables that denote the constraints we have here. 

- We change the preconditioner for the solver. 


You may want to read up on  [2.x.130]  if you want to understand the current program. [1.x.73] [1.x.74] 


[1.x.75]  [1.x.76] 




As usual, at the beginning we include all the header files we need in here. With the exception of the various files that provide interfaces to the Trilinos library, there are no surprises: 

[1.x.77] 




[1.x.78]  [1.x.79] 




This class supplies all function and variables needed to describe the obstacle problem. It is close to what we had to do in  [2.x.131] , and so relatively simple. The only real new components are the update_solution_and_constraints function that computes the active set and a number of variables that are necessary to describe the original (unconstrained) form of the linear system ( [2.x.132]  and  [2.x.133] ) as well as the active set itself and the diagonal of the mass matrix  [2.x.134]  used in scaling Lagrange multipliers in the active set formulation. The rest is as in  [2.x.135] : 

[1.x.80] 




[1.x.81]  [1.x.82] 




In the following, we define classes that describe the right hand side function, the Dirichlet boundary values, and the height of the obstacle as a function of  [2.x.136] . In all three cases, we derive these classes from Function@<dim@>, although in the case of  [2.x.137]  and  [2.x.138]  this is more out of convention than necessity since we never pass such objects to the library. In any case, the definition of the right hand side and boundary values classes is obvious given our choice of  [2.x.139] ,  [2.x.140] : 

[1.x.83] 



We describe the obstacle function by a cascaded barrier (think: stair steps): 

[1.x.84] 




[1.x.85]  [1.x.86] 










[1.x.87]  [1.x.88] 




To everyone who has taken a look at the first few tutorial programs, the constructor is completely obvious: 

[1.x.89] 




[1.x.90]  [1.x.91] 




We solve our obstacle problem on the square  [2.x.141]  in 2D. This function therefore just sets up one of the simplest possible meshes. 

[1.x.92] 




[1.x.93]  [1.x.94] 




In this first function of note, we set up the degrees of freedom handler, resize vectors and matrices, and deal with the constraints. Initially, the constraints are, of course, only given by boundary values, so we interpolate them towards the top of the function. 

[1.x.95] 



The only other thing to do here is to compute the factors in the  [2.x.142]  matrix which is used to scale the residual. As discussed in the introduction, we'll use a little trick to make this mass matrix diagonal, and in the following then first compute all of this as a matrix and then extract the diagonal elements for later use: 

[1.x.96] 




[1.x.97]  [1.x.98] 




This function at once assembles the system matrix and right-hand-side and applied the constraints (both due to the active set as well as from boundary values) to our system. Otherwise, it is functionally equivalent to the corresponding function in, for example,  [2.x.143] . 

[1.x.99] 




[1.x.100]  [1.x.101] 




The next function is used in the computation of the diagonal mass matrix  [2.x.144]  used to scale variables in the active set method. As discussed in the introduction, we get the mass matrix to be diagonal by choosing the trapezoidal rule for quadrature. Doing so we don't really need the triple loop over quadrature points, indices  [2.x.145]  and indices  [2.x.146]  any more and can, instead, just use a double loop. The rest of the function is obvious given what we have discussed in many of the previous tutorial programs.    


Note that at the time this function is called, the constraints object only contains boundary value constraints; we therefore do not have to pay attention in the last copy-local-to-global step to preserve the values of matrix entries that may later on be constrained by the active set.    


Note also that the trick with the trapezoidal rule only works if we have in fact  [2.x.147]  elements. For higher order elements, one would need to use a quadrature formula that has quadrature points at all the support points of the finite element. Constructing such a quadrature formula isn't really difficult, but not the point here, and so we simply assert at the top of the function that our implicit assumption about the finite element is in fact satisfied. 

[1.x.102] 




[1.x.103]  [1.x.104] 




In a sense, this is the central function of this program.  It updates the active set of constrained degrees of freedom as discussed in the introduction and computes an AffineConstraints object from it that can then be used to eliminate constrained degrees of freedom from the solution of the next iteration. At the same time we set the constrained degrees of freedom of the solution to the correct value, namely the height of the obstacle.    


Fundamentally, the function is rather simple: We have to loop over all degrees of freedom and check the sign of the function  [2.x.148]  because in our case  [2.x.149] . To this end, we use the formula given in the introduction by which we can compute the Lagrange multiplier as the residual of the original linear system (given via the variables  [2.x.150] . At the top of this function, we compute this residual using a function that is part of the matrix classes. 

[1.x.105] 



compute contact_force[i] = - lambda[i] * diagonal_of_mass_matrix[i] 

[1.x.106] 



The next step is to reset the active set and constraints objects and to start the loop over all degrees of freedom. This is made slightly more complicated by the fact that we can't just loop over all elements of the solution vector since there is no way for us then to find out what location a DoF is associated with; however, we need this location to test whether the displacement of a DoF is larger or smaller than the height of the obstacle at this location.      


We work around this by looping over all cells and DoFs defined on each of these cells. We use here that the displacement is described using a  [2.x.151]  function for which degrees of freedom are always located on the vertices of the cell; thus, we can get the index of each degree of freedom and its location by asking the vertex for this information. On the other hand, this clearly wouldn't work for higher order elements, and so we add an assertion that makes sure that we only deal with elements for which all degrees of freedom are located in vertices to avoid tripping ourselves with non-functional code in case someone wants to play with increasing the polynomial degree of the solution.      


The price to pay for having to loop over cells rather than DoFs is that we may encounter some degrees of freedom more than once, namely each time we visit one of the cells adjacent to a given vertex. We will therefore have to keep track which vertices we have already touched and which we haven't so far. We do so by using an array of flags  [2.x.152] : 

[1.x.107] 



Now that we know that we haven't touched this DoF yet, let's get the value of the displacement function there as well as the value of the obstacle function and use this to decide whether the current DoF belongs to the active set. For that we use the function given above and in the introduction.            


If we decide that the DoF should be part of the active set, we add its index to the active set, introduce an inhomogeneous equality constraint in the AffineConstraints object, and reset the solution value to the height of the obstacle. Finally, the residual of the non-contact part of the system serves as an additional control (the residual equals the remaining, unaccounted forces, and should be zero outside the contact zone), so we zero out the components of the residual vector (i.e., the Lagrange multiplier lambda) that correspond to the area where the body is in contact; at the end of the loop over all cells, the residual will therefore only consist of the residual in the non-contact zone. We output the norm of this residual along with the size of the active set after the loop. 

[1.x.108] 



In a final step, we add to the set of constraints on DoFs we have so far from the active set those that result from Dirichlet boundary values, and close the constraints object: 

[1.x.109] 




[1.x.110]  [1.x.111] 




There is nothing to say really about the solve function. In the context of a Newton method, we are not typically interested in very high accuracy (why ask for a highly accurate solution of a linear problem that we know only gives us an approximation of the solution of the nonlinear problem), and so we use the ReductionControl class that stops iterations when either an absolute tolerance is reached (for which we choose  [2.x.153] ) or when the residual is reduced by a certain factor (here,  [2.x.154] ). 

[1.x.112] 




[1.x.113]  [1.x.114] 




We use the vtk-format for the output.  The file contains the displacement and a numerical representation of the active set. 

[1.x.115] 




[1.x.116]  [1.x.117] 




This is the function which has the top-level control over everything.  It is not very long, and in fact rather straightforward: in every iteration of the active set method, we assemble the linear system, solve it, update the active set and project the solution back to the feasible set, and then output the results. The iteration is terminated whenever the active set has not changed in the previous iteration.    


The only trickier part is that we have to save the linear system (i.e., the matrix and right hand side) after assembling it in the first iteration. The reason is that this is the only step where we can access the linear system as built without any of the contact constraints active. We need this to compute the residual of the solution at other iterations, but in other iterations that linear system we form has the rows and columns that correspond to constrained degrees of freedom eliminated, and so we can no longer access the full residual of the original equation. 

[1.x.118] 




[1.x.119]  [1.x.120] 




And this is the main function. It follows the pattern of all other main functions. The call to initialize MPI exists because the Trilinos library upon which we build our linear solvers in this program requires it. 

[1.x.121] 



This program can only be run in serial. Otherwise, throw an exception. 

[1.x.122] 

[1.x.123][1.x.124] 


Running the program produces output like this: 

[1.x.125] 



The iterations end once the active set doesn't change any more (it has 5,399 constrained degrees of freedom at that point). The algebraic precondition is apparently working nicely since we only need 4-6 CG iterations to solve the linear system (although this also has a lot to do with the fact that we are not asking for very high accuracy of the linear solver). 

More revealing is to look at a sequence of graphical output files (every third step is shown, with the number of the iteration in the leftmost column): 

 [2.x.155]  

The pictures show that in the first step, the solution (which has been computed without any of the constraints active) bends through so much that pretty much every interior point has to be bounced back to the stairstep function, producing a discontinuous solution. Over the course of the active set iterations, this unphysical membrane shape is smoothed out, the contact with the lower-most stair step disappears, and the solution stabilizes. 

In addition to this, the program also outputs the values of the Lagrange multipliers. Remember that these are the contact forces and so should only be positive on the contact set, and zero outside. If, on the other hand, a Lagrange multiplier is negative in the active set, then this degree of freedom must be removed from the active set. The following pictures show the multipliers in iterations 1, 9 and 18, where we use red and browns to indicate positive values, and blue for negative values. 

 [2.x.156]  

It is easy to see that the positive values converge nicely to moderate values in the interior of the contact set and large upward forces at the edges of the steps, as one would expect (to support the large curvature of the membrane there); at the fringes of the active set, multipliers are initially negative, causing the set to shrink until, in iteration 18, there are no more negative multipliers and the algorithm has converged. 




[1.x.126] [1.x.127][1.x.128] 


As with any of the programs of this tutorial, there are a number of obvious possibilities for extensions and experiments. The first one is clear: introduce adaptivity. Contact problems are prime candidates for adaptive meshes because the solution has lines along which it is less regular (the places where contact is established between membrane and obstacle) and other areas where the solution is very smooth (or, in the present context, constant wherever it is in contact with the obstacle). Adding this to the current program should not pose too many difficulties, but it is not trivial to find a good error estimator for that purpose. 

A more challenging task would be an extension to 3d. The problem here is not so much to simply make everything run in 3d. Rather, it is that when a 3d body is deformed and gets into contact with an obstacle, then the obstacle does not act as a constraining body force within the domain as is the case here. Rather, the contact force only acts on the boundary of the object. The inequality then is not in the differential equation but in fact in the (Neumann-type) boundary conditions, though this leads to a similar kind of variational inequality. Mathematically, this means that the Lagrange multiplier only lives on the surface, though it can of course be extended by zero into the domain if that is convenient. As in the current program, one does not need to form and store this Lagrange multiplier explicitly. 

A further interesting problem for the 3d case is to consider contact problems with friction. In almost every mechanical process friction has a big influence. For the modelling we have to take into account tangential stresses at the contact surface. Also we have to observe that friction adds another nonlinearity to our problem. 

Another nontrivial modification is to implement a more complex constitutive law like nonlinear elasticity or elasto-plastic  material behavior. The difficulty here is to handle the additional nonlinearity arising through the nonlinear constitutive law. [1.x.129] [1.x.130]  [2.x.157]  

 [2.x.158] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38] 

 [2.x.4]  

[1.x.39][1.x.40] 




[1.x.41] [1.x.42][1.x.43] 


This example is an extension of  [2.x.5] , considering a 3d contact problem with an elasto-plastic material behavior with isotropic hardening in three dimensions. In other words, it considers how a three-dimensional body deforms if one pushes into it a rigid obstacle (the contact problem) where deformation is governed by an elasto-plastic material law (a material that can only accommodate a certain maximal stress) that hardens as deformation accumulates. To show what we intend to do before going into too many details, let us just show a picture of what the solution will look like (the deformable body is a cube - only half of which is actually shown -, the obstacle corresponds to a Chinese character that is discussed below): 

 [2.x.6]  


This problem description implies that we have to take care of an additional nonlinearity compared to  [2.x.7] : the material behavior. Since we consider a three dimensional problem here, we also have to account for the fact that the contact area is at the boundary of the deformable body now, rather than in the interior. Finally, compared to  [2.x.8] , we also have to deal with hanging nodes in both the handling of the linear system as well as of the inequality constraints as we would like to use an adaptive mesh; in the latter case, we will have to deal with prioritizing whether the constraints from the hanging nodes or from the inequalities are more important. 

Since you can very easily reach a few million degrees of freedom in three dimensions, even with adaptive mesh refinement, we decided to use Trilinos and p4est to run our code in parallel, building on the framework of  [2.x.9]  for the parallelization. Additional pointers for parallelization can be found in  [2.x.10] . 


[1.x.44][1.x.45] 


The classical formulation of the problem possesses the following form: 

[1.x.46] 

Here, the first of these equations defines the relationship between strain  [2.x.11]  and stress  [2.x.12]  via the fourth-order compliance tensor  [2.x.13] ;  [2.x.14]  provides the plastic component of the strain to ensure that the stress does not exceed the yield stress. We will only consider isotropic materials for which  [2.x.15]  can be expressed in terms of the Lam&eacute; moduli  [2.x.16]  and  [2.x.17]  or alternatively in terms of the bulk modulus  [2.x.18]  and  [2.x.19] . The second equation is the force balance; we will here not consider any body forces and henceforth assume that  [2.x.20] . The complementarity condition in the third line implies that  [2.x.21]  if  [2.x.22]  but that  [2.x.23]  may be a nonzero tensor if and only if  [2.x.24] , and in particular that in this case  [2.x.25]  must point in the direction  [2.x.26] . The inequality  [2.x.27]  is a statement of the fact that plastic materials can only support a finite amount of stress; in other words, they react with plastic deformations  [2.x.28]  if external forces would result in a stress  [2.x.29]  for which  [2.x.30]  would result. A typical form for this [1.x.47] is  [2.x.31]  where  [2.x.32]  is the deviatoric part of a tensor and  [2.x.33]  denotes the Frobenius norm. 

Further equations describe a fixed, zero displacement on  [2.x.34]  and that on the surface  [2.x.35]  where contact may appear, the normal force  [2.x.36]  exerted by the obstacle is inward (no "pull" by the obstacle on our body) and with zero tangential component  [2.x.37] . The last condition is again a complementarity condition that implies that on  [2.x.38] , the normal force can only be nonzero if the body is in contact with the obstacle; the second part describes the impenetrability of the obstacle and the body. The last two equations are commonly referred to as the Signorini contact conditions. 

Most materials - especially metals - have the property that they show some hardening as a result of deformation. In other words,  [2.x.39]  increases with deformation. In practice, it is not the elastic deformation that results in hardening, but the plastic component. There are different constitutive laws to describe those material behaviors. The simplest one is called linear isotropic hardening described by the flow function  [2.x.40] . 


[1.x.48][1.x.49] 


It is generally rather awkward to deal with inequalities. Here, we have to deal with two: plasticity and the contact problem. As described in more detail in the paper mentioned at the top of this page, one can at least reformulate the plasticity in a way that makes it look like a nonlinearity that we can then treat with Newton's method. This is slightly tricky mathematically since the nonlinearity is not just some smooth function but instead has kinks where the stress reaches the yield stress; however, it can be shown for such [1.x.50] functions that Newton's method still converges. 

Without going into details, we will also get rid of the stress as an independent variable and instead work exclusively with the displacements  [2.x.41] . Ultimately, the goal of this reformulation is that we will want to end up with a symmetric, positive definite problem - such as a linearized elasticity problem with spatially variable coefficients resulting from the plastic behavior - that needs to be solved in each Newton step. We want this because there are efficient and scalable methods for the solution of such linear systems, such as CG preconditioned with an algebraic multigrid. This is opposed to the saddle point problem akin to the mixed Laplace (see  [2.x.42] ) we would get were we to continue with the mixed formulation containing both displacements and stresses, and for which  [2.x.43]  already gives a hint at how difficult it is to construct good solvers and preconditioners. 

With this said, let us simply state the problem we obtain after reformulation (again, details can be found in the paper): Find a displacement  [2.x.44]  so that 

[1.x.51] 

where the projector  [2.x.45]  is defined as 

[1.x.52] 

and the space  [2.x.46]  is the space of all displacements that satisfy the contact condition: 

[1.x.53] 



In the actual code, we will use the abbreviation  [2.x.47] . 

Given this formulation, we will apply two techniques: 

- Run a Newton method to iterate out the nonlinearity in the projector. 

- Run an active set method for the contact condition, in much the same   way as we did in  [2.x.48] . 

A strict approach would keep the active set fixed while we iterate the Newton method to convergence (or maybe the other way around: find the final active set before moving on to the next Newton iteration). In practice, it turns out that it is sufficient to do only a single Newton step per active set iteration, and so we will iterate over them concurrently. We will also, every once in a while, refine the mesh. 


[1.x.54][1.x.55] 


As mentioned, we will treat the nonlinearity of the operator  [2.x.49]  by applying a Newton method, despite the fact that the operator is not differentiable in the strict sense. However, it satisfies the conditions of [1.x.56] differentiability and this turns out to be enough for Newton's method to work. The resulting method then goes by the name [1.x.57], which sounds impressive but is, in reality, just a Newton method applied to a semi-smooth function with an appropriately chosen "derivative". 

In the current case, we will run our iteration by solving in each iteration  [2.x.50]  the following equation (still an inequality, but linearized): 

[1.x.58] 

where the rank-4 tensor  [2.x.51]  given by 

[1.x.59] 

This tensor is the (formal) linearization of  [2.x.52]  around  [2.x.53] . For the linear isotropic material we consider here, the bulk and shear components of the projector are given by [1.x.60] 

where  [2.x.54]  and  [2.x.55]  are the identity tensors of rank 2 and 4, respectively. 

Note that this problem corresponds to a linear elastic contact problem where  [2.x.56]  plays the role of the elasticity tensor  [2.x.57] . Indeed, if the material is not plastic at a point, then  [2.x.58] . However, at places where the material is plastic,  [2.x.59]  is a spatially varying function. In any case, the system we have to solve for the Newton iterate  [2.x.60]  gets us closer to the goal of rewriting our problem in a way that allows us to use well-known solvers and preconditioners for elliptic systems. 

As a final note about the Newton method let us mention that as is common with Newton methods we need to globalize it by controlling the step length. In other words, while the system above solves for  [2.x.61] , the final iterate will rather be 

[1.x.61] 

where the difference in parentheses on the right takes the role of the traditional Newton direction,  [2.x.62] . We will determine  [2.x.63]  using a standard line search. 


[1.x.62][1.x.63] 


This linearized problem to be solved in each Newton step is essentially like in  [2.x.64] . The only difference consists in the fact that the contact area is at the boundary instead of in the domain. But this has no further consequence so that we refer to the documentation of  [2.x.65]  with the only hint that  [2.x.66]  contains all the vertices at the contact boundary  [2.x.67]  this time. As there, what we need to do is keep a subset of degrees of freedom fixed, leading to additional constraints that one can write as a saddle point problem. However, as discussed in the paper, by writing these constraints in an appropriate way that removes the coupling between degrees of freedom, we end up with a set of nodes that essentially just have Dirichlet values attached to them. 


[1.x.64][1.x.65] 


The algorithm outlined above combines the damped semismooth Newton-method, which we use for the nonlinear constitutive law, with the semismooth Newton method for the contact. It works as follows:  [2.x.68]    [2.x.69]  Initialize the active and inactive sets  [2.x.70]  and  [2.x.71]   such that  [2.x.72]  and  [2.x.73]  and set  [2.x.74] . Here,  [2.x.75]  is the set of  all degrees of freedom located at the surface of the domain where contact  may happen.  The start value  [2.x.76]  fulfills our obstacle condition, i.e., we project an  initial zero displacement onto the set of feasible displacements. 

  [2.x.77]  Assemble the Newton matrix  [2.x.78]  and the right-hand-side  [2.x.79] .  These correspond to the linearized Newton step, ignoring for the moment  the contact inequality. 

  [2.x.80]  Find the primal-dual pair  [2.x.81]  that satisfies  [1.x.66] 

 As in  [2.x.82] , we can obtain the solution to this problem by eliminating  those degrees of freedom in  [2.x.83]  from the first equation and  obtain a linear system  [2.x.84] . 




  [2.x.85]  Damp the Newton iteration for  [2.x.86]  by applying a line search and  calculating a linear combination of  [2.x.87]  and  [2.x.88] . This  requires finding an   [2.x.89]  so that  [1.x.67] 

 satisfies  [1.x.68]  with  [2.x.90]  with  the exceptions of (i) elements  [2.x.91]  where we set  [2.x.92] ,  and (ii) elements that correspond to hanging nodes, which we eliminate in the usual manner. 

  [2.x.93]  Define the new active and inactive sets by  [1.x.69] 

 [1.x.70] 



  [2.x.94] Project  [2.x.95]  so that it satisfies the contact inequality,  [1.x.71] 

 Here,   [2.x.96]  is the projection of the active  components in  [2.x.97]  to the gap  [1.x.72] 

 where  [2.x.98]  is the [1.x.73] denoting the distance of the obstacle  from the undisplaced configuration of the body. 

  [2.x.99]  If  [2.x.100]  and  [2.x.101]  then stop, else set  [2.x.102]  and go to  step (1). This step ensures that we only stop iterations if both the correct  active set has been found and the plasticity has been iterated to sufficient  accuracy.  [2.x.103]  

In step 3 of this algorithm, the matrix  [2.x.104] ,  [2.x.105]  describes the coupling of the bases for the displacements and Lagrange multiplier (contact forces) and it is not quadratic in our situation since  [2.x.106]  is only defined on  [2.x.107] , i.e., the surface where contact may happen. As shown in the paper, we can choose  [2.x.108]  to be a matrix that has only one entry per row, (see also H&uuml;eber, Wohlmuth: A primal-dual active set strategy for non-linear multibody contact problems, Comput. Methods Appl. Mech. Engrg. 194, 2005, pp. 3147-3166). The vector  [2.x.109]  is defined by a suitable approximation  [2.x.110]  of the gap  [2.x.111]  [1.x.74] 




[1.x.75][1.x.76] 


Since we run our program in 3d, the computations the program performs are expensive. Consequently using adaptive mesh refinement is an important step towards staying within acceptable run-times. To make our lives easier we simply choose the KellyErrorEstimator that is already implemented in deal.II. We hand the solution vector to it which contains the displacement  [2.x.112] . As we will see in the results it yields a quite reasonable adaptive mesh for the contact zone as well as for plasticity. 


[1.x.77][1.x.78] 


This tutorial is essentially a mixture of  [2.x.113]  and  [2.x.114]  but instead of PETSc we let the Trilinos library deal with parallelizing the linear algebra (like in  [2.x.115] ). Since we are trying to solve a similar problem like in  [2.x.116]  we will use the same methods but now in parallel. 

A difficulty is handling of the constraints from the Dirichlet conditions, hanging nodes and the inequality condition that arises from the contact. For this purpose we create three objects of type AffineConstraints that describe the various constraints and that we will combine as appropriate in each iteration. 

Compared to  [2.x.117] , the programs has a few new classes: 

 [2.x.118]   [2.x.119]   [2.x.120]  describes the plastic behavior of the   material 

 [2.x.121]   [2.x.122]  describes a sphere that serves as the   obstacle that is pushed into the deformable, elastoplastic body.   Whether this or the next class is used to describe the obstacle is   determined from the input parameter file. 

 [2.x.123]   [2.x.124]  (and a helper class) is a class that   allows us to read in an obstacle from a file. In the example we   will show in the results section, this file will be    [2.x.125]  and will correspond to data that shows the   Chinese, Japanese or   Korean symbol for force or power (see http://www.orientaloutpost.com/ :   "This word can be used for motivation - it   can also mean power/motion/propulsion/force. It can be anything   internal or external that keeps you going. This is the safest way to express   motivation in Chinese. If your audience is Japanese, please see the other entry   for motivation. This is a word in Japanese and Korean, but it means "motive   power" or "kinetic energy" (without the motivation meaning that you are   probably looking for)"). In essence, we will pretend that we have a stamp   (i.e., a mask that corresponds to a flat bottomed obstacle with no pieces   of intermediate height) that we press into the body. The symbol in question   looks as follows (see also the picture at   the top of this section on how the end result looks like): 

   [2.x.126]   [2.x.127]  

Other than that, let us comment only on the following aspects:  [2.x.128]   [2.x.129]  The program allows you to select from two different coarse meshes   through the parameter file. These are either a cube  [2.x.130]  or   a half sphere with the open side facing the positive  [2.x.131]  direction. 

 [2.x.132] In either case, we will assume the convention that the part of the   boundary that may be in contact with the obstacle has boundary   indicator one. For both kinds of meshes, we assume that this is a free   surface, i.e., the body is either in contact there or there is no force   acting on it. For the half sphere, the curved part has boundary   indicator zero and we impose zero displacement there. For the box,   we impose zero displacement along the bottom but allow vertical   displacement along the sides (though no horizontal displacement).  [2.x.133]  [1.x.79] [1.x.80] 


[1.x.81]  [1.x.82] The set of include files is not much of a surprise any more at this time: 

[1.x.83] 



Finally, we include two system headers that let us create a directory for output files. The first header provides the  [2.x.134]  function and the second lets us determine what happened if  [2.x.135]  fails. 

[1.x.84] 




[1.x.85]  [1.x.86] 




This class provides an interface for a constitutive law, i.e., for the relationship between strain  [2.x.136]  and stress  [2.x.137] . In this example we are using an elastoplastic material behavior with linear, isotropic hardening. Such materials are characterized by Young's modulus  [2.x.138] , Poisson's ratio  [2.x.139] , the initial yield stress  [2.x.140]  and the isotropic hardening parameter  [2.x.141] .  For  [2.x.142]  we obtain perfect elastoplastic behavior.    


As explained in the paper that describes this program, the first Newton steps are solved with a completely elastic material model to avoid having to deal with both nonlinearities (plasticity and contact) at once. To this end, this class has a function  [2.x.143]  that we use later on to simply set  [2.x.144]  to a very large value -- essentially guaranteeing that the actual stress will not exceed it, and thereby producing an elastic material. When we are ready to use a plastic model, we set  [2.x.145]  back to its proper value, using the same function.  As a result of this approach, we need to leave  [2.x.146]  as the only non-const member variable of this class. 

[1.x.87] 



The constructor of the ConstitutiveLaw class sets the required material parameter for our deformable body. Material parameters for elastic isotropic media can be defined in a variety of ways, such as the pair  [2.x.147]  (elastic modulus and Poisson's number), using the Lam&eacute; parameters  [2.x.148]  or several other commonly used conventions. Here, the constructor takes a description of material parameters in the form of  [2.x.149] , but since this turns out to these are not the coefficients that appear in the equations of the plastic projector, we immediately convert them into the more suitable set  [2.x.150]  of bulk and shear moduli.  In addition, the constructor takes  [2.x.151]  (the yield stress absent any plastic strain) and  [2.x.152]  (the hardening parameter) as arguments. In this constructor, we also compute the two principal components of the stress-strain relation and its linearization. 

[1.x.88] 




[1.x.89]  [1.x.90] 




This is the principal component of the constitutive law. It computes the fourth order symmetric tensor that relates the strain to the stress according to the projection given above, when evaluated at a particular strain point. We need this function to calculate the nonlinear residual in  [2.x.153]  where we multiply this tensor with the strain given in a quadrature point. The computations follow the formulas laid out in the introduction. In comparing the formulas there with the implementation below, recall that  [2.x.154]  and that  [2.x.155] .    


The function returns whether the quadrature point is plastic to allow for some statistics downstream on how many of the quadrature points are plastic and how many are elastic. 

[1.x.91] 




[1.x.92]  [1.x.93] 




This function returns the linearized stress strain tensor, linearized around the solution  [2.x.156]  of the previous Newton step  [2.x.157] .  The parameter  [2.x.158]  (commonly denoted  [2.x.159] ) must be passed as an argument, and serves as the linearization point. The function returns the derivative of the nonlinear constitutive law in the variable stress_strain_tensor, as well as the stress-strain tensor of the linearized problem in stress_strain_tensor_linearized.  See  [2.x.160]  where this function is used. 

[1.x.94] 



[1.x.95]    


The following should be relatively standard. We need classes for the boundary forcing term (which we here choose to be zero) and boundary values on those part of the boundary that are not part of the contact surface (also chosen to be zero here). 

[1.x.96] 




[1.x.97]  [1.x.98] 




The following class is the first of two obstacles that can be selected from the input file. It describes a sphere centered at position  [2.x.161]  and radius  [2.x.162] , where  [2.x.163]  is the vertical position of the (flat) surface of the deformable body. The function's  [2.x.164]  returns the location of the obstacle for a given  [2.x.165]  value if the point actually lies below the sphere, or a large positive value that can't possibly interfere with the deformation if it lies outside the "shadow" of the sphere. 

[1.x.99] 



preceding Assert 

[1.x.100] 




[1.x.101]  [1.x.102] 




The following two classes describe the obstacle outlined in the introduction, i.e., the Chinese character. The first of the two,  [2.x.166]  is responsible for reading in data from a picture file stored in pbm ascii format. This data will be bilinearly interpolated and thereby provides a function that describes the obstacle. (The code below shows how one can construct a function by interpolating between given data points. One could use the  [2.x.167]  introduced after this tutorial program was written, which does exactly what we want here, but it is instructive to see how to do it by hand.)      


The data which we read from the file will be stored in a double  [2.x.168]  named obstacle_data.  This vector composes the base to calculate a piecewise bilinear function as a polynomial interpolation. The data we will read from a file consists of zeros (white) and ones (black).      


The  [2.x.169]  variables denote the spacing between pixels in  [2.x.170]  and  [2.x.171]  directions.  [2.x.172]  are the numbers of pixels in each of these directions.   [2.x.173]  returns the value of the image at a given location, interpolated from the adjacent pixel values. 

[1.x.103] 



The constructor of this class reads in the data that describes the obstacle from the given file name. 

[1.x.104] 



The following two functions return the value of a given pixel with coordinates  [2.x.174] , which we identify with the values of a function defined at positions  [2.x.175] , and at arbitrary coordinates  [2.x.176]  where we do a bilinear interpolation between point values returned by the first of the two functions. In the second function, for each  [2.x.177] , we first compute the (integer) location of the nearest pixel coordinate to the bottom left of  [2.x.178] , and then compute the coordinates  [2.x.179]  within this pixel. We truncate both kinds of variables from both below and above to avoid problems when evaluating the function outside of its defined range as may happen due to roundoff errors. 

[1.x.105] 



Finally, this is the class that actually uses the class above. It has a BitmapFile object as a member that describes the height of the obstacle. As mentioned above, the BitmapFile class will provide us with a mask, i.e., values that are either zero or one (and, if you ask for locations between pixels, values that are interpolated between zero and one). This class translates this to heights that are either 0.001 below the surface of the deformable body (if the BitmapFile class reports a one at this location) or 0.999 above the obstacle (if the BitmapFile class reports a zero). The following function should then be self-explanatory. 

[1.x.106] 



preceding Assert 

[1.x.107] 




[1.x.108]  [1.x.109] 




This is the main class of this program and supplies all functions and variables needed to describe the nonlinear contact problem. It is close to  [2.x.180]  but with some additional features like handling hanging nodes, a Newton method, using Trilinos and p4est for parallel distributed computing. To deal with hanging nodes makes life a bit more complicated since we need another AffineConstraints object now. We create a Newton method for the active set method for the contact situation and to handle the nonlinear operator for the constitutive law.    


The general layout of this class is very much like for most other tutorial programs. To make our life a bit easier, this class reads a set of input parameters from an input file. These parameters, using the ParameterHandler class, are declared in the  [2.x.181]  function (which is static so that it can be called before we even create an object of the current type), and a ParameterHandler object that has been used to read an input file will then be passed to the constructor of this class.    


The remaining member functions are by and large as we have seen in several of the other tutorial programs, though with additions for the current nonlinear system. We will comment on their purpose as we get to them further below. 

[1.x.110] 



As far as member variables are concerned, we start with ones that we use to indicate the MPI universe this program runs on, a stream we use to let exactly one processor produce output to the console (see  [2.x.182] ) and a variable that is used to time the various sections of the program: 

[1.x.111] 



The next group describes the mesh and the finite element space. In particular, for this parallel program, the finite element space has associated with it variables that indicate which degrees of freedom live on the current processor (the index sets, see also  [2.x.183]  and the  [2.x.184]  documentation module) as well as a variety of constraints: those imposed by hanging nodes, by Dirichlet boundary conditions, and by the active set of contact nodes. Of the three AffineConstraints variables defined here, the first only contains hanging node constraints, the second also those associated with Dirichlet boundary conditions, and the third these plus the contact constraints.      


The variable  [2.x.185]  consists of those degrees of freedom constrained by the contact, and we use  [2.x.186]  to keep track of the fraction of quadrature points on each cell where the stress equals the yield stress. The latter is only used to create graphical output showing the plastic zone, but not for any further computation; the variable is a member variable of this class since the information is computed as a by-product of computing the residual, but is used only much later. (Note that the vector is a vector of length equal to the number of active cells on the [1.x.112]; it is never used to exchange information between processors and can therefore be a regular deal.II vector.) 

[1.x.113] 



The next block of variables corresponds to the solution and the linear systems we need to form. In particular, this includes the Newton matrix and right hand side; the vector that corresponds to the residual (i.e., the Newton right hand side) but from which we have not eliminated the various constraints and that is used to determine which degrees of freedom need to be constrained in the next iteration; and a vector that corresponds to the diagonal of the  [2.x.187]  matrix briefly mentioned in the introduction and discussed in the accompanying paper. 

[1.x.114] 



The next block contains the variables that describe the material response: 

[1.x.115] 



And then there is an assortment of other variables that are used to identify the mesh we are asked to build as selected by the parameter file, the obstacle that is being pushed into the deformable body, the mesh refinement strategy, whether to transfer the solution from one mesh to the next, and how many mesh refinement cycles to perform. As possible, we mark these kinds of variables as  [2.x.188]  to help the reader identify which ones may or may not be modified later on (the output directory being an exception -- it is never modified outside the constructor but it is awkward to initialize in the member-initializer-list following the colon in the constructor since there we have only one shot at setting it; the same is true for the mesh refinement criterion): 

[1.x.116] 




[1.x.117]  [1.x.118] 





[1.x.119]  [1.x.120] 




Let us start with the declaration of run-time parameters that can be selected in the input file. These values will be read back in the constructor of this class to initialize the member variables of this class: 

[1.x.121] 




[1.x.122]  [1.x.123] 




Given the declarations of member variables as well as the declarations of run-time parameters that are read from the input file, there is nothing surprising in this constructor. In the body we initialize the mesh refinement strategy and the output directory, creating such a directory if necessary. 

[1.x.124] 



If necessary, create a new directory for the output. 

[1.x.125] 




[1.x.126]  [1.x.127] 




The next block deals with constructing the starting mesh. We will use the following helper function and the first block of the  [2.x.189]  to construct a mesh that corresponds to a half sphere. deal.II has a function that creates such a mesh, but it is in the wrong location and facing the wrong direction, so we need to shift and rotate it a bit before using it.    


For later reference, as described in the documentation of  [2.x.190]  the flat surface of the halfsphere has boundary indicator zero, while the remainder has boundary indicator one. 

[1.x.128] 



Since we will attach a different manifold below, we immediately clear the default manifold description: 

[1.x.129] 



Alternatively, create a hypercube mesh. After creating it, assign boundary indicators as follows:  [2.x.191]  In other words, the boundary indicators of the sides of the cube are 8. The boundary indicator of the bottom is 6 and the top has indicator 1. We set these by looping over all cells of all faces and looking at coordinate values of the cell center, and will make use of these indicators later when evaluating which boundary will carry Dirichlet boundary conditions or will be subject to potential contact. (In the current case, the mesh contains only a single cell, and all of its faces are on the boundary, so both the loop over all cells and the query whether a face is on the boundary are, strictly speaking, unnecessary; we retain them simply out of habit: this kind of code can be found in many programs in essentially this form.) 

[1.x.131] 




[1.x.132]  [1.x.133] 




The next piece in the puzzle is to set up the DoFHandler, resize vectors and take care of various other status variables such as index sets and constraint matrices.    


In the following, each group of operations is put into a brace-enclosed block that is being timed by the variable declared at the top of the block (the constructor of the  [2.x.192]  variable starts the timed section, the destructor that is called at the end of the block stops it again). 

[1.x.134] 



Finally, we set up sparsity patterns and matrices. We temporarily (ab)use the system matrix to also build the (diagonal) matrix that we use in eliminating degrees of freedom that are in contact with the obstacle, but we then immediately set the Newton matrix back to zero. 

[1.x.135] 




[1.x.136]  [1.x.137] 




This function, broken out of the preceding one, computes the constraints associated with Dirichlet-type boundary conditions and puts them into the  [2.x.193]  variable by merging with the constraints that come from hanging nodes.    


As laid out in the introduction, we need to distinguish between two cases: 

- If the domain is a box, we set the displacement to zero at the bottom, and allow vertical movement in z-direction along the sides. As shown in the  [2.x.194]  function, the former corresponds to boundary indicator 6, the latter to 8. 

- If the domain is a half sphere, then we impose zero displacement along the curved part of the boundary, associated with boundary indicator zero. 

[1.x.138] 



interpolate all components of the solution 

[1.x.139] 



interpolate x- and y-components of the solution (this is a bit mask, so apply operator| ) 

[1.x.140] 




[1.x.141]  [1.x.142] 




The next helper function computes the (diagonal) mass matrix that is used to determine the active set of the active set method we use in the contact algorithm. This matrix is of mass matrix type, but unlike the standard mass matrix, we can make it diagonal (even in the case of higher order elements) by using a quadrature formula that has its quadrature points at exactly the same locations as the interpolation points for the finite element are located. We achieve this by using a QGaussLobatto quadrature formula here, along with initializing the finite element with a set of interpolation points derived from the same quadrature formula. The remainder of the function is relatively straightforward: we put the resulting matrix into the given argument; because we know the matrix is diagonal, it is sufficient to have a loop over only  [2.x.195]  and not over  [2.x.196] . Strictly speaking, we could even avoid multiplying the shape function's values at quadrature point  [2.x.197]  by itself because we know the shape value to be a vector with exactly one one which when dotted with itself yields one. Since this function is not time critical we add this term for clarity. 

[1.x.143] 




[1.x.144]  [1.x.145] 




The following function is the first function we call in each Newton iteration in the  [2.x.198]  function. What it does is to project the solution onto the feasible set and update the active set for the degrees of freedom that touch or penetrate the obstacle.    


In order to function, we first need to do some bookkeeping: We need to write into the solution vector (which we can only do with fully distributed vectors without ghost elements) and we need to read the Lagrange multiplier and the elements of the diagonal mass matrix from their respective vectors (which we can only do with vectors that do have ghost elements), so we create the respective vectors. We then also initialize the constraints object that will contain constraints from contact and all other sources, as well as an object that contains an index set of all locally owned degrees of freedom that are part of the contact: 

[1.x.146] 



The second part is a loop over all cells in which we look at each point where a degree of freedom is defined whether the active set condition is true and we need to add this degree of freedom to the active set of contact nodes. As we always do, if we want to evaluate functions at individual points, we do this with an FEValues object (or, here, an FEFaceValues object since we need to check contact at the surface) with an appropriately chosen quadrature object. We create this face quadrature object by choosing the "support points" of the shape functions defined on the faces of cells (for more on support points, see this  [2.x.199]  "glossary entry"). As a consequence, we have as many quadrature points as there are shape functions per face and looping over quadrature points is equivalent to looping over shape functions defined on a face. With this, the code looks as follows: 

[1.x.147] 



At each quadrature point (i.e., at each support point of a degree of freedom located on the contact boundary), we then ask whether it is part of the z-displacement degrees of freedom and if we haven't encountered this degree of freedom yet (which can happen for those on the edges between faces), we need to evaluate the gap between the deformed object and the obstacle. If the active set condition is true, then we add a constraint to the AffineConstraints object that the next Newton update needs to satisfy, set the solution vector's corresponding element to the correct value, and add the index to the IndexSet object that stores which degree of freedom is part of the contact: 

[1.x.148] 



At the end of this function, we exchange data between processors updating those ghost elements in the  [2.x.200]  variable that have been written by other processors. We then merge the Dirichlet constraints and those from hanging nodes into the AffineConstraints object that already contains the active set. We finish the function by outputting the total number of actively constrained degrees of freedom for which we sum over the number of actively constrained degrees of freedom owned by each of the processors. This number of locally owned constrained degrees of freedom is of course the number of elements of the intersection of the active set and the set of locally owned degrees of freedom, which we can get by using  [2.x.201]  on two IndexSets: 

[1.x.149] 




[1.x.150]  [1.x.151] 




Given the complexity of the problem, it may come as a bit of a surprise that assembling the linear system we have to solve in each Newton iteration is actually fairly straightforward. The following function builds the Newton right hand side and Newton matrix. It looks fairly innocent because the heavy lifting happens in the call to  [2.x.202]  and in particular in  [2.x.203]  using the constraints we have previously computed. 

[1.x.152] 



Having computed the stress-strain tensor and its linearization, we can now put together the parts of the matrix and right hand side. In both, we need the linearized stress-strain tensor times the symmetric gradient of  [2.x.204] , i.e. the term  [2.x.205] , so we introduce an abbreviation of this term. Recall that the matrix corresponds to the bilinear form  [2.x.206]  in the notation of the accompanying publication, whereas the right hand side is  [2.x.207]  where  [2.x.208]  is the current linearization points (typically the last solution). This might suggest that the right hand side will be zero if the material is completely elastic (where  [2.x.209] ) but this ignores the fact that the right hand side will also contain contributions from non-homogeneous constraints due to the contact.                    


The code block that follows this adds contributions that are due to boundary forces, should there be any. 

[1.x.153] 




[1.x.154]  [1.x.155] 




The following function computes the nonlinear residual of the equation given the current solution (or any other linearization point). This is needed in the linear search algorithm where we need to try various linear combinations of previous and current (trial) solution to compute the (real, globalized) solution of the current Newton step.    


That said, in a slight abuse of the name of the function, it actually does significantly more. For example, it also computes the vector that corresponds to the Newton residual but without eliminating constrained degrees of freedom. We need this vector to compute contact forces and, ultimately, to compute the next active set. Likewise, by keeping track of how many quadrature points we encounter on each cell that show plastic yielding, we also compute the  [2.x.210]  vector that we can later output to visualize the plastic zone. In both of these cases, the results are not necessary as part of the line search, and so we may be wasting a small amount of time computing them. At the same time, this information appears as a natural by-product of what we need to do here anyway, and we want to collect it once at the end of each Newton step, so we may as well do it here.    


The actual implementation of this function should be rather obvious: 

[1.x.156] 




[1.x.157]  [1.x.158] 




The last piece before we can discuss the actual Newton iteration on a single mesh is the solver for the linear systems. There are a couple of complications that slightly obscure the code, but mostly it is just setup then solve. Among the complications are: 

   




- For the hanging nodes we have to apply the  [2.x.211]  function to newton_rhs. This is necessary if a hanging node with solution value  [2.x.212]  has one neighbor with value  [2.x.213]  which is in contact with the obstacle and one neighbor  [2.x.214]  which is not in contact. Because the update for the former will be prescribed, the hanging node constraint will have an inhomogeneity and will look like  [2.x.215] . So the corresponding entries in the right-hand-side are non-zero with a meaningless value. These values we have to set to zero. 

- Like in  [2.x.216] , we need to shuffle between vectors that do and do not have ghost elements when solving or using the solution.    


The rest of the function is similar to  [2.x.217]  and  [2.x.218]  except that we use a BiCGStab solver instead of CG. This is due to the fact that for very small hardening parameters  [2.x.219] , the linear system becomes almost semidefinite though still symmetric. BiCGStab appears to have an easier time with such linear systems. 

[1.x.159] 




[1.x.160]  [1.x.161] 




This is, finally, the function that implements the damped Newton method on the current mesh. There are two nested loops: the outer loop for the Newton iteration and the inner loop for the line search which will be used only if necessary. To obtain a good and reasonable starting value we solve an elastic problem in the very first Newton step on each mesh (or only on the first mesh if we transfer solutions between meshes). We do so by setting the yield stress to an unreasonably large value in these iterations and then setting it back to the correct value in subsequent iterations.    


Other than this, the top part of this function should be reasonably obvious. We initialize the variable  [2.x.220]  to the most negative value representable with double precision numbers so that the comparison whether the current residual is less than that of the previous step will always fail in the first step. 

[1.x.162] 



It gets a bit more hairy after we have computed the trial solution  [2.x.221]  of the current Newton step. We handle a highly nonlinear problem so we have to damp Newton's method using a line search. To understand how we do this, recall that in our formulation, we compute a trial solution in each Newton step and not the update between old and new solution. Since the solution set is a convex set, we will use a line search that tries linear combinations of the previous and the trial solution to guarantee that the damped solution is in our solution set again. At most we apply 5 damping steps.          


There are exceptions to when we use a line search. First, if this is the first Newton step on any mesh, then we don't have any point to compare the residual to, so we always accept a full step. Likewise, if this is the second Newton step on the first mesh (or the second on any mesh if we don't transfer solutions from mesh to mesh), then we have computed the first of these steps using just an elastic model (see how we set the yield stress sigma to an unreasonably large value above). In this case, the first Newton solution was a purely elastic one, the second one a plastic one, and any linear combination would not necessarily be expected to lie in the feasible set -- so we just accept the solution we just got.          


In either of these two cases, we bypass the line search and just update residual and other vectors as necessary. 

[1.x.163] 



The final step is to check for convergence. If the active set has not changed across all processors and the residual is less than a threshold of  [2.x.222] , then we terminate the iteration on the current mesh: 

[1.x.164] 




[1.x.165]  [1.x.166] 




If you've made it this far into the deal.II tutorial, the following function refining the mesh should not pose any challenges to you any more. It refines the mesh, either globally or using the Kelly error estimator, and if so asked also transfers the solution from the previous to the next mesh. In the latter case, we also need to compute the active set and other quantities again, for which we need the information computed by  [2.x.223] . 

[1.x.167] 



enforce constraints to make the interpolated solution conforming on the new mesh: 

[1.x.168] 




[1.x.169]  [1.x.170] 




The remaining three functions before we get to  [2.x.224]  have to do with generating output. The following one is an attempt at showing the deformed body in its deformed configuration. To this end, this function takes a displacement vector field and moves every vertex of the (local part) of the mesh by the previously computed displacement. We will call this function with the current displacement field before we generate graphical output, and we will call it again after generating graphical output with the negative displacement field to undo the changes to the mesh so made.    


The function itself is pretty straightforward. All we have to do is keep track which vertices we have already touched, as we encounter the same vertices multiple times as we loop over cells. 

[1.x.171] 




[1.x.172]  [1.x.173] 




Next is the function we use to actually generate graphical output. The function is a bit tedious, but not actually particularly complicated. It moves the mesh at the top (and moves it back at the end), then computes the contact forces along the contact surface. We can do so (as shown in the accompanying paper) by taking the untreated residual vector and identifying which degrees of freedom correspond to those with contact by asking whether they have an inhomogeneous constraints associated with them. As always, we need to be mindful that we can only write into completely distributed vectors (i.e., vectors without ghost elements) but that when we want to generate output, we need vectors that do indeed have ghost entries for all locally relevant degrees of freedom. 

[1.x.174] 



Calculation of the contact forces 

[1.x.175] 



In the remainder of the function, we generate one VTU file on every processor, indexed by the subdomain id of this processor. On the first processor, we then also create a  [2.x.225]  file that indexes [1.x.176] of the VTU files so that the entire set of output files can be read at once. These  [2.x.226]  are used by Paraview to describe an entire parallel computation's output files. We then do the same again for the competitor of Paraview, the VisIt visualization program, by creating a matching  [2.x.227]  file. 

[1.x.177] 




[1.x.178]  [1.x.179] 




This last auxiliary function computes the contact force by calculating an integral over the contact pressure in z-direction over the contact area. For this purpose we set the contact pressure lambda to 0 for all inactive dofs (whether a degree of freedom is part of the contact is determined just as we did in the previous function). For all active dofs, lambda contains the quotient of the nonlinear residual (newton_rhs_uncondensed) and corresponding diagonal entry of the mass matrix (diag_mass_matrix_vector). Because it is not unlikely that hanging nodes show up in the contact area it is important to apply constraints_hanging_nodes.distribute to the distributed_lambda vector. 

[1.x.180] 




[1.x.181]  [1.x.182] 




As in all other tutorial programs, the  [2.x.228]  function contains the overall logic. There is not very much to it here: in essence, it performs the loops over all mesh refinement cycles, and within each, hands things over to the Newton solver in  [2.x.229]  on the current mesh and calls the function that creates graphical output for the so-computed solution. It then outputs some statistics concerning both run times and memory consumption that has been collected over the course of computations on this mesh. 

[1.x.183] 




[1.x.184]  [1.x.185] 




There really isn't much to the  [2.x.230]  function. It looks like they always do: 

[1.x.186] 

[1.x.187][1.x.188] 


The directory that contains this program also contains a number of input parameter files that can be used to create various different simulations. For example, running the program with the  [2.x.231]  parameter file (using a ball as obstacle and the box as domain) on 16 cores produces output like this: 

[1.x.189] 



The tables at the end of each cycle show information about computing time (these numbers are of course specific to the machine on which this output was produced) and the number of calls of different parts of the program like assembly or calculating the residual, for the most recent mesh refinement cycle. Some of the numbers above can be improved by transferring the solution from one mesh to the next, an option we have not exercised here. Of course, you can also make the program run faster, especially on the later refinement cycles, by just using more processors: the accompanying paper shows good scaling to at least 1000 cores. 

In a typical run, you can observe that for every refinement step, the active set - the contact points - are iterated out at first. After that the Newton method has only to resolve the plasticity. For the finer meshes, quadratic convergence can be observed for the last 4 or 5 Newton iterations. 

We will not discuss here in all detail what happens with each of the input files. Rather, let us just show pictures of the solution (the left half of the domain is omitted if cells have zero quadrature points at which the plastic inequality is active): 

 [2.x.232]  

The picture shows the adaptive refinement and as well how much a cell is plastified during the contact with the ball. Remember that we consider the norm of the deviator part of the stress in each quadrature point to see if there is elastic or plastic behavior. The blue color means that this cell contains only elastic quadrature points in contrast to the red cells in which all quadrature points are plastified. In the middle of the top surface - where the mesh is finest - a very close look shows the dimple caused by the obstacle. This is the result of the  [2.x.233]  function. However, because the indentation of the obstacles we consider here is so small, it is hard to discern this effect; one could play with displacing vertices of the mesh by a multiple of the computed displacement. 

Further discussion of results that can be obtained using this program is provided in the publication mentioned at the very top of this page. 


[1.x.190] [1.x.191][1.x.192] 


There are, as always, multiple possibilities for extending this program. From an algorithmic perspective, this program goes about as far as one can at the time of writing, using the best available algorithms for the contact inequality, the plastic nonlinearity, and the linear solvers. However, there are things one would like to do with this program as far as more realistic situations are concerned:  [2.x.234]   [2.x.235]  Extend the program from a static to a quasi-static situation, perhaps by choosing a backward-Euler-scheme for the time discretization. Some theoretical results can be found in the PhD thesis by Jörg Frohne, [1.x.193], University of Siegen, Germany, 2011. 

 [2.x.236]  It would also be an interesting advance to consider a contact problem with friction. In almost every mechanical process friction has a big influence.  To model this situation, we have to take into account tangential stresses at the contact surface. Friction also adds another inequality to our problem since body and obstacle will typically stick together as long as the tangential stress does not exceed a certain limit, beyond which the two bodies slide past each other. 

 [2.x.237]  If we already simulate a frictional contact, the next step to consider is heat generation over the contact zone. The heat that is caused by friction between two bodies raises the temperature in the deformable body and entails an change of some material parameters. 

 [2.x.238]  It might be of interest to implement more accurate, problem-adapted error estimators for contact as well as for the plasticity.  [2.x.239]  [1.x.194] [1.x.195]  [2.x.240]  

 [2.x.241] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43] 

 [2.x.3]  

[1.x.44] 


[1.x.45][1.x.46] [1.x.47] 


The simulation of multiphase flow in porous media is a ubiquitous problem, and we have previously addressed it already in some form in  [2.x.4]  and  [2.x.5] . However, as was easy to see there, it faces two major difficulties: numerical accuracy and efficiency. The first is easy to see in the stationary solver  [2.x.6] : using lowest order Raviart-Thomas elements can not be expected to yield highly accurate solutions. We need more accurate methods. The second reason is apparent from the time dependent  [2.x.7] : that program is excruciatingly slow, and there is no hope to get highly accurate solutions in 3d within reasonable time frames. 

In this program, in order to overcome these two problems, there are five areas which we are trying to improve for a high performance simulator: 

 [2.x.8]   [2.x.9]  Higher order spatial discretizations  [2.x.10]  Adaptive mesh refinement  [2.x.11]  Adaptive time stepping  [2.x.12]  Operator splitting  [2.x.13]  Efficient solver and preconditioning  [2.x.14]  

Much inspiration for this program comes from  [2.x.15]  but several of the techniques discussed here are original. 


[1.x.48][1.x.49] 


We consider the flow of a two-phase immiscible, incompressible fluid. Capillary and gravity effects are neglected, and viscous effects are assumed dominant. The governing equations for such a flow that are identical to those used in  [2.x.16]  and are 

[1.x.50] 

where  [2.x.17]  is the saturation (volume fraction between zero and one) of the second (wetting) phase,  [2.x.18]  is the pressure,  [2.x.19]  is the permeability tensor,  [2.x.20]  is the total mobility,  [2.x.21]  is the porosity,  [2.x.22]  is the fractional flow of the wetting phase,  [2.x.23]  is the source term and  [2.x.24]  is the total velocity. The total mobility, fractional flow of the wetting phase and total velocity are respectively given by 

[1.x.51] 

where subscripts  [2.x.25]  represent the wetting and non-wetting phases, respectively. 

For convenience, the porosity  [2.x.26]  in the saturation equation, which can be considered a scaling factor for the time variable, is set to one. Following a commonly used prescription for the dependence of the relative permeabilities  [2.x.27]  and  [2.x.28]  on saturation, we use 

[1.x.52] 



The porous media equations above are augmented by initial conditions for the saturation and boundary conditions for the pressure. Since saturation and the gradient of the pressure uniquely determine the velocity, no boundary conditions are necessary for the velocity. Since the flow equations do not contain time derivatives, initial conditions for the velocity and pressure variables are not required. The flow field separates the boundary into inflow or outflow parts. Specifically, [1.x.53] and we arrive at a complete model by also imposing boundary values for the saturation variable on the inflow boundary  [2.x.29] . 


[1.x.54][1.x.55] 


As seen in  [2.x.30] , solving the flow equations for velocity and pressure are the parts of the program that take far longer than the (explicit) updating step for the saturation variable once we know the flow variables. On the other hand,  the pressure and velocity depend only weakly on saturation, so one may think about only solving for pressure and velocity every few time steps while updating the saturation in every step. If we can find a criterion for when the flow variables need to be updated, we call this splitting an "adaptive operator splitting" scheme. 

Here, we use the following a posteriori criterion to decide when to re-compute pressure and velocity variables (detailed derivations and descriptions can be found in [Chueh, Djilali and Bangerth 2011]): 

[1.x.56] 

where superscripts in parentheses denote the number of the saturation time step at which any quantity is defined and  [2.x.31]  represents the last step where we actually computed the pressure and velocity. If  [2.x.32]  exceeds a certain threshold we re-compute the flow variables; otherwise, we skip this computation in time step  [2.x.33]  and only move the saturation variable one time step forward. 

In short, the algorithm allows us to perform a number of saturation time steps of length  [2.x.34]  until the criterion above tells us to re-compute velocity and pressure variables, leading to a macro time step of length [1.x.57] We choose the length of (micro) steps subject to the Courant-Friedrichs-Lewy (CFL) restriction according to the criterion [1.x.58] which we have confirmed to be stable for the choice of finite element and time stepping scheme for the saturation equation discussed below ( [2.x.35]  denotes the diameter of cell  [2.x.36] ). The result is a scheme where neither micro nor macro time steps are of uniform length, and both are chosen adaptively. 

[1.x.59][1.x.60] 

Using this time discretization, we obtain the following set of equations for each time step from the IMPES approach (see  [2.x.37] ): 

[1.x.61] 




Using the fact that  [2.x.38] , the time discrete saturation equation becomes 

[1.x.62] 



[1.x.63][1.x.64] 


By multiplying the equations defining the total velocity  [2.x.39]  and the equation that expresses its divergence in terms of source terms, with test functions  [2.x.40]  and  [2.x.41]  respectively and then integrating terms by parts as necessary, the weak form of the problem reads: Find  [2.x.42]  so that for all test functions  [2.x.43]  there holds [1.x.65] 

Here,  [2.x.44]  represents the unit outward normal vector to  [2.x.45]  and the pressure  [2.x.46]  can be prescribed weakly on the open part of the boundary  [2.x.47]  whereas on those parts where a velocity is prescribed (for example impermeable boundaries with  [2.x.48]  the term disappears altogether because  [2.x.49] . 

We use continuous finite elements to discretize the velocity and pressure equations. Specifically, we use mixed finite elements to ensure high order approximation for both vector (e.g. a fluid velocity) and scalar variables (e.g. pressure) simultaneously. For saddle point problems, it is well established that the so-called Babuska-Brezzi or Ladyzhenskaya-Babuska-Brezzi (LBB) conditions [Brezzi 1991, Chen 2005] need to be satisfied to ensure stability of the pressure-velocity system. These stability conditions are satisfied in the present work by using elements for velocity that are one order higher than for the pressure, i.e.  [2.x.50]  and  [2.x.51] , where  [2.x.52] ,  [2.x.53]  is the space dimension, and  [2.x.54]  denotes the space of tensor product Lagrange polynomials of degree  [2.x.55]  in each variable. 

[1.x.66][1.x.67] 

The chosen  [2.x.56]  elements for the saturation equation do not lead to a stable discretization without upwinding or other kinds of stabilization, and spurious oscillations will appear in the numerical solution. Adding an artificial diffusion term is one approach to eliminating these oscillations [Chen 2005]. On the other hand, adding too much diffusion smears sharp fronts in the solution and suffers from grid-orientation difficulties [Chen 2005]. To avoid these effects, we use the artificial diffusion term proposed by [Guermond and Pasquetti 2008] and validated in [Chueh, Djilali, Bangerth 2011] and [Kronbichler, Heister and Bangerth, 2011], as well as in  [2.x.57] . 

This method modifies the (discrete) weak form of the saturation equation to read 

[1.x.68] 

where  [2.x.58]  is the artificial diffusion parameter and  [2.x.59]  is an appropriately chosen numerical flux on the boundary of the domain (we choose the obvious full upwind flux for this). 

Following [Guermond and Pasquetti 2008] (and as detailed in [Chueh, Djilali and Bangerth 2011]), we use the parameter as a piecewise constant function set on each cell  [2.x.60]  with the diameter  [2.x.61]  as [1.x.69] where  [2.x.62]  is a stabilization exponent and  [2.x.63]  is a dimensionless user-defined stabilization constant. Following [Guermond and Pasquetti 2008] as well as the implementation in  [2.x.64] , the velocity and saturation global normalization constant,  [2.x.65] , and the residual  [2.x.66]  are respectively given by [1.x.70] and [1.x.71] where  [2.x.67]  is a second dimensionless user-defined constant,  [2.x.68]  is the diameter of the domain and  [2.x.69]  is the range of the present saturation values in the entire computational domain  [2.x.70] . 

This stabilization scheme has a number of advantages over simpler schemes such as finite volume (or discontinuous Galerkin) methods or streamline upwind Petrov Galerkin (SUPG) discretizations. In particular, the artificial diffusion term acts primarily in the vicinity of discontinuities since the residual is small in areas where the saturation is smooth. It therefore provides for a higher degree of accuracy. On the other hand, it is nonlinear since  [2.x.71]  depends on the saturation  [2.x.72] . We avoid this difficulty by treating all nonlinear terms explicitly, which leads to the following fully discrete problem at time step  [2.x.73] : 

[1.x.72] 

where  [2.x.74]  is the velocity linearly extrapolated from  [2.x.75]  and  [2.x.76]  to the current time  [2.x.77]  if  [2.x.78]  while  [2.x.79]  is  [2.x.80]  if  [2.x.81] . Consequently, the equation is linear in  [2.x.82]  and all that is required is to solve with a mass matrix on the saturation space. 

Since the Dirichlet boundary conditions for saturation are only imposed on the inflow boundaries, the third term on the left hand side of the equation above needs to be split further into two parts: 

[1.x.73] 

where  [2.x.83]  and  [2.x.84]  represent inflow and outflow boundaries, respectively. We choose values using an upwind formulation, i.e.  [2.x.85]  and  [2.x.86]  correspond to the values taken from the present cell, while the values of  [2.x.87]  and  [2.x.88]  are those taken from the neighboring boundary  [2.x.89] . 


[1.x.74][1.x.75] 


Choosing meshes adaptively to resolve sharp saturation fronts is an essential ingredient to achieve efficiency in our algorithm. Here, we use the same shock-type refinement approach used in [Chueh, Djilali and Bangerth 2011] to select those cells that should be refined or coarsened. The refinement indicator for each cell  [2.x.90]  of the triangulation is computed by [1.x.76] where  [2.x.91]  is the gradient of the discrete saturation variable evaluated at the center  [2.x.92]  of cell  [2.x.93] . This approach is analogous to ones frequently used in compressible flow problems, where density gradients are used to indicate refinement. That said, as we will discuss at the end of the [1.x.77], this turns out to not be a very useful criterion since it leads to refinement basically everywhere. We only show it here for illustrative purposes. 


[1.x.78][1.x.79] 


Following the discretization of the governing equations discussed above, we obtain a linear system of equations in time step  [2.x.94]  of the following form: [1.x.80] where the individual matrices and vectors are defined as follows using shape functions  [2.x.95]  for velocity, and  [2.x.96]  for both pressure and saturation: 

[1.x.81] 

and  [2.x.97]  as given in the definition of the stabilized transport equation. 

The linear system above is of block triangular form if we consider the top left  [2.x.98]  panel of matrices as one block. We can therefore first solve for the velocity and pressure (unless we decide to use  [2.x.99]  in place of the velocity) followed by a solve for the saturation variable. The first of these steps requires us to solve [1.x.82] We apply the Generalized Minimal Residual (GMRES) method [Saad and Schultz 1986] to this linear system. The ideal preconditioner for the velocity-pressure system is 

[1.x.83] 

where  [2.x.100]  is the Schur complement [Zhang 2005] of the system. This preconditioner is optimal since 

[1.x.84] 

for which it can be shown that GMRES converges in two iterations. 

However, we cannot of course expect to use exact inverses of the velocity mass matrix and the Schur complement. We therefore follow the approach by [Silvester and Wathen 1994] originally proposed for the Stokes system. Adapting it to the current set of equations yield the preconditioner 

[1.x.85] 

where a tilde indicates an approximation of the exact inverse matrix. In particular, since  [2.x.101]  is a sparse symmetric and positive definite matrix, we choose for  [2.x.102]  a single application of a sparse incomplete Cholesky decomposition of this matrix [Golub and Van Loan 1996]. We note that the Schur complement that corresponds to the porous media flow operator in non-mixed form,  [2.x.103]  and  [2.x.104]  should be a good approximation of the actual Schur complement matrix  [2.x.105] . Since both of these matrices are again symmetric and positive definite, we use an incomplete Cholesky decomposition of  [2.x.106]  for  [2.x.107] . It is important to note that  [2.x.108]  needs to be built with Dirichlet boundary conditions to ensure its invertibility. 

Once the velocity  [2.x.109]   is available, we can assemble  [2.x.110]  and  [2.x.111]  and solve for the saturations using 

[1.x.86] 

where the mass matrix  [2.x.112]  is solved by the conjugate gradient method, using an incomplete Cholesky decomposition as preconditioner once more. 

[1.x.87][1.x.88] 




 [2.x.113]  The implementation discussed here uses and extends parts of the  [2.x.114] ,  [2.x.115]  and  [2.x.116]  tutorial programs of this library. In particular, if you want to understand how it works, please consult  [2.x.117]  for a discussion of the mathematical problem, and  [2.x.118]  from which most of the implementation is derived. We will not discuss aspects of the implementation that have already been discussed in  [2.x.119] . 

We show numerical results for some two-phase flow equations augmented by appropriate initial and boundary conditions in conjunction with two different choices of the permeability model. In the problems considered, there is no internal source term ( [2.x.120] ). As mentioned above, quantitative numerical results are presented in [Chueh, Djilali and Bangerth 2011]. 

For simplicity, we choose  [2.x.121] , though all methods (as well as our implementation) should work equally well on general unstructured meshes. 

Initial conditions are only required for the saturation variable, and we choose  [2.x.122] , i.e. the porous medium is initially filled by a mixture of the non-wetting (80%) and wetting (20%) phases. This differs from the initial condition in  [2.x.123]  where we had taken  [2.x.124] , but for complicated mathematical reasons that are mentioned there in a longish remark, the current method using an entropy-based artificial diffusion term does not converge to the viscosity solution with this initial condition without additional modifications to the method. We therefore choose this modified version for the current program. 

Furthermore, we prescribe a linear pressure on the boundaries: [1.x.89] Pressure and saturation uniquely determine a velocity, and the velocity determines whether a boundary segment is an inflow or outflow boundary. On the inflow part of the boundary,  [2.x.125] , we impose 

[1.x.90] 

In other words, the domain is flooded by the wetting phase from the left. No boundary conditions for the saturation are required for the outflow parts of the boundary. 

All the numerical and physical parameters used for the 2D/3D cases are listed in the following table: 

 [2.x.126]  


[1.x.91][1.x.92] 




 [2.x.127]   [2.x.128]  CC Chueh, N Djilali and W Bangerth.  [2.x.129]  An h-adaptive operator splitting method for two-phase flow in 3D   heterogeneous porous media.  [2.x.130]  SIAM Journal on Scientific Computing, vol. 35 (2013), pp. B149-B175 

 [2.x.131]  M. Kronbichler, T. Heister, and W. Bangerth  [2.x.132]  High Accuracy Mantle Convection Simulation through Modern Numerical Methods.  [2.x.133]  Geophysics Journal International, vol. 191 (2012), pp. 12-29 

 [2.x.134]  F Brezzi and M Fortin.  [2.x.135]  [1.x.93].  [2.x.136]  Springer-Verlag, 1991. 

 [2.x.137]  Z Chen.  [2.x.138]  [1.x.94].  [2.x.139]  Springer, 2005. 

 [2.x.140]  JL Guermond and R Pasquetti.  [2.x.141]  Entropy-based nonlinear viscosity for Fourier approximations of   conservation laws.  [2.x.142]  [1.x.95], 346(13-14):801-806, 2008. 

 [2.x.143]  CC Chueh, M Secanell, W Bangerth, and N Djilali.  [2.x.144]  Multi-level adaptive simulation of transient two-phase flow in   heterogeneous porous media.  [2.x.145]  [1.x.96], 39:1585-1596, 2010. 

 [2.x.146]  Y Saad and MH Schultz.  [2.x.147]  Gmres: A generalized minimal residual algorithm for solving   nonsymmetric linear systems.  [2.x.148]  [1.x.97],   7(3):856-869, 1986. 

 [2.x.149]  F Zhang.  [2.x.150]  [1.x.98].  [2.x.151]  Springer, 2005. 

 [2.x.152]  D Silvester and A Wathen.  [2.x.153]  Fast iterative solution of stabilised Stokes systems part ii: Using   general block preconditioners.  [2.x.154]  [1.x.99], 31(5):1352-1367, 1994. 

 [2.x.155]  GH Golub and CF van Loan.  [2.x.156]  [1.x.100].  [2.x.157]  3rd Edition, Johns Hopkins, 1996. 

 [2.x.158]  SE Buckley and MC Leverett.  [2.x.159]  Mechanism of fluid displacements in sands.  [2.x.160]  [1.x.101], 146:107-116, 1942. 

 [2.x.161]  [1.x.102] [1.x.103] 


[1.x.104]  [1.x.105] 




The first step, as always, is to include the functionality of a number of deal.II and C++ header files. 




The list includes some header files that provide vector, matrix, and preconditioner classes that implement interfaces to the respective Trilinos classes; some more information on these may be found in  [2.x.162] . 

[1.x.106] 



At the end of this top-matter, we open a namespace for the current project into which all the following material will go, and then import all deal.II names into this namespace: 

[1.x.107] 




[1.x.108]  [1.x.109] 




The following part is taken directly from  [2.x.163]  so there is no need to repeat the descriptions found there. 

[1.x.110] 




[1.x.111]  [1.x.112] 




In this tutorial, we still use the two permeability models previously used in  [2.x.164]  so we again refrain from commenting in detail about them. 

[1.x.113] 




[1.x.114]  [1.x.115] 




The implementations of all the physical quantities such as total mobility  [2.x.165]  and fractional flow of water  [2.x.166]  are taken from  [2.x.167]  so again we don't have do any comment about them. Compared to  [2.x.168]  we have added checks that the saturation passed to these functions is in fact within the physically valid range. Furthermore, given that the wetting phase moves at speed  [2.x.169]  it is clear that  [2.x.170]  must be greater or equal to zero, so we assert that as well to make sure that our calculations to get at the formula for the derivative made sense. 

[1.x.116] 




[1.x.117]  [1.x.118] 




In this first part we define a number of classes that we need in the construction of linear solvers and preconditioners. This part is essentially the same as that used in  [2.x.171] . The only difference is that the original variable name stokes_matrix is replaced by another name darcy_matrix to match our problem. 

[1.x.119] 




[1.x.120]  [1.x.121] 




The definition of the class that defines the top-level logic of solving the time-dependent advection-dominated two-phase flow problem (or Buckley-Leverett problem [Buckley 1942]) is mainly based on tutorial programs  [2.x.172]  and  [2.x.173] , and in particular on  [2.x.174]  where we have used basically the same general structure as done here. As in  [2.x.175] , the key routines to look for in the implementation below are the  [2.x.176]  functions.    


The main difference to  [2.x.177]  is that, since adaptive operator splitting is considered, we need a couple more member variables to hold the last two computed Darcy (velocity/pressure) solutions in addition to the current one (which is either computed directly, or extrapolated from the previous two), and we need to remember the last two times we computed the Darcy solution. We also need a helper function that figures out whether we do indeed need to recompute the Darcy solution.    


Unlike  [2.x.178] , this step uses one more AffineConstraints object called darcy_preconditioner_constraints. This constraint object is used only for assembling the matrix for the Darcy preconditioner and includes hanging node constraints as well as Dirichlet boundary value constraints for the pressure variable. We need this because we are building a Laplace matrix for the pressure as an approximation of the Schur complement) which is only positive definite if boundary conditions are applied.    


The collection of member functions and variables thus declared in this class is then rather similar to those in  [2.x.179] : 

[1.x.122] 



We follow with a number of helper functions that are used in a variety of places throughout the program: 

[1.x.123] 



This all is followed by the member variables, most of which are similar to the ones in  [2.x.180] , with the exception of the ones that pertain to the macro time stepping for the velocity/pressure system: 

[1.x.124] 



At the very end we declare a variable that denotes the material model. Compared to  [2.x.181] , we do this here as a member variable since we will want to use it in a variety of places and so having a central place where such a variable is declared will make it simpler to replace one class by another (e.g. replace  [2.x.182]  by  [2.x.183]  

[1.x.125] 




[1.x.126]  [1.x.127] 




The constructor of this class is an extension of the constructors in  [2.x.184]  and  [2.x.185] . We need to add the various variables that concern the saturation. As discussed in the introduction, we are going to use  [2.x.186]  (Taylor-Hood) elements again for the Darcy system, an element combination that fulfills the Ladyzhenskaya-Babuska-Brezzi (LBB) conditions [Brezzi and Fortin 1991, Chen 2005], and  [2.x.187]  elements for the saturation. However, by using variables that store the polynomial degree of the Darcy and temperature finite elements, it is easy to consistently modify the degree of the elements as well as all quadrature formulas used on them downstream. Moreover, we initialize the time stepping variables related to operator splitting as well as the option for matrix assembly and preconditioning: 

[1.x.128] 




[1.x.129]  [1.x.130] 




This is the function that sets up the DoFHandler objects we have here (one for the Darcy part and one for the saturation part) as well as set to the right sizes the various objects required for the linear algebra in this program. Its basic operations are similar to what  [2.x.188]  did.    


The body of the function first enumerates all degrees of freedom for the Darcy and saturation systems. For the Darcy part, degrees of freedom are then sorted to ensure that velocities precede pressure DoFs so that we can partition the Darcy matrix into a  [2.x.189]  matrix.    


Then, we need to incorporate hanging node constraints and Dirichlet boundary value constraints into darcy_preconditioner_constraints.  The boundary condition constraints are only set on the pressure component since the Schur complement preconditioner that corresponds to the porous media flow operator in non-mixed form,  [2.x.190] , acts only on the pressure variable. Therefore, we use a component_mask that filters out the velocity component, so that the condensation is performed on pressure degrees of freedom only.    


After having done so, we count the number of degrees of freedom in the various blocks. This information is then used to create the sparsity pattern for the Darcy and saturation system matrices as well as the preconditioner matrix from which we build the Darcy preconditioner. As in  [2.x.191] , we choose to create the pattern using the blocked version of DynamicSparsityPattern. So, for this, we follow the same way as  [2.x.192]  did and we don't have to repeat descriptions again for the rest of the member function. 

[1.x.131] 




[1.x.132]  [1.x.133] 




The next few functions are devoted to setting up the various system and preconditioner matrices and right hand sides that we have to deal with in this program. 





[1.x.134]  [1.x.135] 




This function assembles the matrix we use for preconditioning the Darcy system. What we need are a vector mass matrix weighted by  [2.x.193]  on the velocity components and a mass matrix weighted by  [2.x.194]  on the pressure component. We start by generating a quadrature object of appropriate order, the FEValues object that can give values and gradients at the quadrature points (together with quadrature weights). Next we create data structures for the cell matrix and the relation between local and global DoFs. The vectors phi_u and grad_phi_p are going to hold the values of the basis functions in order to faster build up the local matrices, as was already done in  [2.x.195] . Before we start the loop over all active cells, we have to specify which components are pressure and which are velocity.    


The creation of the local matrix is rather simple. There are only a term weighted by  [2.x.196]  (on the velocity) and a Laplace matrix weighted by  [2.x.197]  to be generated, so the creation of the local matrix is done in essentially two lines. Since the material model functions at the top of this file only provide the inverses of the permeability and mobility, we have to compute  [2.x.198]  and  [2.x.199]  by hand from the given values, once per quadrature point.    


Once the local matrix is ready (loop over rows and columns in the local matrix on each quadrature point), we get the local DoF indices and write the local information into the global matrix. We do this by directly applying the constraints (i.e. darcy_preconditioner_constraints) that takes care of hanging node and zero Dirichlet boundary condition constraints. By doing so, we don't have to do that afterwards, and we later don't have to use  [2.x.200]  and  [2.x.201]  both functions that would need to modify matrix and vector entries and so are difficult to write for the Trilinos classes where we don't immediately have access to individual memory locations. 

[1.x.136] 




[1.x.137]  [1.x.138] 




After calling the above functions to assemble the preconditioner matrix, this function generates the inner preconditioners that are going to be used for the Schur complement block preconditioner. The preconditioners need to be regenerated at every saturation time step since they depend on the saturation  [2.x.202]  that varies with time.    


In here, we set up the preconditioner for the velocity-velocity matrix  [2.x.203]  and the Schur complement  [2.x.204] . As explained in the introduction, we are going to use an IC preconditioner based on the vector matrix  [2.x.205]  and another based on the scalar Laplace matrix  [2.x.206]  (which is spectrally close to the Schur complement of the Darcy matrix). Usually, the  [2.x.207]  class can be seen as a good black-box preconditioner which does not need any special knowledge of the matrix structure and/or the operator that's behind it. 

[1.x.139] 




[1.x.140]  [1.x.141] 




This is the function that assembles the linear system for the Darcy system.    


Regarding the technical details of implementation, the procedures are similar to those in  [2.x.208]  and  [2.x.209] . We reset matrix and vector, create a quadrature formula on the cells, and then create the respective FEValues object.    


There is one thing that needs to be commented: since we have a separate finite element and DoFHandler for the saturation, we need to generate a second FEValues object for the proper evaluation of the saturation solution. This isn't too complicated to realize here: just use the saturation structures and set an update flag for the basis function values which we need for evaluation of the saturation solution. The only important part to remember here is that the same quadrature formula is used for both FEValues objects to ensure that we get matching information when we loop over the quadrature points of the two objects.    


The declarations proceed with some shortcuts for array sizes, the creation of the local matrix, right hand side as well as the vector for the indices of the local dofs compared to the global system. 

[1.x.142] 



Next we need a vector that will contain the values of the saturation solution at the previous time level at the quadrature points to assemble the saturation dependent coefficients in the Darcy equations.      


The set of vectors we create next hold the evaluations of the basis functions as well as their gradients that will be used for creating the matrices. Putting these into their own arrays rather than asking the FEValues object for this information each time it is needed is an optimization to accelerate the assembly process, see  [2.x.210]  for details.      


The last two declarations are used to extract the individual blocks (velocity, pressure, saturation) from the total FE system. 

[1.x.143] 



Now start the loop over all cells in the problem. We are working on two different DoFHandlers for this assembly routine, so we must have two different cell iterators for the two objects in use. This might seem a bit peculiar, but since both the Darcy system and the saturation system use the same grid we can assume that the two iterators run in sync over the cells of the two DoFHandler objects.      


The first statements within the loop are again all very familiar, doing the update of the finite element data as specified by the update flags, zeroing out the local arrays and getting the values of the old solution at the quadrature points.  At this point we also have to get the values of the saturation function of the previous time step at the quadrature points. To this end, we can use the  [2.x.211]  (previously already used in  [2.x.212] ,  [2.x.213]  and  [2.x.214] ), a function that takes a solution vector and returns a list of function values at the quadrature points of the present cell. In fact, it returns the complete vector-valued solution at each quadrature point, i.e. not only the saturation but also the velocities and pressure.      


Then we are ready to loop over the quadrature points on the cell to do the integration. The formula for this follows in a straightforward way from what has been discussed in the introduction.      


Once this is done, we start the loop over the rows and columns of the local matrix and feed the matrix with the relevant products.      


The last step in the loop over all cells is to enter the local contributions into the global matrix and vector structures to the positions specified in local_dof_indices. Again, we let the AffineConstraints class do the insertion of the cell matrix elements to the global matrix, which already condenses the hanging node constraints. 

[1.x.144] 




[1.x.145]  [1.x.146] 




This function is to assemble the linear system for the saturation transport equation. It calls, if necessary, two other member functions: assemble_saturation_matrix() and assemble_saturation_rhs(). The former function then assembles the saturation matrix that only needs to be changed occasionally. On the other hand, the latter function that assembles the right hand side must be called at every saturation time step. 

[1.x.147] 




[1.x.148]  [1.x.149] 




This function is easily understood since it only forms a simple mass matrix for the left hand side of the saturation linear system by basis functions phi_i_s and phi_j_s only. Finally, as usual, we enter the local contribution into the global matrix by specifying the position in local_dof_indices. This is done by letting the AffineConstraints class do the insertion of the cell matrix elements to the global matrix, which already condenses the hanging node constraints. 

[1.x.150] 




[1.x.151]  [1.x.152] 




This function is to assemble the right hand side of the saturation transport equation. Before going about it, we have to create two FEValues objects for the Darcy and saturation systems respectively and, in addition, two FEFaceValues objects for the two systems because we have a boundary integral term in the weak form of saturation equation. For the FEFaceValues object of the saturation system, we also require normal vectors, which we request using the update_normal_vectors flag.    


Next, before looping over all the cells, we have to compute some parameters (e.g. global_u_infty, global_S_variation, and global_Omega_diameter) that the artificial viscosity  [2.x.215]  needs. This is largely the same as was done in  [2.x.216] , so you may see there for more information.    


The real works starts with the loop over all the saturation and Darcy cells to put the local contributions into the global vector. In this loop, in order to simplify the implementation, we split some of the work into two helper functions: assemble_saturation_rhs_cell_term and assemble_saturation_rhs_boundary_term.  We note that we insert cell or boundary contributions into the global vector in the two functions rather than in this present function. 

[1.x.153] 




[1.x.154]  [1.x.155] 




This function takes care of integrating the cell terms of the right hand side of the saturation equation, and then assembling it into the global right hand side vector. Given the discussion in the introduction, the form of these contributions is clear. The only tricky part is getting the artificial viscosity and all that is necessary to compute it. The first half of the function is devoted to this task.    


The last part of the function is copying the local contributions into the global vector with position specified in local_dof_indices. 

[1.x.156] 




[1.x.157]  [1.x.158] 




The next function is responsible for the boundary integral terms in the right hand side form of the saturation equation.  For these, we have to compute the upwinding flux on the global boundary faces, i.e. we impose Dirichlet boundary conditions weakly only on inflow parts of the global boundary. As before, this has been described in  [2.x.217]  so we refrain from giving more descriptions about that. 

[1.x.159] 




[1.x.160]  [1.x.161] 




This function implements the operator splitting algorithm, i.e. in each time step it either re-computes the solution of the Darcy system or extrapolates velocity/pressure from previous time steps, then determines the size of the time step, and then updates the saturation variable. The implementation largely follows similar code in  [2.x.218] . It is, next to the run() function, the central one in this program.    


At the beginning of the function, we ask whether to solve the pressure-velocity part by evaluating the a posteriori criterion (see the following function). If necessary, we will solve the pressure-velocity part using the GMRES solver with the Schur complement block preconditioner as is described in the introduction. 

[1.x.162] 



On the other hand, if we have decided that we don't want to compute the solution of the Darcy system for the current time step, then we need to simply extrapolate the previous two Darcy solutions to the same time as we would have computed the velocity/pressure at. We do a simple linear extrapolation, i.e. given the current length  [2.x.219]  of the macro time step from the time when we last computed the Darcy solution to now (given by  [2.x.220] ), and  [2.x.221]  the length of the last macro time step (given by  [2.x.222] ), then we get  [2.x.223] , where  [2.x.224]  and  [2.x.225]  are the last two computed Darcy solutions. We can implement this formula using just two lines of code.      


Note that the algorithm here only works if we have at least two previously computed Darcy solutions from which we can extrapolate to the current time, and this is ensured by requiring re-computation of the Darcy solution for the first 2 time steps. 

[1.x.163] 



With the so computed velocity vector, compute the optimal time step based on the CFL criterion discussed in the introduction... 

[1.x.164] 



...and then also update the length of the macro time steps we use while we're dealing with time step sizes. In particular, this involves: (i) If we have just recomputed the Darcy solution, then the length of the previous macro time step is now fixed and the length of the current macro time step is, up to now, simply the length of the current (micro) time step. (ii) If we have not recomputed the Darcy solution, then the length of the current macro time step has just grown by  [2.x.226] . 

[1.x.165] 



The last step in this function is to recompute the saturation solution based on the velocity field we've just obtained. This naturally happens in every time step, and we don't skip any of these computations. At the end of computing the saturation, we project back into the allowed interval  [2.x.227]  to make sure our solution remains physical. 

[1.x.166] 




[1.x.167]  [1.x.168] 




The next function does the refinement and coarsening of the mesh. It does its work in three blocks: (i) Compute refinement indicators by looking at the gradient of a solution vector extrapolated linearly from the previous two using the respective sizes of the time step (or taking the only solution we have if this is the first time step). (ii) Flagging those cells for refinement and coarsening where the gradient is larger or smaller than a certain threshold, preserving minimal and maximal levels of mesh refinement. (iii) Transferring the solution from the old to the new mesh. None of this is particularly difficult. 

[1.x.169] 




[1.x.170]  [1.x.171] 




This function generates graphical output. It is in essence a copy of the implementation in  [2.x.228] . 

[1.x.172] 




[1.x.173]  [1.x.174] 





[1.x.175]  [1.x.176] 




This function implements the a posteriori criterion for adaptive operator splitting. The function is relatively straightforward given the way we have implemented other functions above and given the formula for the criterion derived in the paper.    


If one decides that one wants the original IMPES method in which the Darcy equation is solved in every time step, then this can be achieved by setting the threshold value  [2.x.229]  (with a default of  [2.x.230] ) to zero, thereby forcing the function to always return true.    


Finally, note that the function returns true unconditionally for the first two time steps to ensure that we have always solved the Darcy system at least twice when skipping its solution, thereby allowing us to extrapolate the velocity from the last two solutions in  [2.x.231] . 

[1.x.177] 




[1.x.178]  [1.x.179] 




The next function simply makes sure that the saturation values always remain within the physically reasonable range of  [2.x.232] . While the continuous equations guarantee that this is so, the discrete equations don't. However, if we allow the discrete solution to escape this range we get into trouble because terms like  [2.x.233]  and  [2.x.234]  will produce unreasonable results (e.g.  [2.x.235]  for  [2.x.236] , which would imply that the wetting fluid phase flows [1.x.180] the direction of the bulk fluid velocity)). Consequently, at the end of each time step, we simply project the saturation field back into the physically reasonable region. 

[1.x.181] 




[1.x.182]  [1.x.183]    


Another simpler helper function: Compute the maximum of the total velocity times the derivative of the fraction flow function, i.e., compute  [2.x.237] . This term is used in both the computation of the time step as well as in normalizing the entropy-residual term in the artificial viscosity. 

[1.x.184] 




[1.x.185]  [1.x.186]    


For computing the stabilization term, we need to know the range of the saturation variable. Unlike in  [2.x.238] , this range is trivially bounded by the interval  [2.x.239]  but we can do a bit better by looping over a collection of quadrature points and seeing what the values are there. If we can, i.e., if there are at least two timesteps around, we can even take the values extrapolated to the next time step.    


As before, the function is taken with minimal modifications from  [2.x.240] . 

[1.x.187] 




[1.x.188]  [1.x.189]    


The final tool function is used to compute the artificial viscosity on a given cell. This isn't particularly complicated if you have the formula for it in front of you, and looking at the implementation in  [2.x.241] . The major difference to that tutorial program is that the velocity here is not simply  [2.x.242]  but  [2.x.243]  and some of the formulas need to be adjusted accordingly. 

[1.x.190] 




[1.x.191]  [1.x.192] 




This function is, besides  [2.x.244] , the primary function of this program as it controls the time iteration as well as when the solution is written into output files and when to do mesh refinement.    


With the exception of the startup code that loops back to the beginning of the function through the  [2.x.245]  label, everything should be relatively straightforward. In any case, it mimics the corresponding function in  [2.x.246] . 

[1.x.193] 




[1.x.194]  [1.x.195] 




The main function looks almost the same as in all other programs. The need to initialize the MPI subsystem for a program that uses Trilinos -- even for programs that do not actually run in parallel -- is explained in  [2.x.247] . 

[1.x.196] 



This program can only be run in serial. Otherwise, throw an exception. 

[1.x.197] 

[1.x.198][1.x.199] 




The output of this program is not really much different from that of  [2.x.248] : it solves the same problem, after all. Of more importance are quantitative metrics such as the accuracy of the solution as well as the time needed to compute it. These are documented in detail in the two publications listed at the top of this page and we won't repeat them here. 

That said, no tutorial program is complete without a couple of good pictures, so here is some output of a run in 3d: 

 [2.x.249]  


[1.x.200] [1.x.201][1.x.202] 


The primary objection one may have to this program is that it is still too slow: 3d computations on reasonably fine meshes are simply too expensive to be done routinely and with reasonably quick turn-around. This is similar to the situation we were in when we wrote  [2.x.250] , from which this program has taken much inspiration. The solution is similar as it was there as well: We need to parallelize the program in a way similar to how we derived  [2.x.251]  out of  [2.x.252] . In fact, all of the techniques used in  [2.x.253]  would be transferable to this program as well, making the program run on dozens or hundreds of processors immediately. 

A different direction is to make the program more relevant to many other porous media applications. Specifically, one avenue is to go to the primary user of porous media flow simulators, namely the oil industry. There, applications in this area are dominated by multiphase flow (i.e., more than the two phases we have here), and the reactions they may have with each other (or any other way phases may exchange mass, such as through dissolution in and bubbling out of gas from the oil phase). Furthermore, the presence of gas often leads to compressibility effects of the fluid. Jointly, these effects are typically formulated in the widely-used "black oil model". True reactions between multiple phases also play a role in oil reservoir modeling when considering controlled burns of oil in the reservoir to raise pressure and temperature. These are much more complex problems, though, and left for future projects. 

Finally, from a mathematical perspective, we have derived the criterion for re-computing the velocity/pressure solution at a given time step under the assumption that we want to compare the solution we would get at the current time step with that computed the last time we actually solved this system. However, in the program, whenever we did not re-compute the solution, we didn't just use the previously computed solution but instead extrapolated from the previous two times we solved the system. Consequently, the criterion was pessimistically stated: what we should really compare is the solution we would get at the current time step with the extrapolated one. Re-stating the theorem in this regard is left as an exercise. 

There are also other ways to extend the mathematical foundation of this program; for example, one may say that it isn't the velocity we care about, but in fact the saturation. Thus, one may ask whether the criterion we use here to decide whether  [2.x.254]  needs to be recomputed is appropriate; one may, for example, suggest that it is also important to decide whether (and by how much) a wrong velocity field in fact affects the solution of the saturation equation. This would then naturally lead to a sensitivity analysis. 

From an algorithmic viewpoint, we have here used a criterion for refinement that is often used in engineering, namely by looking at the gradient of the solution. However, if you inspect the solution, you will find that it quickly leads to refinement almost everywhere, even in regions where it is clearly not necessary: frequently used therefore does not need to imply that it is a useful criterion to begin with. On the other hand, replacing this criterion by a different and better one should not be very difficult. For example, the KellyErrorEstimator class used in many other programs should certainly be applicable to the current problem as well. [1.x.203] [1.x.204]  [2.x.255]  

 [2.x.256] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39][1.x.40][1.x.41][1.x.42][1.x.43][1.x.44][1.x.45][1.x.46][1.x.47][1.x.48][1.x.49][1.x.50][1.x.51] 

 [2.x.3]  

[1.x.52] 

 [2.x.4]  

[1.x.53] [1.x.54][1.x.55] 


The subject of this tutorial is nonlinear solid mechanics. Classical single-field approaches (see e.g.  [2.x.5] ) can not correctly describe the response of quasi-incompressible materials. The response is overly stiff; a phenomenon known as locking. Locking problems can be circumvented using a variety of alternative strategies. One such strategy is the  three-field formulation. It is used here  to model the three-dimensional, fully-nonlinear (geometrical and material) response of an isotropic continuum body. The material response is approximated as hyperelastic. Additionally, the three-field formulation employed is valid for quasi-incompressible as well as compressible materials. 

The objective of this presentation is to provide a basis for using deal.II for problems in nonlinear solid mechanics. The linear problem was addressed in  [2.x.6] . A non-standard, hypoelastic-type form of the geometrically nonlinear problem was partially considered in  [2.x.7] : a rate form of the linearised constitutive relations is used and the problem domain evolves with the motion. Important concepts surrounding the nonlinear kinematics are absent in the theory and implementation.  [2.x.8]  does, however, describe many of the key concepts to implement elasticity within the framework of deal.II. 

We begin with a crash-course in nonlinear kinematics. For the sake of simplicity, we restrict our attention to the quasi-static problem. Thereafter, various key stress measures are introduced and the constitutive model described. We then describe the three-field formulation in detail prior to explaining the structure of the class used to manage the material. The setup of the example problem is then presented. 

 [2.x.9]  This tutorial has been developed (and is described in the introduction) for the problem of elasticity in three dimensions.  While the space dimension could be changed in the main() routine, care needs to be taken.  Two-dimensional elasticity problems, in general, exist only as idealizations of three-dimensional ones.  That is, they are either plane strain or plane stress.  The assumptions that follow either of these choices needs to be consistently imposed.  For more information see the note in  [2.x.10] . 

[1.x.56][1.x.57] 


The three-field formulation implemented here was pioneered by Simo et al. (1985) and is known as the mixed Jacobian-pressure formulation. Important related contributions include those by Simo and Taylor (1991), and Miehe (1994). The notation adopted here draws heavily on the excellent overview of the theoretical aspects of nonlinear solid mechanics by Holzapfel (2001). A nice overview of issues pertaining to incompressible elasticity (at small strains) is given in Hughes (2000). 

 [2.x.11]  	 [2.x.12]  J.C. Simo, R.L. Taylor and K.S. Pister (1985), 		Variational and projection methods for the volume constraint in finite deformation elasto-plasticity, 		 [2.x.13]  Computer Methods in Applied Mechanics and Engineering  [2.x.14] , 		<strong> 51 </strong>, 1-3, 		177-208. 		DOI: [1.x.58]; 	 [2.x.15]  J.C. Simo and R.L. Taylor (1991),   		Quasi-incompressible finite elasticity in principal stretches. Continuum 			basis and numerical algorithms, 		 [2.x.16]  Computer Methods in Applied Mechanics and Engineering  [2.x.17] , 		<strong> 85 </strong>, 3, 		273-310. 		DOI: [1.x.59]; 	 [2.x.18]  C. Miehe (1994), 		Aspects of the formulation and finite element implementation of large strain isotropic elasticity 		 [2.x.19]  International Journal for Numerical Methods in Engineering  [2.x.20]  		<strong> 37 </strong>, 12, 		1981-2004. 		DOI: [1.x.60]; 	 [2.x.21]  G.A. Holzapfel (2001), 		Nonlinear Solid Mechanics. A Continuum Approach for Engineering, 		John Wiley & Sons. 		ISBN: 0-471-82304-X; 	 [2.x.22]  T.J.R. Hughes (2000), 		The Finite Element Method: Linear Static and Dynamic Finite Element Analysis, 		Dover. 		ISBN: 978-0486411811  [2.x.23]  

An example where this three-field formulation is used in a coupled problem is documented in  [2.x.24]  	 [2.x.25]  J-P. V. Pelteret, D. Davydov, A. McBride, D. K. Vu, and P. Steinmann (2016), 		Computational electro- and magneto-elasticity for quasi-incompressible media immersed in free space, 		 [2.x.26]  International Journal for Numerical Methods in Engineering  [2.x.27] . 		DOI: [1.x.61]  [2.x.28]  

[1.x.62][1.x.63] 


One can think of fourth-order tensors as linear operators mapping second-order tensors (matrices) onto themselves in much the same way as matrices map vectors onto vectors. There are various fourth-order unit tensors that will be required in the forthcoming presentation. The fourth-order unit tensors  [2.x.29]  and  [2.x.30]  are defined by [1.x.64] Note  [2.x.31] . Furthermore, we define the symmetric and skew-symmetric fourth-order unit tensors by [1.x.65] such that [1.x.66] The fourth-order  [2.x.32]  returned by identity_tensor() is  [2.x.33] . 


[1.x.67][1.x.68] 


Let the time domain be denoted  [2.x.34] , where  [2.x.35]  and  [2.x.36]  is the total problem duration. Consider a continuum body that occupies the reference configuration  [2.x.37]  at time  [2.x.38] . %Particles in the reference configuration are identified by the position vector  [2.x.39] . The configuration of the body at a later time  [2.x.40]  is termed the current configuration, denoted  [2.x.41] , with particles identified by the vector  [2.x.42] . The nonlinear map between the reference and current configurations, denoted  [2.x.43] , acts as follows: [1.x.69] The material description of the displacement of a particle is defined by [1.x.70] 

The deformation gradient  [2.x.44]  is defined as the material gradient of the motion: [1.x.71] The determinant of the of the deformation gradient  [2.x.45]  maps corresponding volume elements in the reference and current configurations, denoted  [2.x.46]  and  [2.x.47] , respectively, as [1.x.72] 

Two important measures of the deformation in terms of the spatial and material coordinates are the left and right Cauchy-Green tensors, respectively, and denoted  [2.x.48]  and  [2.x.49] . They are both symmetric and positive definite. 

The Green-Lagrange strain tensor is defined by [1.x.73] If the assumption of infinitesimal deformations is made, then the second term on the right can be neglected, and  [2.x.50]  (the linearised strain tensor) is the only component of the strain tensor. This assumption is, looking at the setup of the problem, not valid in  [2.x.51] , making the use of the linearized  [2.x.52]  as the strain measure in that tutorial program questionable. 

In order to handle the different response that materials exhibit when subjected to bulk and shear type deformations we consider the following decomposition of the deformation gradient  [2.x.53]   and the left Cauchy-Green tensor  [2.x.54]  into volume-changing (volumetric) and volume-preserving (isochoric) parts: [1.x.74] Clearly,  [2.x.55] . 

The spatial velocity field is denoted  [2.x.56] . The derivative of the spatial velocity field with respect to the spatial coordinates gives the spatial velocity gradient  [2.x.57] , that is [1.x.75] where  [2.x.58] . 


[1.x.76][1.x.77] 


Cauchy's stress theorem equates the Cauchy traction  [2.x.59]  acting on an infinitesimal surface element in the current configuration  [2.x.60]  to the product of the Cauchy stress tensor  [2.x.61]  (a spatial quantity)  and the outward unit normal to the surface  [2.x.62]  as [1.x.78] The Cauchy stress is symmetric. Similarly,  the first Piola-Kirchhoff traction  [2.x.63]  which acts on an infinitesimal surface element in the reference configuration  [2.x.64]  is the product of the first Piola-Kirchhoff stress tensor  [2.x.65]  (a two-point tensor)  and the outward unit normal to the surface  [2.x.66]  as [1.x.79] The Cauchy traction  [2.x.67]  and the first Piola-Kirchhoff traction  [2.x.68]  are related as [1.x.80] This can be demonstrated using [1.x.81]. 

The first Piola-Kirchhoff stress tensor is related to the Cauchy stress as [1.x.82] Further important stress measures are the (spatial) Kirchhoff stress   [2.x.69]  and the (referential) second Piola-Kirchhoff stress  [2.x.70] . 


[1.x.83][1.x.84] 


Push-forward and pull-back operators allow one to transform various measures between the material and spatial settings. The stress measures used here are contravariant, while the strain measures are covariant. 

The push-forward and-pull back operations for second-order covariant tensors  [2.x.71]  are respectively given by: [1.x.85] 

The push-forward and pull back operations for second-order contravariant tensors  [2.x.72]  are respectively given by: [1.x.86] For example  [2.x.73] . 


[1.x.87][1.x.88] 


A hyperelastic material response is governed by a Helmholtz free energy function  [2.x.74]  which serves as a potential for the stress. For example, if the Helmholtz free energy depends on the right Cauchy-Green tensor  [2.x.75]  then the isotropic hyperelastic response is [1.x.89] If the Helmholtz free energy depends on the left Cauchy-Green tensor  [2.x.76]  then the isotropic hyperelastic response is [1.x.90] 

Following the multiplicative decomposition of the deformation gradient, the Helmholtz free energy can be decomposed as [1.x.91] Similarly, the Kirchhoff stress can be decomposed into volumetric and isochoric parts as  [2.x.77]  where: 

[1.x.92] 

where  [2.x.78]  is the pressure response.  [2.x.79]  is the projection tensor which provides the deviatoric operator in the Eulerian setting. The fictitious Kirchhoff stress tensor  [2.x.80]  is defined by [1.x.93] 




 [2.x.81]  The pressure response as defined above differs from the widely-used definition of the pressure in solid mechanics as  [2.x.82] . Here  [2.x.83]  is the hydrostatic pressure. We make use of the pressure response throughout this tutorial (although we refer to it as the pressure). 

[1.x.94][1.x.95] 


The Helmholtz free energy corresponding to a compressible [1.x.96] is given by [1.x.97] where  [2.x.84]  is the bulk modulus ( [2.x.85]  and  [2.x.86]  are the Lam&eacute; parameters) and  [2.x.87] . The function  [2.x.88]  is required to be strictly convex and satisfy the condition  [2.x.89] , among others, see Holzapfel (2001) for further details. In this work  [2.x.90] . 

Incompressibility imposes the isochoric constraint that  [2.x.91]  for all motions  [2.x.92] . The Helmholtz free energy corresponding to an incompressible neo-Hookean material is given by [1.x.98] where  [2.x.93] . Thus, the incompressible response is obtained by removing the volumetric component from the compressible free energy and enforcing  [2.x.94] . 


[1.x.99][1.x.100] 


We will use a Newton-Raphson strategy to solve the nonlinear boundary value problem. Thus, we will need to linearise the constitutive relations. 

The fourth-order elasticity tensor in the material description is defined by [1.x.101] The fourth-order elasticity tensor in the spatial description  [2.x.95]  is obtained from the push-forward of  [2.x.96]  as [1.x.102] The fourth-order elasticity tensors (for hyperelastic materials) possess both major and minor symmetries. 

The fourth-order spatial elasticity tensor can be written in the following decoupled form: [1.x.103] where 

[1.x.104] 

where the fictitious elasticity tensor  [2.x.97]  in the spatial description is defined by [1.x.105] 

[1.x.106][1.x.107] 


The total potential energy of the system  [2.x.98]  is the sum of the internal and external potential energies, denoted  [2.x.99]  and  [2.x.100] , respectively. We wish to find the equilibrium configuration by minimising the potential energy. 

As mentioned above, we adopt a three-field formulation. We denote the set of primary unknowns by  [2.x.101] . The independent kinematic variable  [2.x.102]  enters the formulation as a constraint on  [2.x.103]  enforced by the Lagrange multiplier  [2.x.104]  (the pressure, as we shall see). 

The three-field variational principle used here is given by [1.x.108] where the external potential is defined by [1.x.109] The boundary of the current configuration   [2.x.105]  is composed into two parts as  [2.x.106] , where  [2.x.107] . The prescribed Cauchy traction, denoted  [2.x.108] , is applied to  [2.x.109]  while the motion is prescribed on the remaining portion of the boundary  [2.x.110] . The body force per unit current volume is denoted  [2.x.111] . 




The stationarity of the potential follows as 

[1.x.110] 

for all virtual displacements  [2.x.112]  subject to the constraint that  [2.x.113]  on  [2.x.114] , and all virtual pressures  [2.x.115]  and virtual dilatations  [2.x.116] . 

One should note that the definitions of the volumetric Kirchhoff stress in the three field formulation  [2.x.117]   and the subsequent volumetric tangent differs slightly from the general form given in the section on hyperelastic materials where  [2.x.118] . This is because the pressure  [2.x.119]  is now a primary field as opposed to a constitutively derived quantity. One needs to carefully distinguish between the primary fields and those obtained from the constitutive relations. 

 [2.x.120]  Although the variables are all expressed in terms of spatial quantities, the domain of integration is the initial configuration. This approach is called a  [2.x.121]  total-Lagrangian formulation  [2.x.122] . The approach given in  [2.x.123] , where the domain of integration is the current configuration, could be called an  [2.x.124]  updated Lagrangian formulation  [2.x.125] . The various merits of these two approaches are discussed widely in the literature. It should be noted however that they are equivalent. 


The Euler-Lagrange equations corresponding to the residual are: 

[1.x.111] 

The first equation is the (quasi-static) equilibrium equation in the spatial setting. The second is the constraint that  [2.x.126] . The third is the definition of the pressure  [2.x.127] . 

 [2.x.128]  The simplified single-field derivation ( [2.x.129]  is the only primary variable) below makes it clear how we transform the limits of integration to the reference domain: 

[1.x.112] 

where  [2.x.130] . 

We will use an iterative Newton-Raphson method to solve the nonlinear residual equation  [2.x.131] . For the sake of simplicity we assume dead loading, i.e. the loading does not change due to the deformation. 

The change in a quantity between the known state at  [2.x.132]  and the currently unknown state at  [2.x.133]  is denoted  [2.x.134] . The value of a quantity at the current iteration  [2.x.135]  is denoted  [2.x.136] . The incremental change between iterations  [2.x.137]  and  [2.x.138]  is denoted  [2.x.139] . 

Assume that the state of the system is known for some iteration  [2.x.140] . The linearised approximation to nonlinear governing equations to be solved using the  Newton-Raphson method is: Find  [2.x.141]  such that [1.x.113] then set  [2.x.142] . The tangent is given by 

[1.x.114] Thus, 

[1.x.115] 

where 

[1.x.116] 



Note that the following terms are termed the geometrical stress and  the material contributions to the tangent matrix: 

[1.x.117] 




[1.x.118][1.x.119] 


The three-field formulation used here is effective for quasi-incompressible materials, that is where  [2.x.143]  (where  [2.x.144]  is [1.x.120]), subject to a good choice of the interpolation fields for  [2.x.145]  and  [2.x.146] . Typically a choice of  [2.x.147]  is made. Here  [2.x.148]  is the FE_DGPMonomial class. A popular choice is  [2.x.149]  which is known as the mean dilatation method (see Hughes (2000) for an intuitive discussion). This code can accommodate a  [2.x.150]  formulation. The discontinuous approximation allows  [2.x.151]  and  [2.x.152]  to be condensed out and a classical displacement based method is recovered. 

For fully-incompressible materials  [2.x.153]  and the three-field formulation will still exhibit locking behavior. This can be overcome by introducing an additional constraint into the free energy of the form  [2.x.154] . Here  [2.x.155]  is a Lagrange multiplier to enforce the isochoric constraint. For further details see Miehe (1994). 

The linearised problem can be written as [1.x.121] where 

[1.x.122] 



There are no derivatives of the pressure and dilatation (primary) variables present in the formulation. Thus the discontinuous finite element interpolation of the pressure and dilatation yields a block diagonal matrix for  [2.x.156] ,  [2.x.157]  and  [2.x.158] . Therefore we can easily express the fields  [2.x.159]  and  [2.x.160]  on each cell simply by inverting a local matrix and multiplying it by the local right hand side. We can then insert the result into the remaining equations and recover a classical displacement-based method. In order to condense out the pressure and dilatation contributions at the element level we need the following results: 

[1.x.123] 

and thus [1.x.124] where [1.x.125] Note that due to the choice of  [2.x.161]  and  [2.x.162]  as discontinuous at the element level, all matrices that need to be inverted are defined at the element level. 

The procedure to construct the various contributions is as follows: 

- Construct  [2.x.163] . 

- Form  [2.x.164]  for element and store where  [2.x.165]  was stored in  [2.x.166] . 

- Form  [2.x.167]  and add to  [2.x.168]  to get  [2.x.169]  

- The modified system matrix is called  [2.x.170] .   That is   [1.x.126] 


[1.x.127][1.x.128] 


A good object-oriented design of a Material class would facilitate the extension of this tutorial to a wide range of material types. In this tutorial we simply have one Material class named Material_Compressible_Neo_Hook_Three_Field. Ideally this class would derive from a class HyperelasticMaterial which would derive from the base class Material. The three-field nature of the formulation used here also complicates the matter. 

The Helmholtz free energy function for the three field formulation is  [2.x.171] . The isochoric part of the Kirchhoff stress  [2.x.172]  is identical to that obtained using a one-field formulation for a hyperelastic material. However, the volumetric part of the free energy is now a function of the primary variable  [2.x.173] . Thus, for a three field formulation the constitutive response for the volumetric part of the Kirchhoff stress  [2.x.174]  (and the tangent) is not given by the hyperelastic constitutive law as in a one-field formulation. One can label the term  [2.x.175]  as the volumetric Kirchhoff stress, but the pressure  [2.x.176]  is not derived from the free energy; it is a primary field. 

In order to have a flexible approach, it was decided that the Material_Compressible_Neo_Hook_Three_Field would still be able to calculate and return a volumetric Kirchhoff stress and tangent. In order to do this, we choose to store the interpolated primary fields  [2.x.177]  and  [2.x.178]  in the Material_Compressible_Neo_Hook_Three_Field class associated with the quadrature point. This decision should be revisited at a later stage when the tutorial is extended to account for other materials. 


[1.x.129][1.x.130] 


The numerical example considered here is a nearly-incompressible block under compression. This benchmark problem is taken from 

- S. Reese, P. Wriggers, B.D. Reddy (2000),   A new locking-free brick element technique for large deformation problems in elasticity,    [2.x.179]  Computers and Structures  [2.x.180] ,   <strong> 75 </strong>,   291-304.   DOI: [1.x.131] 

  [2.x.181]  

The material is quasi-incompressible neo-Hookean with [1.x.132]  [2.x.182]  and  [2.x.183] . For such a choice of material properties a conventional single-field  [2.x.184]  approach would lock. That is, the response would be overly stiff. The initial and final configurations are shown in the image above. Using symmetry, we solve for only one quarter of the geometry (i.e. a cube with dimension  [2.x.185] ). The inner-quarter of the upper surface of the domain is subject to a load of  [2.x.186] . [1.x.133] [1.x.134] 

We start by including all the necessary deal.II header files and some C++ related ones. They have been discussed in detail in previous tutorial programs, so you need only refer to past tutorials for details. 

[1.x.135] 



This header gives us the functionality to store data at quadrature points 

[1.x.136] 



Here are the headers necessary to use the LinearOperator class. These are also all conveniently packaged into a single header file, namely <deal.II/lac/linear_operator_tools.h> but we list those specifically required here for the sake of transparency. 

[1.x.137] 



Defined in these two headers are some operations that are pertinent to finite strain elasticity. The first will help us compute some kinematic quantities, and the second provides some stanard tensor definitions. 

[1.x.138] 



We then stick everything that relates to this tutorial program into a namespace of its own, and import all the deal.II function and class names into it: 

[1.x.139] 




[1.x.140]  [1.x.141]    


There are several parameters that can be set in the code so we set up a ParameterHandler object to read in the choices at run-time. 

[1.x.142] 




[1.x.143]  [1.x.144] 




As mentioned in the introduction, a different order interpolation should be used for the displacement  [2.x.187]  than for the pressure  [2.x.188]  and the dilatation  [2.x.189] .  Choosing  [2.x.190]  and  [2.x.191]  as discontinuous (constant) functions at the element level leads to the mean-dilatation method. The discontinuous approximation allows  [2.x.192]  and  [2.x.193]  to be condensed out and a classical displacement based method is recovered. Here we specify the polynomial order used to approximate the solution. The quadrature order should be adjusted accordingly. 

[1.x.145] 




[1.x.146]  [1.x.147] 




Make adjustments to the problem geometry and the applied load.  Since the problem modelled here is quite specific, the load scale can be altered to specific values to compare with the results given in the literature. 

[1.x.148] 




[1.x.149]  [1.x.150] 




We also need the shear modulus  [2.x.194]  and Poisson ration  [2.x.195]  for the neo-Hookean material. 

[1.x.151] 




[1.x.152]  [1.x.153] 




Next, we choose both solver and preconditioner settings.  The use of an effective preconditioner is critical to ensure convergence when a large nonlinear motion occurs within a Newton increment. 

[1.x.154] 




[1.x.155]  [1.x.156] 




A Newton-Raphson scheme is used to solve the nonlinear system of governing equations.  We now define the tolerances and the maximum number of iterations for the Newton-Raphson nonlinear solver. 

[1.x.157] 




[1.x.158]  [1.x.159] 




Set the timestep size  [2.x.196]  and the simulation end-time. 

[1.x.160] 




[1.x.161]  [1.x.162] 




Finally we consolidate all of the above structures into a single container that holds all of our run-time selections. 

[1.x.163] 




[1.x.164]  [1.x.165] 




A simple class to store time data. Its functioning is transparent so no discussion is necessary. For simplicity we assume a constant time step size. 

[1.x.166] 




[1.x.167]  [1.x.168] 




As discussed in the Introduction, Neo-Hookean materials are a type of hyperelastic materials.  The entire domain is assumed to be composed of a compressible neo-Hookean material.  This class defines the behavior of this material within a three-field formulation.  Compressible neo-Hookean materials can be described by a strain-energy function (SEF)  [2.x.197] .    


The isochoric response is given by  [2.x.198]  where  [2.x.199]  and  [2.x.200]  is the first invariant of the left- or right-isochoric Cauchy-Green deformation tensors. That is  [2.x.201] . In this example the SEF that governs the volumetric response is defined as  [2.x.202] , where  [2.x.203]  is the [1.x.169] and  [2.x.204]  is [1.x.170].    


The following class will be used to characterize the material we work with, and provides a central point that one would need to modify if one were to implement a different material model. For it to work, we will store one object of this type per quadrature point, and in each of these objects store the current state (characterized by the values or measures  of the three fields) so that we can compute the elastic coefficients linearized around the current state. 

[1.x.171] 



We update the material model with various deformation dependent data based on  [2.x.205]  and the pressure  [2.x.206]  and dilatation  [2.x.207] , and at the end of the function include a physical check for internal consistency: 

[1.x.172] 



The second function determines the Kirchhoff stress  [2.x.208]  

[1.x.173] 



The fourth-order elasticity tensor in the spatial setting  [2.x.209]  is calculated from the SEF  [2.x.210]  as  [2.x.211]  where  [2.x.212]  

[1.x.174] 



Derivative of the volumetric free energy with respect to  [2.x.213]  return  [2.x.214]  

[1.x.175] 



Second derivative of the volumetric free energy wrt  [2.x.215] . We need the following computation explicitly in the tangent so we make it public.  We calculate  [2.x.216]  

[1.x.176] 



The next few functions return various data that we choose to store with the material: 

[1.x.177] 



Define constitutive model parameters  [2.x.217]  (bulk modulus) and the neo-Hookean model parameter  [2.x.218] : 

[1.x.178] 



Model specific data that is convenient to store with the material: 

[1.x.179] 



The following functions are used internally in determining the result of some of the public functions above. The first one determines the volumetric Kirchhoff stress  [2.x.219] : 

[1.x.180] 



Next, determine the isochoric Kirchhoff stress  [2.x.220] : 

[1.x.181] 



Then, determine the fictitious Kirchhoff stress  [2.x.221] : 

[1.x.182] 



Calculate the volumetric part of the tangent  [2.x.222] : 

[1.x.183] 



Calculate the isochoric part of the tangent  [2.x.223] : 

[1.x.184] 



Calculate the fictitious elasticity tensor  [2.x.224] . For the material model chosen this is simply zero: 

[1.x.185] 




[1.x.186]  [1.x.187] 




As seen in  [2.x.225] , the  [2.x.226]  class offers a method for storing data at the quadrature points.  Here each quadrature point holds a pointer to a material description.  Thus, different material models can be used in different regions of the domain.  Among other data, we choose to store the Kirchhoff stress  [2.x.227]  and the tangent  [2.x.228]  for the quadrature points. 

[1.x.188] 



The first function is used to create a material object and to initialize all tensors correctly: The second one updates the stored values and stresses based on the current deformation measure  [2.x.229] , pressure  [2.x.230]  and dilation  [2.x.231]  field values. 

[1.x.189] 



To this end, we calculate the deformation gradient  [2.x.232]  from the displacement gradient  [2.x.233] , i.e.  [2.x.234]  and then let the material model associated with this quadrature point update itself. When computing the deformation gradient, we have to take care with which data types we compare the sum  [2.x.235] : Since  [2.x.236]  has data type SymmetricTensor, just writing  [2.x.237]  would convert the second argument to a symmetric tensor, perform the sum, and then cast the result to a Tensor (i.e., the type of a possibly nonsymmetric tensor). However, since  [2.x.238]  is nonsymmetric in general, the conversion to SymmetricTensor will fail. We can avoid this back and forth by converting  [2.x.239]  to Tensor first, and then performing the addition as between nonsymmetric tensors: 

[1.x.190] 



The material has been updated so we now calculate the Kirchhoff stress  [2.x.240] , the tangent  [2.x.241]  and the first and second derivatives of the volumetric free energy.        


We also store the inverse of the deformation gradient since we frequently use it: 

[1.x.191] 



We offer an interface to retrieve certain data.  Here are the kinematic variables: 

[1.x.192] 



...and the kinetic variables.  These are used in the material and global tangent matrix and residual assembly operations: 

[1.x.193] 



And finally the tangent: 

[1.x.194] 



In terms of member functions, this class stores for the quadrature point it represents a copy of a material type in case different materials are used in different regions of the domain, as well as the inverse of the deformation gradient... 

[1.x.195] 



... and stress-type variables along with the tangent  [2.x.242] : 

[1.x.196] 




[1.x.197]  [1.x.198] 




The Solid class is the central class in that it represents the problem at hand. It follows the usual scheme in that all it really has is a constructor, destructor and a  [2.x.243]  function that dispatches all the work to private functions of this class: 

[1.x.199] 



In the private section of this class, we first forward declare a number of objects that are used in parallelizing work using the WorkStream object (see the  [2.x.244]  module for more information on this).      


We declare such structures for the computation of tangent (stiffness) matrix and right hand side vector, static condensation, and for updating quadrature points: 

[1.x.200] 



We start the collection of member functions with one that builds the grid: 

[1.x.201] 



Set up the finite element system to be solved: 

[1.x.202] 



Create Dirichlet constraints for the incremental displacement field: 

[1.x.203] 



Several functions to assemble the system and right hand side matrices using multithreading. Each of them comes as a wrapper function, one that is executed to do the work in the WorkStream model on one cell, and one that copies the work done on this one cell into the global object that represents it: 

[1.x.204] 



And similar to perform global static condensation: 

[1.x.205] 



Create and update the quadrature points. Here, no data needs to be copied into a global object, so the copy_local_to_global function is empty: 

[1.x.206] 



Solve for the displacement using a Newton-Raphson method. We break this function into the nonlinear loop and the function that solves the linearized Newton-Raphson step: 

[1.x.207] 



Solution retrieval as well as post-processing and writing data to file: 

[1.x.208] 



Finally, some member variables that describe the current state: A collection of the parameters used to describe the problem setup... 

[1.x.209] 



...the volume of the reference configuration... 

[1.x.210] 



...and description of the geometry on which the problem is solved: 

[1.x.211] 



Also, keep track of the current time and the time spent evaluating certain functions 

[1.x.212] 



A storage object for quadrature point information. As opposed to  [2.x.245] , deal.II's native quadrature point data manager is employed here. 

[1.x.213] 



A description of the finite-element system including the displacement polynomial degree, the degree-of-freedom handler, number of DoFs per cell and the extractor objects used to retrieve information from the solution vectors: 

[1.x.214] 



Description of how the block-system is arranged. There are 3 blocks, the first contains a vector DOF  [2.x.246]  while the other two describe scalar DOFs,  [2.x.247]  and  [2.x.248] . 

[1.x.215] 



Rules for Gauss-quadrature on both the cell and faces. The number of quadrature points on both cells and faces is recorded. 

[1.x.216] 



Objects that store the converged solution and right-hand side vectors, as well as the tangent matrix. There is an AffineConstraints object used to keep track of constraints.  We make use of a sparsity pattern designed for a block system. 

[1.x.217] 



Then define a number of variables to store norms and update norms and normalization factors. 

[1.x.218] 



Methods to calculate error measures 

[1.x.219] 



Compute the volume in the spatial configuration 

[1.x.220] 



Print information to screen in a pleasing way... 

[1.x.221] 




[1.x.222]  [1.x.223] 





[1.x.224]  [1.x.225] 




We initialize the Solid class using data extracted from the parameter file. 

[1.x.226] 



The Finite Element System is composed of dim continuous displacement DOFs, and discontinuous pressure and dilatation DOFs. In an attempt to satisfy the Babuska-Brezzi or LBB stability conditions (see Hughes (2000)), we setup a  [2.x.249]  system.  [2.x.250]  elements satisfy this condition, while  [2.x.251]  elements do not. However, it has been shown that the latter demonstrate good convergence characteristics nonetheless. 

[1.x.227] 



In solving the quasi-static problem, the time becomes a loading parameter, i.e. we increasing the loading linearly with time, making the two concepts interchangeable. We choose to increment time linearly using a constant time step size.    


We start the function with preprocessing, setting the initial dilatation values, and then output the initial grid before starting the simulation proper with the first time (and loading) increment.    


Care must be taken (or at least some thought given) when imposing the constraint  [2.x.252]  on the initial solution field. The constraint corresponds to the determinant of the deformation gradient in the undeformed configuration, which is the identity tensor. We use FE_DGPMonomial bases to interpolate the dilatation field, thus we can't simply set the corresponding dof to unity as they correspond to the monomial coefficients. Thus we use the  [2.x.253]  function to do the work for us. The  [2.x.254]  function requires an argument indicating the hanging node constraints. We have none in this program So we have to create a constraint object. In its original state, constraint objects are unsorted, and have to be sorted (using the  [2.x.255]  function) before they can be used. Have a look at  [2.x.256]  for more information. We only need to enforce the initial condition on the dilatation. In order to do this, we make use of a ComponentSelectFunction which acts as a mask and sets the J_component of n_components to 1. This is exactly what we want. Have a look at its usage in  [2.x.257]  for more information. 

[1.x.228] 



We then declare the incremental solution update  [2.x.258]  and start the loop over the time domain.      


At the beginning, we reset the solution update for this time step... 

[1.x.229] 



...solve the current time step and update total solution vector  [2.x.259] ... 

[1.x.230] 



...and plot the results before moving on happily to the next time step: 

[1.x.231] 




[1.x.232]  [1.x.233] 





[1.x.234]  [1.x.235] 




The first group of private member functions is related to parallelization. We use the Threading Building Blocks library (TBB) to perform as many computationally intensive distributed tasks as possible. In particular, we assemble the tangent matrix and right hand side vector, the static condensation contributions, and update data stored at the quadrature points using TBB. Our main tool for this is the WorkStream class (see the  [2.x.260]  module for more information). 




Firstly we deal with the tangent matrix and right-hand side assembly structures. The PerTaskData object stores local contributions to the global system. 

[1.x.236] 



On the other hand, the ScratchData object stores the larger objects such as the shape-function values array ( [2.x.261] ) and a shape function gradient and symmetric gradient vector which we will use during the assembly. 

[1.x.237] 



Then we define structures to assemble the statically condensed tangent matrix. Recall that we wish to solve for a displacement-based formulation. We do the condensation at the element level as the  [2.x.262]  and  [2.x.263]  fields are element-wise discontinuous.  As these operations are matrix-based, we need to setup a number of matrices to store the local contributions from a number of the tangent matrix sub-blocks.  We place these in the PerTaskData struct.    


We choose not to reset any data in the  [2.x.264]  function as the matrix extraction and replacement tools will take care of this 

[1.x.238] 



The ScratchData object for the operations we wish to perform here is empty since we need no temporary data, but it still needs to be defined for the current implementation of TBB in deal.II.  So we create a dummy struct for this purpose. 

[1.x.239] 



And finally we define the structures to assist with updating the quadrature point information. Similar to the SC assembly process, we do not need the PerTaskData object (since there is nothing to store here) but must define one nonetheless. Note that this is because for the operation that we have here -- updating the data on quadrature points -- the operation is purely local: the things we do on every cell get consumed on every cell, without any global aggregation operation as is usually the case when using the WorkStream class. The fact that we still have to define a per-task data structure points to the fact that the WorkStream class may be ill-suited to this operation (we could, in principle simply create a new task using  [2.x.265]  for each cell) but there is not much harm done to doing it this way anyway. Furthermore, should there be different material models associated with a quadrature point, requiring varying levels of computational expense, then the method used here could be advantageous. 

[1.x.240] 



The ScratchData object will be used to store an alias for the solution vector so that we don't have to copy this large data structure. We then define a number of vectors to extract the solution values and gradients at the quadrature points. 

[1.x.241] 




[1.x.242]  [1.x.243] 




On to the first of the private member functions. Here we create the triangulation of the domain, for which we choose the scaled cube with each face given a boundary ID number.  The grid must be refined at least once for the indentation problem.    


We then determine the volume of the reference configuration and print it for comparison: 

[1.x.244] 



Since we wish to apply a Neumann BC to a patch on the top surface, we must find the cell faces in this part of the domain and mark them with a distinct boundary ID number.  The faces we are looking for are on the +y surface and will get boundary ID 6 (zero through five are already used when creating the six faces of the cube domain): 

[1.x.245] 




[1.x.246]  [1.x.247] 




Next we describe how the FE system is setup.  We first determine the number of components per block. Since the displacement is a vector component, the first dim components belong to it, while the next two describe scalar pressure and dilatation DOFs. 

[1.x.248] 



The DOF handler is then initialized and we renumber the grid in an efficient manner. We also record the number of DOFs per block. 

[1.x.249] 



Setup the sparsity pattern and tangent matrix 

[1.x.250] 



The global system matrix initially has the following structure 

[1.x.251] 

We optimize the sparsity pattern to reflect this structure and prevent unnecessary data creation for the right-diagonal block components. 

[1.x.252] 



We then set up storage vectors 

[1.x.253] 



...and finally set up the quadrature point history: 

[1.x.254] 




[1.x.255]  [1.x.256] Next we compute some information from the FE system that describes which local element DOFs are attached to which block component.  This is used later to extract sub-blocks from the global matrix.    


In essence, all we need is for the FESystem object to indicate to which block component a DOF on the reference cell is attached to.  Currently, the interpolation fields are setup such that 0 indicates a displacement DOF, 1 a pressure DOF and 2 a dilatation DOF. 

[1.x.257] 




[1.x.258]  [1.x.259] The method used to store quadrature information is already described in  [2.x.266] . Here we implement a similar setup for a SMP machine.    


Firstly the actual QPH data objects are created. This must be done only once the grid is refined to its finest level. 

[1.x.260] 



Next we setup the initial quadrature point data. Note that when the quadrature point data is retrieved, it is returned as a vector of smart pointers. 

[1.x.261] 




[1.x.262]  [1.x.263] As the update of QP information occurs frequently and involves a number of expensive operations, we define a multithreaded approach to distributing the task across a number of CPU cores.    


To start this, we first we need to obtain the total solution as it stands at this Newton increment and then create the initial copy of the scratch and copy data objects: 

[1.x.264] 



We then pass them and the one-cell update function to the WorkStream to be processed: 

[1.x.265] 



Now we describe how we extract data from the solution vector and pass it along to each QP storage object for processing. 

[1.x.266] 



We first need to find the values and gradients at quadrature points inside the current cell and then we update each local QP using the displacement gradient and total pressure and dilatation solution values: 

[1.x.267] 




[1.x.268]  [1.x.269] 




The next function is the driver method for the Newton-Raphson scheme. At its top we create a new vector to store the current Newton update step, reset the error storage objects and print solver header. 

[1.x.270] 



We now perform a number of Newton iterations to iteratively solve the nonlinear problem.  Since the problem is fully nonlinear and we are using a full Newton method, the data stored in the tangent matrix and right-hand side vector is not reusable and must be cleared at each Newton step. We then initially build the linear system and check for convergence (and store this value in the first iteration). The unconstrained DOFs of the rhs vector hold the out-of-balance forces, and collectively determine whether or not the equilibrium solution has been attained.      


Although for this particular problem we could potentially construct the RHS vector before assembling the system matrix, for the sake of extensibility we choose not to do so. The benefit to assembling the RHS vector and system matrix separately is that the latter is an expensive operation and we can potentially avoid an extra assembly process by not assembling the tangent matrix when convergence is attained. However, this makes parallelizing the code using MPI more difficult. Furthermore, when extending the problem to the transient case additional contributions to the RHS may result from the time discretization and application of constraints for the velocity and acceleration fields. 

[1.x.271] 



We construct the linear system, but hold off on solving it (a step that should be significantly more expensive than assembly): 

[1.x.272] 



We can now determine the normalized residual error and check for solution convergence: 

[1.x.273] 



If we have decided that we want to continue with the iteration, we solve the linearized system: 

[1.x.274] 



We can now determine the normalized Newton update error: 

[1.x.275] 



Lastly, since we implicitly accept the solution step we can perform the actual update of the solution increment for the current time step, update all quadrature point information pertaining to this new displacement and stress state and continue iterating: 

[1.x.276] 



At the end, if it turns out that we have in fact done more iterations than the parameter file allowed, we raise an exception that can be caught in the main() function. The call <code>AssertThrow(condition, exc_object)</code> is in essence equivalent to <code>if (!cond) throw exc_object;</code> but the former form fills certain fields in the exception object that identify the location (filename and line number) where the exception was raised to make it simpler to identify where the problem happened. 

[1.x.277] 




[1.x.278]  [1.x.279] 




This program prints out data in a nice table that is updated on a per-iteration basis. The next two functions set up the table header and footer: 

[1.x.280] 




[1.x.281]  [1.x.282] 




Calculate the volume of the domain in the spatial configuration 

[1.x.283] 



In contrast to that which was previously called for, in this instance the quadrature point data is specifically non-modifiable since we will only be accessing data. We ensure that the right get_data function is called by marking this update function as constant. 

[1.x.284] 



Calculate how well the dilatation  [2.x.267]  agrees with  [2.x.268]  from the  [2.x.269]  error  [2.x.270] . We also return the ratio of the current volume of the domain to the reference volume. This is of interest for incompressible media where we want to check how well the isochoric constraint has been enforced. 

[1.x.285] 




[1.x.286]  [1.x.287] 




Determine the true residual error for the problem.  That is, determine the error in the residual for the unconstrained degrees of freedom.  Note that to do so, we need to ignore constrained DOFs by setting the residual in these vector components to zero. 

[1.x.288] 




[1.x.289]  [1.x.290] 




Determine the true Newton update error for the problem 

[1.x.291] 




[1.x.292]  [1.x.293] 




This function provides the total solution, which is valid at any Newton step. This is required as, to reduce computational error, the total solution is only updated at the end of the timestep. 

[1.x.294] 




[1.x.295]  [1.x.296] 




Since we use TBB for assembly, we simply setup a copy of the data structures required for the process and pass them, along with the assembly functions to the WorkStream object for processing. Note that we must ensure that the matrix and RHS vector are reset before any assembly operations can occur. Furthermore, since we are describing a problem with Neumann BCs, we will need the face normals and so must specify this in the face update flags. 

[1.x.297] 



The syntax used here to pass data to the WorkStream class is discussed in  [2.x.271] . 

[1.x.298] 



Of course, we still have to define how we assemble the tangent matrix contribution for a single cell.  We first need to reset and initialize some of the scratch data structures and retrieve some basic information regarding the DOF numbering on this cell.  We can precalculate the cell shape function values and gradients. Note that the shape function gradients are defined with regard to the current configuration.  That is  [2.x.272] . 

[1.x.299] 



Now we build the local cell stiffness matrix and RHS vector. Since the global and local system matrices are symmetric, we can exploit this property by building only the lower half of the local matrix and copying the values to the upper half.  So we only assemble half of the  [2.x.273] ,  [2.x.274] ,  [2.x.275]  blocks, while the whole  [2.x.276] ,  [2.x.277] ,  [2.x.278]  blocks are built.      


In doing so, we first extract some configuration dependent variables from our quadrature history objects for the current quadrature point. 

[1.x.300] 



These two tensors store some precomputed data. Their use will explained shortly. 

[1.x.301] 



Next we define some aliases to make the assembly process easier to follow. 

[1.x.302] 



We first compute the contributions from the internal forces.  Note, by definition of the rhs as the negative of the residual, these contributions are subtracted. 

[1.x.303] 



Before we go into the inner loop, we have one final chance to introduce some optimizations. We've already taken into account the symmetry of the system, and we can now precompute some common terms that are repeatedly applied in the inner loop. We won't be excessive here, but will rather focus on expensive operations, namely those involving the rank-4 material stiffness tensor and the rank-2 stress tensor.              


What we may observe is that both of these tensors are contracted with shape function gradients indexed on the "i" DoF. This implies that this particular operation remains constant as we loop over the "j" DoF. For that reason, we can extract this from the inner loop and save the many operations that, for each quadrature point and DoF index "i" and repeated over index "j" are required to double contract a rank-2 symmetric tensor with a rank-4 symmetric tensor, and a rank-1 tensor with a rank-2 tensor.              


At the loss of some readability, this small change will reduce the assembly time of the symmetrized system by about half when using the simulation default parameters, and becomes more significant as the h-refinement level increases. 

[1.x.304] 



Now we're prepared to compute the tangent matrix contributions: 

[1.x.305] 



This is the  [2.x.279]  contribution. It comprises a material contribution, and a geometrical stress contribution which is only added along the local matrix diagonals: 

[1.x.306] 



The material contribution: 

[1.x.307] 



The geometrical stress contribution: 

[1.x.308] 



Next is the  [2.x.280]  contribution 

[1.x.309] 



and lastly the  [2.x.281]  and  [2.x.282]  contributions: 

[1.x.310] 



Next we assemble the Neumann contribution. We first check to see it the cell face exists on a boundary on which a traction is applied and add the contribution if this is the case. 

[1.x.311] 



Using the face normal at this quadrature point we specify the traction in reference configuration. For this problem, a defined pressure is applied in the reference configuration. The direction of the applied traction is assumed not to evolve with the deformation of the domain. The traction is defined using the first Piola-Kirchhoff stress is simply  [2.x.283]  We use the time variable to linearly ramp up the pressure load.                


Note that the contributions to the right hand side vector we compute here only exist in the displacement components of the vector. 

[1.x.312] 



Finally, we need to copy the lower half of the local matrix into the upper half: 

[1.x.313] 




[1.x.314]  [1.x.315] The constraints for this problem are simple to describe. In this particular example, the boundary values will be calculated for the two first iterations of Newton's algorithm. In general, one would build non-homogeneous constraints in the zeroth iteration (that is, when `apply_dirichlet_bc == true` in the code block that follows) and build only the corresponding homogeneous constraints in the following step. While the current example has only homogeneous constraints, previous experiences have shown that a common error is forgetting to add the extra condition when refactoring the code to specific uses. This could lead to errors that are hard to debug. In this spirit, we choose to make the code more verbose in terms of what operations are performed at each Newton step. 

[1.x.316] 



Since we (a) are dealing with an iterative Newton method, (b) are using an incremental formulation for the displacement, and (c) apply the constraints to the incremental displacement field, any non-homogeneous constraints on the displacement update should only be specified at the zeroth iteration. No subsequent contributions are to be made since the constraints will be exactly satisfied after that iteration. 

[1.x.317] 



Furthermore, after the first Newton iteration within a timestep, the constraints remain the same and we do not need to modify or rebuild them so long as we do not clear the  [2.x.284]  object. 

[1.x.318] 



At the zeroth Newton iteration we wish to apply the full set of non-homogeneous and homogeneous constraints that represent the boundary conditions on the displacement increment. Since in general the constraints may be different at each time step, we need to clear the constraints matrix and completely rebuild it. An example case would be if a surface is accelerating; in such a scenario the change in displacement is non-constant between each time step. 

[1.x.319] 



The boundary conditions for the indentation problem in 3D are as follows: On the -x, -y and -z faces (IDs 0,2,4) we set up a symmetry condition to allow only planar movement while the +x and +z faces (IDs 1,5) are traction free. In this contrived problem, part of the +y face (ID 3) is set to have no motion in the x- and z-component. Finally, as described earlier, the other part of the +y face has an the applied pressure but is also constrained in the x- and z-directions.          


In the following, we will have to tell the function interpolation boundary values which components of the solution vector should be constrained (i.e., whether it's the x-, y-, z-displacements or combinations thereof). This is done using ComponentMask objects (see  [2.x.285] ) which we can get from the finite element if we provide it with an extractor object for the component we wish to select. To this end we first set up such extractor objects and later use it when generating the relevant component masks: 

[1.x.320] 



As all Dirichlet constraints are fulfilled exactly after the zeroth Newton iteration, we want to ensure that no further modification are made to those entries. This implies that we want to convert all non-homogeneous Dirichlet constraints into homogeneous ones.          


In this example the procedure to do this is quite straightforward, and in fact we can (and will) circumvent any unnecessary operations when only homogeneous boundary conditions are applied. In a more general problem one should be mindful of hanging node and periodic constraints, which may also introduce some inhomogeneities. It might then be advantageous to keep disparate objects for the different types of constraints, and merge them together once the homogeneous Dirichlet constraints have been constructed. 

[1.x.321] 



Since the affine constraints were finalized at the previous Newton iteration, they may not be modified directly. So we need to copy them to another temporary object and make modification there. Once we're done, we'll transfer them back to the main  [2.x.286]  object. 

[1.x.322] 




[1.x.323]  [1.x.324] Solving the entire block system is a bit problematic as there are no contributions to the  [2.x.287]  block, rendering it noninvertible (when using an iterative solver). Since the pressure and dilatation variables DOFs are discontinuous, we can condense them out to form a smaller displacement-only system which we will then solve and subsequently post-process to retrieve the pressure and dilatation solutions. 




The static condensation process could be performed at a global level but we need the inverse of one of the blocks. However, since the pressure and dilatation variables are discontinuous, the static condensation (SC) operation can also be done on a per-cell basis and we can produce the inverse of the block-diagonal  [2.x.288]  block by inverting the local blocks. We can again use TBB to do this since each operation will be independent of one another.    


Using the TBB via the WorkStream class, we assemble the contributions to form  [2.x.289]  from each element's contributions. These contributions are then added to the global stiffness matrix. Given this description, the following two functions should be clear: 

[1.x.325] 



Now we describe the static condensation process. As per usual, we must first find out which global numbers the degrees of freedom on this cell have and reset some data structures: 

[1.x.326] 



We now extract the contribution of the dofs associated with the current cell to the global stiffness matrix.  The discontinuous nature of the  [2.x.290]  and  [2.x.291]  interpolations mean that their is no coupling of the local contributions at the global level. This is not the case with the  [2.x.292]  dof.  In other words,  [2.x.293] ,  [2.x.294]  and  [2.x.295] , when extracted from the global stiffness matrix are the element contributions.  This is not the case for  [2.x.296] .      


Note: A lower-case symbol is used to denote element stiffness matrices. 




Currently the matrix corresponding to the dof associated with the current element (denoted somewhat loosely as  [2.x.297] ) is of the form: 

[1.x.327] 

     


We now need to modify it such that it appear as 

[1.x.328] 

with  [2.x.298]  where  [2.x.299]  and  [2.x.300] .      


At this point, we need to take note of the fact that global data already exists in the  [2.x.301] ,  [2.x.302]  and  [2.x.303]  sub-blocks.  So if we are to modify them, we must account for the data that is already there (i.e. simply add to it or remove it if necessary).  Since the copy_local_to_global operation is a "+=" operation, we need to take this into account      


For the  [2.x.304]  block in particular, this means that contributions have been added from the surrounding cells, so we need to be careful when we manipulate this block.  We can't just erase the sub-blocks.      


This is the strategy we will employ to get the sub-blocks we want: 

     




-  [2.x.305] : Since we don't have access to  [2.x.306] , but we know its contribution is added to the global  [2.x.307]  matrix, we just want to add the element wise static-condensation  [2.x.308] . 

     




-  [2.x.309] : Similarly,  [2.x.310]  exists in the subblock. Since the copy operation is a += operation, we need to subtract the existing  [2.x.311]  submatrix in addition to "adding" that which we wish to replace it with. 

     




-  [2.x.312] : Since the global matrix is symmetric, this block is the same as the one above and we can simply use  [2.x.313]  as a substitute for this one.      


We first extract element data from the system matrix. So first we get the entire subblock for the cell, then extract  [2.x.314]  for the dofs associated with the current element 

[1.x.329] 



and next the local matrices for  [2.x.315]   [2.x.316]  and  [2.x.317] : 

[1.x.330] 



To get the inverse of  [2.x.318] , we invert it directly.  This operation is relatively inexpensive since  [2.x.319]  since block-diagonal. 

[1.x.331] 



Now we can make condensation terms to add to the  [2.x.320]  block and put them in the cell local matrix  [2.x.321] : 

[1.x.332] 



 [2.x.322]  

[1.x.333] 



 [2.x.323]  

[1.x.334] 



 [2.x.324]  

[1.x.335] 



Next we place  [2.x.325]  in the  [2.x.326]  block for post-processing.  Note again that we need to remove the contribution that already exists there. 

[1.x.336] 




[1.x.337]  [1.x.338] We now have all of the necessary components to use one of two possible methods to solve the linearised system. The first is to perform static condensation on an element level, which requires some alterations to the tangent matrix and RHS vector. Alternatively, the full block system can be solved by performing condensation on a global level. Below we implement both approaches. 

[1.x.339] 



Firstly, here is the approach using the (permanent) augmentation of the tangent matrix. For the following, recall that 

[1.x.340] 

and 

[1.x.341] 

and thus [1.x.342] where [1.x.343] 




At the top, we allocate two temporary vectors to help with the static condensation, and variables to store the number of linear solver iterations and the (hopefully converged) residual. 

[1.x.344] 



In the first step of this function, we solve for the incremental displacement  [2.x.327] .  To this end, we perform static condensation to make  [2.x.328]  and put  [2.x.329]  in the original  [2.x.330]  block. That is, we make  [2.x.331] . 

[1.x.345] 



 [2.x.332]  

[1.x.346] 



 [2.x.333]  

[1.x.347] 



 [2.x.334]  

[1.x.348] 



 [2.x.335]  

[1.x.349] 



 [2.x.336]  

[1.x.350] 



 [2.x.337]  

[1.x.351] 



We've chosen by default a SSOR preconditioner as it appears to provide the fastest solver convergence characteristics for this problem on a single-thread machine.  However, this might not be true for different problem sizes. 

[1.x.352] 



Otherwise if the problem is small enough, a direct solver can be utilised. 

[1.x.353] 



Now that we have the displacement update, distribute the constraints back to the Newton update: 

[1.x.354] 



The next step after solving the displacement problem is to post-process to get the dilatation solution from the substitution:  [2.x.338]  

[1.x.355] 



 [2.x.339]  

[1.x.356] 



 [2.x.340]  

[1.x.357] 



 [2.x.341]  

[1.x.358] 



 [2.x.342]  

[1.x.359] 



we ensure here that any Dirichlet constraints are distributed on the updated solution: 

[1.x.360] 



Finally we solve for the pressure update with the substitution:  [2.x.343]  

[1.x.361] 



 [2.x.344]  

[1.x.362] 



 [2.x.345]  

[1.x.363] 



 [2.x.346]  

[1.x.364] 



and finally....  [2.x.347]  

[1.x.365] 



We are now at the end, so we distribute all constrained dofs back to the Newton update: 

[1.x.366] 



Manual condensation of the dilatation and pressure fields on a local level, and subsequent post-processing, took quite a bit of effort to achieve. To recap, we had to produce the inverse matrix  [2.x.348] , which was permanently written into the global tangent matrix. We then permanently modified  [2.x.349]  to produce  [2.x.350] . This involved the extraction and manipulation of local sub-blocks of the tangent matrix. After solving for the displacement, the individual matrix-vector operations required to solve for dilatation and pressure were carefully implemented. Contrast these many sequence of steps to the much simpler and transparent implementation using functionality provided by the LinearOperator class. 




For ease of later use, we define some aliases for blocks in the RHS vector 

[1.x.367] 



... and for blocks in the Newton update vector. 

[1.x.368] 



We next define some linear operators for the tangent matrix sub-blocks We will exploit the symmetry of the system, so not all blocks are required. 

[1.x.369] 



We then construct a LinearOperator that represents the inverse of (square block)  [2.x.351] . Since it is diagonal (or, when a higher order ansatz it used, nearly diagonal), a Jacobi preconditioner is suitable. 

[1.x.370] 



Now we can construct that transpose of  [2.x.352]  and a linear operator that represents the condensed operations  [2.x.353]  and  [2.x.354]  and the final augmented matrix  [2.x.355] . Note that the schur_complement() operator could also be of use here, but for clarity and the purpose of demonstrating the similarities between the formulation and implementation of the linear solution scheme, we will perform these operations manually. 

[1.x.371] 



Lastly, we define an operator for inverse of augmented stiffness matrix, namely  [2.x.356] . Note that the preconditioner for the augmented stiffness matrix is different to the case when we use static condensation. In this instance, the preconditioner is based on a non-modified  [2.x.357] , while with the first approach we actually modified the entries of this sub-block. However, since  [2.x.358]  and  [2.x.359]  operate on the same space, it remains adequate for this problem. 

[1.x.372] 



Now we are in a position to solve for the displacement field. We can nest the linear operations, and the result is immediately written to the Newton update vector. It is clear that the implementation closely mimics the derivation stated in the introduction. 

[1.x.373] 



The operations need to post-process for the dilatation and pressure fields are just as easy to express. 

[1.x.374] 



Solve the full block system with a direct solver. As it is relatively robust, it may be immune to problem arising from the presence of the zero  [2.x.360]  block. 

[1.x.375] 



Finally, we again ensure here that any Dirichlet constraints are distributed on the updated solution: 

[1.x.376] 




[1.x.377]  [1.x.378] Here we present how the results are written to file to be viewed using ParaView or VisIt. The method is similar to that shown in previous tutorials so will not be discussed in detail. 

[1.x.379] 



Since we are dealing with a large deformation problem, it would be nice to display the result on a displaced grid!  The MappingQEulerian class linked with the DataOut class provides an interface through which this can be achieved without physically moving the grid points in the Triangulation object ourselves.  We first need to copy the solution to a temporary vector and then create the Eulerian mapping. We also specify the polynomial degree to the DataOut object in order to produce a more refined output data set when higher order polynomials are used. 

[1.x.380] 




[1.x.381]  [1.x.382] Lastly we provide the main driver function which appears no different to the other tutorials. 

[1.x.383] 

[1.x.384][1.x.385] 


Firstly, we present a comparison of a series of 3-d results with those in the literature (see Reese et al (2000)) to demonstrate that the program works as expected. 

We begin with a comparison of the convergence with mesh refinement for the  [2.x.361]  and  [2.x.362]  formulations, as summarised in the figure below. The vertical displacement of the midpoint of the upper surface of the block is used to assess convergence. Both schemes demonstrate good convergence properties for varying values of the load parameter  [2.x.363] . The results agree with those in the literature. The lower-order formulation typically overestimates the displacement for low levels of refinement, while the higher-order interpolation scheme underestimates it, but be a lesser degree. This benchmark, and a series of others not shown here, give us confidence that the code is working as it should. 

 [2.x.364]  


A typical screen output generated by running the problem is shown below. The particular case demonstrated is that of the  [2.x.365]  formulation. It is clear that, using the Newton-Raphson method, quadratic convergence of the solution is obtained. Solution convergence is achieved within 5 Newton increments for all time-steps. The converged displacement's  [2.x.366] -norm is several orders of magnitude less than the geometry scale. 

[1.x.386] 






Using the Timer class, we can discern which parts of the code require the highest computational expense. For a case with a large number of degrees-of-freedom (i.e. a high level of refinement), a typical output of the Timer is given below. Much of the code in the tutorial has been developed based on the optimizations described, discussed and demonstrated in  [2.x.367]  and others. With over 93% of the time being spent in the linear solver, it is obvious that it may be necessary to invest in a better solver for large three-dimensional problems. The SSOR preconditioner is not multithreaded but is effective for this class of solid problems. It may be beneficial to investigate the use of another solver such as those available through the Trilinos library. 




[1.x.387] 




We then used ParaView to visualize the results for two cases. The first was for the coarsest grid and the lowest-order interpolation method:  [2.x.368] . The second was on a refined grid using a  [2.x.369]  formulation. The vertical component of the displacement, the pressure  [2.x.370]  and the dilatation  [2.x.371]  fields are shown below. 


For the first case it is clear that the coarse spatial discretization coupled with large displacements leads to a low quality solution (the loading ratio is   [2.x.372] ). Additionally, the pressure difference between elements is very large. The constant pressure field on the element means that the large pressure gradient is not captured. However, it should be noted that locking, which would be present in a standard  [2.x.373]  displacement formulation does not arise even in this poorly discretised case. The final vertical displacement of the tracked node on the top surface of the block is still within 12.5% of the converged solution. The pressure solution is very coarse and has large jumps between adjacent cells. It is clear that the volume nearest to the applied traction undergoes compression while the outer extents of the domain are in a state of expansion. The dilatation solution field and pressure field are clearly linked, with positive dilatation indicating regions of positive pressure and negative showing regions placed in compression. As discussed in the Introduction, a compressive pressure has a negative sign while an expansive pressure takes a positive sign. This stems from the definition of the volumetric strain energy function and is opposite to the physically realistic interpretation of pressure. 


 [2.x.374]  

Combining spatial refinement and a higher-order interpolation scheme results in a high-quality solution. Three grid refinements coupled with a  [2.x.375]  formulation produces a result that clearly captures the mechanics of the problem. The deformation of the traction surface is well resolved. We can now observe the actual extent of the applied traction, with the maximum force being applied at the central point of the surface causing the largest compression. Even though very high strains are experienced in the domain, especially at the boundary of the region of applied traction, the solution remains accurate. The pressure field is captured in far greater detail than before. There is a clear distinction and transition between regions of compression and expansion, and the linear approximation of the pressure field allows a refined visualization of the pressure at the sub-element scale. It should however be noted that the pressure field remains discontinuous and could be smoothed on a continuous grid for the post-processing purposes. 




 [2.x.376]  

This brief analysis of the results demonstrates that the three-field formulation is effective in circumventing volumetric locking for highly-incompressible media. The mixed formulation is able to accurately simulate the displacement of a near-incompressible block under compression. The command-line output indicates that the volumetric change under extreme compression resulted in less than 0.01% volume change for a Poisson's ratio of 0.4999. 

In terms of run-time, the  [2.x.377]  formulation tends to be more computationally expensive than the  [2.x.378]  for a similar number of degrees-of-freedom (produced by adding an extra grid refinement level for the lower-order interpolation). This is shown in the graph below for a batch of tests run consecutively on a single 4-core (8-thread) machine. The increase in computational time for the higher-order method is likely due to the increased band-width required for the higher-order elements. As previously mentioned, the use of a better solver and preconditioner may mitigate the expense of using a higher-order formulation. It was observed that for the given problem using the multithreaded Jacobi preconditioner can reduce the computational runtime by up to 72% (for the worst case being a higher-order formulation with a large number of degrees-of-freedom) in comparison to the single-thread SSOR preconditioner. However, it is the author's experience that the Jacobi method of preconditioning may not be suitable for some finite-strain problems involving alternative constitutive models. 


 [2.x.379]  


Lastly, results for the displacement solution for the 2-d problem are showcased below for two different levels of grid refinement. It is clear that due to the extra constraints imposed by simulating in 2-d that the resulting displacement field, although qualitatively similar, is different to that of the 3-d case. 


 [2.x.380]  

[1.x.388] [1.x.389][1.x.390] 


There are a number of obvious extensions for this work: 

- Firstly, an additional constraint could be added to the free-energy   function in order to enforce a high degree of incompressibility in   materials. An additional Lagrange multiplier would be introduced,   but this could most easily be dealt with using the principle of   augmented Lagrange multipliers. This is demonstrated in  [2.x.381] Simo and   Taylor (1991)  [2.x.382] . 

- The constitutive relationship used in this   model is relatively basic. It may be beneficial to split the material   class into two separate classes, one dealing with the volumetric   response and the other the isochoric response, and produce a generic   materials class (i.e. having abstract virtual functions that derived   classes have to implement) that would allow for the addition of more complex   material models. Such models could include other hyperelastic   materials, plasticity and viscoelastic materials and others. 

- The program has been developed for solving problems on single-node   multicore machines. With a little effort, the program could be   extended to a large-scale computing environment through the use of   Petsc or Trilinos, using a similar technique to that demonstrated in    [2.x.383] . This would mostly involve changes to the setup, assembly,    [2.x.384]  and linear solver routines. 

- As this program assumes quasi-static equilibrium, extensions to   include dynamic effects would be necessary to study problems where   inertial effects are important, e.g. problems involving impact. 

- Load and solution limiting procedures may be necessary for highly   nonlinear problems. It is possible to add a linesearch algorithm to   limit the step size within a Newton increment to ensure optimum   convergence. It may also be necessary to use a load limiting method,   such as the Riks method, to solve unstable problems involving   geometric instability such as buckling and snap-through. 

- Many physical problems involve contact. It is possible to include   the effect of frictional or frictionless contact between objects   into this program. This would involve the addition of an extra term   in the free-energy functional and therefore an addition to the   assembly routine. One would also need to manage the contact problem   (detection and stress calculations) itself. An alternative to   additional penalty terms in the free-energy functional would be to   use active set methods such as the one used in  [2.x.385] . 

- The complete condensation procedure using LinearOperators has been   coded into the linear solver routine. This could also have been   achieved through the application of the schur_complement()   operator to condense out one or more of the fields in a more   automated manner. 

- Finally, adaptive mesh refinement, as demonstrated in  [2.x.386]  and    [2.x.387] , could provide additional solution accuracy. [1.x.391] [1.x.392]  [2.x.388]  

 [2.x.389] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6] 

 [2.x.3]  

[1.x.7] [1.x.8] [1.x.9][1.x.10] 


In this example we present how to use periodic boundary conditions in deal.II. Periodic boundary conditions are algebraic constraints that typically occur in computations on representative regions of a larger domain that repeat in one or more directions. 

An example is the simulation of the electronic structure of photonic crystals, because they have a lattice-like structure and, thus, it often suffices to do the actual computation on only one box of the lattice. To be able to proceed this way one has to assume that the model can be periodically extended to the other boxes; this requires the solution to have a periodic structure. 

[1.x.11] [1.x.12][1.x.13] 


deal.II provides a number of high level entry points to impose periodic boundary conditions. The general approach to apply periodic boundary conditions consists of three steps (see also the  [2.x.4]  "Glossary entry on periodic boundary conditions"): 

-# Create a mesh 

-# Identify those pairs of faces on different parts of the boundary across which    the solution should be symmetric, using  [2.x.5]  

-# Add the periodicity information to the mesh    using  [2.x.6]  

-# Add periodicity constraints using  [2.x.7]  

The second and third step are necessary for parallel meshes using the  [2.x.8]  class to ensure that cells on opposite sides of the domain but connected by periodic faces are part of the ghost layer if one of them is stored on the local processor. If the Triangulation is not a  [2.x.9]  these steps are not necessary. 

The first step consists of collecting matching periodic faces and storing them in a  [2.x.10]  of  [2.x.11]  This is done with the function  [2.x.12]  that can be invoked for example like this: 

[1.x.14] 



This call loops over all faces of the container dof_handler on the periodic boundaries with boundary indicator  [2.x.13]  and  [2.x.14]  respectively. (You can assign these boundary indicators by hand after creating the coarse mesh, see  [2.x.15]  "Boundary indicator". Alternatively, you can also let many of the functions in namespace GridGenerator do this for if you specify the "colorize" flag; in that case, these functions will assign different boundary indicators to different parts of the boundary, with the details typically spelled out in the documentation of these functions.) 

Concretely, if  [2.x.16]  are the vertices of two faces  [2.x.17] , then the function call above will match pairs of faces (and dofs) such that the difference between  [2.x.18]  and  [2.x.19]  vanishes in every component apart from direction and stores the resulting pairs with associated data in  [2.x.20]  (See  [2.x.21]  for detailed information about the matching process.) 

Consider, for example, the colored unit square  [2.x.22]  with boundary indicator 0 on the left, 1 on the right, 2 on the bottom and 3 on the top faces. (See the documentation of  [2.x.23]  for this convention on how boundary indicators are assigned.) Then, 

[1.x.15] 

would yield periodicity constraints such that  [2.x.24]  for all  [2.x.25] . 

If we instead consider the parallelogram given by the convex hull of  [2.x.26] ,  [2.x.27] ,  [2.x.28] ,  [2.x.29]  we can achieve the constraints  [2.x.30]  by specifying an  [2.x.31]  

[1.x.16] 

or 

[1.x.17] 

Here, again, the assignment of boundary indicators 0 and 1 stems from what  [2.x.32]  documents. 

The resulting  [2.x.33]  can be used in  [2.x.34]  for populating an AffineConstraints object with periodicity constraints: 

[1.x.18] 



Apart from this high level interface there are also variants of  [2.x.35]  available that combine those two steps (see the variants of  [2.x.36]  

There is also a low level interface to  [2.x.37]  if more flexibility is needed. The low level variant allows to directly specify two faces that shall be constrained: 

[1.x.19] 

Here, we need to specify the orientation of the two faces using  [2.x.38]   [2.x.39]  and  [2.x.40]  For a closer description have a look at the documentation of  [2.x.41]  The remaining parameters are the same as for the high level interface apart from the self-explaining  [2.x.42]  and  [2.x.43]  


[1.x.20] [1.x.21][1.x.22] 


In the following, we show how to use the above functions in a more involved example. The task is to enforce rotated periodicity constraints for the velocity component of a Stokes flow. 

On a quarter-circle defined by  [2.x.44]  we are going to solve the Stokes problem [1.x.23] 

where the boundary  [2.x.45]  is defined as  [2.x.46] . For the remaining parts of the boundary we are going to use periodic boundary conditions, i.e. 

[1.x.24] 



The mesh will be generated by  [2.x.47]  which also documents how it assigns boundary indicators to its various boundaries if its `colorize` argument is set to `true`. [1.x.25] [1.x.26] 

This example program is a slight modification of  [2.x.48]  running in parallel using Trilinos to demonstrate the usage of periodic boundary conditions in deal.II. We thus omit to discuss the majority of the source code and only comment on the parts that deal with periodicity constraints. For the rest have a look at  [2.x.49]  and the full source code at the bottom. 




In order to implement periodic boundary conditions only two functions have to be modified: 

-  [2.x.50] : To populate an AffineConstraints object with periodicity constraints 

-  [2.x.51] : To supply a distributed triangulation with periodicity information. 




The rest of the program is identical to  [2.x.52] , so let us skip this part and only show these two functions in the following. (The full program can be found in the "Plain program" section below, though.) 








   




[1.x.27]  [1.x.28] 

[1.x.29] 



Before we can prescribe periodicity constraints, we need to ensure that cells on opposite sides of the domain but connected by periodic faces are part of the ghost layer if one of them is stored on the local processor. At this point we need to think about how we want to prescribe periodicity. The vertices  [2.x.53]  of a face on the left boundary should be matched to the vertices  [2.x.54]  of a face on the lower boundary given by  [2.x.55]  where the rotation matrix  [2.x.56]  and the offset  [2.x.57]  are given by 

[1.x.30] 

The data structure we are saving the resulting information into is here based on the Triangulation. 

[1.x.31] 



Now telling the triangulation about the desired periodicity is particularly easy by just calling  [2.x.58]  

[1.x.32] 



After we provided the mesh with the necessary information for the periodicity constraints, we are now able to actual create them. For describing the matching we are using the same approach as before, i.e., the  [2.x.59]  of a face on the left boundary should be matched to the vertices  [2.x.60]  of a face on the lower boundary given by  [2.x.61]  where the rotation matrix  [2.x.62]  and the offset  [2.x.63]  are given by 

[1.x.33] 

These two objects not only describe how faces should be matched but also in which sense the solution should be transformed from  [2.x.64]  to  [2.x.65] . 

[1.x.34] 



For setting up the constraints, we first store the periodicity information in an auxiliary object of type  [2.x.66]   [2.x.67]  </code>. The periodic boundaries have the boundary indicators 2 (x=0) and 3 (y=0). All the other parameters we have set up before. In this case the direction does not matter. Due to  [2.x.68]  this is exactly what we want. 

[1.x.35] 



Next, we need to provide information on which vector valued components of the solution should be rotated. Since we choose here to just constraint the velocity and this starts at the first component of the solution vector, we simply insert a 0: 

[1.x.36] 



After setting up all the information in periodicity_vector all we have to do is to tell make_periodicity_constraints to create the desired constraints. 

[1.x.37] 



The rest of the program is then again identical to  [2.x.69] . We will omit it here now, but as before, you can find these parts in the "Plain program" section below. 




[1.x.38][1.x.39] 


The created output is not very surprising. We simply see that the solution is periodic with respect to the left and lower boundary: 

 [2.x.70]  

Without the periodicity constraints we would have ended up with the following solution: 

 [2.x.71]  [1.x.40] [1.x.41]  [2.x.72]  

 [2.x.73] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] ,  [2.x.4] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34] 

 [2.x.5]  

[1.x.35] 


[1.x.36] [1.x.37][1.x.38] 


This program deals with the problem of coupling different physics in different parts of the domain. Specifically, let us consider the following situation that couples a Stokes fluid with an elastic solid (these two problems were previously discussed separately in  [2.x.6]  and  [2.x.7] , where you may want to read up on the individual equations): 

- In a part  [2.x.8]  of  [2.x.9] , we have a fluid flowing that satisfies the   time independent Stokes equations (in the form that involves the strain   tensor):   [1.x.39] 

  Here,  [2.x.10]  are the fluid velocity and pressure, respectively.   We prescribe the velocity on part of the external boundary,   [1.x.40] 

  while we assume free-flow conditions on the remainder of the external   boundary,   [1.x.41] 



- The remainder of the domain,  [2.x.11]  is   occupied by a solid whose deformation field  [2.x.12]  satisfies the   elasticity equation,   [1.x.42] 

  where  [2.x.13]  is the rank-4 elasticity tensor (for which we will use a   particularly simple form by assuming that the solid is isotropic).   It deforms in reaction to the forces exerted by the   fluid flowing along the boundary of the solid. We assume this deformation to   be so small that it has no feedback effect on the fluid, i.e. the coupling   is only in one direction. For simplicity, we will assume that the   solid's external boundary is clamped, i.e.   [1.x.43] 



- As a consequence of the small displacement assumption, we will pose the   following boundary conditions on the interface between the fluid and solid:   first, we have no slip boundary conditions for the fluid,   [1.x.44] 

  Secondly, the forces (traction) on the solid equal the normal stress from the fluid,   [1.x.45] 

  where  [2.x.14]  is the normal vector on  [2.x.15]  pointing from   the solid to the fluid. 

We get a weak formulation of this problem by following our usual rule of multiplying from the left by a test function and integrating over the domain. It then looks like this: Find  [2.x.16]  such that 

[1.x.46] 

for all test functions  [2.x.17] ; the first, second, and third lines correspond to the fluid, solid, and interface contributions, respectively. Note that  [2.x.18]  is only a subspace of the spaces listed above to accommodate for the various Dirichlet boundary conditions. 

This sort of coupling is of course possible by simply having two Triangulation and two DoFHandler objects, one each for each of the two subdomains. On the other hand, deal.II is much simpler to use if there is a single DoFHandler object that knows about the discretization of the entire problem. 

This program is about how this can be achieved. Note that the goal is not to present a particularly useful physical model (a realistic fluid-structure interaction model would have to take into account the finite deformation of the solid and the effect this has on the fluid): this is, after all, just a tutorial program intended to demonstrate techniques, not to solve actual problems. Furthermore, we will make the assumption that the interface between the subdomains is aligned with coarse mesh cell faces. 


[1.x.47][1.x.48] 


Before going into more details let us state the obvious: this is a problem with multiple solution variables; for this, you will probably want to read the  [2.x.19]  documentation module first, which presents the basic philosophical framework in which we address problems with more than one solution variable. But back to the problem at hand: 

The fundamental idea to implement these sort of problems in deal.II goes as follows: in the problem formulation, the velocity and pressure variables  [2.x.20]  only live in the fluid subdomain  [2.x.21] . But let's assume that we extend them by zero to the entire domain  [2.x.22]  (in the general case this means that they will be discontinuous along  [2.x.23] ). So what is the appropriate function space for these variables? We know that on  [2.x.24]  we should require  [2.x.25] , so for the extensions  [2.x.26]  to the whole domain the following appears a useful set of function spaces: 

[1.x.49] 

(Since this is not important for the current discussion, we have omitted the question of boundary values from the choice of function spaces; this question also affects whether we can choose  [2.x.27]  for the pressure or whether we have to choose the space  [2.x.28]  for the pressure. None of these questions are relevant to the following discussion, however.) 

Note that these are indeed a linear function spaces with obvious norm. Since no confusion is possible in practice, we will henceforth omit the tilde again to denote the extension of a function to the whole domain and simply refer by  [2.x.29]  to both the original and the extended function. 

For discretization, we need finite dimensional subspaces  [2.x.30]  of  [2.x.31] . For Stokes, we know from  [2.x.32]  that an appropriate choice is  [2.x.33]  but this only holds for that part of the domain occupied by the fluid. For the extended field, let's use the following subspaces defined on the triangulation  [2.x.34] : 

[1.x.50] 

In other words, on  [2.x.35]  we choose the usual discrete spaces but we keep the (discontinuous) extension by zero. The point to make is that we now need a description of a finite element space for functions that are zero on a cell &mdash; and this is where the FE_Nothing class comes in: it describes a finite dimensional function space of functions that are constant zero. A particular property of this peculiar linear vector space is that it has no degrees of freedom: it isn't just finite dimensional, it is in fact zero dimensional, and consequently for objects of this type,  [2.x.36]  will return zero. For discussion below, let us give this space a proper symbol: [1.x.51] The symbol  [2.x.37]  reminds of the fact that functions in this space are zero. Obviously, we choose  [2.x.38] . 

This entire discussion above can be repeated for the variables we use to describe the elasticity equation. Here, for the extended variables, we have 

[1.x.52] 

and we will typically use a finite element space of the kind 

[1.x.53] 

of polynomial degree  [2.x.39] . 

So to sum up, we are going to look for a discrete vector-valued solution  [2.x.40]  in the following space: 

[1.x.54] 






[1.x.55][1.x.56] 


So how do we implement this sort of thing? First, we realize that the discrete space  [2.x.41]  essentially calls for two different finite elements: First, on the fluid subdomain, we need the element  [2.x.42]  which in deal.II is readily implemented by 

[1.x.57] 

where  [2.x.43]  implements the space of functions that are always zero. Second, on the solid subdomain, we need the element  [2.x.44] , which we get using 

[1.x.58] 



The next step is that we associate each of these two elements with the cells that occupy each of the two subdomains. For this we realize that in a sense the two elements are just variations of each other in that they have the same number of vector components but have different polynomial degrees &mdash; this smells very much like what one would do in  [2.x.45]  finite element methods, and it is exactly what we are going to do here: we are going to (ab)use the classes and facilities of the hp-namespace to assign different elements to different cells. In other words, we will use collect the two finite elements in an  [2.x.46]  will integrate with an appropriate  [2.x.47]  using an  [2.x.48]  object, and our DoFHandler will be in [1.x.59]-mode. You may wish to take a look at  [2.x.49]  for an overview of all of these concepts. 

Before going on describing the testcase, let us clarify a bit [1.x.60] this approach of extending the functions by zero to the entire domain and then mapping the problem on to the hp-framework makes sense: 

- It makes things uniform: On all cells, the number of vector components is   the same (here,  [2.x.50] ). This makes all sorts of   things possible since a uniform description allows for code   re-use. For example, counting degrees of freedom per vector   component  [2.x.51]  sorting degrees of   freedom by component  [2.x.52]  subsequent   partitioning of matrices and vectors into blocks and many other   functions work as they always did without the need to add special   logic to them that describes cases where some of the variables only   live on parts of the domain. Consequently, you have all sorts of   tools already available to you in programs like the current one that   weren't originally written for the multiphysics case but work just   fine in the current context. 

- It allows for easy graphical output: All graphical output formats we support   require that each field in the output is defined on all nodes of the   mesh. But given that now all solution components live everywhere,   our existing DataOut routines work as they always did, and produce   graphical output suitable for visualization -- the fields will   simply be extended by zero, a value that can easily be filtered out   by visualization programs if not desired. 

- There is essentially no cost: The trick with the FE_Nothing does not add any   degrees of freedom to the overall problem, nor do we ever have to handle a   shape function that belongs to these components &mdash; the FE_Nothing has   no degrees of freedom, not does it have shape functions, all it does is take   up vector components. 


[1.x.61][1.x.62] 


More specifically, in the program we have to address the following points: 

- Implementing the bilinear form, and in particular dealing with the   interface term, both in the matrix and the sparsity pattern. 

- Implementing Dirichlet boundary conditions on the external and   internal parts of the boundaries    [2.x.53] . 


[1.x.63][1.x.64] 


Let us first discuss implementing the bilinear form, which at the discrete level we recall to be 

[1.x.65] 

Given that we have extended the fields by zero, we could in principle write the integrals over subdomains to the entire domain  [2.x.54] , though it is little additional effort to first ask whether a cell is part of the elastic or fluid region before deciding which terms to integrate. Actually integrating these terms is not very difficult; for the Stokes equations, the relevant steps have been shown in  [2.x.55] , whereas for the elasticity equation we take essentially the form shown in the  [2.x.56]  module (rather than the one from  [2.x.57] ). 

The term that is of more interest is the interface term, [1.x.66] Based on our assumption that the interface  [2.x.58]  coincides with cell boundaries, this can in fact be written as a set of face integrals. If we denote the velocity, pressure and displacement components of shape function  [2.x.59]  using the extractor notation  [2.x.60] , then the term above yields the following contribution to the global matrix entry  [2.x.61] : [1.x.67] Although it isn't immediately obvious, this term presents a slight complication: while  [2.x.62]  and  [2.x.63]  are evaluated on the solid side of the interface (they are test functions for the displacement and the normal vector to  [2.x.64] , respectively, we need to evaluate  [2.x.65]  on the fluid side of the interface since they correspond to the stress/force exerted by the fluid. In other words, in our implementation, we will need FEFaceValue objects for both sides of the interface. To make things slightly worse, we may also have to deal with the fact that one side or the other may be refined, leaving us with the need to integrate over parts of a face. Take a look at the implementation below on how to deal with this. 

As an additional complication, the matrix entries that result from this term need to be added to the sparsity pattern of the matrix somehow. This is the realm of various functions in the DoFTools namespace like  [2.x.66]  and  [2.x.67]  Essentially, what these functions do is simulate what happens during assembly of the system matrix: whenever assembly would write a nonzero entry into the global matrix, the functions in DoFTools would add an entry to the sparsity pattern. We could therefore do the following: let  [2.x.68]  add all those entries to the sparsity pattern that arise from the regular cell-by-cell integration, and then do the same by hand that arise from the interface terms. If you look at the implementation of the interface integrals in the program below, it should be obvious how to do that and would require no more than maybe 100 lines of code at most. 

But we're lazy people: the interface term couples degrees of freedom from two adjacent cells along a face, which is exactly the kind of thing one would do in discontinuous Galerkin schemes for which the function  [2.x.69]  was written. This is a superset of matrix entries compared to the usual  [2.x.70]  it will also add all entries that result from computing terms coupling the degrees of freedom from both sides of all faces. Unfortunately, for the simplest version of this function, this is a pretty big superset. Consider for example the following mesh with two cells and a  [2.x.71]  finite element: 

[1.x.68] 

Here, the sparsity pattern produced by  [2.x.72]  will only have entries for degrees of freedom that couple on a cell. However, it will not have sparsity pattern entries  [2.x.73] . The sparsity pattern generated by  [2.x.74]  will have these entries, however: it assumes that you want to build a sparsity pattern for a bilinear form that couples [1.x.69] degrees of freedom from adjacent cells. This is not what we want: our interface term acts only on a small subset of cells, and we certainly don't need all the extra couplings between two adjacent fluid cells, or two adjacent solid cells. Furthermore, the fact that we use higher order elements means that we would really generate many many more entries than we actually need: on the coarsest mesh, in 2d, 44,207 nonzero entries instead of 16,635 for  [2.x.75]  leading to plenty of zeros in the matrix we later build (of course, the 16,635 are not enough since they don't include the interface entries). This ratio would be even worse in 3d. 

So being extremely lazy comes with a cost: too many entries in the matrix. But we can get away with being moderately lazy: there is a variant of  [2.x.76]  that allows us to specify which vector components of the finite element couple with which other components, both in cell terms as well as in face terms. For cells that are in the solid subdomain, we couple all displacements with each other; for fluid cells, all velocities with all velocities and the pressure, but not the pressure with itself. Since no cell has both sets of variables, there is no need to distinguish between the two kinds of cells, so we can write the mask like this: 

[1.x.70] 

Here, we have used the fact that the first  [2.x.77]  components of the finite element are the velocities, then the pressure, and then the  [2.x.78]  displacements. (We could as well have stated that the velocities/pressure also couple with the displacements since no cell ever has both sets of variables.) On the other hand, the interface terms require a mask like this: 

[1.x.71] 

In other words, all displacement test functions (components  [2.x.79] ) couple with all velocity and pressure shape functions on the other side of an interface. This is not entirely true, though close: in fact, the exact form of the interface term only those pressure displacement shape functions that are indeed nonzero on the common interface, which is not true for all shape functions; on the other hand, it really couples all velocities (since the integral involves gradients of the velocity shape functions, which are all nonzero on all faces of the cell). However, the mask we build above, is not capable of these subtleties. Nevertheless, through these masks we manage to get the number of sparsity pattern entries down to 21,028 &mdash; good enough for now. 




[1.x.72][1.x.73] 


The second difficulty is that while we know how to enforce a zero velocity or stress on the external boundary (using  [2.x.80]  called with an appropriate component mask and setting different boundary indicators for solid and fluid external boundaries), we now also needed the velocity to be zero on the interior interface, i.e.  [2.x.81] . At the time of writing this, there is no function in deal.II that handles this part, but it isn't particularly difficult to implement by hand: essentially, we just have to loop over all cells, and if it is a fluid cell and its neighbor is a solid cell, then add constraints that ensure that the velocity degrees of freedom on this face are zero. Some care is necessary to deal with the case that the adjacent solid cell is refined, yielding the following code: 

[1.x.74] 



The call  [2.x.82]  tells the AffineConstraints to start a new constraint for degree of freedom  [2.x.83]  of the form  [2.x.84] . Typically, one would then proceed to set individual coefficients  [2.x.85]  to nonzero values (using  [2.x.86]  or set  [2.x.87]  to something nonzero (using  [2.x.88]  doing nothing as above, funny as it looks, simply leaves the constraint to be  [2.x.89] , which is exactly what we need in the current context. The call to  [2.x.90]  makes sure that we only set boundary values to zero for velocity but not pressure components. 

Note that there are cases where this may yield incorrect results: notably, once we find a solid neighbor child to a current fluid cell, we assume that all neighbor children on the common face are in the solid subdomain. But that need not be so; consider, for example, the following mesh: 

[1.x.75] 



In this case, we would set all velocity degrees of freedom on the right face of the left cell to zero, which is incorrect for the top degree of freedom on that face. That said, that can only happen if the fluid and solid subdomains do not coincide with a set of complete coarse mesh cells &mdash; but this is a contradiction to the assumption stated at the end of the first section of this introduction. 




[1.x.76][1.x.77] 


We will consider the following situation as a testcase: 

 [2.x.91]  

As discussed at the top of this document, we need to assume in a few places that a cell is either entirely in the fluid or solid part of the domain and, furthermore, that all children of an inactive cell also belong to the same subdomain. This can definitely be ensured if the coarse mesh already subdivides the mesh into solid and fluid coarse mesh cells; given the geometry outlined above, we can do that by using an  [2.x.92]  coarse mesh, conveniently provided by the  [2.x.93]  function. 

The fixed boundary at the bottom implies  [2.x.94] , and we also prescribe Dirichlet conditions for the flow at the top so that we get inflow at the left and outflow at the right. At the left and right boundaries, no boundary conditions are imposed explicitly for the flow, yielding the implicit no-stress condition  [2.x.95] . The conditions on the interface between the two domains has already been discussed above. 

For simplicity, we choose the material parameters to be  [2.x.96] . In the results section below, we will also show a 3d simulation that can be obtained from the same program. The boundary conditions and geometry are defined nearly analogously to the 2d situation above. 


[1.x.78][1.x.79] 


In the program, we need a way to identify which part of the domain a cell is in. There are many different ways of doing this. A typical way would be to use the  [2.x.97]  "subdomain_id" tag available with each cell, though this field has a special meaning in %parallel computations. An alternative is the  [2.x.98]  "material_id" field also available with every cell. It has the additional advantage that it is inherited from the mother to the child cell upon mesh refinement; in other words, we would set the material id once upon creating the mesh and it will be correct for all active cells even after several refinement cycles. We therefore go with this alternative: we define an  [2.x.99]  with symbolic names for material_id numbers and will use them to identify which part of the domain a cell is on. 

Secondly, we use an object of type DoFHandler operating in [1.x.80]-mode. This class needs to know which cells will use the Stokes and which the elasticity finite element. At the beginning of each refinement cycle we will therefore have to walk over all cells and set the (in hp-parlance) active FE index to whatever is appropriate in the current situation. While we can use symbolic names for the material id, the active FE index is in fact a number that will frequently be used to index into collections of objects (e.g. of type  [2.x.100]  and  [2.x.101]  that means that the active FE index actually has to have value zero for the fluid and one for the elastic part of the domain. 


[1.x.81][1.x.82] 


This program is primarily intended to show how to deal with different physics in different parts of the domain, and how to implement such models in deal.II. As a consequence, we won't bother coming up with a good solver: we'll just use the SparseDirectUMFPACK class which always works, even if not with optimal complexity. We will, however, comment on possible other solvers in the [1.x.83] section. 


[1.x.84][1.x.85] 


One of the trickier aspects of this program is how to estimate the error. Because it works on almost any program, we'd like to use the KellyErrorEstimator, and we can relatively easily do that here as well using code like the following: 

[1.x.86] 

This gives us two sets of error indicators for each cell. We would then somehow combine them into one for mesh refinement, for example using something like the following (note that we normalize the squared error indicator in the two vectors because error quantities have physical units that do not match in the current situation, leading to error indicators that may differ by orders of magnitude between the two subdomains): 

[1.x.87] 

(In the code, we actually weigh the error indicators 4:1 in favor of the ones computed on the Stokes subdomain since refinement is otherwise heavily biased towards the elastic subdomain, but this is just a technicality. The factor 4 has been determined heuristically to work reasonably well.) 

While this principle is sound, it doesn't quite work as expected. The reason is that the KellyErrorEstimator class computes error indicators by integrating the jump in the solution's gradient around the faces of each cell. This jump is likely to be very large at the locations where the solution is discontinuous and extended by zero; it also doesn't become smaller as the mesh is refined. The KellyErrorEstimator class can't just ignore the interface because it essentially only sees a DoFHandler in [1.x.88]-mode where the element type changes from one cell to another &mdash; precisely the thing that the [1.x.89]-mode was designed for, the interface in the current program looks no different than the interfaces in  [2.x.102] , for example, and certainly no less legitimate. Be that as it may, the end results is that there is a layer of cells on both sides of the interface between the two subdomains where error indicators are irrationally large. Consequently, most of the mesh refinement is focused on the interface. 

This clearly wouldn't happen if we had a refinement indicator that actually understood something about the problem and simply ignore the interface between subdomains when integrating jump terms. On the other hand, this program is about showing how to represent problems where we have different physics in different subdomains, not about the peculiarities of the KellyErrorEstimator, and so we resort to the big hammer called "heuristics": we simply set the error indicators of cells at the interface to zero. This cuts off the spikes in the error indicators. At first sight one would also think that it prevents the mesh from being refined at the interface, but the requirement that neighboring cells may only differ by one level of refinement will still lead to a reasonably refined mesh. 

While this is clearly a suboptimal solution, it works for now and leaves room for future improvement. [1.x.90] [1.x.91] 


[1.x.92]  [1.x.93] 




The include files for this program are the same as for many others before. The only new one is the one that declares FE_Nothing as discussed in the introduction. The ones in the hp directory have already been discussed in  [2.x.103] . 







[1.x.94] 




[1.x.95]  [1.x.96] 




This is the main class. It is, if you want, a combination of  [2.x.104]  and  [2.x.105]  in that it has member variables that either address the global problem (the Triangulation and DoFHandler objects, as well as the  [2.x.106]  and various linear algebra objects) or that pertain to either the elasticity or Stokes sub-problems. The general structure of the class, however, is like that of most of the other programs implementing stationary problems.    


There are a few helper functions (<code>cell_is_in_fluid_domain, cell_is_in_solid_domain</code>) of self-explanatory nature (operating on the symbolic names for the two subdomains that will be used as material_ids for cells belonging to the subdomains, as explained in the introduction) and a few functions (<code>make_grid, set_active_fe_indices, assemble_interface_terms</code>) that have been broken out of other functions that can be found in many of the other tutorial programs and that will be discussed as we get to their implementation.    


The final set of variables ( [2.x.107] ) describes the material properties used for the two physics models. 

[1.x.97] 




[1.x.98]  [1.x.99] 




The following class does as its name suggests. The boundary values for the velocity are  [2.x.108]  in 2d and  [2.x.109]  in 3d, respectively. The remaining boundary conditions for this problem are all homogeneous and have been discussed in the introduction. The right hand side forcing term is zero for both the fluid and the solid so we don't need an extra class for it. 

[1.x.100] 




[1.x.101]  [1.x.102] 





[1.x.103]  [1.x.104] 




Let's now get to the implementation of the primary class of this program. The first few functions are the constructor and the helper functions that can be used to determine which part of the domain a cell is in. Given the discussion of these topics in the introduction, their implementation is rather obvious. In the constructor, note that we have to construct the  [2.x.110]  object from the base elements for Stokes and elasticity; using the  [2.x.111]  function assigns them spots zero and one in this collection, an order that we have to remember and use consistently in the rest of the program. 

[1.x.105] 




[1.x.106]  [1.x.107] 




The next pair of functions deals with generating a mesh and making sure all flags that denote subdomains are correct.  [2.x.112] , as discussed in the introduction, generates an  [2.x.113]  mesh (or an  [2.x.114]  mesh in 3d) to make sure that each coarse mesh cell is completely within one of the subdomains. After generating this mesh, we loop over its boundary and set the boundary indicator to one at the top boundary, the only place where we set nonzero Dirichlet boundary conditions. After this, we loop again over all cells to set the material indicator &mdash; used to denote which part of the domain we are in, to either the fluid or solid indicator. 

[1.x.108] 



The second part of this pair of functions determines which finite element to use on each cell. Above we have set the material indicator for each coarse mesh cell, and as mentioned in the introduction, this information is inherited from mother to child cell upon mesh refinement.    


In other words, whenever we have refined (or created) the mesh, we can rely on the material indicators to be a correct description of which part of the domain a cell is in. We then use this to set the active FE index of the cell to the corresponding element of the  [2.x.115]  member variable of this class: zero for fluid cells, one for solid cells. 

[1.x.109] 




[1.x.110]  [1.x.111] 




The next step is to setup the data structures for the linear system. To this end, we first have to set the active FE indices with the function immediately above, then distribute degrees of freedom, and then determine constraints on the linear system. The latter includes hanging node constraints as usual, but also the inhomogeneous boundary values at the top fluid boundary, and zero boundary values along the perimeter of the solid subdomain. 

[1.x.112] 



There are more constraints we have to handle, though: we have to make sure that the velocity is zero at the interface between fluid and solid. The following piece of code was already presented in the introduction: 

[1.x.113] 



At the end of all this, we can declare to the constraints object that we now have all constraints ready to go and that the object can rebuild its internal data structures for better efficiency: 

[1.x.114] 



In the rest of this function we create a sparsity pattern as discussed extensively in the introduction, and use it to initialize the matrix; then also set vectors to their correct sizes: 

[1.x.115] 




[1.x.116]  [1.x.117] 




Following is the central function of this program: the one that assembles the linear system. It has a long section of setting up auxiliary functions at the beginning: from creating the quadrature formulas and setting up the FEValues, FEFaceValues and FESubfaceValues objects necessary to integrate the cell terms as well as the interface terms for the case where cells along the interface come together at same size or with differing levels of refinement... 

[1.x.118] 



...to objects that are needed to describe the local contributions to the global linear system... 

[1.x.119] 



...to variables that allow us to extract certain components of the shape functions and cache their values rather than having to recompute them at every quadrature point: 

[1.x.120] 



Then comes the main loop over all cells and, as in  [2.x.116] , the initialization of the  [2.x.117]  object for the current cell and the extraction of a FEValues object that is appropriate for the current cell: 

[1.x.121] 



With all of this done, we continue to assemble the cell terms for cells that are part of the Stokes and elastic regions. While we could in principle do this in one formula, in effect implementing the one bilinear form stated in the introduction, we realize that our finite element spaces are chosen in such a way that on each cell, one set of variables (either velocities and pressure, or displacements) are always zero, and consequently a more efficient way of computing local integrals is to do only what's necessary based on an  [2.x.118]  clause that tests which part of the domain we are in.          


The actual computation of the local matrix is the same as in  [2.x.119]  as well as that given in the  [2.x.120]  documentation module for the elasticity equations: 

[1.x.122] 



Once we have the contributions from cell integrals, we copy them into the global matrix (taking care of constraints right away, through the  [2.x.121]  function). Note that we have not written anything into the  [2.x.122]  variable, though we still need to pass it along since the elimination of nonzero boundary values requires the modification of local and consequently also global right hand side values: 

[1.x.123] 



The more interesting part of this function is where we see about face terms along the interface between the two subdomains. To this end, we first have to make sure that we only assemble them once even though a loop over all faces of all cells would encounter each part of the interface twice. We arbitrarily make the decision that we will only evaluate interface terms if the current cell is part of the solid subdomain and if, consequently, a face is not at the boundary and the potential neighbor behind it is part of the fluid domain. Let's start with these conditions: 

[1.x.124] 



At this point we know that the current cell is a candidate for integration and that a neighbor behind face  [2.x.123]  exists. There are now three possibilities: 

                 




- The neighbor is at the same refinement level and has no children. 

- The neighbor has children. 

- The neighbor is coarser.                  


In all three cases, we are only interested in it if it is part of the fluid subdomain. So let us start with the first and simplest case: if the neighbor is at the same level, has no children, and is a fluid cell, then the two cells share a boundary that is part of the interface along which we want to integrate interface terms. All we have to do is initialize two FEFaceValues object with the current face and the face of the neighboring cell (note how we find out which face of the neighboring cell borders on the current cell) and pass things off to the function that evaluates the interface terms (the third through fifth arguments to this function provide it with scratch arrays). The result is then again copied into the global matrix, using a function that knows that the DoF indices of rows and columns of the local matrix result from different cells: 

[1.x.125] 



The second case is if the neighbor has further children. In that case, we have to loop over all the children of the neighbor to see if they are part of the fluid subdomain. If they are, then we integrate over the common interface, which is a face for the neighbor and a subface of the current cell, requiring us to use an FEFaceValues for the neighbor and an FESubfaceValues for the current cell: 

[1.x.126] 



The last option is that the neighbor is coarser. In that case we have to use an FESubfaceValues object for the neighbor and a FEFaceValues for the current cell; the rest is the same as before: 

[1.x.127] 



In the function that assembles the global system, we passed computing interface terms to a separate function we discuss here. The key is that even though we can't predict the combination of FEFaceValues and FESubfaceValues objects, they are both derived from the FEFaceValuesBase class and consequently we don't have to care: the function is simply called with two such objects denoting the values of the shape functions on the quadrature points of the two sides of the face. We then do what we always do: we fill the scratch arrays with the values of shape functions and their derivatives, and then loop over all entries of the matrix to compute the local integrals. The details of the bilinear form we evaluate here are given in the introduction. 

[1.x.128] 




[1.x.129]  [1.x.130] 




As discussed in the introduction, we use a rather trivial solver here: we just pass the linear system off to the SparseDirectUMFPACK direct solver (see, for example,  [2.x.124] ). The only thing we have to do after solving is ensure that hanging node and boundary value constraints are correct. 

[1.x.131] 




[1.x.132]  [1.x.133] 




Generating graphical output is rather trivial here: all we have to do is identify which components of the solution vector belong to scalars and/or vectors (see, for example,  [2.x.125]  for a previous example), and then pass it all on to the DataOut class: 

[1.x.134] 




[1.x.135]  [1.x.136] 




The next step is to refine the mesh. As was discussed in the introduction, this is a bit tricky primarily because the fluid and the solid subdomains use variables that have different physical dimensions and for which the absolute magnitude of error estimates is consequently not directly comparable. We will therefore have to scale them. At the top of the function, we therefore first compute error estimates for the different variables separately (using the velocities but not the pressure for the fluid domain, and the displacements in the solid domain): 

[1.x.137] 



We then normalize error estimates by dividing by their norm and scale the fluid error indicators by a factor of 4 as discussed in the introduction. The results are then added together into a vector that contains error indicators for all cells: 

[1.x.138] 



The second to last part of the function, before actually refining the mesh, involves a heuristic that we have already mentioned in the introduction: because the solution is discontinuous, the KellyErrorEstimator class gets all confused about cells that sit at the boundary between subdomains: it believes that the error is large there because the jump in the gradient is large, even though this is entirely expected and a feature that is in fact present in the exact solution as well and therefore not indicative of any numerical error.      


Consequently, we set the error indicators to zero for all cells at the interface; the conditions determining which cells this affects are slightly awkward because we have to account for the possibility of adaptively refined meshes, meaning that the neighboring cell can be coarser than the current one, or could in fact be refined some more. The structure of these nested conditions is much the same as we encountered when assembling interface terms in  [2.x.126] . 

[1.x.139] 




[1.x.140]  [1.x.141] 




This is, as usual, the function that controls the overall flow of operation. If you've read through tutorial programs  [2.x.127]  through  [2.x.128] , for example, then you are already quite familiar with the following structure: 

[1.x.142] 




[1.x.143]  [1.x.144] 




This, final, function contains pretty much exactly what most of the other tutorial programs have: 

[1.x.145] 

[1.x.146] [1.x.147][1.x.148] 


[1.x.149][1.x.150] 




When running the program, you should get output like the following: 

[1.x.151] 



The results are easily visualized: 

 [2.x.129]  

The plots are easily interpreted: as the flow drives down on the left side and up on the right side of the upright part of the solid, it produces a pressure that is high on the left and low on the right, and these forces bend the vertical part of the solid to the right. 


[1.x.152][1.x.153] 


By changing the dimension of the  [2.x.130]  class in  [2.x.131]  to 3, we can also run the same problem 3d. You'd get output along the following lines: 

[1.x.154] 

You'll notice that the big bottleneck is the solver: SparseDirectUmfpack needs nearly 5 hours and some 80 GB of memory to solve the last iteration of this problem on a 2016 workstation (the second to last iteration took only 16 minutes). Clearly a better solver is needed here, a topic discussed below. 

The results can also be visualized and yield good pictures as well. Here is one, showing both a vector plot for the velocity (in oranges), the solid displacement (in blues), and shading the solid region: 

 [2.x.132]  

In addition to the lack of a good solver, the mesh is a bit unbalanced: mesh refinement heavily favors the fluid subdomain (in 2d, it was the other way around, prompting us to weigh the fluid error indicators higher). Clearly, some tweaking of the relative importance of error indicators in the two subdomains is important if one wanted to go on doing more 3d computations. 


[1.x.155] [1.x.156][1.x.157] 


[1.x.158][1.x.159] 


An obvious place to improve the program would be to use a more sophisticated solver &mdash; in particular one that scales well and will also work for realistic 3d problems. This shouldn't actually be too hard to achieve here, because of the one-way coupling from fluid into solid. To this end, assume we had re-ordered degrees of freedom in such a way that we first have all velocity and pressure degrees of freedom, and then all displacements (this is easily possible using  [2.x.133]  Then the system matrix could be split into the following block form: [1.x.160] where  [2.x.134]  is the Stokes matrix for velocity and pressure (it could be further subdivided into a  [2.x.135]  matrix as in  [2.x.136] , though this is immaterial for the current purpose),  [2.x.137]  results from the elasticity equations for the displacements, and  [2.x.138]  is the matrix that comes from the interface conditions. Now notice that the matrix [1.x.161] is the inverse of  [2.x.139] . Applying this matrix requires only one solve with  [2.x.140]  and  [2.x.141]  each since [1.x.162] can be computed as  [2.x.142]  followed by  [2.x.143] . 

One can therefore expect that [1.x.163] would be a good preconditioner if  [2.x.144] . 

That means, we only need good preconditioners for Stokes and the elasticity equations separately. These are well known: for Stokes, we can use the preconditioner discussed in the results section of  [2.x.145] ; for elasticity, a good preconditioner would be a single V-cycle of a geometric or algebraic multigrid. There are more open questions, however: For an "optimized" solver block-triangular preconditioner built from two sub-preconditioners, one point that often comes up is that, when choosing parameters for the sub-preconditioners, values that work well when solving the two problems separately may not be optimal when combined into a multiphysics preconditioner.  In particular, when solving just a solid or fluid mechanics problem separately, the balancing act between the number of iterations to convergence and the cost of applying the preconditioner on a per iteration basis may lead one to choose an expensive preconditioner for the Stokes problem and a cheap preconditioner for the elasticity problem (or vice versa).  When combined, however, there is the additional constraint that you want the two sub-preconditioners to converge at roughly the same rate, or else the cheap one may drive up the global number of iterations while the expensive one drives up the cost-per-iteration. For example, while a single AMG V-cycle is a good approach for elasticity by itself, when combined into a multiphysics problem there may be an incentive to using a full W-cycle or multiple cycles to help drive down the total solve time. 


[1.x.164][1.x.165] 


As mentioned in the introduction, the refinement indicator we use for this program is rather ad hoc. A better one would understand that the jump in the gradient of the solution across the interface is not indicative of the error but to be expected and ignore the interface when integrating the jump terms. Nevertheless, this is not what the KellyErrorEstimator class does. Another, bigger question, is whether this kind of estimator is a good strategy in the first place: for example, if we want to have maximal accuracy in one particular aspect of the displacement (e.g. the displacement at the top right corner of the solid), then is it appropriate to scale the error indicators for fluid and solid to the same magnitude? Maybe it is necessary to solve the fluid problem with more accuracy than the solid because the fluid solution directly affects the solids solution? Maybe the other way around? 

Consequently, an obvious possibility for improving the program would be to implement a better refinement criterion. There is some literature on this topic; one of a variety of possible starting points would be the paper by Thomas Wick on "Adaptive finite elements for monolithic fluid-structure interaction on a prolongated domain: Applied to an heart valve simulation", Proceedings of the Computer Methods in Mechanics Conference 2011 (CMM-2011), 9-12 May 2011, Warszaw, Poland. 


[1.x.166][1.x.167] 


The results above are purely qualitative as there is no evidence that our scheme in fact converges. An obvious thing to do would therefore be to add some quantitative measures to check that the scheme at least converges to [1.x.168]. For example, we could output for each refinement cycle the deflection of the top right corner of the part of the solid that protrudes into the fluid subdomain. Or we could compute the net force vector or torque the fluid exerts on the solid. 


[1.x.169][1.x.170] 


In reality, most fluid structure interaction problems are so that the movement of the solid does affect the flow of the fluid. For example, the forces of the air around an air foil cause it to flex and to change its shape. Likewise, a flag flaps in the wind, completely changing its shape. 

Such problems where the coupling goes both ways are typically handled in an Arbitrary Lagrangian Eulerian (ALE) framework, in which the displacement of the solid is extended into the fluid domain in some smooth way, rather than by zero as we do here. The extended displacement field is then used to deform the mesh on which we compute the fluid flow. Furthermore, the boundary conditions for the fluid on the interface are no longer that the velocity is zero; rather, in a time dependent program, the fluid velocity must be equal to the time derivative of the displacement along the interface. [1.x.171] [1.x.172]  [2.x.146]  

 [2.x.147] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34] 

 [2.x.3]  

[1.x.35] 

[1.x.36] [1.x.37][1.x.38] 


This program deals with the [1.x.39], 

[1.x.40] 

This equation appears in the modeling of thin structures such as roofs of stadiums. These objects are of course in reality three-dimensional with a large aspect ratio of lateral extent to perpendicular thickness, but one can often very accurately model these structures as two dimensional by making assumptions about how internal forces vary in the perpendicular direction. These assumptions lead to the equation above. 

The model typically comes in two different kinds, depending on what kinds of boundary conditions are imposed. The first case, 

[1.x.41] 

corresponds to the edges of the thin structure attached to the top of a wall of height  [2.x.4]  in such a way that the bending forces that act on the structure are  [2.x.5] ; in most physical situations, one will have  [2.x.6] , corresponding to the structure simply sitting atop the wall. 

In the second possible case of boundary values, one would have 

[1.x.42] 

This corresponds to a "clamped" structure for which a nonzero  [2.x.7]  implies a certain angle against the horizontal. 

As with Dirichlet and Neumann boundary conditions for the Laplace equation, it is of course possible to have one kind of boundary conditions on one part of the boundary, and the other on the remainder. 


[1.x.43][1.x.44] 


The fundamental issue with the equation is that it takes four derivatives of the solution. In the case of the Laplace equation we treated in  [2.x.8] ,  [2.x.9] , and several other tutorial programs, one multiplies by a test function, integrates, integrates by parts, and ends up with only one derivative on both the test function and trial function -- something one can do with functions that are continuous globally, but may have kinks at the interfaces between cells: The derivative may not be defined at the interfaces, but that is on a lower-dimensional manifold (and so doesn't show up in the integrated value). 

But for the biharmonic equation, if one followed the same procedure using integrals over the entire domain (i.e., the union of all cells), one would end up with two derivatives on the test functions and trial functions each. If one were to use the usual piecewise polynomial functions with their kinks on cell interfaces, the first derivative would yield a discontinuous gradient, and the second derivative with delta functions on the interfaces -- but because both the second derivatives of the test functions and of the trial functions yield a delta function, we would try to integrate the product of two delta functions. For example, in 1d, where  [2.x.10]  are the usual piecewise linear "hat functions", we would get integrals of the sort 

[1.x.45] 

where  [2.x.11]  is the node location at which the shape function  [2.x.12]  is defined, and  [2.x.13]  is the mesh size (assumed uniform). The problem is that delta functions in integrals are defined using the relationship 

[1.x.46] 

But that only works if (i)  [2.x.14]  is actually well defined at  [2.x.15] , and (ii) if it is finite. On the other hand, an integral of the form 

[1.x.47] 

does not make sense. Similar reasoning can be applied for 2d and 3d situations. 

In other words: This approach of trying to integrate over the entire domain and then integrating by parts can't work. 

Historically, numerical analysts have tried to address this by inventing finite elements that are "C<sup>1</sup> continuous", i.e., that use shape functions that are not just continuous but also have continuous first derivatives. This is the realm of elements such as the Argyris element, the Clough-Tocher element and others, all developed in the late 1960s. From a twenty-first century perspective, they can only be described as bizarre in their construction. They are also exceedingly cumbersome to implement if one wants to use general meshes. As a consequence, they have largely fallen out of favor and deal.II currently does not contain implementations of these shape functions. 


[1.x.48][1.x.49] 


So how does one approach solving such problems then? That depends a bit on the boundary conditions. If one has the first set of boundary conditions, i.e., if the equation is 

[1.x.50] 

then the following trick works (at least if the domain is convex, see below): In the same way as we obtained the mixed Laplace equation of  [2.x.16]  from the regular Laplace equation by introducing a second variable, we can here introduce a variable  [2.x.17]  and can then replace the equations above by the following, "mixed" system: 

[1.x.51] 

In other words, we end up with what is in essence a system of two coupled Laplace equations for  [2.x.18] , each with Dirichlet-type boundary conditions. We know how to solve such problems, and it should not be very difficult to construct good solvers and preconditioners for this system either using the techniques of  [2.x.19]  or  [2.x.20] . So this case is pretty simple to deal with. 

 [2.x.21]  It is worth pointing out that this only works for domains whose   boundary has corners if the domain is also convex -- in other words,   if there are no re-entrant corners.   This sounds like a rather random condition, but it makes   sense in view of the following two facts: The solution of the   original biharmonic equation must satisfy  [2.x.22] . On the   other hand, the mixed system reformulation above suggests that both    [2.x.23]  and  [2.x.24]  satisfy  [2.x.25]  because both variables only   solve a Poisson equation. In other words, if we want to ensure that   the solution  [2.x.26]  of the mixed problem is also a solution of the   original biharmonic equation, then we need to be able to somehow   guarantee that the solution of  [2.x.27]  is in fact more smooth   than just  [2.x.28] . This can be argued as follows: For convex   domains,   [1.x.52] implies that if the right hand side  [2.x.29] , then    [2.x.30]  if the domain is convex and the boundary is smooth   enough. (This could also be guaranteed if the domain boundary is   sufficiently smooth -- but domains whose boundaries have no corners   are not very practical in real life.)   We know that  [2.x.31]  because it solves the equation    [2.x.32] , but we are still left with the condition on convexity   of the boundary; one can show that polygonal, convex domains are   good enough to guarantee that  [2.x.33]  in this case (smoothly   bounded, convex domains would result in  [2.x.34] , but we don't   need this much regularity). On the other hand, if the domain is not   convex, we can not guarantee that the solution of the mixed system   is in  [2.x.35] , and consequently may obtain a solution that can't be   equal to the solution of the original biharmonic equation. 

The more complicated situation is if we have the "clamped" boundary conditions, i.e., if the equation looks like this: 

[1.x.53] 

The same trick with the mixed system does not work here, because we would end up with [1.x.54] Dirichlet and Neumann boundary conditions for  [2.x.36] , but none for  [2.x.37] . 


The solution to this conundrum arrived with the Discontinuous Galerkin method wave in the 1990s and early 2000s: In much the same way as one can use [1.x.55] shape functions for the Laplace equation by penalizing the size of the discontinuity to obtain a scheme for an equation that has one derivative on each shape function, we can use a scheme that uses [1.x.56] (but not  [2.x.38]  continuous) shape functions and penalize the jump in the derivative to obtain a scheme for an equation that has two derivatives on each shape function. In analogy to the Interior Penalty (IP) method for the Laplace equation, this scheme for the biharmonic equation is typically called the  [2.x.39]  IP (or C0IP) method, since it uses  [2.x.40]  (continuous but not continuously differentiable) shape functions with an interior penalty formulation. 


[1.x.57][1.x.58] 


We base this program on the  [2.x.41]  IP method presented by Susanne Brenner and Li-Yeng Sung in the paper "C [2.x.42]  Interior Penalty Method for Linear Fourth Order Boundary Value Problems on polygonal domains''  [2.x.43]  , where the method is derived for the biharmonic equation with "clamped" boundary conditions. 

As mentioned, this method relies on the use of  [2.x.44]  Lagrange finite elements where the  [2.x.45]  continuity requirement is relaxed and has been replaced with interior penalty techniques. To derive this method, we consider a  [2.x.46]  shape function  [2.x.47]  which vanishes on  [2.x.48] . We introduce notation  [2.x.49]  as the set of all faces of  [2.x.50] ,  [2.x.51]  as the set of boundary faces, and  [2.x.52]  as the set of interior faces for use further down below. Since the higher order derivatives of  [2.x.53]  have two values on each interface  [2.x.54]  (shared by the two cells  [2.x.55] ), we cope with this discontinuity by defining the following single-valued functions on  [2.x.56] : 

[1.x.59] 

for  [2.x.57]  (i.e., for the gradient and the matrix of second derivatives), and where  [2.x.58]  denotes a unit vector normal to  [2.x.59]  pointing from  [2.x.60]  to  [2.x.61] . In the literature, these functions are referred to as the "jump" and "average" operations, respectively. 

To obtain the  [2.x.62]  IP approximation  [2.x.63] , we left multiply the biharmonic equation by  [2.x.64] , and then integrate over  [2.x.65] . As explained above, we can't do the integration by parts on all of  [2.x.66]  with these shape functions, but we can do it on each cell individually since the shape functions are just polynomials on each cell. Consequently, we start by using the following integration-by-parts formula on each mesh cell  [2.x.67] : 

[1.x.60] 

At this point, we have two options: We can integrate the domain term's  [2.x.68]  one more time to obtain 

[1.x.61] 

For a variety of reasons, this turns out to be a variation that is not useful for our purposes. 

Instead, what we do is recognize that  [2.x.69] , and we can re-sort these operations as  [2.x.70]  where we typically write  [2.x.71]  to indicate that this is the "Hessian" matrix of second derivatives. With this re-ordering, we can now integrate the divergence, rather than the gradient operator, and we get the following instead: 

[1.x.62] 

Here, the colon indicates a double-contraction over the indices of the matrices to its left and right, i.e., the scalar product between two tensors. The outer product of two vectors  [2.x.72]  yields the matrix  [2.x.73] . 

Then, we sum over all cells  [2.x.74] , and take into account that this means that every interior face appears twice in the sum. If we therefore split everything into a sum of integrals over cell interiors and a separate sum over cell interfaces, we can use the jump and average operators defined above. There are two steps left: First, because our shape functions are continuous, the gradients of the shape functions may be discontinuous, but the continuity guarantees that really only the normal component of the gradient is discontinuous across faces whereas the tangential component(s) are continuous. Second, the discrete formulation that results is not stable as the mesh size goes to zero, and to obtain a stable formulation that converges to the correct solution, we need to add the following terms: 

[1.x.63] 

Then, after making cancellations that arise, we arrive at the following C0IP formulation of the biharmonic equation: find  [2.x.75]  such that  [2.x.76]  on  [2.x.77]  and 

[1.x.64] 

where 

[1.x.65] 

and 

[1.x.66] 

Here,  [2.x.78]  is the penalty parameter which both weakly enforces the boundary condition 

[1.x.67] 

on the boundary interfaces  [2.x.79] , and also ensures that in the limit  [2.x.80] ,  [2.x.81]  converges to a  [2.x.82]  continuous function.  [2.x.83]  is chosen to be large enough to guarantee the stability of the method. We will discuss our choice in the program below. 


[1.x.68][1.x.69] 

On polygonal domains, the weak solution  [2.x.84]  to the biharmonic equation lives in  [2.x.85]  where  [2.x.86]  is determined by the interior angles at the corners of  [2.x.87] . For instance, whenever  [2.x.88]  is convex,  [2.x.89] ;  [2.x.90]  may be less than one if the domain has re-entrant corners but  [2.x.91]  is close to  [2.x.92]  if one of all interior angles is close to  [2.x.93] . 

Now suppose that the  [2.x.94]  IP solution  [2.x.95]  is approximated by  [2.x.96]  shape functions with polynomial degree  [2.x.97] . Then the discretization outlined above yields the convergence rates as discussed below. 


[1.x.70] 

Ideally, we would like to measure convergence in the "energy norm"  [2.x.98] . However, this does not work because, again, the discrete solution  [2.x.99]  does not have two (weak) derivatives. Instead, one can define a discrete ( [2.x.100]  IP) seminorm that is "equivalent" to the energy norm, as follows: 

[1.x.71] 



In this seminorm, the theory in the paper mentioned above yields that we can expect 

[1.x.72] 

much as one would expect given the convergence rates we know are true for the usual discretizations of the Laplace equation. 

Of course, this is true only if the exact solution is sufficiently smooth. Indeed, if  [2.x.101]  with  [2.x.102] ,  [2.x.103]  where  [2.x.104] , then the convergence rate of the  [2.x.105]  IP method is  [2.x.106] . In other words, the optimal convergence rate can only be expected if the solution is so smooth that  [2.x.107] ; this can only happen if (i) the domain is convex with a sufficiently smooth boundary, and (ii)  [2.x.108] . In practice, of course, the solution is what it is (independent of the polynomial degree we choose), and the last condition can then equivalently be read as saying that there is definitely no point in choosing  [2.x.109]  large if  [2.x.110]  is not also large. In other words, the only reasonably choices for  [2.x.111]  are  [2.x.112]  because larger polynomial degrees do not result in higher convergence orders. 

For the purposes of this program, we're a bit too lazy to actually implement this equivalent seminorm -- though it's not very difficult and would make for a good exercise. Instead, we'll simply check in the program what the "broken"  [2.x.113]  seminorm 

[1.x.73] 

yields. The convergence rate in this norm can, from a theoretical perspective, of course not be [1.x.74] than the one for  [2.x.114]  because it contains only a subset of the necessary terms, but it could at least conceivably be better. It could also be the case that we get the optimal convergence rate even though there is a bug in the program, and that that bug would only show up in sub-optimal rates for the additional terms present in  [2.x.115] . But, one might hope that if we get the optimal rate in the broken norm and the norms discussed below, then the program is indeed correct. The results section will demonstrate that we obtain optimal rates in all norms shown. 


[1.x.75] 

The optimal convergence rate in the  [2.x.116] -norm is  [2.x.117]  provided  [2.x.118] . More details can be found in Theorem 4.6 of  [2.x.119]  . 

The default in the program below is to choose  [2.x.120] . In that case, the theorem does not apply, and indeed one only gets  [2.x.121]  instead of  [2.x.122]  as we will show in the results section. 


[1.x.76] 

Given that we expect  [2.x.123]  in the best of cases for a norm equivalent to the  [2.x.124]  seminorm, and  [2.x.125]  for the  [2.x.126]  norm, one may ask about what happens in the  [2.x.127]  seminorm that is intermediate to the two others. A reasonable guess is that one should expect  [2.x.128] . There is probably a paper somewhere that proves this, but we also verify that this conjecture is experimentally true below. 




[1.x.77][1.x.78] 


We remark that the derivation of the  [2.x.129]  IP method for the biharmonic equation with other boundary conditions -- for instance, for the first set of boundary conditions namely  [2.x.130]  and  [2.x.131]  on  [2.x.132]  -- can be obtained with suitable modifications to  [2.x.133]  and  [2.x.134]  described in the book chapter  [2.x.135]  . 


[1.x.79][1.x.80] 


The last step that remains to describe is what this program solves for. As always, a trigonometric function is both a good and a bad choice because it does not lie in any polynomial space in which we may seek the solution while at the same time being smoother than real solutions typically are (here, it is in  [2.x.136]  while real solutions are typically only in  [2.x.137]  or so on convex polygonal domains, or somewhere between  [2.x.138]  and  [2.x.139]  if the domain is not convex). But, since we don't have the means to describe solutions of realistic problems in terms of relatively simple formulas, we just go with the following, on the unit square for the domain  [2.x.140] : 

[1.x.81] 

As a consequence, we then need choose as boundary conditions the following: 

[1.x.82] 

The right hand side is easily computed as 

[1.x.83] 

The program has classes  [2.x.141]  and  [2.x.142]  that encode this information. [1.x.84] [1.x.85] 


[1.x.86]  [1.x.87] 




The first few include files have already been used in the previous example, so we will not explain their meaning here again. The principal structure of the program is very similar to that of, for example,  [2.x.143]  and so we include many of the same header files. 







[1.x.88] 



The two most interesting header files will be these two: 

[1.x.89] 



The first of these is responsible for providing the class FEInterfaceValues that can be used to evaluate quantities such as the jump or average of shape functions (or their gradients) across interfaces between cells. This class will be quite useful in evaluating the penalty terms that appear in the C0IP formulation. 










[1.x.90] 



In the following namespace, let us define the exact solution against which we will compare the numerically computed one. It has the form  [2.x.144]  (only the 2d case is implemented), and the namespace also contains a class that corresponds to the right hand side that produces this solution. 

[1.x.91] 




[1.x.92]  [1.x.93]    


The following is the principal class of this tutorial program. It has the structure of many of the other tutorial programs and there should really be nothing particularly surprising about its contents or the constructor that follows it. 

[1.x.94] 



Next up are the functions that create the initial mesh (a once refined unit square) and set up the constraints, vectors, and matrices on each mesh. Again, both of these are essentially unchanged from many previous tutorial programs. 

[1.x.95] 




[1.x.96]  [1.x.97]    


The following pieces of code are more interesting. They all relate to the assembly of the linear system. While assembling the cell-interior terms is not of great difficulty -- that works in essence like the assembly of the corresponding terms of the Laplace equation, and you have seen how this works in  [2.x.145]  or  [2.x.146] , for example -- the difficulty is with the penalty terms in the formulation. These require the evaluation of gradients of shape functions at interfaces of cells. At the least, one would therefore need to use two FEFaceValues objects, but if one of the two sides is adaptively refined, then one actually needs an FEFaceValues and one FESubfaceValues objects; one also needs to keep track which shape functions live where, and finally we need to ensure that every face is visited only once. All of this is a substantial overhead to the logic we really want to implement (namely the penalty terms in the bilinear form). As a consequence, we will make use of the FEInterfaceValues class -- a helper class in deal.II that allows us to abstract away the two FEFaceValues or FESubfaceValues objects and directly access what we really care about: jumps, averages, etc.    


But this doesn't yet solve our problem of having to keep track of which faces we have already visited when we loop over all cells and all of their faces. To make this process simpler, we use the  [2.x.147]  function that provides a simple interface for this task: Based on the ideas outlined in the WorkStream namespace documentation,  [2.x.148]  requires three functions that do work on cells, interior faces, and boundary faces. These functions work on scratch objects for intermediate results, and then copy the result of their computations into copy data objects from where a copier function copies them into the global matrix and right hand side objects.    


The following structures then provide the scratch and copy objects that are necessary for this approach. You may look up the WorkStream namespace as well as the  [2.x.149]  "Parallel computing with multiple processors" module for more information on how they typically work. 

[1.x.98] 



The more interesting part is where we actually assemble the linear system. Fundamentally, this function has five parts: 

- The definition of the `cell_worker` lambda function, a small function that is defined within the `assemble_system()` function and that will be responsible for computing the local integrals on an individual cell. It will work on a copy of the `ScratchData` class and put its results into the corresponding `CopyData` object. 

- The definition of the `face_worker` lambda function that does the integration of all terms that live on the interfaces between cells. 

- The definition of the `boundary_worker` function that does the same but for cell faces located on the boundary of the domain. 

- The definition of the `copier` function that is responsible for copying all of the data the previous three functions have put into copy objects for a single cell, into the global matrix and right hand side.    


The fifth part is the one where we bring all of this together.    


Let us go through each of these pieces necessary for the assembly in turns. 

[1.x.99] 



The first piece is the `cell_worker` that does the assembly on the cell interiors. It is a (lambda) function that takes a cell (input), a scratch object, and a copy object (output) as arguments. It looks like the assembly functions of many other of the tutorial programs, or at least the body of the loop over all cells.      


The terms we integrate here are the cell contribution 

[1.x.100] 

to the global matrix, and 

[1.x.101] 

to the right hand side vector.      


We use the same technique as used in the assembly of  [2.x.150]  to accelerate the function: Instead of calling `fe_values.shape_hessian(i, qpoint)` in the innermost loop, we create a variable `hessian_i` that evaluates this value once in the loop over `i` and re-use the so-evaluated value in the loop over `j`. For symmetry, we do the same with a variable `hessian_j`, although it is indeed only used once and we could have left the call to `fe_values.shape_hessian(j,qpoint)` in the instruction that computes the scalar product between the two terms. 

[1.x.102] 



The next building block is the one that assembles penalty terms on each of the interior faces of the mesh. As described in the documentation of  [2.x.151]  this function receives arguments that denote a cell and its neighboring cell, as well as (for each of the two cells) the face (and potentially sub-face) we have to integrate over. Again, we also get a scratch object, and a copy object for putting the results in.      


The function has three parts itself. At the top, we initialize the FEInterfaceValues object and create a new  [2.x.152]  object to store our input in. This gets pushed to the end of the `copy_data.face_data` variable. We need to do this because the number of faces (or subfaces) over which we integrate for a given cell differs from cell to cell, and the sizes of these matrices also differ, depending on what degrees of freedom are adjacent to the face or subface. As discussed in the documentation of  [2.x.153]  the copy object is reset every time a new cell is visited, so that what we push to the end of `copy_data.face_data()` is really all that the later `copier` function gets to see when it copies the contributions of each cell to the global matrix and right hand side objects. 

[1.x.103] 



The second part deals with determining what the penalty parameter should be. By looking at the units of the various terms in the bilinear form, it is clear that the penalty has to have the form  [2.x.154]  (i.e., one over length scale), but it is not a priori obvious how one should choose the dimension-less number  [2.x.155] . From the discontinuous Galerkin theory for the Laplace equation, one might conjecture that the right choice is  [2.x.156]  is the right choice, where  [2.x.157]  is the polynomial degree of the finite element used. We will discuss this choice in a bit more detail in the results section of this program.        


In the formula above,  [2.x.158]  is the size of cell  [2.x.159] . But this is not quite so straightforward either: If one uses highly stretched cells, then a more involved theory says that  [2.x.160]  should be replaced by the diameter of cell  [2.x.161]  normal to the direction of the edge in question.  It turns out that there is a function in deal.II for that. Secondly,  [2.x.162]  may be different when viewed from the two different sides of a face.        


To stay on the safe side, we take the maximum of the two values. We will note that it is possible that this computation has to be further adjusted if one were to use hanging nodes resulting from adaptive mesh refinement. 

[1.x.104] 



Finally, and as usual, we loop over the quadrature points and indices `i` and `j` to add up the contributions of this face or sub-face. These are then stored in the `copy_data.face_data` object created above. As for the cell worker, we pull the evaluation of averages and jumps out of the loops if possible, introducing local variables that store these results. The assembly then only needs to use these local variables in the innermost loop. Regarding the concrete formula this code implements, recall that the interface terms of the bilinear form were as follows: 

[1.x.105] 



[1.x.106] 



The third piece is to do the same kind of assembly for faces that are at the boundary. The idea is the same as above, of course, with only the difference that there are now penalty terms that also go into the right hand side.      


As before, the first part of the function simply sets up some helper objects: 

[1.x.107] 



Positively, because we now only deal with one cell adjacent to the face (as we are on the boundary), the computation of the penalty factor  [2.x.163]  is substantially simpler: 

[1.x.108] 



The third piece is the assembly of terms. This is now slightly more involved since these contains both terms for the matrix and for the right hand side. The former is exactly the same as for the interior faces stated above if one just defines the jump and average appropriately (which is what the FEInterfaceValues class does). The latter requires us to evaluate the boundary conditions  [2.x.164] , which in the current case (where we know the exact solution) we compute from  [2.x.165] . The term to be added to the right hand side vector is then  [2.x.166] . 

[1.x.109] 



Part 4 is a small function that copies the data produced by the cell, interior, and boundary face assemblers above into the global matrix and right hand side vector. There really is not very much to do here: We distribute the cell matrix and right hand side contributions as we have done in almost all of the other tutorial programs using the constraints objects. We then also have to do the same for the face matrix contributions that have gained content for the faces (interior and boundary) and that the `face_worker` and `boundary_worker` have added to the `copy_data.face_data` array. 

[1.x.110] 



Having set all of this up, what remains is to just create a scratch and copy data object and call the  [2.x.167]  function that then goes over all cells and faces, calls the respective workers on them, and then the copier function that puts things into the global matrix and right hand side. As an additional benefit,  [2.x.168]  does all of this in parallel, using as many processor cores as your machine happens to have. 

[1.x.111] 




[1.x.112]  [1.x.113]    


The show is essentially over at this point: The remaining functions are not overly interesting or novel. The first one simply uses a direct solver to solve the linear system (see also  [2.x.169] ): 

[1.x.114] 



The next function evaluates the error between the computed solution and the exact solution (which is known here because we have chosen the right hand side and boundary values in a way so that we know the corresponding solution). In the first two code blocks below, we compute the error in the  [2.x.170]  norm and the  [2.x.171]  semi-norm. 

[1.x.115] 



Now also compute an approximation to the  [2.x.172]  seminorm error. The actual  [2.x.173]  seminorm would require us to integrate second derivatives of the solution  [2.x.174] , but given the Lagrange shape functions we use,  [2.x.175]  of course has kinks at the interfaces between cells, and consequently second derivatives are singular at interfaces. As a consequence, we really only integrate over the interior of cells and ignore the interface contributions. This is *not* an equivalent norm to the energy norm for the problem, but still gives us an idea of how fast the error converges.      


We note that one could address this issue by defining a norm that is equivalent to the energy norm. This would involve adding up not only the integrals over cell interiors as we do below, but also adding penalty terms for the jump of the derivative of  [2.x.176]  across interfaces, with an appropriate scaling of the two kinds of terms. We will leave this for later work. 

[1.x.116] 



Equally uninteresting is the function that generates graphical output. It looks exactly like the one in  [2.x.177] , for example. 

[1.x.117] 



The same is true for the `run()` function: Just like in previous programs. 

[1.x.118] 




[1.x.119]  [1.x.120] 




Finally for the `main()` function. There is, again, not very much to see here: It looks like the ones in previous tutorial programs. There is a variable that allows selecting the polynomial degree of the element we want to use for solving the equation. Because the C0IP formulation we use requires the element degree to be at least two, we check with an assertion that whatever one sets for the polynomial degree actually makes sense. 

[1.x.121] 

[1.x.122][1.x.123] 


We run the program with right hand side and boundary values as discussed in the introduction. These will produce the solution  [2.x.178]  on the domain  [2.x.179] . We test this setup using  [2.x.180] ,  [2.x.181] , and  [2.x.182]  elements, which one can change via the `fe_degree` variable in the `main()` function. With mesh refinement, the  [2.x.183]  convergence rates,  [2.x.184] -seminorm rate, and  [2.x.185] -seminorm convergence of  [2.x.186]  should then be around 2, 2, 1 for  [2.x.187]  (with the  [2.x.188]  norm sub-optimal as discussed in the introduction); 4, 3, 2 for  [2.x.189] ; and 5, 4, 3 for  [2.x.190] , respectively. 

From the literature, it is not immediately clear what the penalty parameter  [2.x.191]  should be. For example,  [2.x.192]  state that it needs to be larger than one, and choose  [2.x.193] . The FEniCS/Dolphin tutorial chooses it as  [2.x.194] , see https://fenicsproject.org/docs/dolfin/1.6.0/python/demo/documented/biharmonic/python/documentation.html .  [2.x.195]  uses a value for  [2.x.196]  larger than the number of edges belonging to an element for Kirchhoff plates (see their Section 4.2). This suggests that maybe  [2.x.197] ,  [2.x.198] , are too small; on the other hand, a value  [2.x.199]  would be reasonable, where  [2.x.200]  is the degree of polynomials. The last of these choices is the one one would expect to work by comparing to the discontinuous Galerkin formulations for the Laplace equation (see, for example, the discussions in  [2.x.201]  and  [2.x.202] ), and it will turn out to also work here. But we should check what value of  [2.x.203]  is right, and we will do so below; changing  [2.x.204]  is easy in the two `face_worker` and `boundary_worker` functions defined in `assemble_system()`. 


[1.x.124][1.x.125][1.x.126][1.x.127] 


We run the code with differently refined meshes and get the following convergence rates. 

 [2.x.205]  We can see that the  [2.x.206]  convergence rates are around 2,  [2.x.207] -seminorm convergence rates are around 2, and  [2.x.208] -seminorm convergence rates are around 1. The latter two match the theoretically expected rates; for the former, we have no theorem but are not surprised that it is sub-optimal given the remark in the introduction. 


[1.x.128][1.x.129][1.x.130][1.x.131] 




 [2.x.209]  We can see that the  [2.x.210]  convergence rates are around 4,  [2.x.211] -seminorm convergence rates are around 3, and  [2.x.212] -seminorm convergence rates are around 2. This, of course, matches our theoretical expectations. 


[1.x.132][1.x.133][1.x.134][1.x.135] 


 [2.x.213]  We can see that the  [2.x.214]  norm convergence rates are around 5,  [2.x.215] -seminorm convergence rates are around 4, and  [2.x.216] -seminorm convergence rates are around 3. On the finest mesh, the  [2.x.217]  norm convergence rate is much smaller than our theoretical expectations because the linear solver becomes the limiting factor due to round-off. Of course the  [2.x.218]  error is also very small already in that case. 


[1.x.136][1.x.137][1.x.138][1.x.139] 


For comparison with the results above, let us now also consider the case where we simply choose  [2.x.219] : 

 [2.x.220]  Although  [2.x.221]  norm convergence rates of  [2.x.222]  more or less follows the theoretical expectations, the  [2.x.223] -seminorm and  [2.x.224] -seminorm do not seem to converge as expected. Comparing results from  [2.x.225]  and  [2.x.226] , it is clear that  [2.x.227]  is a better penalty. Given that  [2.x.228]  is already too small for  [2.x.229]  elements, it may not be surprising that if one repeated the experiment with a  [2.x.230]  element, the results are even more disappointing: One again only obtains convergence rates of 2, 1, zero -- i.e., no better than for the  [2.x.231]  element (although the errors are smaller in magnitude). Maybe surprisingly, however, one obtains more or less the expected convergence orders when using  [2.x.232]  elements. Regardless, this uncertainty suggests that  [2.x.233]  is at best a risky choice, and at worst an unreliable one and that we should choose  [2.x.234]  larger. 


[1.x.140][1.x.141][1.x.142][1.x.143] 


Since  [2.x.235]  is clearly too small, one might conjecture that  [2.x.236]  might actually work better. Here is what one obtains in that case: 

 [2.x.237]  In this case, the convergence rates more or less follow the theoretical expectations, but, compared to the results from  [2.x.238] , are more variable. Again, we could repeat this kind of experiment for  [2.x.239]  and  [2.x.240]  elements. In both cases, we will find that we obtain roughly the expected convergence rates. Of more interest may then be to compare the absolute size of the errors. While in the table above, for the  [2.x.241]  case, the errors on the finest grid are comparable between the  [2.x.242]  and  [2.x.243]  case, for  [2.x.244]  the errors are substantially larger for  [2.x.245]  than for  [2.x.246] . The same is true for the  [2.x.247]  case. 


[1.x.144][1.x.145] 


The conclusions for which of the "reasonable" choices one should use for the penalty parameter is that  [2.x.248]  yields the expected results. It is, consequently, what the code uses as currently written. 


[1.x.146][1.x.147] 


There are a number of obvious extensions to this program that would make sense: 

- The program uses a square domain and a uniform mesh. Real problems   don't come this way, and one should verify convergence also on   domains with other shapes and, in particular, curved boundaries. One   may also be interested in resolving areas of less regularity by   using adaptive mesh refinement. 

- From a more theoretical perspective, the convergence results above   only used the "broken"  [2.x.249]  seminorm  [2.x.250]  instead   of the "equivalent" norm  [2.x.251] . This is good enough to   convince ourselves that the program isn't fundamentally   broken. However, it might be interesting to measure the error in the   actual norm for which we have theoretical results. Implementing this   addition should not be overly difficult using, for example, the   FEInterfaceValues class combined with  [2.x.252]  in the   same spirit as we used for the assembly of the linear system. 


[1.x.148]  [1.x.149] 


  Similar to the "clamped" boundary condition addressed in the implementation,   we will derive the  [2.x.253]  IP finite element scheme for simply supported plates:   [1.x.150] 

  We multiply the biharmonic equation by the test function  [2.x.254]  and integrate over  [2.x.255]  and get:   [1.x.151] 



  Summing up over all cells  [2.x.256] ,since normal directions of  [2.x.257]  are pointing at   opposite directions on each interior edge shared by two cells and  [2.x.258]  on  [2.x.259] ,   [1.x.152] 

  and by the definition of jump over cell interfaces,   [1.x.153] 

  We separate interior faces and boundary faces of the domain,   [1.x.154] 

  where  [2.x.260]  is the set of interior faces.   This leads us to   [1.x.155] 



  In order to symmetrize and stabilize the discrete problem,   we add symmetrization and stabilization term.   We finally get the  [2.x.261]  IP finite element scheme for the biharmonic equation:   find  [2.x.262]  such that  [2.x.263]  on  [2.x.264]  and   [1.x.156] 

  where   [1.x.157] 

  and   [1.x.158] 

  The implementation of this boundary case is similar to the "clamped" version   except that `boundary_worker` is no longer needed for system assembling   and the right hand side is changed according to the formulation. [1.x.159] [1.x.160]  [2.x.265]  

 [2.x.266] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23] 



[1.x.24][1.x.25] 

[1.x.26] [1.x.27][1.x.28] 


This program demonstrates how to use the cell-based implementation of finite element operators with the MatrixFree class, first introduced in  [2.x.4] , to solve nonlinear partial differential equations. Moreover, we have another look at the handling of constraints within the matrix-free framework. Finally, we will use an explicit time-stepping method to solve the problem and introduce Gauss-Lobatto finite elements that are very convenient in this case since their mass matrix can be accurately approximated by a diagonal, and thus trivially invertible, matrix. The two ingredients to this property are firstly a distribution of the nodal points of Lagrange polynomials according to the point distribution of the Gauss-Lobatto quadrature rule. Secondly, the quadrature is done with the same Gauss-Lobatto quadrature rule. In this formula, the integrals  [2.x.5]  become zero whenever  [2.x.6] , because exactly one function  [2.x.7]  is one and all others zero in the points defining the Lagrange polynomials. Moreover, the Gauss-Lobatto distribution of nodes of Lagrange polynomials clusters the nodes towards the element boundaries. This results in a well-conditioned polynomial basis for high-order discretization methods. Indeed, the condition number of an FE_Q elements with equidistant nodes grows exponentially with the degree, which destroys any benefit for orders of about five and higher. For this reason, Gauss-Lobatto points are the default distribution for the FE_Q element (but at degrees one and two, those are equivalent to the equidistant points). 

[1.x.29][1.x.30] 


As an example, we choose to solve the sine-Gordon soliton equation [1.x.31] 

that was already introduced in  [2.x.8] . As a simple explicit time integration method, we choose leap frog scheme using the second-order formulation of the equation. With this time stepping, the scheme reads in weak form 

[1.x.32] where [1.x.33] denotes a test function and the index [1.x.34] stands for the time step number. 

For the spatial discretization, we choose FE_Q elements with basis functions defined to interpolate the support points of the Gauss-Lobatto quadrature rule. Moreover, when we compute the integrals over the basis functions to form the mass matrix and the operator on the right hand side of the equation above, we use the Gauss-Lobatto quadrature rule with the same support points as the node points of the finite element to evaluate the integrals. Since the finite element is Lagrangian, this will yield a diagonal mass matrix on the left hand side of the equation, making the solution of the linear system in each time step trivial. 

Using this quadrature rule, for a [1.x.35]th order finite element, we use a [1.x.36]th order accurate formula to evaluate the integrals. Since the product of two [1.x.37]th order basis functions when computing a mass matrix gives a function with polynomial degree [1.x.38] in each direction, the integrals are not computed exactly.  However, the overall convergence properties are not disturbed by the quadrature error on meshes with affine element shapes with L2 errors proportional to [1.x.39]. Note however that order reduction with sub-optimal convergence rates of the L2 error of [1.x.40] or even [1.x.41] for some 3D setups has been reported [1.x.42] on deformed (non-affine) element shapes for wave equations when the integrand is not a polynomial any more. 

Apart from the fact that we avoid solving linear systems with this type of elements when using explicit time-stepping, they come with two other advantages. When we are using the sum-factorization approach to evaluate the finite element operator (cf.  [2.x.9] ), we have to evaluate the function at the quadrature points. In the case of Gauss-Lobatto elements, where quadrature points and node points of the finite element coincide, this operation is trivial since the value of the function at the quadrature points is given by its one-dimensional coefficients. In this way, the arithmetic work for the finite element operator evaluation is reduced by approximately a factor of two compared to the generic Gaussian quadrature. 

To sum up the discussion, by using the right finite element and quadrature rule combination, we end up with a scheme where we only need to compute the right hand side vector corresponding to the formulation above and then multiply it by the inverse of the diagonal mass matrix in each time step. In practice, of course, we extract the diagonal elements and invert them only once at the beginning of the program. 

[1.x.43][1.x.44] 


The usual way to handle constraints in  [2.x.10]  is to use the AffineConstraints class that builds a sparse matrix storing information about which degrees of freedom (DoF) are constrained and how they are constrained. This format uses an unnecessarily large amount of memory since there are not so many different types of constraints: for example, in the case of hanging nodes when using linear finite element on every cell, most constraints have the form  [2.x.11]  where the coefficients  [2.x.12]  are always the same and only  [2.x.13]  are different. While storing this redundant information is not a problem in general because it is only needed once during matrix and right hand side assembly, it becomes a bottleneck in the matrix-free approach since there this information has to be accessed every time we apply the operator, and the remaining components of the operator evaluation are so fast. Thus, instead of an AffineConstraints object, MatrixFree uses a variable that we call  [2.x.14]  that collects the weights of the different constraints. Then, only an identifier of each constraint in the mesh instead of all the weights have to be stored. Moreover, the constraints are not applied in a pre- and postprocessing step but rather as we evaluate the finite element operator. Therefore, the constraint information is embedded into the variable  [2.x.15]  that is used to extract the cell information from the global vector. If a DoF is constrained, the  [2.x.16]  variable contains the global indices of the DoFs that it is constrained to. Then, we have another variable  [2.x.17]  at hand that holds, for each cell, the local indices of DoFs that are constrained as well as the identifier of the type of constraint. Fortunately, you will not see these data structures in the example program since the class  [2.x.18]  takes care of the constraints without user interaction. 

In the presence of hanging nodes, the diagonal mass matrix obtained on the element level via the Gauss-Lobatto quadrature/node point procedure does not directly translate to a diagonal global mass matrix, as following the constraints on rows and columns would also add off-diagonal entries. As explained in [1.x.45], interpolating constraints on a vector, which maintains the diagonal shape of the mass matrix, is consistent with the equations up to an error of the same magnitude as the quadrature error. In the program below, we will simply assemble the diagonal of the mass matrix as if it were a vector to enable this approximation. 


[1.x.46][1.x.47] 


The MatrixFree class comes with the option to be parallelized on three levels: MPI parallelization on clusters of distributed nodes, thread parallelization scheduled by the Threading Building Blocks library, and finally with a vectorization by working on a batch of two (or more) cells via SIMD data type (sometimes called cross-element or external vectorization). As we have already discussed in  [2.x.19] , you will get best performance by using an instruction set specific to your system, e.g. with the cmake variable <tt>-DCMAKE_CXX_FLAGS="-march=native"</tt>. The MPI parallelization was already exploited in  [2.x.20] . Here, we additionally consider thread parallelization with TBB. This is fairly simple, as all we need to do is to tell the initialization of the MatrixFree object about the fact that we want to use a thread parallel scheme through the variable  [2.x.21]  During setup, a dependency graph is set up similar to the one described in the  [2.x.22]  , which allows to schedule the work of the  [2.x.23]  function on chunks of cells without several threads accessing the same vector indices. As opposed to the WorkStream loops, some additional clever tricks to avoid global synchronizations as described in [1.x.48] are also applied. 

Note that this program is designed to be run with a distributed triangulation  [2.x.24]  which requires deal.II to be configured with [1.x.49] as described in the [1.x.50] file. However, a non-distributed triangulation is also supported, in which case the computation will be run in serial. 

[1.x.51][1.x.52] 


In our example, we choose the initial value to be [1.x.53] and solve the equation over the time interval [-10,10]. The constants are chosen to be  [2.x.25]  and [1.x.54]. As mentioned in  [2.x.26] , in one dimension [1.x.55] as a function of [1.x.56] is the exact solution of the sine-Gordon equation. For higher dimension, this is however not the case. [1.x.57] [1.x.58] 

The necessary files from the deal.II library. 

[1.x.59] 



This includes the data structures for the efficient implementation of matrix-free methods. 

[1.x.60] 



We start by defining two global variables to collect all parameters subject to changes at one place: One for the dimension and one for the finite element degree. The dimension is used in the main function as a template argument for the actual classes (like in all other deal.II programs), whereas the degree of the finite element is more crucial, as it is passed as a template argument to the implementation of the Sine-Gordon operator. Therefore, it needs to be a compile-time constant. 

[1.x.61] 




[1.x.62]  [1.x.63] 




The  [2.x.27]  class implements the cell-based operation that is needed in each time step. This nonlinear operation can be implemented straight-forwardly based on the  [2.x.28]  class, in the same way as a linear operation would be treated by this implementation of the finite element operator application. We apply two template arguments to the class, one for the dimension and one for the degree of the finite element. This is a difference to other functions in deal.II where only the dimension is a template argument. This is necessary to provide the inner loops in  [2.x.29]  with information about loop lengths etc., which is essential for efficiency. On the other hand, it makes it more challenging to implement the degree as a run-time parameter. 

[1.x.64] 




[1.x.65]  [1.x.66] 




This is the constructor of the SineGordonOperation class. It receives a reference to the MatrixFree holding the problem information and the time step size as input parameters. The initialization routine sets up the mass matrix. Since we use Gauss-Lobatto elements, the mass matrix is a diagonal matrix and can be stored as a vector. The computation of the mass matrix diagonal is simple to achieve with the data structures provided by FEEvaluation: Just loop over all cell batches, i.e., collections of cells due to SIMD vectorization, and integrate over the function that is constant one on all quadrature points by using the  [2.x.30]  function with  [2.x.31]  argument at the slot for values. Finally, we invert the diagonal entries to have the inverse mass matrix directly available in each time step. 

[1.x.67] 




[1.x.68]  [1.x.69] 




This operator implements the core operation of the program, the integration over a range of cells for the nonlinear operator of the Sine-Gordon problem. The implementation is based on the FEEvaluation class as in  [2.x.32] . Due to the special structure in Gauss-Lobatto elements, certain operations become simpler, in particular the evaluation of shape function values on quadrature points which is simply the injection of the values of cell degrees of freedom. The MatrixFree class detects possible structure of the finite element at quadrature points when initializing, which is then automatically used by FEEvaluation for selecting the most appropriate numerical kernel. 




The nonlinear function that we have to evaluate for the time stepping routine includes the value of the function at the present time  [2.x.33]  as well as the value at the previous time step  [2.x.34]  Both values are passed to the operator in the collection of source vectors  [2.x.35]  which is simply a  [2.x.36]  of pointers to the actual solution vectors. This construct of collecting several source vectors into one is necessary as the cell loop in  [2.x.37]  takes exactly one source and one destination vector, even if we happen to use many vectors like the two in this case. Note that the cell loop accepts any valid class for input and output, which does not only include vectors but general data types.  However, only in case it encounters a  [2.x.38]  or a  [2.x.39]  collecting these vectors, it calls functions that exchange ghost data due to MPI at the beginning and the end of the loop. In the loop over the cells, we first have to read in the values in the vectors related to the local values.  Then, we evaluate the value and the gradient of the current solution vector and the values of the old vector at the quadrature points. Next, we combine the terms in the scheme in the loop over the quadrature points. Finally, we integrate the result against the test function and accumulate the result to the global solution vector @p dst. 

[1.x.70] 




[1.x.71]  [1.x.72] 




This function performs the time stepping routine based on the cell-local strategy. Note that we need to set the destination vector to zero before we add the integral contributions of the current time step (via the  [2.x.40]  call). In this tutorial, we let the cell-loop do the zero operation via the fifth `true` argument passed to  [2.x.41]  The loop can schedule the zero operation closer to the operations on vector entries for supported vector entries, thereby possibly increasing data locality (the vector entries that first get zeroed are later re-used in the `distribute_local_to_global()` call). The structure of the cell loop is implemented in the cell finite element operator class. On each cell it applies the routine defined as the  [2.x.42]  method of the class  [2.x.43] . One could also provide a function with the same signature that is not part of a class. Finally, the result of the integration is multiplied by the inverse mass matrix. 

[1.x.73] 




[1.x.74]  [1.x.75] 




We define a time-dependent function that is used as initial value. Different solutions can be obtained by varying the starting time. This function, taken from  [2.x.44] , would represent an analytic solution in 1D for all times, but is merely used for setting some starting solution of interest here. More elaborate choices that could test the convergence of this program are given in  [2.x.45] . 

[1.x.76] 




[1.x.77]  [1.x.78] 




This is the main class that builds on the class in  [2.x.46] .  However, we replaced the SparseMatrix<double> class by the MatrixFree class to store the geometry data. Also, we use a distributed triangulation in this example. 

[1.x.79] 




[1.x.80]  [1.x.81] 




This is the constructor of the SineGordonProblem class. The time interval and time step size are defined here. Moreover, we use the degree of the finite element that we defined at the top of the program to initialize a FE_Q finite element based on Gauss-Lobatto support points. These points are convenient because in conjunction with a QGaussLobatto quadrature rule of the same order they give a diagonal mass matrix without compromising accuracy too much (note that the integration is inexact, though), see also the discussion in the introduction. Note that FE_Q selects the Gauss-Lobatto nodal points by default due to their improved conditioning versus equidistant points. To make things more explicit, we state the selection of the nodal points nonetheless. 

[1.x.82] 




[1.x.83]  [1.x.84] 




As in  [2.x.47]  this functions sets up a cube grid in  [2.x.48]  dimensions of extent  [2.x.49] . We refine the mesh more in the center of the domain since the solution is concentrated there. We first refine all cells whose center is within a radius of 11, and then refine once more for a radius 6.  This simple ad hoc refinement could be done better by adapting the mesh to the solution using error estimators during the time stepping as done in other example programs, and using  [2.x.50]  to transfer the solution to the new mesh. 

[1.x.85] 



We generate hanging node constraints for ensuring continuity of the solution. As in  [2.x.51] , we need to equip the constraint matrix with the IndexSet of locally relevant degrees of freedom to avoid it to consume too much memory for big problems. Next, the <code> MatrixFree </code> object for the problem is set up. Note that we specify a particular scheme for shared-memory parallelization (hence one would use multithreading for intra-node parallelism and not MPI; we here choose the standard option &mdash; if we wanted to disable shared memory parallelization even in case where there is more than one TBB thread available in the program, we would choose  [2.x.52]  Also note that, instead of using the default QGauss quadrature argument, we supply a QGaussLobatto quadrature formula to enable the desired behavior. Finally, three solution vectors are initialized. MatrixFree expects a particular layout of ghost indices (as it handles index access in MPI-local numbers that need to match between the vector and MatrixFree), so we just ask it to initialize the vectors to be sure the ghost exchange is properly handled. 

[1.x.86] 




[1.x.87]  [1.x.88] 




This function prints the norm of the solution and writes the solution vector to a file. The norm is standard (except for the fact that we need to accumulate the norms over all processors for the parallel grid which we do via the  [2.x.53]  function), and the second is similar to what we did in  [2.x.54]  or  [2.x.55] . Note that we can use the same vector for output as the one used during computations: The vectors in the matrix-free framework always provide full information on all locally owned cells (this is what is needed in the local evaluations, too), including ghost vector entries on these cells. This is the only data that is needed in the  [2.x.56]  function as well as in DataOut. The only action to take at this point is to make sure that the vector updates its ghost values before we read from them, and to reset ghost values once done. This is a feature present only in the  [2.x.57]  class. Distributed vectors with PETSc and Trilinos, on the other hand, need to be copied to special vectors including ghost values (see the relevant section in  [2.x.58] ). If we also wanted to access all degrees of freedom on ghost cells (e.g. when computing error estimators that use the jump of solution over cell boundaries), we would need more information and create a vector initialized with locally relevant dofs just as in  [2.x.59] . Observe also that we need to distribute constraints for output - they are not filled during computations (rather, they are interpolated on the fly in the matrix-free method  [2.x.60]  

[1.x.89] 




[1.x.90]  [1.x.91] 




This function is called by the main function and steps into the subroutines of the class.    


After printing some information about the parallel setup, the first action is to set up the grid and the cell operator. Then, the time step is computed from the CFL number given in the constructor and the finest mesh size. The finest mesh size is computed as the diameter of the last cell in the triangulation, which is the last cell on the finest level of the mesh. This is only possible for meshes where all elements on a level have the same size, otherwise, one needs to loop over all cells. Note that we need to query all the processors for their finest cell since not all processors might hold a region where the mesh is at the finest level. Then, we readjust the time step a little to hit the final time exactly. 

[1.x.92] 



Next the initial value is set. Since we have a two-step time stepping method, we also need a value of the solution at time-time_step. For accurate results, one would need to compute this from the time derivative of the solution at initial time, but here we ignore this difficulty and just set it to the initial value function at that artificial time. 




We then go on by writing the initial state to file and collecting the two starting solutions in a  [2.x.61]  of pointers that get later consumed by the  [2.x.62]  function. Next, an instance of the  [2.x.63]  based on the finite element degree specified at the top of this file is set up. 

[1.x.93] 



Now loop over the time steps. In each iteration, we shift the solution vectors by one and call the `apply` function of the `SineGordonOperator` class. Then, we write the solution to a file. We clock the wall times for the computational time needed as wall as the time needed to create the output and report the numbers when the time stepping is finished.      


Note how this shift is implemented: We simply call the swap method on the two vectors which swaps only some pointers without the need to copy data around, a relatively expensive operation within an explicit time stepping method. Let us see what happens in more detail: First, we exchange  [2.x.64] , which means that  [2.x.65]  gets  [2.x.66] , which is what we expect. Similarly,  [2.x.67]  in the next step. After this,  [2.x.68]  holds  [2.x.69] , but that will be overwritten during this step. 

[1.x.94] 




[1.x.95]  [1.x.96] 




As in  [2.x.70] , we initialize MPI at the start of the program. Since we will in general mix MPI parallelization with threads, we also set the third argument in MPI_InitFinalize that controls the number of threads to an invalid number, which means that the TBB library chooses the number of threads automatically, typically to the number of available cores in the system. As an alternative, you can also set this number manually if you want to set a specific number of threads (e.g. when MPI-only is required). 

[1.x.97] 

[1.x.98][1.x.99] 


[1.x.100][1.x.101] 


In order to demonstrate the gain in using the MatrixFree class instead of the standard  [2.x.71]  assembly routines for evaluating the information from old time steps, we study a simple serial run of the code on a nonadaptive mesh. Since much time is spent on evaluating the sine function, we do not only show the numbers of the full sine-Gordon equation but also for the wave equation (the sine-term skipped from the sine-Gordon equation). We use both second and fourth order elements. The results are summarized in the following table. 

 [2.x.72]  

It is apparent that the matrix-free code outperforms the standard assembly routines in deal.II by far. In 3D and for fourth order elements, one operator evaluation is also almost ten times as fast as a sparse matrix-vector product. 

[1.x.102][1.x.103] 


We start with the program output obtained on a workstation with 12 cores / 24 threads (one Intel Xeon E5-2687W v4 CPU running at 3.2 GHz, hyperthreading enabled), running the program in release mode: 

[1.x.104] 



In 3D, the respective output looks like 

[1.x.105] 



It takes 0.008 seconds for one time step with more than a million degrees of freedom (note that we would need many processors to reach such numbers when solving linear systems). 

If we replace the thread-parallelization by a pure MPI parallelization, the timings change into: 

[1.x.106] 



We observe a dramatic speedup for the output (which makes sense, given that most code of the output is not parallelized via threads, whereas it is for MPI), but less than the theoretical factor of 12 we would expect from the parallelism. More interestingly, the computations also get faster when switching from the threads-only variant to the MPI-only variant. This is a general observation for the MatrixFree framework (as of updating this data in 2019). The main reason is that the decisions regarding work on conflicting cell batches made to enable execution in parallel are overly pessimistic: While they ensure that no work on neighboring cells is done on different threads at the same time, this conservative setting implies that data from neighboring cells is also evicted from caches by the time neighbors get touched. Furthermore, the current scheme is not able to provide a constant load for all 24 threads for the given mesh with 17,592 cells. 

The current program allows to also mix MPI parallelization with thread parallelization. This is most beneficial when running programs on clusters with multiple nodes, using MPI for the inter-node parallelization and threads for the intra-node parallelization. On the workstation used above, we can run threads in the hyperthreading region (i.e., using 2 threads for each of the 12 MPI ranks). An important setting for mixing MPI with threads is to ensure proper binning of tasks to CPUs. On many clusters the placing is either automatically via the `mpirun/mpiexec` environment, or there can be manual settings. Here, we simply report the run times the plain version of the program (noting that things could be improved towards the timings of the MPI-only program when proper pinning is done): 

[1.x.107] 






[1.x.108][1.x.109] 


There are several things in this program that could be improved to make it even more efficient (besides improved boundary conditions and physical stuff as discussed in  [2.x.73] ): 

 [2.x.74]   [2.x.75]  [1.x.110] As becomes obvious   from the comparison of the plain wave equation and the sine-Gordon   equation above, the evaluation of the sine terms dominates the total   time for the finite element operator application. There are a few   reasons for this: Firstly, the deal.II sine computation of a   VectorizedArray field is not vectorized (as opposed to the rest of   the operator application). This could be cured by handing the sine   computation to a library with vectorized sine computations like   Intel's math kernel library (MKL). By using the function    [2.x.76]  in MKL, the program uses half the computing time   in 2D and 40 percent less time in 3D. On the other hand, the sine   computation is structurally much more complicated than the simple   arithmetic operations like additions and multiplications in the rest   of the local operation. 

   [2.x.77]  [1.x.111] While the implementation allows for   arbitrary order in the spatial part (by adjusting the degree of the finite   element), the time stepping scheme is a standard second-order leap-frog   scheme. Since solutions in wave propagation problems are usually very   smooth, the error is likely dominated by the time stepping part. Of course,   this could be cured by using smaller time steps (at a fixed spatial   resolution), but it would be more efficient to use higher order time   stepping as well. While it would be straight-forward to do so for a   first-order system (use some Runge&ndash;Kutta scheme of higher order,   probably combined with adaptive time step selection like the [1.x.112]), it is more challenging for the second-order formulation. At   least in the finite difference community, people usually use the PDE to find   spatial correction terms that improve the temporal error. 

 [2.x.78]  [1.x.113] [1.x.114]  [2.x.79]  

 [2.x.80] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31] 

[1.x.32] 

[1.x.33] [1.x.34][1.x.35] 

This tutorial is an extension to  [2.x.3]  and demonstrates several ways to obtain more involved meshes than the ones shown there. 

 [2.x.4]  This tutorial is also available as a Jupyter Python notebook that   uses the deal.II python interface. The notebook is available in the   same directory as the original C++ program. 

Generating complex geometries is a challenging task, especially in three space dimensions. We will discuss several ways to do this, but this list is not exhaustive. Additionally, there is not one approach that fits all problems. 

This example program shows some of ways to create and modify meshes for computations and outputs them as  [2.x.5]  files in much the same way as we do in  [2.x.6] . No other computations or adaptive refinements are done; the idea is that you can use the techniques used here as building blocks in other, more involved simulators. Please note that the example program does not show all the ways to generate meshes that are discussed in this introduction. 


[1.x.36][1.x.37] 


When you use adaptive mesh refinement, you definitely want the initial mesh to be as coarse as possible. The reason is that you can make it as fine as you want using adaptive refinement as long as you have memory and CPU time available. However, this requires that you don't waste mesh cells in parts of the domain where they don't pay off. As a consequence, you don't want to start with a mesh that is too fine to start with, because that takes up a good part of your cell budget already, and because you can't coarsen away cells that are in the initial mesh. 

That said, your mesh needs to capture the given geometry adequately. 


[1.x.38][1.x.39] 


There are several ways to create an initial mesh. Meshes can be modified or combined in many ways as discussed later on. 

[1.x.40][1.x.41] 


The easiest way to generate meshes is to use the functions in namespace GridGenerator, as already discussed in  [2.x.7] .  There are many different helper functions available, including  [2.x.8]   [2.x.9]   [2.x.10]  and  [2.x.11]  


[1.x.42][1.x.43] 


If there is no good fit in the GridGenerator namespace for what you want to do, you can always create a Triangulation in your program "by hand". For that, you need a list of vertices with their coordinates and a list of cells referencing those vertices. You can find an example in the function <tt>create_coarse_grid()</tt> in  [2.x.12] . All the functions in GridGenerator are implemented in this fashion. 

We are happy to accept more functions to be added to GridGenerator. So, if you end up writing a function that might be useful for a larger audience, please contribute it. 


[1.x.44][1.x.45] 


The class GridIn can read many different mesh formats from a file from disk. How this is done is explained in  [2.x.13]  and can be seen in the function  [2.x.14]  in this example, see the code below. 

Meshes can be generated from different tools like [1.x.46], [1.x.47] and [1.x.48]. See the documentation of GridIn for more information. The problem is that deal.II needs meshes that only consist of quadrilaterals and hexahedra -- tetrahedral meshes won't work (this means tools like tetgen can not be used directly). 

We will describe a possible workflow using %Gmsh. %Gmsh is the smallest and most quickly set up open source tool we are aware of. It can generate unstructured 2d quad meshes. In 3d, it can extrude 2d meshes to get hexahedral meshes; 3D meshing of unstructured geometry into hexahedra is possible, though there are some issues with the quality of these meshes that imply that these meshes only sometimes work in deal.II. 

In %Gmsh, a mesh is fundamentally described in a text-based  [2.x.15]  file whose format can contain computations, loops, variables, etc. This format is quite flexible in allowing the description of complex geometries. The mesh is then generated from a surface representation, which is built from a list of line loops, which is built from a list of lines, which are in turn built from points. The  [2.x.16]  script can be written and edited by hand or it can be generated automatically by creating objects graphically inside %Gmsh. In many cases it is best to combine both approaches. The file can be easily reloaded by pressing "reload" under the "Geometry" tab if you want to write it by hand and see the effects in the graphical user interface of gmsh. 

This tutorial contains an example  [2.x.17]  file that describes a box with two objects cut out in the interior. This is how  [2.x.18]  looks like in %Gmsh (displaying the boundary indicators as well as the mesh discussed further down below): 

 [2.x.19]  

You might want to open the  [2.x.20]  file in a text editor (it is located in the same directory as the <tt> [2.x.21] .cc</tt> source file) to see how it is structured. You can see how the boundary of the domain is composed of a number of lines and how later on we combine several lines into "physical lines" (or "physical surfaces") that list the logical lines' numbers. "Physical" object are the ones that carry information about the boundary indicator (see  [2.x.22]  "this glossary entry"). 

 [2.x.23]  It is important that this file contain "physical lines" and "physical   surfaces". These give the boundary indicators and material ids for use   in deal.II. Without these physical entities, nothing will be imported into   deal.II. 

deal.II's GridIn class can read the  [2.x.24]  format written by %Gmsh and that contains a mesh created for the geometry described by the  [2.x.25]  from the  [2.x.26]  by running the commands 

[1.x.49] 



on the command line, or by clicking "Mesh" and then "2D" inside %Gmsh after loading the file.  Now this is the mesh read from the  [2.x.27]  file and saved again by deal.II as an image (see the  [2.x.28]  function of the current program): 

 [2.x.29]  

 [2.x.30]  %Gmsh has a number of other interfaces by which one can describe   geometries to it. In particular, it has the ability to interface with   scripting languages like Python and Julia, but it can also be scripted   from C++. These interfaces are useful if one doesn't just want to generate   a mesh for a single geometry (in which case the graphical interface or,   in simple cases, a hand-written `.geo` file is probably the simplest   approach), but instead wants to do parametric studies over the geometry   for which it is necessary to generate many meshes for geometries that   differ in certain parameters. Another case where this is useful is if there   is already a CAD geometry for which one only needs a mesh; indeed, this   can be done from within deal.II using the    [2.x.31]  function. 


[1.x.50][1.x.51] 


After acquiring one (or several) meshes in the ways described above, there are many ways to manipulate them before using them in a finite element computation. 


[1.x.52][1.x.53] 


The GridTools namespace contains a collection of small functions to transform a given mesh in various ways. The usage of the functions  [2.x.32]   [2.x.33]   [2.x.34]  is fairly obvious, so we won't discuss those functions here. 

The function  [2.x.35]  allows you to transform the vertices of a given mesh using a smooth function. An example of its use is also given in the results section of  [2.x.36]  but let us show a simpler example here: In the function  [2.x.37]  of the current program, we perturb the y coordinate of a mesh with a sine curve: 

 [2.x.38]  

Similarly, we can transform a regularly refined unit square to a wall-adapted mesh in y direction using the formula  [2.x.39] . This is done in  [2.x.40]  of this tutorial:  [2.x.41]  

Finally, the function  [2.x.42]  allows you to move vertices in the mesh (optionally ignoring boundary nodes) by a random amount. This is demonstrated in  [2.x.43]  and the result is as follows: 

 [2.x.44]  

This function is primarily intended to negate some of the superconvergence effects one gets when studying convergence on regular meshes, as well as to suppress some optimizations in deal.II that can exploit the fact that cells are similar in shape. (Superconvergence refers to the fact that if a mesh has certain symmetries -- for example, if the edges running into a vertex are symmetric to this vertex, and if this is so for all vertices of a cell 

-- that the solution is then often convergent with a higher order than one would have expected from the usual error analysis. In the end, this is a result of the fact that if one were to make a Taylor expansion of the error, the symmetry leads to the fact that the expected next term of the expansion happens to be zero, and the error order is determined by the *second next* term. A distorted mesh does not have these symmetries and consequently the error reflects what one will see when solving the equation on *any* kind of mesh, rather than showing something that is only reflective of a particular situation.) 


[1.x.54][1.x.55] 


The function  [2.x.45]  allows you to merge two given Triangulation objects into a single one.  For this to work, the vertices of the shared edge or face have to match exactly.  Lining up the two meshes can be achieved using  [2.x.46]  and  [2.x.47]   In the function  [2.x.48]  of this tutorial, we merge a square with a round hole (generated with  [2.x.49]  and a rectangle (generated with  [2.x.50]  The function  [2.x.51]  allows you to specify the number of repetitions and the positions of the corners, so there is no need to shift the triangulation manually here. You should inspect the mesh graphically to make sure that cells line up correctly and no unpaired nodes exist in the merged Triangulation. 

These are the input meshes and the output mesh: 

 [2.x.52]  


[1.x.56][1.x.57] 


The function  [2.x.53]  demonstrates the ability to pick individual vertices and move them around in an existing mesh. Note that this has the potential to produce degenerate or inverted cells and you shouldn't expect anything useful to come of using such meshes. Here, we create a box with a cylindrical hole that is not exactly centered by moving the top vertices upwards: 

 [2.x.54]  

For the exact way how this is done, see the code below. 


[1.x.58][1.x.59] 


If you need a 3d mesh that can be created by extruding a given 2d mesh (that can be created in any of the ways given above), you can use the function  [2.x.55]  See the  [2.x.56]  function in this tutorial for an example. Note that for this particular case, the given result could also be achieved using the 3d version of  [2.x.57]  The main usage is a 2d mesh, generated for example with %Gmsh, that is read in from a  [2.x.58]  file as described above. This is the output from grid_4(): 

 [2.x.59]  


[1.x.60][1.x.61] 


Creating a coarse mesh using the methods discussed above is only the first step. When you have it, it will typically serve as the basis for further mesh refinement. This is not difficult &mdash; in fact, there is nothing else to do &mdash; if your geometry consists of only straight faces. However, this is often not the case if you have a more complex geometry and more steps than just creating the mesh are necessary. We will go over some of these steps in the [1.x.62] below. [1.x.63] [1.x.64] 

This tutorial program is odd in the sense that, unlike for most other steps, the introduction already provides most of the information on how to use the various strategies to generate meshes. Consequently, there is little that remains to be commented on here, and we intersperse the code with relatively little text. In essence, the code here simply provides a reference implementation of what has already been described in the introduction. 





[1.x.65]  [1.x.66] 







[1.x.67] 




[1.x.68]  [1.x.69] 




The following function generates some output for any of the meshes we will be generating in the remainder of this program. In particular, it generates the following information: 




- Some general information about the number of space dimensions in which this mesh lives and its number of cells. 

- The number of boundary faces that use each boundary indicator, so that it can be compared with what we expect. 




Finally, the function outputs the mesh in VTU format that can easily be visualized in Paraview or VisIt. 

[1.x.70] 



Next loop over all faces of all cells and find how often each boundary indicator is used (recall that if you access an element of a  [2.x.60]  object that doesn't exist, it is implicitly created and default initialized -- to zero, in the current case -- before we then increment it): 

[1.x.71] 



Finally, produce a graphical representation of the mesh to an output file: 

[1.x.72] 




[1.x.73]  [1.x.74] 





[1.x.75]  [1.x.76] 




In this first example, we show how to load the mesh for which we have discussed in the introduction how to generate it. This follows the same pattern as used in  [2.x.61]  to load a mesh, although there it was written in a different file format (UCD instead of MSH). 

[1.x.77] 




[1.x.78]  [1.x.79] 




Here, we first create two triangulations and then merge them into one.  As discussed in the introduction, it is important to ensure that the vertices at the common interface are located at the same coordinates. 

[1.x.80] 




[1.x.81]  [1.x.82] 




In this function, we move vertices of a mesh. This is simpler than one usually expects: if you ask a cell using  [2.x.62]  for the coordinates of its  [2.x.63] th vertex, it doesn't just provide the location of this vertex but in fact a reference to the location where these coordinates are stored. We can then modify the value stored there. 




So this is what we do in the first part of this function: We create a square of geometry  [2.x.64]  with a circular hole with radius 0.25 located at the origin. We then loop over all cells and all vertices and if a vertex has a  [2.x.65]  coordinate equal to one, we move it upward by 0.5. 




Note that this sort of procedure does not usually work this way because one will typically encounter the same vertices multiple times and may move them more than once. It works here because we select the vertices we want to use based on their geometric location, and a vertex moved once will fail this test in the future. A more general approach to this problem would have been to keep a  [2.x.66]  of those vertex indices that we have already moved (which we can obtain using  [2.x.67]  and only move those vertices whose index isn't in the set yet. 

[1.x.83] 



In the second step we will refine the mesh twice. To do this correctly, we should place new points on the interior boundary along the surface of a circle centered at the origin. Fortunately,  [2.x.68]  already attaches a Manifold object to the interior boundary, so we do not need to do anything but refine the mesh (see the [1.x.84] for a fully worked example where we  [2.x.69] do [2.x.70]  attach a Manifold object). 

[1.x.85] 



There is one snag to doing things as shown above: If one moves the nodes on the boundary as shown here, one often ends up with cells in the interior that are badly distorted since the interior nodes were not moved around. This is not that much of a problem in the current case since the mesh did not contain any internal nodes when the nodes were moved -- it was the coarse mesh and it so happened that all vertices are at the boundary. It's also the case that the movement we had here was, compared to the average cell size not overly dramatic. Nevertheless, sometimes one does want to move vertices by a significant distance, and in that case one needs to move internal nodes as well. One way to do that automatically is to call the function  [2.x.71]  that takes a set of transformed vertex coordinates and moves all of the other vertices in such a way that the resulting mesh has, in some sense, a small distortion. 














[1.x.86]  [1.x.87] 




This example takes the initial grid from the previous function and simply extrudes it into the third space dimension: 

[1.x.88] 




[1.x.89]  [1.x.90] 




This and the next example first create a mesh and then transform it by moving every node of the mesh according to a function that takes a point and returns a mapped point. In this case, we transform  [2.x.72] . 




 [2.x.73]  takes a triangulation and an argument that can be called like a function taking a Point and returning a Point. There are different ways of providing such an argument: It could be a pointer to a function; it could be an object of a class that has an `operator()`; it could be a lambda function; or it could be anything that is described via a  [2.x.74]  object. 




Decidedly the more modern way is to use a lambda function that takes a Point and returns a Point, and that is what we do in the following: 

[1.x.91] 




[1.x.92]  [1.x.93] 




In this second example of transforming points from an original to a new mesh, we will use the mapping  [2.x.75] . To make things more interesting, rather than doing so in a single function as in the previous example, we here create an object with an  [2.x.76]  that will be called by  [2.x.77]  Of course, this object may in reality be much more complex: the object may have member variables that play a role in computing the new locations of vertices. 

[1.x.94] 




[1.x.95]  [1.x.96] 




In this last example, we create a mesh and then distort its (interior) vertices by a random perturbation. This is not something you want to do for production computations (because results are generally better on meshes with "nicely shaped" cells than on the deformed cells produced by  [2.x.78]  but it is a useful tool for testing discretizations and codes to make sure they don't work just by accident because the mesh happens to be uniformly structured and supporting superconvergence properties. 

[1.x.97] 




[1.x.98]  [1.x.99] 




Finally, the main function. There isn't much to do here, only to call all the various functions we wrote above. 

[1.x.100] 

[1.x.101][1.x.102] 


The program produces a series of  [2.x.79]  files of the triangulations. The methods are discussed above. 


[1.x.103][1.x.104] 


As mentioned in the introduction, creating a coarse mesh using the methods discussed here is only the first step. In order to refine a mesh, the Triangulation needs to know where to put new vertices on the mid-points of edges, faces, and cells. By default, these new points will be placed at the arithmetic mean of the surrounding points, but this isn't what you want if you need curved boundaries that aren't already adequately resolved by the coarse mesh. For example, for this mesh the central hole is supposed to be round: 

 [2.x.80]  

If you simply refine it, the Triangulation class can not know whether you wanted the hole to be round or to be an octagon. The default is to place new points along existing straight lines. After two mesh refinement steps, this would yield the following mesh, which is not what we wanted: 

 [2.x.81]  

What needs to happen is that you tell the triangulation that you in fact want to use a curved geometry. The way to do this requires three steps: 

- Create an object that describes the desired geometry. This object will be   queried when refining the Triangulation for new point placement. It will also   be used to calculate shape function values if a high degree mapping, like   MappingQ or MappingQGeneric, is used during system assembly.   In deal.II the Manifold class and classes inheriting from it (e.g.,   PolarManifold and FlatManifold) perform these calculations. 

- Notify the Triangulation object which Manifold classes to use. By default, a   Triangulation uses FlatManifold to do all geometric calculations,   which assumes that all cell edges are straight lines and all quadrilaterals   are flat. You can attach Manifold classes to a Triangulation by calling    [2.x.82]  function, which associates a    [2.x.83]  with a Manifold object. For more information on this   see the  [2.x.84]  "glossary entry on this topic". 

- Finally, you must mark cells and cell faces with the correct    [2.x.85] . For example, you could get an annular sector with   curved cells in Cartesian coordinates (but rectangles in polar coordinates)   by doing the following:   [1.x.105] 

  Now, when the grid is refined, all cell splitting calculations will be done in   polar coordinates. 

All functions in the GridGenerator namespace which create a mesh where some cells should be curved also attach the correct Manifold object to the provided Triangulation: i.e., for those functions we get the correct behavior by default. For a hand-generated mesh, however, the situation is much more interesting. 

To illustrate this process in more detail, let us consider an example created by Yuhan Zhou as part of a 2013 semester project at Texas A&amp;M University. The goal was to generate (and use) a geometry that describes a microstructured electric device. In a CAD program, the geometry looks like this: 

 [2.x.86]  

In the following, we will walk you through the entire process of creating a mesh for this geometry, including a number of common pitfalls by showing the things that can go wrong. 

The first step in getting there was to create a coarse mesh, which was done by creating a 2d coarse mesh for each of cross sections, extruding them into the third direction, and gluing them together. The following code does this, using the techniques previously described: 

[1.x.106] 



This creates the following mesh: 

<img src="https://www.dealii.org/images/steps/developer/ [2.x.87] .yuhan.8.png"      alt="" width="400" height="355"> 

This mesh has the right general shape, but the top cells are now polygonal: their edges are no longer along circles and we do not have a very accurate representation of the original geometry. The next step is to teach the top part of the domain that it should be curved. Put another way, all calculations done on the top boundary cells should be done in cylindrical coordinates rather than Cartesian coordinates. We can do this by creating a CylindricalManifold object and associating it with the cells above  [2.x.88] . This way, when we refine the cells on top, we will place new points along concentric circles instead of straight lines. 

In deal.II we describe all geometries with classes that inherit from Manifold. The default geometry is Cartesian and is implemented in the FlatManifold class. As the name suggests, Manifold and its inheriting classes provide a way to describe curves and curved cells in a general way with ideas and terminology from differential geometry: for example, CylindricalManifold inherits from ChartManifold, which describes a geometry through pull backs and push forwards. In general, one should think that the Triangulation class describes the topology of a domain (in addition, of course, to storing the locations of the vertices) while the Manifold classes describe the geometry of a domain (e.g., whether or not a pair of vertices lie along a circular arc or a straight line). A Triangulation will refine cells by doing computations with the Manifold associated with that cell regardless of whether or not the cell is on the boundary. Put another way: the Manifold classes do not need any information about where the boundary of the Triangulation actually is: it is up to the Triangulation to query the right Manifold for calculations on a cell. Most Manifold functions (e.g.,  [2.x.89]  know nothing about the domain itself and just assume that the points given to it lie along a geodesic. In this case, with the CylindricalManifold constructed below, the geodesics are arcs along circles orthogonal to the  [2.x.90] -axis centered along the line  [2.x.91] . 

Since all three top parts of the domain use the same geodesics, we will mark all cells with centers above the  [2.x.92]  line as being cylindrical in nature: 

[1.x.107] 



With this code, we get a mesh that looks like this: 

<img src="https://www.dealii.org/images/steps/developer/ [2.x.93] .yuhan.9.png"      alt="" width="400" height="355"> 

This change fixes the boundary but creates a new problem: the cells adjacent to the cylinder's axis are badly distorted. We should use Cartesian coordinates for calculations on these central cells to avoid this issue. The cells along the center line all have a face that touches the line  [2.x.94]  so, to implement this, we go back and overwrite the  [2.x.95] s on these cells to be zero (which is the default): 

[1.x.108] 



This gives us the following grid: 

<img src="https://www.dealii.org/images/steps/developer/ [2.x.96] .yuhan.10.png"      alt="" width="400" height="355"> 

This gives us a good mesh, where cells at the center of each circle are still Cartesian and cells around the boundary lie along a circle. We can really see the nice detail of the boundary fitted mesh if we refine two more times: 

<img src="https://www.dealii.org/images/steps/developer/ [2.x.97] .yuhan.11.png"      alt="" width="400" height="355"> 


[1.x.109][1.x.110] 


[1.x.111][1.x.112] 


It is often useful to assign different boundary ids to a mesh that is generated in one form or another as described in this tutorial to apply different boundary conditions. 

For example, you might want to apply a different boundary condition for the right boundary of the first grid in this program. To do this, iterate over the cells and their faces and identify the correct faces (for example using `cell->center()` to query the coordinates of the center of a cell as we do in  [2.x.98] , or using `cell->face(f)->get_boundary_id()` to query the current boundary indicator of the  [2.x.99] th face of the cell). You can then use `cell->face(f)->set_boundary_id()` to set the boundary id to something different. You can take a look back at  [2.x.100]  how iteration over the meshes is done there. 

[1.x.113][1.x.114] 


Computations on manifolds, like they are done in  [2.x.101] , require a surface mesh embedded into a higher dimensional space. While some can be constructed using the GridGenerator namespace or loaded from a file, it is sometimes useful to extract a surface mesh from a volume mesh. 

Use the function  [2.x.102]  to extract the surface elements of a mesh. Using the function on a 3d mesh (a `Triangulation<3,3>`, for example from `grid_4()`), this will return a `Triangulation<2,3>` that you can use in  [2.x.103] .  Also try extracting the boundary mesh of a `Triangulation<2,2>`. 


<!-- 

Possible Extensions for this tutorial: 

- Database of unstructured meshes for convergence studies 

- how to remove or disable a cell inside a mesh 

--> [1.x.115] [1.x.116]  [2.x.104]  

 [2.x.105] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27] 

 [2.x.4]  

[1.x.28] 

 [2.x.5]  

 [2.x.6]  As a prerequisite of this program, you need to have both p4est and either the PETSc or Trilinos library installed. The installation of deal.II together with these additional libraries is described in the [1.x.29] file. 


[1.x.30] [1.x.31][1.x.32] 




This example shows the usage of the multilevel functions in deal.II on parallel, distributed meshes and gives a comparison between geometric and algebraic multigrid methods. The algebraic multigrid (AMG) preconditioner is the same used in  [2.x.7] . Two geometric multigrid (GMG) preconditioners are considered: a matrix-based version similar to that in  [2.x.8]  (but for parallel computations) and a matrix-free version discussed in  [2.x.9] . The goal is to find out which approach leads to the best solver for large parallel computations. 

This tutorial is based on one of the numerical examples in  [2.x.10] . Please see that publication for a detailed background on the multigrid implementation in deal.II. We will summarize some of the results in the following text. 

Algebraic multigrid methods are obviously the easiest to implement with deal.II since classes such as  [2.x.11]  and  [2.x.12]  are, in essence, black box preconditioners that require only a couple of lines to set up even for parallel computations. On the other hand, geometric multigrid methods require changes throughout a code base -- not very many, but one has to know what one is doing. 

What the results of this program will show is that algebraic and geometric multigrid methods are roughly comparable in performance [1.x.33], and that matrix-free geometric multigrid methods are vastly better for the problem under consideration here. A secondary conclusion will be that matrix-based geometric multigrid methods really don't scale well strongly when the number of unknowns per processor becomes smaller than 20,000 or so. 


[1.x.34][1.x.35] 


We consider the variable-coefficient Laplacian weak formulation 

[1.x.36] 

on the domain  [2.x.13]  (an L-shaped domain for 2D and a Fichera corner for 3D) with  [2.x.14]  if  [2.x.15]  and  [2.x.16]  otherwise. In other words,  [2.x.17]  is small along the edges or faces of the domain that run into the reentrant corner, as will be visible in the figure below. 

The boundary conditions are  [2.x.18]  on the whole boundary and the right-hand side is  [2.x.19] . We use continuous  [2.x.20]  elements for the discrete finite element space  [2.x.21] , and use a residual-based, cell-wise a posteriori error estimator  [2.x.22]  from  [2.x.23]  with 

[1.x.37] 

to adaptively refine the mesh. (This is a generalization of the Kelly error estimator used in the KellyErrorEstimator class that drives mesh refinement in most of the other tutorial programs.) The following figure visualizes the solution and refinement for 2D:  [2.x.24]  In 3D, the solution looks similar (see below). On the left you can see the solution and on the right we show a slice for  [2.x.25]  close to the center of the domain showing the adaptively refined mesh.  [2.x.26]  Both in 2D and 3D you can see the adaptive refinement picking up the corner singularity and the inner singularity where the viscosity jumps, while the interface along the line that separates the two viscosities is (correctly) not refined as it is resolved adequately. This is because the kink in the solution that results from the jump in the coefficient is aligned with cell interfaces. 


[1.x.38][1.x.39] 


As mentioned above, the purpose of this program is to demonstrate the use of algebraic and geometric multigrid methods for this problem, and to do so for parallel computations. An important component of making algorithms scale to large parallel machines is ensuring that every processor has the same amount of work to do. (More precisely, what matters is that there are no small fraction of processors that have substantially more work than the rest since, if that were so, a large fraction of processors will sit idle waiting for the small fraction to finish. Conversely, a small fraction of processors having substantially [1.x.40] work is not a problem because the majority of processors continues to be productive and only the small fraction sits idle once finished with their work.) 

For the active mesh, we use the  [2.x.27]  class as done in  [2.x.28]  which uses functionality in the external library [1.x.41] for the distribution of the active cells among processors. For the non-active cells in the multilevel hierarchy, deal.II implements what we will refer to as the "first-child rule" where, for each cell in the hierarchy, we recursively assign the parent of a cell to the owner of the first child cell. The following figures give an example of such a distribution. Here the left image represents the active cells for a sample 2D mesh partitioned using a space-filling curve (which is what p4est uses to partition cells); the center image gives the tree representation of the active mesh; and the right image gives the multilevel hierarchy of cells. The colors and numbers represent the different processors. The circular nodes in the tree are the non-active cells which are distributed using the "first-child rule". 

 [2.x.29]  

Included among the output to screen in this example is a value "Partition efficiency" given by one over  [2.x.30]  This value, which will be denoted by  [2.x.31] ,  quantifies the overhead produced by not having a perfect work balance on each level of the multigrid hierarchy. This imbalance is evident from the example above: while level  [2.x.32]  is about as well balanced as is possible with four cells among three processors, the coarse level  [2.x.33]  has work for only one processor, and level  [2.x.34]  has work for only two processors of which one has three times as much work as the other. 

For defining  [2.x.35] , it is important to note that, as we are using local smoothing to define the multigrid hierarchy (see the  [2.x.36]  "multigrid paper" for a description of local smoothing), the refinement level of a cell corresponds to that cell's multigrid level. Now, let  [2.x.37]  be the number of cells on level  [2.x.38]  (both active and non-active cells) and  [2.x.39]  be the subset owned by process  [2.x.40] . We will also denote by  [2.x.41]  the total number of processors. Assuming that the workload for any one processor is proportional to the number of cells owned by that processor, the optimal workload per processor is given by 

[1.x.42] 

Next, assuming a synchronization of work on each level (i.e., on each level of a V-cycle, work must be completed by all processors before moving on to the next level), the limiting effort on each level is given by 

[1.x.43] 

and the total parallel complexity 

[1.x.44] 

Then we define  [2.x.42]  as a ratio of the optimal partition to the parallel complexity of the current partition 

[1.x.45] 

For the example distribution above, we have 

[1.x.46] 

The value  [2.x.43]   [2.x.44]  then represents the factor increase in timings we expect for GMG methods (vmults, assembly, etc.) due to the imbalance of the mesh partition compared to a perfectly load-balanced workload. We will report on these in the results section below for a sequence of meshes, and compare with the observed slow-downs as we go to larger and larger processor numbers (where, typically, the load imbalance becomes larger as well). 

These sorts of considerations are considered in much greater detail in  [2.x.45] , which contains a full discussion of the partition efficiency model and the effect the imbalance has on the GMG V-cycle timing. In summary, the value of  [2.x.46]  is highly dependent on the degree of local mesh refinement used and has an optimal value  [2.x.47]  for globally refined meshes. Typically for adaptively refined meshes, the number of processors used to distribute a single mesh has a negative impact on  [2.x.48]  but only up to a leveling off point, where the imbalance remains relatively constant for an increasing number of processors, and further refinement has very little impact on  [2.x.49] . Finally,  [2.x.50]  was shown to give an accurate representation of the slowdown in parallel scaling expected for the timing of a V-cycle. 

It should be noted that there is potential for some asynchronous work between multigrid levels, specifically with purely nearest neighbor MPI communication, and an adaptive mesh could be constructed such that the efficiency model would far overestimate the V-cycle slowdown due to the asynchronous work "covering up" the imbalance (which assumes synchronization over levels). However, for most realistic adaptive meshes the expectation is that this asynchronous work will only cover up a very small portion of the imbalance and the efficiency model will describe the slowdown very well. 


[1.x.47][1.x.48] 


The considerations above show that one has to expect certain limits on the scalability of the geometric multigrid algorithm as it is implemented in deal.II because even in cases where the finest levels of a mesh are perfectly load balanced, the coarser levels may not be. At the same time, the coarser levels are weighted less (the contributions of  [2.x.51]  to  [2.x.52]  are small) because coarser levels have fewer cells and, consequently, do not contribute to the overall run time as much as finer levels. In other words, imbalances in the coarser levels may not lead to large effects in the big picture. 

Algebraic multigrid methods are of course based on an entirely different approach to creating a hierarchy of levels. In particular, they create these purely based on analyzing the system matrix, and very sophisticated algorithms for ensuring that the problem is well load-balanced on every level are implemented in both the hypre and ML/MueLu packages that underly the  [2.x.53]  and  [2.x.54]  classes. In some sense, these algorithms are simpler than for geometric multigrid methods because they only deal with the matrix itself, rather than all of the connotations of meshes, neighbors, parents, and other geometric entities. At the same time, much work has also been put into making algebraic multigrid methods scale to very large problems, including questions such as reducing the number of processors that work on a given level of the hierarchy to a subset of all processors, if otherwise processors would spend less time on computations than on communication. (One might note that it is of course possible to implement these same kinds of ideas also in geometric multigrid algorithms where one purposefully idles some processors on coarser levels to reduce the amount of communication. deal.II just doesn't do this at this time.) 

These are not considerations we typically have to worry about here, however: For most purposes, we use algebraic multigrid methods as black-box methods. 




[1.x.49][1.x.50] 


As mentioned above, this program can use three different ways of solving the linear system: matrix-based geometric multigrid ("MB"), matrix-free geometric multigrid ("MF"), and algebraic multigrid ("AMG"). The directory in which this program resides has input files with suffix `.prm` for all three of these options, and for both 2d and 3d. 

You can execute the program as in 

[1.x.51] 

and this will take the run-time parameters from the given input file (here, `gmg_mb_2d.prm`). 

The program is intended to be run in parallel, and you can achieve this using a command such as 

[1.x.52] 

if you want to, for example, run on four processors. (That said, the program is also ready to run with, say, `-np 28672` if that's how many processors you have available.) [1.x.53] [1.x.54] 


[1.x.55]  [1.x.56] 




The include files are a combination of  [2.x.55] ,  [2.x.56] , and  [2.x.57] : 







[1.x.57] 



We use the same strategy as in  [2.x.58]  to switch between PETSc and Trilinos: 







[1.x.58] 



Comment the following preprocessor definition in or out if you have PETSc and Trilinos installed and you prefer using PETSc in this example: 

[1.x.59] 



The following files are used to assemble the error estimator like in  [2.x.59] : 

[1.x.60] 




[1.x.61]  [1.x.62] 




MatrixFree operators must use the  [2.x.60]  vector type. Here we define operations which copy to and from Trilinos vectors for compatibility with the matrix-based code. Note that this functionality does not currently exist for PETSc vector types, so Trilinos must be installed to use the MatrixFree solver in this tutorial. 

[1.x.63] 



Let's move on to the description of the problem we want to solve. We set the right-hand side function to 1.0. The  [2.x.61]  function returning a VectorizedArray is used by the matrix-free code path. 

[1.x.64] 



This next class represents the diffusion coefficient. We use a variable coefficient which is 100.0 at any point where at least one coordinate is less than -0.5, and 1.0 at all other points. As above, a separate value() returning a VectorizedArray is used for the matrix-free code. An @p average() function computes the arithmetic average for a set of points. 

[1.x.65] 



When using a coefficient in the MatrixFree framework, we also need a function that creates a Table of coefficient values for a set of cells provided by the MatrixFree operator argument here. 

[1.x.66] 




[1.x.67]  [1.x.68] 




We will use ParameterHandler to pass in parameters at runtime.  The structure  [2.x.62]  parses and stores these parameters to be queried throughout the program. 

[1.x.69] 




[1.x.70]  [1.x.71] 




This is the main class of the program. It looks very similar to  [2.x.63] ,  [2.x.64] , and  [2.x.65] . For the MatrixFree setup, we use the  [2.x.66]  class which defines `local_apply()`, `compute_diagonal()`, and `set_coefficient()` functions internally. Note that the polynomial degree is a template parameter of this class. This is necesary for the matrix-free code. 

[1.x.72] 



We will use the following types throughout the program. First the matrix-based types, after that the matrix-free classes. For the matrix-free implementation, we use  [2.x.67]  for the level operators. 

[1.x.73] 



The only interesting part about the constructor is that we construct the multigrid hierarchy unless we use AMG. For that, we need to parse the run time parameters before this constructor completes. 

[1.x.74] 




[1.x.75]  [1.x.76] 




Unlike  [2.x.68]  and  [2.x.69] , we split the set up into two parts, setup_system() and setup_multigrid(). Here is the typical setup_system() function for the active mesh found in most tutorials. For matrix-free, the active mesh set up is similar to  [2.x.70] ; for matrix-based (GMG and AMG solvers), the setup is similar to  [2.x.71] . 

[1.x.77] 




[1.x.78]  [1.x.79] 




This function does the multilevel setup for both matrix-free and matrix-based GMG. The matrix-free setup is similar to that of  [2.x.72] , and the matrix-based is similar to  [2.x.73] , except we must use appropriate distributed sparsity patterns. 




The function is not called for the AMG approach, but to err on the safe side, the main `switch` statement of this function nevertheless makes sure that the function only operates on known multigrid settings by throwing an assertion if the function were called for anything other than the two geometric multigrid methods. 

[1.x.80] 




[1.x.81]  [1.x.82] 




The assembly is split into three parts: `assemble_system()`, `assemble_multigrid()`, and `assemble_rhs()`. The `assemble_system()` function here assembles and stores the (global) system matrix and the right-hand side for the matrix-based methods. It is similar to the assembly in  [2.x.74] . 




Note that the matrix-free method does not execute this function as it does not need to assemble a matrix, and it will instead assemble the right-hand side in assemble_rhs(). 

[1.x.83] 




[1.x.84]  [1.x.85] 




The following function assembles and stores the multilevel matrices for the matrix-based GMG method. This function is similar to the one found in  [2.x.75] , only here it works for distributed meshes. This difference amounts to adding a condition that we only assemble on locally owned level cells and a call to compress() for each matrix that is built. 

[1.x.86] 




[1.x.87]  [1.x.88] 




The final function in this triptych assembles the right-hand side vector for the matrix-free method -- because in the matrix-free framework, we don't have to assemble the matrix and can get away with only assembling the right hand side. We could do this by extracting the code from the `assemble_system()` function above that deals with the right hand side, but we decide instead to go all in on the matrix-free approach and do the assembly using that way as well. 




The result is a function that is similar to the one found in the "Use  [2.x.76]  to avoid resolving constraints" subsection in the "Possibilities for extensions" section of  [2.x.77] . 




The reason for this function is that the MatrixFree operators do not take into account non-homogeneous Dirichlet constraints, instead treating all Dirichlet constraints as homogeneous. To account for this, the right-hand side here is assembled as the residual  [2.x.78] , where  [2.x.79]  is a zero vector except in the Dirichlet values. Then when solving, we have that the solution is  [2.x.80] . This can be seen as a Newton iteration on a linear system with initial guess  [2.x.81] . The CG solve in the `solve()` function below computes  [2.x.82]  and the call to `constraints.distribute()` (which directly follows) adds the  [2.x.83] . 




Obviously, since we are considering a problem with zero Dirichlet boundary, we could have taken a similar approach to  [2.x.84]  `assemble_rhs()`, but this additional work allows us to change the problem declaration if we so choose. 




This function has two parts in the integration loop: applying the negative of matrix  [2.x.85]  to  [2.x.86]  by submitting the negative of the gradient, and adding the right-hand side contribution by submitting the value  [2.x.87] . We must be sure to use `read_dof_values_plain()` for evaluating  [2.x.88]  as `read_dof_vaues()` would set all Dirichlet values to zero. 




Finally, the system_rhs vector is of type  [2.x.89]  but the MatrixFree class only work for  [2.x.90]   Therefore we must compute the right-hand side using MatrixFree funtionality and then use the functions in the `ChangeVectorType` namespace to copy it to the correct type. 

[1.x.89] 




[1.x.90]  [1.x.91] 




Here we set up the multigrid preconditioner, test the timing of a single V-cycle, and solve the linear system. Unsurprisingly, this is one of the places where the three methods differ the most. 

[1.x.92] 



The solver for the matrix-free GMG method is similar to  [2.x.91] , apart from adding some interface matrices in complete analogy to  [2.x.92] . 

[1.x.93] 



Copy the solution vector and right-hand side from  [2.x.93]  to  [2.x.94]  so that we can solve. 

[1.x.94] 



Timing for 1 V-cycle. 

[1.x.95] 



Solve the linear system, update the ghost values of the solution, copy back to  [2.x.95]  and distribute constraints. 

[1.x.96] 



Solver for the matrix-based GMG method, similar to  [2.x.96] , only using a Jacobi smoother instead of a SOR smoother (which is not implemented in parallel). 

[1.x.97] 



Timing for 1 V-cycle. 

[1.x.98] 



Solve the linear system and distribute constraints. 

[1.x.99] 



Solver for the AMG method, similar to  [2.x.97] . 

[1.x.100] 



Timing for 1 V-cycle. 

[1.x.101] 



Solve the linear system and distribute constraints. 

[1.x.102] 




[1.x.103]  [1.x.104] 




We use the FEInterfaceValues class to assemble an error estimator to decide which cells to refine. See the exact definition of the cell and face integrals in the introduction. To use the method, we define Scratch and Copy objects for the  [2.x.98]  with much of the following code being in essence as was set up in  [2.x.99]  already (or at least similar in spirit). 

[1.x.105] 



Assembler for cell residual  [2.x.100]  

[1.x.106] 



Assembler for face term  [2.x.101]  

[1.x.107] 



We need to assemble each interior face once but we need to make sure that both processes assemble the face term between a locally owned and a ghost cell. This is achieved by setting the  [2.x.102]  flag. We need to do this, because we do not communicate the error estimator contributions here. 

[1.x.108] 




[1.x.109]  [1.x.110] 




We use the cell-wise estimator stored in the vector  [2.x.103]  and refine a fixed number of cells (chosen here to roughly double the number of DoFs in each step). 

[1.x.111] 




[1.x.112]  [1.x.113] 




The output_results() function is similar to the ones found in many of the tutorials (see  [2.x.104]  for example). 

[1.x.114] 




[1.x.115]  [1.x.116] 




As in most tutorials, this function calls the various functions defined above to setup, assemble, solve, and output the results. 

[1.x.117] 



We only output level cell data for the GMG methods (same with DoF data below). Note that the partition efficiency is irrelevant for AMG since the level hierarchy is not distributed or used during the computation. 

[1.x.118] 



Only set up the multilevel hierarchy for GMG. 

[1.x.119] 



For the matrix-free method, we only assemble the right-hand side. For both matrix-based methods, we assemble both active matrix and right-hand side, and only assemble the multigrid matrices for matrix-based GMG. 

[1.x.120] 




[1.x.121]  [1.x.122] 




This is a similar main function to  [2.x.105] , with the exception that we require the user to pass a .prm file as a sole command line argument (see  [2.x.106]  and the documentation of the ParameterHandler class for a complete discussion of parameter files). 

[1.x.123] 

[1.x.124][1.x.125] 


When you run the program using the following command 

[1.x.126] 

the screen output should look like the following: 

[1.x.127] 

Here, the timing of the `solve()` function is split up in 3 parts: setting up the multigrid preconditioner, execution of a single multigrid V-cycle, and the CG solver. The V-cycle that is timed is unnecessary for the overall solve and only meant to give an insight at the different costs for AMG and GMG. Also it should be noted that when using the AMG solver, "Workload imbalance" is not included in the output since the hierarchy of coarse meshes is not required. 

All results in this section are gathered on Intel Xeon Platinum 8280 (Cascade Lake) nodes which have 56 cores and 192GB per node and support AVX-512 instructions, allowing for vectorization over 8 doubles (vectorization used only in the matrix-free computations). The code is compiled using gcc 7.1.0 with intel-mpi 17.0.3. Trilinos 12.10.1 is used for the matrix-based GMG/AMG computations. 

We can then gather a variety of information by calling the program with the input files that are provided in the directory in which  [2.x.107]  is located. Using these, and adjusting the number of mesh refinement steps, we can produce information about how well the program scales. 

The following table gives weak scaling timings for this program on up to 256M DoFs and 7,168 processors. (Recall that weak scaling keeps the number of degrees of freedom per processor constant while increasing the number of processors; i.e., it considers larger and larger problems.) Here,  [2.x.108]  is the partition efficiency from the  introduction (also equal to 1.0/workload imbalance), "Setup" is a combination of setup, setup multigrid, assemble, and assemble multigrid from the timing blocks, and "Prec" is the preconditioner setup. Ideally all times would stay constant over each problem size for the individual solvers, but since the partition efficiency decreases from 0.371 to 0.161 from largest to smallest problem size, we expect to see an approximately  [2.x.109]  times increase in timings for GMG. This is, in fact, pretty close to what we really get: 

 [2.x.110]  

On the other hand, the algebraic multigrid in the last set of columns is relatively unaffected by the increasing imbalance of the mesh hierarchy (because it doesn't use the mesh hierarchy) and the growth in time is rather driven by other factors that are well documented in the literature (most notably that the algorithmic complexity of some parts of algebraic multigrid methods appears to be  [2.x.111]  instead of  [2.x.112]  for geometric multigrid). 

The upshort of the table above is that the matrix-free geometric multigrid method appears to be the fastest approach to solving this equation if not by a huge margin. Matrix-based methods, on the other hand, are consistently the worst. 

The following figure provides strong scaling results for each method, i.e., we solve the same problem on more and more processors. Specifically, we consider the problems after 16 mesh refinement cycles (32M DoFs) and 19 cycles (256M DoFs), on between 56 to 28,672 processors: 

 [2.x.113]  

While the matrix-based GMG solver and AMG scale similarly and have a similar time to solution (at least as long as there is a substantial number of unknowns per processor -- say, several 10,000), the matrix-free GMG solver scales much better and solves the finer problem in roughly the same time as the AMG solver for the coarser mesh with only an eighth of the number of processors. Conversely, it can solve the same problem on the same number of processors in about one eighth the time. 


[1.x.128][1.x.129] 


[1.x.130][1.x.131] 


The finite element degree is currently hard-coded as 2, see the template arguments of the main class. It is easy to change. To test, it would be interesting to switch to a test problem with a reference solution. This way, you can compare error rates. 

[1.x.132][1.x.133] 


A more interesting example would involve a more complicated coarse mesh (see  [2.x.114]  for inspiration). The issue in that case is that the coarsest level of the mesh hierarchy is actually quite large, and one would have to think about ways to solve the coarse level problem efficiently. (This is not an issue for algebraic multigrid methods because they would just continue to build coarser and coarser levels of the matrix, regardless of their geometric origin.) 

In the program here, we simply solve the coarse level problem with a Conjugate Gradient method without any preconditioner. That is acceptable if the coarse problem is really small -- for example, if the coarse mesh had a single cell, then the coarse mesh problems has a  [2.x.115]  matrix in 2d, and a  [2.x.116]  matrix in 3d; for the coarse mesh we use on the  [2.x.117] -shaped domain of the current program, these sizes are  [2.x.118]  in 2d and  [2.x.119]  in 3d. But if the coarse mesh consists of hundreds or thousands of cells, this approach will no longer work and might start to dominate the overall run-time of each V-cyle. A common approach is then to solve the coarse mesh problem using an algebraic multigrid preconditioner; this would then, however, require assembling the coarse matrix (even for the matrix-free version) as input to the AMG implementation. [1.x.134] [1.x.135]  [2.x.120]  

 [2.x.121] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] ,  [2.x.4] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38] 

 [2.x.5]  

[1.x.39] 

[1.x.40] [1.x.41][1.x.42] 


This tutorial program presents the implementation of a hybridizable discontinuous Galkerin method for the convection-diffusion equation. 

[1.x.43][1.x.44] 


One common argument against the use of discontinuous Galerkin elements is the large number of globally coupled degrees of freedom that one must solve in an implicit system.  This is because, unlike continuous finite elements, in typical discontinuous elements there is one degree of freedom at each vertex [1.x.45], rather than just one, and similarly for edges and faces.  As an example of how fast the number of unknowns grows, consider the FE_DGPMonomial basis: each scalar solution component is represented by polynomials of degree  [2.x.6]  with  [2.x.7]  degrees of freedom per element. Typically, all degrees of freedom in an element are coupled to all of the degrees of freedom in the adjacent elements.  The resulting discrete equations yield very large linear systems very quickly, especially for systems of equations in 2 or 3 dimensions. 

[1.x.46][1.x.47] 

To alleviate the computational cost of solving such large linear systems, the hybridizable discontinuous Galerkin (HDG) methodology was introduced by Cockburn and co-workers (see the references in the recent HDG overview article by Nguyen and Peraire  [2.x.8] ). 

The HDG method achieves this goal by formulating the mathematical problem using Dirichlet-to-Neumann mappings.  The partial differential equations are first written as a first order system, and each field is then discretized via a DG method.  At this point, the single-valued "trace" values on the skeleton of the mesh, i.e., element faces, are taken to be independent unknown quantities. This yields unknowns in the discrete formulation that fall into two categories: 

- Face unknowns that only couple with the cell unknowns from both sides of the face; 

- Cell unknowns that only couple with the cell and face unknowns   defined within the same cell. Crucially, no cell interior degree of freedom   on one cell ever couples to any interior cell degree of freedom of a   different cell. 

The Dirichlet-to-Neumann map concept then permits the following solution procedure:  [2.x.9]     [2.x.10]   Use local element interior data to enforce a Neumann condition on the skeleton of the triangulation.  The global problem is then to solve for the trace values, which are the only globally coupled unknowns.    [2.x.11]   Use the known skeleton values as Dirichlet data for solving local element-level solutions.  This is known as the 'local solver', and is an [1.x.48] element-by-element solution process.  [2.x.12]  

[1.x.49][1.x.50] 

The above procedure also has a linear algebra interpretation---referred to as [1.x.51]---that was exploited to reduce the size of the global linear system by Guyan in the context of continuous Finite Elements  [2.x.13] , and by Fraeijs de Veubeke for mixed methods  [2.x.14] . In the latter case (mixed formulation), the system reduction was achieved through the use of discontinuous fluxes combined with the introduction of an additional auxiliary [1.x.52] variable that approximates the trace of the unknown at the boundary of every element. This procedure became known as hybridization and---by analogy---is the reason why the local discontinuous Galerkin method introduced by Cockburn, Gopalakrishnan, and Lazarov in 2009  [2.x.15] , and subsequently developed by their collaborators, eventually came to be known as the [1.x.53] (HDG) method. 

Let us write the complete linear system associated to the HDG problem as a block system with the discrete DG (cell interior) variables  [2.x.16]  as first block and the skeleton (face) variables  [2.x.17]  as the second block: [1.x.54] 

Our aim is now to eliminate the  [2.x.18]  block with a Schur complement approach similar to  [2.x.19] , which results in the following two steps: [1.x.55] 

The point is that the presence of  [2.x.20]  is not a problem because  [2.x.21]  is a block diagonal matrix where each block corresponds to one cell and is therefore easy enough to invert. The coupling to other cells is introduced by the matrices  [2.x.22]  and  [2.x.23]  over the skeleton variable. The block-diagonality of  [2.x.24]  and the structure in  [2.x.25]  and  [2.x.26]  allow us to invert the matrix  [2.x.27]  element by element (the local solution of the Dirichlet problem) and subtract  [2.x.28]  from  [2.x.29] . The steps in the Dirichlet-to-Neumann map concept hence correspond to  [2.x.30]     [2.x.31]  constructing the Schur complement matrix  [2.x.32]  and right hand     side  [2.x.33]   [1.x.56]     and inserting the contribution into the global trace matrix in the usual way,    [2.x.34]  solving the Schur complement system for  [2.x.35] , and    [2.x.36]  solving for  [2.x.37]  using the second equation, given  [2.x.38] .  [2.x.39]  


[1.x.57][1.x.58] 

Another criticism of traditional DG methods is that the approximate fluxes converge suboptimally.  The local HDG solutions can be shown to converge as  [2.x.40] , i.e., at optimal order.  Additionally, a super-convergence property can be used to post-process a new approximate solution that converges at the rate  [2.x.41] . 


[1.x.59][1.x.60] 


The hybridizable discontinuous Galerkin method is only one way in which the problems of the discontinuous Galerkin method can be addressed. Another idea is what is called the "weak Galerkin" method. It is explored in  [2.x.42] . 


[1.x.61][1.x.62] 


The HDG formulation used for this example is taken from  [2.x.43]  [1.x.63][1.x.64][1.x.65] 

We consider the convection-diffusion equation over the domain  [2.x.44]  with Dirichlet boundary  [2.x.45]  and Neumann boundary  [2.x.46] : [1.x.66] 



Introduce the auxiliary variable  [2.x.47]  and rewrite the above equation as the first order system: [1.x.67] 



We multiply these equations by the weight functions  [2.x.48]  and integrate by parts over every element  [2.x.49]  to obtain: [1.x.68] 



The terms decorated with a hat denote the numerical traces (also commonly referred to as numerical fluxes).  They are approximations to the interior values on the boundary of the element.  To ensure conservation, these terms must be single-valued on any given element edge  [2.x.50]  even though, with discontinuous shape functions, there may of course be multiple values coming from the cells adjacent to an interface. We eliminate the numerical trace  [2.x.51]  by using traces of the form: [1.x.69] 



The variable  [2.x.52]  is introduced as an additional independent variable and is the one for which we finally set up a globally coupled linear system. As mentioned above, it is defined on the element faces and discontinuous from one face to another wherever faces meet (at vertices in 2d, and at edges and vertices in 3d). Values for  [2.x.53]  and  [2.x.54]  appearing in the numerical trace function are taken to be the cell's interior solution restricted to the boundary  [2.x.55] . 

The local stabilization parameter  [2.x.56]  has effects on stability and accuracy of HDG solutions; see the literature for a further discussion. A stabilization parameter of unity is reported to be the choice which gives best results. A stabilization parameter  [2.x.57]  that tends to infinity prohibits jumps in the solution over the element boundaries, making the HDG solution approach the approximation with continuous finite elements. In the program below, we choose the stabilization parameter as [1.x.70] 

where we set the diffusion  [2.x.58]  and the diffusion length scale to  [2.x.59] . 

The trace/skeleton variables in HDG methods are single-valued on element faces.  As such, they must strongly represent the Dirichlet data on  [2.x.60] .  This means that [1.x.71] 

where the equal sign actually means an  [2.x.61]  projection of the boundary function  [2.x.62]  onto the space of the face variables (e.g. linear functions on the faces). This constraint is then applied to the skeleton variable  [2.x.63]  using inhomogeneous constraints by the method  [2.x.64]  

Summing the elemental contributions across all elements in the triangulation, enforcing the normal component of the numerical flux, and integrating by parts on the equation weighted by  [2.x.65] , we arrive at the final form of the problem: Find  [2.x.66]  such that 

[1.x.72] 



The unknowns  [2.x.67]  are referred to as local variables; they are represented as standard DG variables.  The unknown  [2.x.68]  is the skeleton variable which has support on the codimension-1 surfaces (faces) of the mesh. 

We use the notation  [2.x.69]  to denote the sum of integrals over all cells and  [2.x.70]  to denote integration over all faces of all cells, i.e., interior faces are visited twice, once from each side and with the corresponding normal vectors. When combining the contribution from both elements sharing a face, the above equation yields terms familiar from the DG method, with jumps of the solution over the cell boundaries. 

In the equation above, the space  [2.x.71]  for the scalar variable  [2.x.72]  is defined as the space of functions that are tensor product polynomials of degree  [2.x.73]  on each cell and discontinuous over the element boundaries  [2.x.74] , i.e., the space described by  [2.x.75] . The space for the gradient or flux variable  [2.x.76]  is a vector element space where each component is a locally polynomial and discontinuous  [2.x.77] . In the code below, we collect these two local parts together in one FESystem where the first @p dim components denote the gradient part and the last scalar component corresponds to the scalar variable. For the skeleton component  [2.x.78] , we define a space that consists of discontinuous tensor product polynomials that live on the element faces, which in deal.II is implemented by the class FE_FaceQ. This space is otherwise similar to FE_DGQ, i.e., the solution function is not continuous between two neighboring faces, see also the results section below for an illustration. 

In the weak form given above, we can note the following coupling patterns:  [2.x.79]     [2.x.80]  The matrix  [2.x.81]  consists of local-local coupling terms.  These arise when the   local weighting functions  [2.x.82]  multiply the local solution terms    [2.x.83] . Because the elements are discontinuous,  [2.x.84]    is block diagonal.    [2.x.85]  The matrix  [2.x.86]  represents the local-face coupling.  These are the terms   with weighting functions  [2.x.87]  multiplying the skeleton variable    [2.x.88] .    [2.x.89]  The matrix  [2.x.90]  represents the face-local coupling, which involves the   weighting function  [2.x.91]  multiplying the local solutions  [2.x.92] .    [2.x.93]   The matrix  [2.x.94]  is the face-face coupling;   terms involve both  [2.x.95]  and  [2.x.96] .  [2.x.97]  

[1.x.73][1.x.74] 


One special feature of the HDG methods is that they typically allow for constructing an enriched solution that gains accuracy. This post-processing takes the HDG solution in an element-by-element fashion and combines it such that one can get  [2.x.98]  order of accuracy when using polynomials of degree  [2.x.99] . For this to happen, there are two necessary ingredients:  [2.x.100]     [2.x.101]  The computed solution gradient  [2.x.102]  converges at optimal rate,    i.e.,  [2.x.103] .    [2.x.104]  The cell-wise average of the scalar part of the solution,     [2.x.105] , super-converges at rate     [2.x.106] .  [2.x.107]  

We now introduce a new variable  [2.x.108] , which we find by minimizing the expression  [2.x.109]  over the cell  [2.x.110]  under the constraint  [2.x.111] . The constraint is necessary because the minimization functional does not determine the constant part of  [2.x.112] . This translates to the following system of equations: [1.x.75] 



Since we test by the whole set of basis functions in the space of tensor product polynomials of degree  [2.x.113]  in the second set of equations, this is an overdetermined system with one more equation than unknowns. We fix this in the code below by omitting one of these equations (since the rows in the Laplacian are linearly dependent when representing a constant function). As we will see below, this form of the post-processing gives the desired super-convergence result with rate  [2.x.114] .  It should be noted that there is some freedom in constructing  [2.x.115]  and this minimization approach to extract the information from the gradient is not the only one. In particular, the post-processed solution defined here does not satisfy the convection-diffusion equation in any sense. As an alternative, the paper by Nguyen, Peraire and Cockburn cited above suggests another somewhat more involved formula for convection-diffusion that can also post-process the flux variable into an  [2.x.116] -conforming variant and better represents the local convection-diffusion operator when the diffusion is small. We leave the implementation of a more sophisticated post-processing as a possible extension to the interested reader. 

Note that for vector-valued problems, the post-processing works similarly. One simply sets the constraint for the mean value of each vector component separately and uses the gradient as the main source of information. 

[1.x.76][1.x.77] 


For this tutorial program, we consider almost the same test case as in  [2.x.117] . The computational domain is  [2.x.118]  and the exact solution corresponds to the one in  [2.x.119] , except for a scaling. We use the following source centers  [2.x.120]  for the exponentials  [2.x.121]     [2.x.122]  1D:   [2.x.123] ,    [2.x.124]  2D:  [2.x.125] ,    [2.x.126]  3D:  [2.x.127] .  [2.x.128]  

With the exact solution given, we then choose the forcing on the right hand side and the Neumann boundary condition such that we obtain this solution (manufactured solution technique). In this example, we choose the diffusion equal to one and the convection as [1.x.78] Note that the convection is divergence-free,  [2.x.129] . 

[1.x.79][1.x.80] 


Besides implementing the above equations, the implementation below provides the following features:  [2.x.130]     [2.x.131]  WorkStream to parallelize local solvers. Workstream has been presented   in detail in  [2.x.132] .    [2.x.133]  Reconstruct the local DG solution from the trace.    [2.x.134]  Post-processing the solution for superconvergence.    [2.x.135]  DataOutFaces for direct output of the global skeleton solution.  [2.x.136]  [1.x.81] [1.x.82] 


[1.x.83]  [1.x.84] 




Most of the deal.II include files have already been covered in previous examples and are not commented on. 

[1.x.85] 



However, we do have a few new includes for the example. The first one defines finite element spaces on the faces of the triangulation, which we refer to as the 'skeleton'. These finite elements do not have any support on the element interior, and they represent polynomials that have a single value on each codimension-1 surface, but admit discontinuities on codimension-2 surfaces. 

[1.x.86] 



The second new file we include defines a new type of sparse matrix.  The regular  [2.x.137]  type stores indices to all non-zero entries.  The  [2.x.138]  takes advantage of the coupled nature of DG solutions.  It stores an index to a matrix sub-block of a specified size.  In the HDG context, this sub-block-size is actually the number of degrees of freedom per face defined by the skeleton solution field. This reduces the memory consumption of the matrix by up to one third and results in similar speedups when using the matrix in solvers. 

[1.x.87] 



The final new include for this example deals with data output.  Since we have a finite element field defined on the skeleton of the mesh, we would like to visualize what that solution actually is. DataOutFaces does exactly this; the interface is the almost the same as the familiar DataOut, but the output only has codimension-1 data for the simulation. 

[1.x.88] 



We start by putting all of our classes into their own namespace. 

[1.x.89] 




[1.x.90]  [1.x.91]    


The structure of the analytic solution is the same as in  [2.x.139] . There are two exceptions. Firstly, we also create a solution for the 3d case, and secondly, we scale the solution so its norm is of order unity for all values of the solution width. 

[1.x.92] 



This class implements a function where the scalar solution and its negative gradient are collected together. This function is used when computing the error of the HDG approximation and its implementation is to simply call value and gradient function of the Solution class. 

[1.x.93] 



Next comes the implementation of the convection velocity. As described in the introduction, we choose a velocity field that is  [2.x.140]  in 2D and  [2.x.141]  in 3D. This gives a divergence-free velocity field. 

[1.x.94] 



The last function we implement is the right hand side for the manufactured solution. It is very similar to  [2.x.142] , with the exception that we now have a convection term instead of the reaction term. Since the velocity field is incompressible, i.e.,  [2.x.143] , the advection term simply reads  [2.x.144] . 

[1.x.95] 




[1.x.96]  [1.x.97] 




The HDG solution procedure follows closely that of  [2.x.145] . The major difference is the use of three different sets of DoFHandler and FE objects, along with the ChunkSparseMatrix and the corresponding solutions vectors. We also use WorkStream to enable a multithreaded local solution process which exploits the embarrassingly parallel nature of the local solver. For WorkStream, we define the local operations on a cell and a copy function into the global matrix and vector. We do this both for the assembly (which is run twice, once when we generate the system matrix and once when we compute the element-interior solutions from the skeleton values) and for the postprocessing where we extract a solution that converges at higher order. 

[1.x.98] 



Data for the assembly and solution of the primal variables. 

[1.x.99] 



Post-processing the solution to obtain  [2.x.146]  is an element-by-element procedure; as such, we do not need to assemble any global data and do not declare any 'task data' for WorkStream to use. 

[1.x.100] 



The following three functions are used by WorkStream to do the actual work of the program. 

[1.x.101] 



The 'local' solutions are interior to each element.  These represent the primal solution field  [2.x.147]  as well as the auxiliary field  [2.x.148] . 

[1.x.102] 



The new finite element type and corresponding  [2.x.149]  are used for the global skeleton solution that couples the element-level local solutions. 

[1.x.103] 



As stated in the introduction, HDG solutions can be post-processed to attain superconvergence rates of  [2.x.150] .  The post-processed solution is a discontinuous finite element solution representing the primal variable on the interior of each cell.  We define a FE type of degree  [2.x.151]  to represent this post-processed solution, which we only use for output after constructing it. 

[1.x.104] 



The degrees of freedom corresponding to the skeleton strongly enforce Dirichlet boundary conditions, just as in a continuous Galerkin finite element method. We can enforce the boundary conditions in an analogous manner via an AffineConstraints object. In addition, hanging nodes are handled in the same way as for continuous finite elements: For the face elements which only define degrees of freedom on the face, this process sets the solution on the refined side to coincide with the representation on the coarse side.      


Note that for HDG, the elimination of hanging nodes is not the only possibility &mdash; in terms of the HDG theory, one could also use the unknowns from the refined side and express the local solution on the coarse side through the trace values on the refined side. However, such a setup is not as easily implemented in terms of deal.II loops and not further analyzed. 

[1.x.105] 



The usage of the ChunkSparseMatrix class is similar to the usual sparse matrices: You need a sparsity pattern of type ChunkSparsityPattern and the actual matrix object. When creating the sparsity pattern, we just have to additionally pass the size of local blocks. 

[1.x.106] 



Same as  [2.x.152] : 

[1.x.107] 




[1.x.108]  [1.x.109] 





[1.x.110]  [1.x.111] The constructor is similar to those in other examples, with the exception of handling multiple DoFHandler and FiniteElement objects. Note that we create a system of finite elements for the local DG part, including the gradient/flux part and the scalar part. 

[1.x.112] 




[1.x.113]  [1.x.114] The system for an HDG solution is setup in an analogous manner to most of the other tutorial programs.  We are careful to distribute dofs with all of our DoFHandler objects.  The  [2.x.153]  and  [2.x.154]  objects go with the global skeleton solution. 

[1.x.115] 



When creating the chunk sparsity pattern, we first create the usual dynamic sparsity pattern and then set the chunk size, which is equal to the number of dofs on a face, when copying this into the final sparsity pattern. 

[1.x.116] 




[1.x.117]  [1.x.118] Next comes the definition of the local data structures for the parallel assembly. The first structure  [2.x.155]  contains the local vector and matrix that are written into the global matrix, whereas the ScratchData contains all data that we need for the local assembly. There is one variable worth noting here, namely the boolean variable @p trace_reconstruct. As mentioned in the introduction, we solve the HDG system in two steps. First, we create a linear system for the skeleton system where we condense the local part into it via the Schur complement  [2.x.156] . Then, we solve for the local part using the skeleton solution. For these two steps, we need the same matrices on the elements twice, which we want to compute by two assembly steps. Since most of the code is similar, we do this with the same function but only switch between the two based on a flag that we set when starting the assembly. Since we need to pass this information on to the local worker routines, we store it once in the task data. 

[1.x.119] 




[1.x.120]  [1.x.121]  [2.x.157]  contains persistent data for each thread within WorkStream.  The FEValues, matrix, and vector objects should be familiar by now.  There are two objects that need to be discussed:  [2.x.158]  int> > fe_local_support_on_face` and  [2.x.159]  int> > fe_support_on_face`.  These are used to indicate whether or not the finite elements chosen have support (non-zero values) on a given face of the reference cell for the local part associated to  [2.x.160]  and the skeleton part  [2.x.161]  We extract this information in the constructor and store it once for all cells that we work on.  Had we not stored this information, we would be forced to assemble a large number of zero terms on each cell, which would significantly slow the program. 

[1.x.122] 




[1.x.123]  [1.x.124]  [2.x.162]  contains the data used by WorkStream when post-processing the local solution  [2.x.163] .  It is similar, but much simpler, than  [2.x.164]  

[1.x.125] 




[1.x.126]  [1.x.127] The  [2.x.165]  function is similar to the one on  [2.x.166] , where the quadrature formula and the update flags are set up, and then  [2.x.167]  is used to do the work in a multi-threaded manner.  The  [2.x.168]  input parameter is used to decide whether we are solving for the global skeleton solution (false) or the local solution (true).    


One thing worth noting for the multi-threaded execution of assembly is the fact that the local computations in `assemble_system_one_cell()` call into BLAS and LAPACK functions if those are available in deal.II. Thus, the underlying BLAS/LAPACK library must support calls from multiple threads at the same time. Most implementations do support this, but some libraries need to be built in a specific way to avoid problems. For example, OpenBLAS compiled without multithreading inside the BLAS/LAPACK calls needs to built with a flag called `USE_LOCKING` set to true. 

[1.x.128] 




[1.x.129]  [1.x.130] The real work of the HDG program is done by  [2.x.169]  Assembling the local matrices  [2.x.170]  is done here, along with the local contributions of the global matrix  [2.x.171] . 

[1.x.131] 



Construct iterator for dof_handler_local for FEValues reinit function. 

[1.x.132] 



We first compute the cell-interior contribution to  [2.x.172]  matrix (referred to as matrix  [2.x.173]  in the introduction) corresponding to local-local coupling, as well as the local right-hand-side vector.  We store the values at each quadrature point for the basis functions, the right-hand-side value, and the convection velocity, in order to have quick access to these fields. 

[1.x.133] 



Face terms are assembled on all faces of all elements. This is in contrast to more traditional DG methods, where each face is only visited once in the assembly procedure. 

[1.x.134] 



The already obtained  [2.x.174]  values are needed when solving for the local variables. 

[1.x.135] 



Here we compute the stabilization parameter discussed in the introduction: since the diffusion is one and the diffusion length scale is set to 1/5, it simply results in a contribution of 5 for the diffusion part and the magnitude of convection through the element boundary in a centered scheme for the convection part. 

[1.x.136] 



We store the non-zero flux and scalar values, making use of the support_on_face information we created in  [2.x.175]  

[1.x.137] 



When  [2.x.176]  we are preparing to assemble the system for the skeleton variable  [2.x.177] . If this is the case, we must assemble all local matrices associated with the problem: local-local, local-face, face-local, and face-face.  The face-face matrix is stored as  [2.x.178]  so that it can be assembled into the global system by @p copy_local_to_global. 

[1.x.138] 



Note the sign of the face_no-local matrix.  We negate the sign during assembly here so that we can use the  [2.x.179]  with addition when computing the Schur complement. 

[1.x.139] 



This last term adds the contribution of the term  [2.x.180]  to the local matrix. As opposed to the face matrices above, we need it in both assembly stages. 

[1.x.140] 



When  [2.x.181]  we are solving for the local solutions on an element by element basis.  The local right-hand-side is calculated by replacing the basis functions @p tr_phi in the  [2.x.182]  computation by the computed values @p trace_values.  Of course, the sign of the matrix is now minus since we have moved everything to the other side of the equation. 

[1.x.141] 



Once assembly of all of the local contributions is complete, we must either: (1) assemble the global system, or (2) compute the local solution values and save them. In either case, the first step is to invert the local-local matrix. 

[1.x.142] 



For (1), we compute the Schur complement and add it to the @p cell_matrix, matrix  [2.x.183]  in the introduction. 

[1.x.143] 



For (2), we are simply solving (ll_matrix).(solution_local) = (l_rhs). Hence, we multiply  [2.x.184]  by our already inverted local-local matrix and store the result using the  [2.x.185]  function. 

[1.x.144] 




[1.x.145]  [1.x.146] If we are in the first step of the solution, i.e.  [2.x.186]  then we assemble the local matrices into the global system. 

[1.x.147] 




[1.x.148]  [1.x.149] The skeleton solution is solved for by using a BiCGStab solver with identity preconditioner. 

[1.x.150] 



Once we have solved for the skeleton solution, we can solve for the local solutions in an element-by-element fashion.  We do this by re-using the same  [2.x.187]  function but switching  [2.x.188]  to true. 

[1.x.151] 




[1.x.152]  [1.x.153] 




The postprocess method serves two purposes. First, we want to construct a post-processed scalar variables in the element space of degree  [2.x.189]  that we hope will converge at order  [2.x.190] . This is again an element-by-element process and only involves the scalar solution as well as the gradient on the local cell. To do this, we introduce the already defined scratch data together with some update flags and run the work stream to do this in parallel.    


Secondly, we want to compute discretization errors just as we did in  [2.x.191] . The overall procedure is similar with calls to  [2.x.192]  The difference is in how we compute the errors for the scalar variable and the gradient variable. In  [2.x.193] , we did this by computing  [2.x.194]  or  [2.x.195]  contributions. Here, we have a DoFHandler with these two contributions computed and sorted by their vector component,  [2.x.196]  for the gradient and  [2.x.197]  for the scalar. To compute their value, we hence use a ComponentSelectFunction with either of them, together with the @p SolutionAndGradient class introduced above that contains the analytic parts of either of them. Eventually, we also compute the L2-error of the post-processed solution and add the results into the convergence table. 

[1.x.154] 




[1.x.155]  [1.x.156]    


This is the actual work done for the postprocessing. According to the discussion in the introduction, we need to set up a system that projects the gradient part of the DG solution onto the gradient of the post-processed variable. Moreover, we need to set the average of the new post-processed variable to equal the average of the scalar DG solution on the cell.    


More technically speaking, the projection of the gradient is a system that would potentially fills our  [2.x.198]  times  [2.x.199]  matrix but is singular (the sum of all rows would be zero because the constant function has zero gradient). Therefore, we take one row away and use it for imposing the average of the scalar value. We pick the first row for the scalar part, even though we could pick any row for  [2.x.200]  elements. However, had we used FE_DGP elements instead, the first row would correspond to the constant part already and deleting e.g. the last row would give us a singular system. This way, our program can also be used for those elements. 

[1.x.157] 



Having assembled all terms, we can again go on and solve the linear system. We invert the matrix and then multiply the inverse by the right hand side. An alternative (and more numerically stable) method would have been to only factorize the matrix and apply the factorization. 

[1.x.158] 




[1.x.159]  [1.x.160] We have 3 sets of results that we would like to output:  the local solution, the post-processed local solution, and the skeleton solution. The former 2 both 'live' on element volumes, whereas the latter lives on codimension-1 surfaces of the triangulation.  Our  [2.x.201]  function writes all local solutions to the same vtk file, even though they correspond to different DoFHandler objects.  The graphical output for the skeleton variable is done through use of the DataOutFaces class. 

[1.x.161] 



We first define the names and types of the local solution, and add the data to  [2.x.202]  

[1.x.162] 



The second data item we add is the post-processed solution. In this case, it is a single scalar variable belonging to a different DoFHandler. 

[1.x.163] 



The  [2.x.203]  class works analogously to the  [2.x.204]  that defines the solution on the skeleton of the triangulation.  We treat it as such here, and the code is similar to that above. 

[1.x.164] 




[1.x.165]  [1.x.166] 




We implement two different refinement cases for HDG, just as in  [2.x.205] : adaptive_refinement and global_refinement.  The global_refinement option recreates the entire triangulation every time. This is because we want to use a finer sequence of meshes than what we would get with one refinement step, namely 2, 3, 4, 6, 8, 12, 16, ... elements per direction. 




The adaptive_refinement mode uses the  [2.x.206]  to give a decent indication of the non-regular regions in the scalar local solutions. 

[1.x.167] 



Just as in  [2.x.207] , we set the boundary indicator of two of the faces to 1 where we want to specify Neumann boundary conditions instead of Dirichlet conditions. Since we re-create the triangulation every time for global refinement, the flags are set in every refinement step, not just at the beginning. 

[1.x.168] 




[1.x.169]  [1.x.170] The functionality here is basically the same as  [2.x.208] . We loop over 10 cycles, refining the grid on each one.  At the end, convergence tables are created. 

[1.x.171] 



There is one minor change for the convergence table compared to  [2.x.209] : Since we did not refine our mesh by a factor two in each cycle (but rather used the sequence 2, 3, 4, 6, 8, 12, ...), we need to tell the convergence rate evaluation about this. We do this by setting the number of cells as a reference column and additionally specifying the dimension of the problem, which gives the necessary information for the relation between number of cells and mesh size. 

[1.x.172] 



Now for the three calls to the main class in complete analogy to  [2.x.210] . 

[1.x.173] 

[1.x.174][1.x.175] 


[1.x.176][1.x.177] 


We first have a look at the output generated by the program when run in 2D. In the four images below, we show the solution for polynomial degree  [2.x.211]  and cycles 2, 3, 4, and 8 of the program. In the plots, we overlay the data generated from the internal data (DG part) with the skeleton part ( [2.x.212] ) into the same plot. We had to generate two different data sets because cells and faces represent different geometric entities, the combination of which (in the same file) is not supported in the VTK output of deal.II. 

The images show the distinctive features of HDG: The cell solution (colored surfaces) is discontinuous between the cells. The solution on the skeleton variable sits on the faces and ties together the local parts. The skeleton solution is not continuous on the vertices where the faces meet, even though its values are quite close along lines in the same coordinate direction. The skeleton solution can be interpreted as a rubber spring between the two sides that balances the jumps in the solution (or rather, the flux  [2.x.213] ). From the picture at the top left, it is clear that the bulk solution frequently over- and undershoots and that the skeleton variable in indeed a better approximation to the exact solution; this explains why we can get a better solution using a postprocessing step. 

As the mesh is refined, the jumps between the cells get small (we represent a smooth solution), and the skeleton solution approaches the interior parts. For cycle 8, there is no visible difference in the two variables. We also see how boundary conditions are implemented weakly and that the interior variables do not exactly satisfy boundary conditions. On the lower and left boundaries, we set Neumann boundary conditions, whereas we set Dirichlet conditions on the right and top boundaries. 

 [2.x.214]  

Next, we have a look at the post-processed solution, again at cycles 2, 3, 4, and 8. This is a discontinuous solution that is locally described by second order polynomials. While the solution does not look very good on the mesh of cycle two, it looks much better for cycles three and four. As shown by the convergence table below, we find that is also converges more quickly to the analytical solution. 

 [2.x.215]  

Finally, we look at the solution for  [2.x.216]  at cycle 2. Despite the coarse mesh with only 64 cells, the post-processed solution is similar in quality to the linear solution (not post-processed) at cycle 8 with 4,096 cells. This clearly shows the superiority of high order methods for smooth solutions. 

 [2.x.217]  

[1.x.178][1.x.179] 


When the program is run, it also outputs information about the respective steps and convergence tables with errors in the various components in the end. In 2D, the convergence tables look the following: 

[1.x.180] 




One can see the error reduction upon grid refinement, and for the cases where global refinement was performed, also the convergence rates. The quadratic convergence rates of Q1 elements in the  [2.x.218]  norm for both the scalar variable and the gradient variable is apparent, as is the cubic rate for the postprocessed scalar variable in the  [2.x.219]  norm. Note this distinctive feature of an HDG solution. In typical continuous finite elements, the gradient of the solution of order  [2.x.220]  converges at rate  [2.x.221]  only, as opposed to  [2.x.222]  for the actual solution. Even though superconvergence results for finite elements are also available (e.g. superconvergent patch recovery first introduced by Zienkiewicz and Zhu), these are typically limited to structured meshes and other special cases. For Q3 HDG variables, the scalar variable and gradient converge at fourth order and the postprocessed scalar variable at fifth order. 

The same convergence rates are observed in 3d. 

[1.x.181] 



[1.x.182][1.x.183] 


[1.x.184][1.x.185] 


The convergence tables verify the expected convergence rates stated in the introduction. Now, we want to show a quick comparison of the computational efficiency of the HDG method compared to a usual finite element (continuous Galkerin) method on the problem of this tutorial. Of course, stability aspects of the HDG method compared to continuous finite elements for transport-dominated problems are also important in practice, which is an aspect not seen on a problem with smooth analytic solution. In the picture below, we compare the  [2.x.223]  error as a function of the number of degrees of freedom (left) and of the computing time spent in the linear solver (right) for two space dimensions of continuous finite elements (CG) and the hybridized discontinuous Galerkin method presented in this tutorial. As opposed to the tutorial where we only use unpreconditioned BiCGStab, the times shown in the figures below use the Trilinos algebraic multigrid preconditioner in  [2.x.224]  For the HDG part, a wrapper around ChunkSparseMatrix for the trace variable has been used in order to utilize the block structure in the matrix on the finest level. 

 [2.x.225]  

The results in the graphs show that the HDG method is slower than continuous finite elements at  [2.x.226] , about equally fast for cubic elements and faster for sixth order elements. However, we have seen above that the HDG method actually produces solutions which are more accurate than what is represented in the original variables. Therefore, in the next two plots below we instead display the error of the post-processed solution for HDG (denoted by  [2.x.227]  for example). We now see a clear advantage of HDG for the same amount of work for both  [2.x.228]  and  [2.x.229] , and about the same quality for  [2.x.230] . 

 [2.x.231]  

Since the HDG method actually produces results converging as  [2.x.232] , we should compare it to a continuous Galerkin solution with the same asymptotic convergence behavior, i.e., FE_Q with degree  [2.x.233] . If we do this, we get the convergence curves below. We see that CG with second order polynomials is again clearly better than HDG with linears. However, the advantage of HDG for higher orders remains. 

 [2.x.234]  

The results are in line with properties of DG methods in general: Best performance is typically not achieved for linear elements, but rather at somewhat higher order, usually around  [2.x.235] . This is because of a volume-to-surface effect for discontinuous solutions with too much of the solution living on the surfaces and hence duplicating work when the elements are linear. Put in other words, DG methods are often most efficient when used at relatively high order, despite their focus on a discontinuous (and hence, seemingly low accurate) representation of solutions. 

[1.x.186][1.x.187] 


We now show the same figures in 3D: The first row shows the number of degrees of freedom and computing time versus the  [2.x.236]  error in the scalar variable  [2.x.237]  for CG and HDG at order  [2.x.238] , the second row shows the post-processed HDG solution instead of the original one, and the third row compares the post-processed HDG solution with CG at order  [2.x.239] . In 3D, the volume-to-surface effect makes the cost of HDG somewhat higher and the CG solution is clearly better than HDG for linears by any metric. For cubics, HDG and CG are of similar quality, whereas HDG is again more efficient for sixth order polynomials. One can alternatively also use the combination of FE_DGP and FE_FaceP instead of (FE_DGQ, FE_FaceQ), which do not use tensor product polynomials of degree  [2.x.240]  but Legendre polynomials of [1.x.188] degree  [2.x.241] . There are fewer degrees of freedom on the skeleton variable for FE_FaceP for a given mesh size, but the solution quality (error vs. number of DoFs) is very similar to the results for FE_FaceQ. 

 [2.x.242]  

One final note on the efficiency comparison: We tried to use general-purpose sparse matrix structures and similar solvers (optimal AMG preconditioners for both without particular tuning of the AMG parameters on any of them) to give a fair picture of the cost versus accuracy of two methods, on a toy example. It should be noted however that geometric multigrid (GMG) for continuous finite elements is about a factor four to five faster for  [2.x.243]  and  [2.x.244] . As of 2019, optimal-complexity iterative solvers for HDG are still under development in the research community. Also, there are other implementation aspects for CG available such as fast matrix-free approaches as shown in  [2.x.245]  that make higher order continuous elements more competitive. Again, it is not clear to the authors of the tutorial whether similar improvements could be made for HDG. We refer to [1.x.189] for a recent efficiency evaluation. 


[1.x.190][1.x.191] 


As already mentioned in the introduction, one possibility is to implement another post-processing technique as discussed in the literature. 

A second item that is not done optimally relates to the performance of this program, which is of course an issue in practical applications (weighing in also the better solution quality of (H)DG methods for transport-dominated problems). Let us look at the computing time of the tutorial program and the share of the individual components: 

 [2.x.246]  

As can be seen from the table, the solver and assembly calls dominate the runtime of the program. This also gives a clear indication of where improvements would make the most sense: 

 [2.x.247]     [2.x.248]  Better linear solvers: We use a BiCGStab iterative solver without   preconditioner, where the number of iteration increases with increasing   problem size (the number of iterations for Q1 elements and global   refinements starts at 35 for the small sizes but increase up to 701 for the   largest size). To do better, one could for example use an algebraic   multigrid preconditioner from Trilinos, or some more advanced variants as   the one discussed in [1.x.192]. For diffusion-dominated problems such as the problem at hand   with finer meshes, such a solver can be designed that uses the matrix-vector   products from the more efficient ChunkSparseMatrix on the finest level, as   long as we are not working in parallel with MPI. For MPI-parallelized   computations, a standard  [2.x.249]  can be used. 

   [2.x.250]  Speed up assembly by pre-assembling parts that do not change from one   cell to another (those that do neither contain variable coefficients nor   mapping-dependent terms).  [2.x.251]  [1.x.193] [1.x.194]  [2.x.252]  

 [2.x.253] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25] 

 [2.x.3]  

[1.x.26] 

 [2.x.4]  In order to run this program, deal.II must be configured to use the UMFPACK sparse direct solver. Refer to the [1.x.27] for instructions how to do this. 

[1.x.28] [1.x.29][1.x.30] 


This program shows how to use Runge-Kutta methods to solve a time-dependent problem. It solves a small variation of the heat equation discussed first in  [2.x.5]  but, since the purpose of this program is only to demonstrate using more advanced ways to interface with deal.II's time stepping algorithms, only solves a simple problem on a uniformly refined mesh. 


[1.x.31][1.x.32] 


In this example, we solve the one-group time-dependent diffusion approximation of the neutron transport equation (see  [2.x.6]  for the time-independent multigroup diffusion). This is a model for how neutrons move around highly scattering media, and consequently it is a variant of the time-dependent diffusion equation -- which is just a different name for the heat equation discussed in  [2.x.7] , plus some extra terms. We assume that the medium is not fissible and therefore, the neutron flux satisfies the following equation: [1.x.33] 

augmented by appropriate boundary conditions. Here,  [2.x.8]  is the velocity of neutrons (for simplicity we assume it is equal to 1 which can be achieved by simply scaling the time variable),  [2.x.9]  is the diffusion coefficient,  [2.x.10]  is the absorption cross section, and  [2.x.11]  is a source. Because we are only interested in the time dependence, we assume that  [2.x.12]  and  [2.x.13]  are constant. 

Since this program only intends to demonstrate how to use advanced time stepping algorithms, we will only look for the solutions of relatively simple problems. Specifically, we are looking for a solution on a square domain  [2.x.14]  of the form [1.x.34] 

By using quadratic finite elements, we can represent this function exactly at any particular time, and all the error will be due to the time discretization. We do this because it is then easy to observe the order of convergence of the various time stepping schemes we will consider, without having to separate spatial and temporal errors. 

We impose the following boundary conditions: homogeneous Dirichlet for  [2.x.15]  and  [2.x.16]  and homogeneous Neumann conditions for  [2.x.17]  and  [2.x.18] . We choose the source term so that the corresponding solution is in fact of the form stated above: [1.x.35] 

Because the solution is a sine in time, we know that the exact solution satisfies  [2.x.19] . Therefore, the error at time  [2.x.20]  is simply the norm of the numerical solution, i.e.,  [2.x.21] , and is particularly easily evaluated. In the code, we evaluate the  [2.x.22]  norm of the vector of nodal values of  [2.x.23]  instead of the  [2.x.24]  norm of the associated spatial function, since the former is simpler to compute; however, on uniform meshes, the two are just related by a constant and we can consequently observe the temporal convergence order with either. 


[1.x.36][1.x.37] 


The Runge-Kutta methods implemented in deal.II assume that the equation to be solved can be written as: [1.x.38] 

On the other hand, when using finite elements, discretized time derivatives always result in the presence of a mass matrix on the left hand side. This can easily be seen by considering that if the solution vector  [2.x.25]  in the equation above is in fact the vector of nodal coefficients  [2.x.26]  for a variable of the form [1.x.39] 

with spatial shape functions  [2.x.27] , then multiplying an equation of the form [1.x.40] 

by test functions, integrating over  [2.x.28] , substituting  [2.x.29]  and restricting the test functions to the  [2.x.30]  from above, then this spatially discretized equation has the form [1.x.41] 

where  [2.x.31]  is the mass matrix and  [2.x.32]  is the spatially discretized version of  [2.x.33]  (where  [2.x.34]  is typically the place where spatial derivatives appear, but this is not of much concern for the moment given that we only consider time derivatives). In other words, this form fits the general scheme above if we write [1.x.42] 



Runke-Kutta methods are time stepping schemes that approximate  [2.x.35]  through a particular one-step approach. They are typically written in the form [1.x.43] 

where for the form of the right hand side above [1.x.44] 

Here  [2.x.36] ,  [2.x.37] , and  [2.x.38]  are known coefficients that identify which particular Runge-Kutta scheme you want to use, and  [2.x.39]  is the time step used. Different time stepping methods of the Runge-Kutta class differ in the number of stages  [2.x.40]  and the values they use for the coefficients  [2.x.41] ,  [2.x.42] , and  [2.x.43]  but are otherwise easy to implement since one can look up tabulated values for these coefficients. (These tables are often called Butcher tableaus.) 

At the time of the writing of this tutorial, the methods implemented in deal.II can be divided in three categories:  [2.x.44]   [2.x.45]  Explicit Runge-Kutta; in order for a method to be explicit, it is necessary that in the formula above defining  [2.x.46] ,  [2.x.47]  does not appear on the right hand side. In other words, these methods have to satisfy  [2.x.48] .  [2.x.49]  Embedded (or adaptive) Runge-Kutta; we will discuss their properties below.  [2.x.50]  Implicit Runge-Kutta; this class of methods require the solution of a possibly nonlinear system the stages  [2.x.51]  above, i.e., they have  [2.x.52]  for at least one of the stages  [2.x.53] .  [2.x.54]  Many well known time stepping schemes that one does not typically associate with the names Runge or Kutta can in fact be written in a way so that they, too, can be expressed in these categories. They oftentimes represent the lowest-order members of these families. 


[1.x.45][1.x.46] 


These methods, only require a function to evaluate  [2.x.55]  but not (as implicit methods) to solve an equation that involves  [2.x.56]  for  [2.x.57] . As all explicit time stepping methods, they become unstable when the time step chosen is too large. 

Well known methods in this class include forward Euler, third order Runge-Kutta, and fourth order Runge-Kutta (often abbreviated as RK4). 


[1.x.47][1.x.48] 


These methods use both a lower and a higher order method to estimate the error and decide if the time step needs to be shortened or can be increased. The term "embedded" refers to the fact that the lower-order method does not require additional evaluates of the function  [2.x.58]  but reuses data that has to be computed for the high order method anyway. It is, in other words, essentially free, and we get the error estimate as a side product of using the higher order method. 

This class of methods include Heun-Euler, Bogacki-Shampine, Dormand-Prince (ode45 in Matlab and often abbreviated as RK45 to indicate that the lower and higher order methods used here are 4th and 5th order Runge-Kutta methods, respectively), Fehlberg, and Cash-Karp. 

At the time of the writing, only embedded explicit methods have been implemented. 


[1.x.49][1.x.50] 


Implicit methods require the solution of (possibly nonlinear) systems of the form  [2.x.59]  for  [2.x.60]  in each (sub-)timestep. Internally, this is done using a Newton-type method and, consequently, they require that the user provide functions that can evaluate  [2.x.61]  and  [2.x.62]  or equivalently  [2.x.63] . 

The particular form of this operator results from the fact that each Newton step requires the solution of an equation of the form 

[1.x.51] 

for some (given)  [2.x.64] . Implicit methods are always stable, regardless of the time step size, but too large time steps of course affect the [1.x.52] of the solution, even if the numerical solution remains stable and bounded. 

Methods in this class include backward Euler, implicit midpoint, Crank-Nicolson, and the two stage SDIRK method (short for "singly diagonally implicit Runge-Kutta", a term coined to indicate that the diagonal elements  [2.x.65]  defining the time stepping method are all equal; this property allows for the Newton matrix  [2.x.66]  to be re-used between stages because  [2.x.67]  is the same every time). 


[1.x.53][1.x.54] 


By expanding the solution of our model problem as always using shape functions  [2.x.68]  and writing [1.x.55] 

we immediately get the spatially discretized version of the diffusion equation as [1.x.56] 

where [1.x.57] 

See also  [2.x.69]  and  [2.x.70]  to understand how we arrive here. Boundary terms are not necessary due to the chosen boundary conditions for the current problem. To use the Runge-Kutta methods, we recast this as follows: [1.x.58] 

In the code, we will need to be able to evaluate this function  [2.x.71]  along with its derivative, [1.x.59] 




[1.x.60][1.x.61] 


To simplify the problem, the domain is two dimensional and the mesh is uniformly refined (there is no need to adapt the mesh since we use quadratic finite elements and the exact solution is quadratic). Going from a two dimensional domain to a three dimensional domain is not very challenging. However if you intend to solve more complex problems where the mesh must be adapted (as is done, for example, in  [2.x.72] ), then it is important to remember the following issues: 

 [2.x.73]   [2.x.74]  You will need to project the solution to the new mesh when the mesh is changed. Of course,      the mesh      used should be the same from the beginning to the end of each time step,      a question that arises because Runge-Kutta methods use multiple      evaluations of the equations within each time step.  [2.x.75]  You will need to update the mass matrix and its inverse every time the      mesh is changed.  [2.x.76]  The techniques for these steps are readily available by looking at  [2.x.77] . [1.x.62] [1.x.63] 


[1.x.64]  [1.x.65] 




The first task as usual is to include the functionality of these well-known deal.II library files and some C++ header files. 

[1.x.66] 



This is the only include file that is new: It includes all the Runge-Kutta methods. 

[1.x.67] 



The next step is like in all previous tutorial programs: We put everything into a namespace of its own and then import the deal.II classes and functions into it. 

[1.x.68] 




[1.x.69]  [1.x.70] 




The next piece is the declaration of the main class. Most of the functions in this class are not new and have been explained in previous tutorials. The only interesting functions are  [2.x.78]  and  [2.x.79]  evaluates the diffusion equation,  [2.x.80] , at a given time and a given  [2.x.81] .  [2.x.82]  evaluates  [2.x.83]  or equivalently  [2.x.84]  at a given time, for a given  [2.x.85]  and  [2.x.86] . This function is needed when an implicit method is used. 

[1.x.71] 



The next three functions are the drivers for the explicit methods, the implicit methods, and the embedded explicit methods respectively. The driver function for embedded explicit methods returns the number of steps executed given that it only takes the number of time steps passed as an argument as a hint, but internally computed the optimal time step itself. 

[1.x.72] 



We choose quadratic finite elements and we initialize the parameters. 

[1.x.73] 




[1.x.74]  [1.x.75] Now, we create the constraint matrix and the sparsity pattern. Then, we initialize the matrices and the solution vector. 

[1.x.76] 




[1.x.77]  [1.x.78] In this function, we compute  [2.x.87]  and the mass matrix  [2.x.88] . The mass matrix is then inverted using a direct solver; the  [2.x.89]  variable will then store the inverse of the mass matrix so that  [2.x.90]  can be applied to a vector using the  [2.x.91]  function of that object. (Internally, UMFPACK does not really store the inverse of the matrix, but its LU factors; applying the inverse matrix is then equivalent to doing one forward and one backward solves with these two factors, which has the same complexity as applying an explicit inverse of the matrix). 

[1.x.79] 




[1.x.80]  [1.x.81]    


In this function, the source term of the equation for a given time and a given point is computed. 

[1.x.82] 




[1.x.83]  [1.x.84]    


Next, we evaluate the weak form of the diffusion equation at a given time  [2.x.92]  and for a given vector  [2.x.93] . In other words, as outlined in the introduction, we evaluate  [2.x.94] . For this, we have to apply the matrix  [2.x.95]  (previously computed and stored in the variable  [2.x.96] ) to  [2.x.97]  and then add the source term which we integrate as we usually do. (Integrating up the solution could be done using  [2.x.98]  if you wanted to save a few lines of code, or wanted to take advantage of doing the integration in parallel.) The result is then multiplied by  [2.x.99] . 

[1.x.85] 




[1.x.86]  [1.x.87]    


We compute  [2.x.100] . This is done in several steps: 

- compute  [2.x.101]  

- invert the matrix to get  [2.x.102]  

- compute  [2.x.103]  

- compute  [2.x.104]  

- return z. 

[1.x.88] 




[1.x.89]  [1.x.90]    


The following function then outputs the solution in vtu files indexed by the number of the time step and the name of the time stepping method. Of course, the (exact) result should really be the same for all time stepping method, but the output here at least allows us to compare them. 

[1.x.91] 




[1.x.92]  [1.x.93]    


This function is the driver for all the explicit methods. At the top it initializes the time stepping and the solution (by setting it to zero and then ensuring that boundary value and hanging node constraints are respected; of course, with the mesh we use here, hanging node constraints are not in fact an issue). It then calls  [2.x.105]  which performs one time step. Time is stored and incremented through a DiscreteTime object.    


For explicit methods,  [2.x.106]  needs to evaluate  [2.x.107] , i.e, it needs  [2.x.108] . Because  [2.x.109]  is a member function, it needs to be bound to  [2.x.110] . After each evolution step, we again apply the correct boundary values and hanging node constraints.    


Finally, the solution is output every 10 time steps. 

[1.x.94] 




[1.x.95]  [1.x.96] This function is equivalent to  [2.x.111]  but for implicit methods. When using implicit methods, we need to evaluate  [2.x.112]  and  [2.x.113]  for which we use the two member functions previously introduced. 

[1.x.97] 




[1.x.98]  [1.x.99] This function is the driver for the embedded explicit methods. It requires more parameters: 

- coarsen_param: factor multiplying the current time step when the error is below the threshold. 

- refine_param: factor multiplying the current time step when the error is above the threshold. 

- min_delta: smallest time step acceptable. 

- max_delta: largest time step acceptable. 

- refine_tol: threshold above which the time step is refined. 

- coarsen_tol: threshold below which the time step is coarsen.    


Embedded methods use a guessed time step. If the error using this time step is too large, the time step will be reduced. If the error is below the threshold, a larger time step will be tried for the next time step.  [2.x.114]  is the guessed time step produced by the embedded method. In summary, time step size is potentially modified in three ways: 

- Reducing or increasing time step size within  [2.x.115]  

- Using the calculated  [2.x.116] . 

- Automatically adjusting the step size of the last time step to ensure simulation ends precisely at  [2.x.117] . This adjustment is handled inside the DiscreteTime instance. 

[1.x.100] 




[1.x.101]  [1.x.102]    


The following is the main function of the program. At the top, we create the grid (a [0,5]x[0,5] square) and refine it four times to get a mesh that has 16 by 16 cells, for a total of 256.  We then set the boundary indicator to 1 for those parts of the boundary where  [2.x.118]  and  [2.x.119] . 

[1.x.103] 



Next, we set up the linear systems and fill them with content so that they can be used throughout the time stepping process: 

[1.x.104] 



Finally, we solve the diffusion problem using several of the Runge-Kutta methods implemented in namespace TimeStepping, each time outputting the error at the end time. (As explained in the introduction, since the exact solution is zero at the final time, the error equals the numerical solution and can be computed by just taking the  [2.x.120]  norm of the solution vector.) 

[1.x.105] 




[1.x.106]  [1.x.107] 




The following  [2.x.121]  function is similar to previous examples and need not be commented on. 

[1.x.108] 

[1.x.109][1.x.110] 


The point of this program is less to show particular results, but instead to show how it is done. This we have already demonstrated simply by discussing the code above. Consequently, the output the program yields is relatively sparse and consists only of the console output and the solutions given in VTU format for visualization. 

The console output contains both errors and, for some of the methods, the number of steps they performed: 

[1.x.111] 



As expected the higher order methods give (much) more accurate solutions. We also see that the (rather inaccurate) Heun-Euler method increased the number of time steps in order to satisfy the tolerance. On the other hand, the other embedded methods used a lot less time steps than what was prescribed. [1.x.112] [1.x.113]  [2.x.122]  

 [2.x.123] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13] 

 [2.x.3]  

[1.x.14] 

 [2.x.4]  This program elaborates on concepts of geometry and the classes that implement it. These classes are grouped into the documentation module on  [2.x.5]  "Manifold description for triangulations". See there for additional information. 

 [2.x.6]  This tutorial is also available as a Jupyter Python notebook that   uses the deal.II python interface. The notebook is available in the   same directory as the original C++ program. Rendered notebook can also   be viewed on the [1.x.15]. 


[1.x.16] [1.x.17][1.x.18] 


Partial differential equations for realistic problems are often posed on domains with complicated geometries. To provide just a few examples, consider these cases: 

- Among the two arguably most important industrial applications for the finite   element method, aerodynamics and more generally fluid dynamics is   one. Computer simulations today are used in the design of every airplane,   car, train and ship. The domain in which the partial differential equation   is posed is, in these cases, the air surrounding the plane with its wings,   flaps and engines; the air surrounding the car with its wheel, wheel wells,   mirrors and, in the case of race cars, all sorts of aerodynamic equipment;   the air surrounding the train with its wheels and gaps between cars. In the   case of ships, the domain is the water surrounding the ship with its rudders   and propellers. 

- The other of the two big applications of the finite element method is   structural engineering in which the domains are bridges, airplane nacelles   and wings, and other solid bodies of often complicated shapes. 

- Finite element modeling is also often used to describe the generation and   propagation of earthquake waves. In these cases, one needs to accurately   represent the geometry of faults in the Earth crust. Since faults intersect,   dip at angles, and are often not completely straight, domains are frequently   very complex. One could cite many more examples of complicated geometries in which one wants to pose and solve a partial differential equation. What this shows is that the "real" world is much more complicated than what we have shown in almost all of the tutorial programs preceding this one. 

This program is therefore devoted to showing how one deals with complex geometries using a concrete application. In particular, what it shows is how we make a mesh fit the domain we want to solve on. On the other hand, what the program does not show is how to create a coarse for a domain. The process to arrive at a coarse mesh is called "mesh generation" and there are a number of high-quality programs that do this much better than we could ever implement. However, deal.II does have the ability to read in meshes in many formats generated by mesh generators and then make them fit a given shape, either by deforming a mesh or refining it a number of times until it fits. The deal.II Frequently Asked Questions page referenced from http://www.dealii.org/ provides resources to mesh generators. 


[1.x.19][1.x.20] 


Let us assume that you have a complex domain and that you already have a coarse mesh that somehow represents the general features of the domain. Then there are two situations in which it is necessary to describe to a deal.II program the details of your geometry: 

- Mesh refinement: Whenever a cell is refined, it is necessary to introduce   new vertices in the Triangulation. In the simplest case, one assumes that   the objects that make up the Triangulation are straight line segments, a   bi-linear surface or a tri-linear volume. The next vertex is then simply put   into the middle of the old ones. However, for curved boundaries or if we   want to solve a PDE on a curved, lower-dimensional manifold embedded in a   higher-dimensional space, this is insufficient since it will not respect the   actual geometry. We will therefore have to tell Triangulation where to put   new points. 

- Integration: When using higher order finite element methods, it is often   necessary to compute integrals using curved approximations of the boundary,   i.e., describe each edge or face of cells as curves, instead of straight   line segments or bilinear patches. The same is, of course, true when   integrating boundary terms (e.g., inhomogeneous Neumann boundary   conditions). For the purpose of integration, the various Mapping classes   then provide the transformation from the reference cell to the actual cell. 

In both cases, we need a way to provide information about the geometry of the domain at the level of an individual cell, its faces and edges. This is where the Manifold class comes into play. Manifold is an abstract base class that only defines an interface by which the Triangulation and Mapping classes can query geometric information about the domain. Conceptually, Manifold sees the world in a way not dissimilar to how the mathematical subdiscipline geometry sees it: a domain is essentially just a collection of points that is somehow equipped with the notion of a distance between points so that we can obtain a point "in the middle" of some other points. 

deal.II provides a number of classes that implement the interface provided by Manifold for a variety of common geometries. On the other hand, in this program we will consider only a very common and much simpler case, namely the situation where (a part of) the domain we want to solve on can be described by transforming a much simpler domain (we will call this the "reference domain"). In the language of mathematics, this means that the (part of the) domain is a [1.x.21]. Charts are described by a smooth function that maps from the simpler domain to the chart (the "push-forward" function) and its inverse (the "pull-back" function). If the domain as a whole is not a chart (e.g., the surface of a sphere), then it can often be described as a collection of charts (e.g., the northern hemisphere and the southern hemisphere are each charts) and the domain can then be describe by an [1.x.22]. 

If a domain can be decomposed into an atlas, all we need to do is provide the pull-back and push-forward functions for each of the charts. In deal.II, this means providing a class derived from ChartManifold, and this is precisely what we will do in this program. 


[1.x.23][1.x.24] 


To illustrate how one describes geometries using charts in deal.II, we will consider a case that originates in an application of the [1.x.25], using a data set provided by D. Sarah Stamps. In the concrete application, we were interested in describing flow in the Earth mantle under the [1.x.26], a zone where two continental plates drift apart. Not to beat around the bush, the geometry we want to describe looks like this: 

 [2.x.7]  

In particular, though you cannot see this here, the top surface is not just colored by the elevation but is, in fact, deformed to follow the correct topography. While the actual application is not relevant here, the geometry is. The domain we are interested in is a part of the Earth that ranges from the surface to a depth of 500km, from 26 to 35 degrees East of the Greenwich meridian, and from 5 degrees North of the equator to 10 degrees South. 

This description of the geometry suggests to start with a box  [2.x.8]  (measured in degrees, degrees, and meters) and to provide a map  [2.x.9]  so that  [2.x.10]  where  [2.x.11]  is the domain we seek.  [2.x.12]  is then a chart,  [2.x.13]  the pull-back operator, and  [2.x.14]  the push-forward operator. If we need a point  [2.x.15]  that is the "average" of other points  [2.x.16] , the ChartManifold class then first applies the pull-back to obtain  [2.x.17] , averages these to a point  [2.x.18]  and then computes  [2.x.19] . 

Our goal here is therefore to implement a class that describes  [2.x.20]  and  [2.x.21] . If Earth was a sphere, then this would not be difficult: if we denote by  [2.x.22]  the points of  [2.x.23]  (i.e., longitude counted eastward, latitude counted northward, and elevation relative to zero depth), then [1.x.27] provides coordinates in a Cartesian coordinate system, where  [2.x.24]  is the radius of the sphere. However, the Earth is not a sphere: 

 [2.x.25]   [2.x.26]  It is flattened at the poles and larger at the equator: the semi-major axis   is approximately 22km longer than the semi-minor axis. We will account for   this using the [1.x.28]   reference standard for the Earth shape. The formula used in WGS 84 to obtain   a position in Cartesian coordinates from longitude, latitude, and elevation   is [1.x.29]   where  [2.x.27] , and radius and   ellipticity are given by  [2.x.28] . In this formula,   we assume that the arguments to sines and cosines are evaluated in degree, not   radians (though we will have to change this assumption in the code). 

 [2.x.29]  It has topography in the form of mountains and valleys. We will account for   this using real topography data (see below for a description of where   this data comes from). Using this data set, we can look up elevations on a   latitude-longitude mesh laid over the surface of the Earth. Starting with   the box  [2.x.30] , we will therefore   first stretch it in vertical direction before handing it off to the WGS 84   function: if  [2.x.31]  is the height at longitude  [2.x.32]    and latitude  [2.x.33] , then we define [1.x.30]   Using this function, the top surface of the box  [2.x.34]  is displaced to the   correct topography, the bottom surface remains where it was, and points in   between are linearly interpolated.  [2.x.35]  

Using these two functions, we can then define the entire push-forward function  [2.x.36]  as [1.x.31] In addition, we will have to define the inverse of this function, the pull-back operation, which we can write as [1.x.32] We can obtain one of the components of this function by inverting the formula above: [1.x.33] Computing  [2.x.37]  is also possible though a lot more awkward. We won't show the formula here but instead only provide the implementation in the program. 


[1.x.34][1.x.35] 


There are a number of issues we need to address in the program. At the largest scale, we need to write a class that implements the interface of ChartManifold. This involves a function  [2.x.38]  that takes a point in the reference domain  [2.x.39]  and transform it into real space using the function  [2.x.40]  outlined above, and its inverse function  [2.x.41]  implementing  [2.x.42] . We will do so in the  [2.x.43]  class below that looks, in essence, like this: 

[1.x.36] 



The transformations above have two parts: the WGS 84 transformations and the topography transformation. Consequently, the  [2.x.44]  class will have additional (non-virtual) member functions  [2.x.45]  and  [2.x.46]  that implement these two pieces, and corresponding pull back functions. 

The WGS 84 transformation functions are not particularly interesting (even though the formulas they implement are impressive). The more interesting part is the topography transformation. Recall that for this, we needed to evaluate the elevation function  [2.x.47] . There is of course no formula for this: Earth is what it is, the best one can do is look up the altitude from some table. This is, in fact what we will do. 

The data we use was originally created by the  [1.x.37], was downloaded from the US Geologic Survey (USGS) and processed by D. Sarah Stamps who also wrote the initial version of the WGS 84 transformation functions. The topography data so processed is stored in a file  [2.x.48]  that, when unpacked looks like this: 

[1.x.38] 

The data is formatted as  [2.x.49]  where the first two columns are provided in degrees North of the equator and degrees East of the Greenwich meridian. The final column is given in meters above the WGS 84 zero elevation. 

In the transformation functions, we need to evaluate  [2.x.50]  for a given longitude  [2.x.51]  and latitude  [2.x.52] . In general, this data point will not be available and we will have to interpolate between adjacent data points. Writing such an interpolation routine is not particularly difficult, but it is a bit tedious and error prone. Fortunately, we can somehow shoehorn this data set into an existing class:  [2.x.53]  . Unfortunately, the class does not fit the bill quite exactly and so we need to work around it a bit. The problem comes from the way we initialize this class: in its simplest form, it takes a stream of values that it assumes form an equispaced mesh in the  [2.x.54]  plane (or, here, the  [2.x.55]  plane). Which is what they do here, sort of: they are ordered latitude first, longitude second; and more awkwardly, the first column starts at the largest values and counts down, rather than the usual other way around. 

Now, while tutorial programs are meant to illustrate how to code with deal.II, they do not necessarily have to satisfy the same quality standards as one would have to do with production codes. In a production code, we would write a function that reads the data and (i) automatically determines the extents of the first and second column, (ii) automatically determines the number of data points in each direction, (iii) does the interpolation regardless of the order in which data is arranged, if necessary by switching the order between reading and presenting it to the  [2.x.56]  class. 

On the other hand, tutorial programs are best if they are short and demonstrate key points rather than dwell on unimportant aspects and, thereby, obscure what we really want to show. Consequently, we will allow ourselves a bit of leeway: 

- since this program is intended solely for a particular geometry around the area   of the East-African rift and since this is precisely the area described by the data   file, we will hardcode in the program that there are    [2.x.57]  pieces of data; 

- we will hardcode the boundaries of the data    [2.x.58] ; 

- we will lie to the  [2.x.59]  class: the class will   only see the data in the last column of this data file, and we will pretend that   the data is arranged in a way that there are 1139 data points in the first   coordinate direction that are arranged in [1.x.39] order but in an   interval  [2.x.60]  (not the negated bounds). Then,   when we need to look something up for a latitude  [2.x.61] , we can ask the   interpolating table class for a value at  [2.x.62] . With this little   trick, we can avoid having to switch around the order of data as read from   file. 

All of this then calls for a class that essentially looks like this: 

[1.x.40] 



Note how the  [2.x.63]  function negates the latitude. It also switches from the format  [2.x.64]  that we use everywhere else to the latitude-longitude format used in the table. Finally, it takes its arguments in radians as that is what we do everywhere else in the program, but then converts them to the degree-based system used for table lookup. As you will see in the implementation below, the function has a few more (static) member functions that we will call in the initialization of the  [2.x.65]  member variable: the class type of this variable has a constructor that allows us to set everything right at construction time, rather than having to fill data later on, but this constructor takes a number of objects that can't be constructed in-place (at least not in C++98). Consequently, the construction of each of the objects we want to pass in the initialization happens in a number of static member functions. 

Having discussed the general outline of how we want to implement things, let us go to the program and show how it is done in practice. [1.x.41] [1.x.42] 

Let us start with the include files we need here. Obviously, we need the ones that describe the triangulation ( [2.x.66] ), and that allow us to create and output triangulations ( [2.x.67]  and  [2.x.68] ). Furthermore, we need the header file that declares the Manifold and ChartManifold classes that we will need to describe the geometry ( [2.x.69] ). We will then also need the  [2.x.70]  function from the last of the following header files; the purpose for this function will become discussed at the point where we use it. 

[1.x.43] 



The remainder of the include files relate to reading the topography data. As explained in the introduction, we will read it from a file and then use the  [2.x.71]  class that is declared in the first of the following header files. Because the data is large, the file we read from is stored as gzip compressed data and we make use of some BOOST-provided functionality to read directly from gzipped data. 

[1.x.44] 



The final part of the top matter is to open a namespace into which to put everything, and then to import the dealii namespace into it. 

[1.x.45] 




[1.x.46]  [1.x.47]    


The first significant part of this program is the class that describes the topography  [2.x.72]  as a function of longitude and latitude. As discussed in the introduction, we will make our life a bit easier here by not writing the class in the most general way possible but by only writing it for the particular purpose we are interested in here: interpolating data obtained from one very specific data file that contains information about a particular area of the world for which we know the extents.    


The general layout of the class has been discussed already above. Following is its declaration, including three static member functions that we will need in initializing the  [2.x.73]  member variable. 

[1.x.48] 



Let us move to the implementation of the class. The interesting parts of the class are the constructor and the  [2.x.74]  function. The former initializes the  [2.x.75]  member variable and we will use the constructor that requires us to pass in the end points of the 2-dimensional data set we want to interpolate (which are here given by the intervals  [2.x.76] , using the trick of switching end points discussed in the introduction, and  [2.x.77] , both given in degrees), the number of intervals into which the data is split (379 in latitude direction and 219 in longitude direction, for a total of  [2.x.78]  data points), and a Table object that contains the data. The data then of course has size  [2.x.79]  and we initialize it by providing an iterator to the first of the 83,600 elements of a  [2.x.80]  object returned by the  [2.x.81]  function below. Note that all of the member functions we call here are static because (i) they do not access any member variables of the class, and (ii) because they are called at a time when the object is not initialized fully anyway. 

[1.x.49] 



The only other function of greater interest is the  [2.x.82]  function. It returns a temporary vector that contains all 83,600 data points describing the altitude and is read from the file  [2.x.83] . Because the file is compressed by gzip, we cannot just read it through an object of type  [2.x.84]  but there are convenient methods in the BOOST library (see http://www.boost.org) that allows us to read from compressed files without first having to uncompress it on disk. The result is, basically, just another input stream that, for all practical purposes, looks just like the ones we always use.    


When reading the data, we read the three columns but throw ignore the first two. The datum in the last column is appended to an array that we the return and that will be copied into the table from which  [2.x.85]  is initialized. Since the BOOST.iostreams library does not provide a very useful exception when the input file does not exist, is not readable, or does not contain the correct number of data lines, we catch all exceptions it may produce and create our own one. To this end, in the  [2.x.86]  clause, we let the program run into an  [2.x.87]  statement. Since the condition is always false, this always triggers an exception. In other words, this is equivalent to writing  [2.x.88]  but it also fills certain fields in the exception object that will later be printed on the screen identifying the function, file and line where the exception happened. 

[1.x.50] 



create a stream where we read from gzipped data 

[1.x.51] 




[1.x.52]  [1.x.53]    


The following class is then the main one of this program. Its structure has been described in much detail in the introduction and does not need much introduction any more. 

[1.x.54] 



The implementation, as well, is pretty straightforward if you have read the introduction. In particular, both of the pull back and push forward functions are just concatenations of the respective functions of the WGS 84 and topography mappings: 

[1.x.55] 



The next function is required by the interface of the Manifold base class, and allows cloning the AfricaGeometry class. Notice that, while the function returns a  [2.x.89]  we internally create a `unique_ptr<AfricaGeometry>`. In other words, the library requires a pointer-to-base-class, which we provide by creating a pointer-to-derived-class. 

[1.x.56] 



The following two functions then define the forward and inverse transformations that correspond to the WGS 84 reference shape of Earth. The forward transform follows the formula shown in the introduction. The inverse transform is significantly more complicated and is, at the very least, not intuitive. It also suffers from the fact that it returns an angle that at the end of the function we need to clip back into the interval  [2.x.90]  if it should have escaped from there. 

[1.x.57] 



In contrast, the topography transformations follow exactly the description in the introduction. There is not consequently not much to add: 

[1.x.58] 




[1.x.59]  [1.x.60]    


Having so described the properties of the geometry, not it is time to deal with the mesh used to discretize it. To this end, we create objects for the geometry and triangulation, and then proceed to create a  [2.x.91]  rectangular mesh that corresponds to the reference domain  [2.x.92] . We choose this number of subdivisions because it leads to cells that are roughly like cubes instead of stretched in one direction or another.    


Of course, we are not actually interested in meshing the reference domain. We are interested in meshing the real domain. Consequently, we will use the  [2.x.93]  function that simply moves every point of a triangulation according to a given transformation. The transformation function it wants is a function that takes as its single argument a point in the reference domain and returns the corresponding location in the domain that we want to map to. This is, of course, exactly the push forward function of the geometry we use. We wrap it by a lambda function to obtain the kind of function object required for the transformation. 

[1.x.61] 



The next step is to explain to the triangulation to use our geometry object whenever a new point is needed upon refining the mesh. We do this by telling the triangulation to use our geometry for everything that has manifold indicator zero, and then proceed to mark all cells and their bounding faces and edges with manifold indicator zero. This ensures that the triangulation consults our geometry object every time a new vertex is needed. Since manifold indicators are inherited from mother to children, this also happens after several recursive refinement steps. 

[1.x.62] 



The last step is to refine the mesh beyond its initial  [2.x.94]  coarse mesh. We could just refine globally a number of times, but since for the purpose of this tutorial program we're really only interested in what is happening close to the surface, we just refine 6 times all of the cells that have a face at a boundary with indicator 5. Looking this up in the documentation of the  [2.x.95]  function we have used above reveals that boundary indicator 5 corresponds to the top surface of the domain (and this is what the last  [2.x.96]  argument in the call to  [2.x.97]  above meant: to "color" the boundaries by assigning each boundary a unique boundary indicator). 

[1.x.63] 



Having done this all, we can now output the mesh into a file of its own: 

[1.x.64] 




[1.x.65]  [1.x.66] 




Finally, the main function, which follows the same scheme used in all tutorial programs starting with  [2.x.98] . There isn't much to do here, only to call the single  [2.x.99]  function. 

[1.x.67] 

[1.x.68][1.x.69] 


Running the program produces a mesh file  [2.x.100]  that we can visualize with any of the usual visualization programs that can read the VTU file format. If one just looks at the mesh itself, it is actually very difficult to see anything that doesn't just look like a perfectly round piece of a sphere (though if one modified the program so that it does produce a sphere and looked at them at the same time, the difference between the overall sphere and WGS 84 shape is quite apparent). Apparently, Earth is actually quite a flat place. Of course we already know this from satellite pictures. However, we can tease out something more by coloring cells by their volume. This both produces slight variations in hue along the top surface and something for the visualization programs to apply their shading algorithms to (because the top surfaces of the cells are now no longer just tangential to a sphere but tilted): 

 [2.x.101]  

Yet, at least as far as visualizations are concerned, this is still not too impressive. Rather, let us visualize things in a way so that we show the actual elevation along the top surface. In other words, we want a picture like this, with an incredible amount of detail: 

 [2.x.102]  

A zoom-in of this picture shows the vertical displacement quite clearly (here, looking from the West-Northwest over the rift valley, the triple peaks of [1.x.70], [1.x.71], and [1.x.72] in the [1.x.73], [1.x.74] and toward the great flatness of [1.x.75]): 

 [2.x.103]  


These image were produced with three small modifications:  [2.x.104]     [2.x.105]  An additional seventh mesh refinement towards the top surface for the   first of these two pictures, and a total of nine for the second. In the   second image, the horizontal mesh size is approximately 1.5km, and just   under 1km in vertical direction. (The picture was also created using a   more resolved data set; however, it is too big to distribute as part of   the tutorial.) 

   [2.x.106]  The addition of the following function that, given a point    [2.x.107]  computes the elevation by converting the point to   reference WGS 84 coordinates and only keeping the depth variable (the   function is, consequently, a simplified version of the    [2.x.108]  function): 

[1.x.76] 



   [2.x.109] Adding the following piece to the bottom of the  [2.x.110]  function: 

[1.x.77] 

 [2.x.111]  This last piece of code first creates a  [2.x.112]  finite element space on the mesh. It then (ab)uses  [2.x.113]  to evaluate the elevation function for every node at the top boundary (the one with boundary indicator 5). We here wrap the call to  [2.x.114]  with the ScalarFunctionFromFunctionObject class to make a regular C++ function look like an object of a class derived from the Function class that we want to use in  [2.x.115]  Having so gotten a list of degrees of freedom located at the top boundary and corresponding elevation values, we just go down this list and set these elevations in the  [2.x.116]  vector (leaving all interior degrees of freedom at their original zero value). This vector is then output using DataOut as usual and can be visualized as shown above. 


[1.x.78][1.x.79] 


If you zoomed in on the mesh shown above and looked closely enough, you would find that at hanging nodes, the two small edges connecting to the hanging nodes are not in exactly the same location as the large edge of the neighboring cell. This can be shown more clearly by using a different surface description in which we enlarge the vertical topography to enhance the effect (courtesy of Alexander Grayver): 

 [2.x.117]  

So what is happening here? Partly, this is only a result of visualization, but there is an underlying real cause as well: 

 [2.x.118]     [2.x.119] When you visualize a mesh using any of the common visualization   programs, what they really show you is just a set of edges that are plotted   as straight lines in three-dimensional space. This is so because almost all   data file formats for visualizing data only describe hexahedral cells as a   collection of eight vertices in 3d space, and do not allow to any more   complicated descriptions. (This is the main reason why    [2.x.120]  takes an argument that can be set to something   larger than one.) These linear edges may be the edges of the cell you do   actual computations on, or they may not, depending on what kind of mapping   you use when you do your integrations using FEValues. By default, of course,   FEValues uses a linear mapping (i.e., an object of class MappingQ1) and in   that case a 3d cell is indeed described exclusively by its 8 vertices and   the volume it fills is a trilinear interpolation between these points,   resulting in linear edges. But, you could also have used tri-quadratic,   tri-cubic, or even higher order mappings and in these cases the volume of   each cell will be bounded by quadratic, cubic or higher order polynomial   curves. Yet, you only get to see these with linear edges in the   visualization program because, as mentioned, file formats do not allow to   describe the real geometry of cells. 

   [2.x.121] That said, let us for simplicity assume that you are indeed using a   trilinear mapping, then the image shown above is a faithful representation   of the cells on which you form your integrals. In this case, indeed the   small cells at a hanging nodes do not, in general, snugly fit against the   large cell but leave a gap or may intersect the larger cell. Why is this?   Because when the triangulation needs a new vertex on an edge it wants to   refine, it asks the manifold description where this new vertex is supposed   to be, and the manifold description duly returns such a point by (in the   case of a geometry derived from ChartManifold) pulling the adjacent points   of the line back to the reference domain, averaging their locations, and   pushing forward this new location to the real domain. But this new location   is not usually along a straight line (in real space) between the adjacent   vertices and consequently the two small straight lines forming the refined   edge do not lie exactly on the one large straight line forming the unrefined   side of the hanging node.  [2.x.122]  

The situation is slightly more complicated if you use a higher order mapping using the MappingQ class, but not fundamentally different. Let's take a quadratic mapping for the moment (nothing fundamental changes with even higher order mappings). Then you need to imagine each edge of the cells you integrate on as a quadratic curve despite the fact that you will never actually see it plotted that way by a visualization program. But imagine it that way for a second. So which quadratic curve does MappingQ take? It is the quadratic curve that goes through the two vertices at the end of the edge as well as a point in the middle that it queries from the manifold. In the case of the long edge on the unrefined side, that's of course exactly the location of the hanging node, so the quadratic curve describing the long edge does go through the hanging node, unlike in the case of the linear mapping. But the two small edges are also quadratic curves; for example, the left small edge will go through the left vertex of the long edge and the hanging node, plus a point it queries halfway in between from the manifold. Because, as before, the point the manifold returns halfway along the left small edge is rarely exactly on the quadratic curve describing the long edge, the quadratic short edge will typically not coincide with the left half of the quadratic long edge, and the same is true for the right short edge. In other words, again, the geometries of the large cell and its smaller neighbors at hanging nodes do not touch snuggly. 

This all begs two questions: first, does it matter, and second, could this be fixed. Let us discuss these in the following: 

 [2.x.123]     [2.x.124] Does it matter? It is almost certainly true that this depends on the   equation you are solving. For example, it is known that solving the Euler   equations of gas dynamics on complex geometries requires highly accurate   boundary descriptions to ensure convergence of quantities that are measure   the flow close to the boundary. On the other hand, equations with elliptic   components (e.g., the Laplace or Stokes equations) are typically rather   forgiving of these issues: one does quadrature anyway to approximate   integrals, and further approximating the geometry may not do as much harm as   one could fear given that the volume of the overlaps or gaps at every   hanging node is only  [2.x.125]  even with a linear mapping and  [2.x.126]  for a mapping of degree  [2.x.127] . (You can see this by considering   that in 2d the gap/overlap is a triangle with base  [2.x.128]  and height  [2.x.129] ; in 3d, it is a pyramid-like structure with base area  [2.x.130]  and   height  [2.x.131] . Similar considerations apply for higher order mappings   where the height of the gaps/overlaps is  [2.x.132] .) In other words,   if you use a linear mapping with linear elements, the error in the volume   you integrate over is already at the same level as the integration error   using the usual Gauss quadrature. Of course, for higher order elements one   would have to choose matching mapping objects. 

  Another point of view on why it is probably not worth worrying too much   about the issue is that there is certainly no narrative in the community of   numerical analysts that these issues are a major concern one needs to watch   out for when using complex geometries. If it does not seem to be discussed   often among practitioners, if ever at all, then it is at least not something   people have identified as a common problem. 

  This issue is not dissimilar to having hanging nodes at curved boundaries   where the geometry description of the boundary typically pulls a hanging   node onto the boundary whereas the large edge remains straight, making the   adjacent small and large cells not match each other. Although this behavior   existed in deal.II since its beginning, 15 years before manifold   descriptions became available, it did not ever come up in mailing list   discussions or conversations with colleagues. 

   [2.x.133] Could it be fixed? In principle, yes, but it's a complicated   issue. Let's assume for the moment that we would only ever use the MappingQ1   class, i.e., linear mappings. In that case, whenever the triangulation class   requires a new vertex along an edge that would become a hanging node, it   would just take the mean value of the adjacent vertices [1.x.80], i.e., without asking the manifold description. This way, the   point lies on the long straight edge and the two short straight edges would   match the one long edge. Only when all adjacent cells have been refined and   the point is no longer a hanging node would we replace its coordinates by   coordinates we get by a manifold. This may be awkward to implement, but it   would certainly be possible. 

  The more complicated issue arises because people may want to use a higher   order MappingQ object. In that case, the Triangulation class may freely   choose the location of the hanging node (because the quadratic curve for the   long edge can be chosen in such a way that it goes through the hanging node)   but the MappingQ class, when determining the location of mid-edge points   must make sure that if the edge is one half of a long edge of a neighboring   coarser cell, then the midpoint cannot be obtained from the manifold but   must be chosen along the long quadratic edge. For cubic (and all other odd)   mappings, the matter is again a bit complicated because one typically   arranges the cubic edge to go through points 1/3 and 2/3 along the edge, and   thus necessarily through the hanging node, but this could probably be worked   out. In any case, even then, there are two problems with this: 

  - When refining the triangulation, the Triangulation class can not know what     mapping will be used. In fact it is not uncommon for a triangulation to be     used differently in different contexts within the same program. If the     mapping used determines whether we can freely choose a point or not, how,     then, should the triangulation locate new vertices? 

  - Mappings are purely local constructs: they only work on a cell in     isolation, and this is one of the important features of the finite element     method. Having to ask whether one of the vertices of an edge is a hanging     node requires querying the neighborhood of a cell; furthermore, such a     query does not just involve the 6 face neighbors of a cell in 3d, but may     require traversing a possibly very large number of other cells that     connect to an edge. Even if it can be done, one still needs to do     different things depending on how the neighborhood looks like, producing     code that is likely very complex, hard to maintain, and possibly slow. 

  Consequently, at least for the moment, none of these ideas are   implemented. This leads to the undesirable consequence of discontinuous   geometries, but, as discussed above, the effects of this do not appear to   pose problem in actual practice. 

 [2.x.134]  [1.x.81] [1.x.82]  [2.x.135]  

 [2.x.136] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16] 

 [2.x.3]  

[1.x.17] 

 [2.x.4]  This program elaborates on concepts of industrial geometry, using tools that interface with the OpenCASCADE library (http://www.opencascade.org) that allow the specification of arbitrary IGES files to describe the boundaries for your geometries. 

 [2.x.5]  

[1.x.18] [1.x.19][1.x.20] 




In some of the previous tutorial programs ( [2.x.6] ,  [2.x.7] ,  [2.x.8] ,  [2.x.9]  and  [2.x.10]  among others) we have learned how to use the mesh refinement methods provided in deal.II. These tutorials have shown how to employ such tools to produce a fine grid for a single simulation, as done in  [2.x.11] ; or to start from a coarse grid and carry out a series of simulations on adaptively refined grids, as is the case of  [2.x.12] . Regardless of which approach is taken, the mesh refinement requires a suitable geometrical description of the computational domain boundary in order to place, at each refinement, the new mesh nodes onto the boundary surface. For instance,  [2.x.13]  shows how creating a circular grid automatically attaches a circular manifold object to the computational domain, so that the faces lying on the boundary are refined onto the circle.  [2.x.14]  shows how to do this with a Manifold defined by experimentally obtained data. But, at least as far as elementary boundary shapes are concerned, deal.II really only provides circles, spheres, boxes and other elementary combinations. In this tutorial, we will show how to use a set of classes developed to import arbitrary CAD geometries, assign them to the desired boundary of the computational domain, and refine a computational grid on such complex shapes. 


[1.x.21][1.x.22] 


In the most common industrial practice, the geometrical models of arbitrarily shaped objects are realized by means of Computer Aided Design (CAD) tools. The use of CAD modelers has spread in the last decades, as they allow for the generation of a full virtual model of each designed object, which through a computer can be visualized, inspected, and analyzed in its finest details well before it is physically crafted.  From a mathematical perspective, the engine lying under the hood of CAD modelers is represented by analytical geometry, and in particular by parametric curves and surfaces such as B-splines and NURBS that are rich enough that they can represent most surfaces of practical interest.  Once a virtual model is ready, all the geometrical features of the desired object are stored in files which materially contain the coefficients of the parametric surfaces and curves composing the object. Depending on the specific CAD tool used to define the geometrical model, there are of course several different file formats in which the information of a CAD model can be organized. To provide a common ground to exchange data across CAD tools, the U.S. National Bureau of Standards published in 1980 the Initial Graphics Exchange Representation (IGES) neutral file format, which is used in this example. 

[1.x.23][1.x.24] 


To import and interrogate CAD models, the deal.II library implements a series of wrapper functions for the OpenCASCADE open source library for CAD modeling. These functions allow to import IGES files into OpenCASCADE native objects, and wrap them inside a series of Manifold classes. 

Once imported from an IGES file, the model is stored in a  [2.x.15] , which is the generic topological entity defined in the OpenCASCADE framework. From a  [2.x.16] , it is then possible to access all the sub-shapes (such as vertices, edges and faces) composing it, along with their geometrical description. In the deal.II framework, the topological entities composing a shape are used to create a corresponding Manifold representation. In  [2.x.17]  we saw how to use  [2.x.18]  to create a hyper sphere, which automatically attaches a SphericalManifold to all boundary faces. This guarantees that boundary faces stay on a sphere or circle during mesh refinement. The functions of the CAD modeling interface have been designed to retain the same structure, allowing the user to build a projector object using the imported CAD shapes, maintaining the same procedure we used in other tutorial programs, i.e., assigning such projector object to cells, faces or edges of a coarse mesh. At each refinement cycle, the new mesh nodes will be then automatically generated by projecting a midpoint of an existing object onto the specified geometry. 

Differently from a spherical or circular boundary, a boundary with a complex geometry poses problems as to where it is best to place the new nodes created upon refinement on the prescribed shape. PolarManifold, for example, transforms the surrounding points to polar coordinates, calculates the average in that coordinate system (for each coordinate individually) and finally transforms the point back to Cartesian coordinates. 

In the case of an arbitrary and complex shape though, an appropriate choice for the placement of a new node cannot be identified that easily. The OpenCASCADE wrappers in deal.II provide several projector classes that employ different projection strategies. A first projector, implemented in the  [2.x.19]  class, is to be used only for edge refinement. It is built assigning it a topological shape of dimension one, either a  [2.x.20]  (which is a compound shape, made of several connected  [2.x.21] s) and refines a mesh edge finding the new vertex as the point splitting in two even parts the curvilinear length of the CAD curve portion that lies between the vertices of the original edge. 

 [2.x.22]  


A different projection strategy has been implemented in the  [2.x.23]  class. The  [2.x.24]  assigned at construction time can be arbitrary (a collection of shapes, faces, edges or a single face or edge will all work). The new cell nodes are first computed by averaging the surrounding points in the same way as FlatManifold does. In a second step, all the new nodes will be projected onto the  [2.x.25]  along the direction normal to the shape. If no normal projection is available, the point which is closest to the shape---typically lying on the shape boundary---is selected.  If the shape is composed of several sub-shapes, the projection is carried out onto every single sub-shape, and the closest projection point is selected. 

 [2.x.26]   [2.x.27]  

As we are about to experience, for some shapes, setting the projection direction as that normal to the CAD surface will not lead to surface mesh elements of suitable quality. This is because the direction normal to the CAD surface has in principle nothing to do with the direction along which the mesh needs the new nodes to be located. The  [2.x.28]  class, in this case, can help. This class is constructed assigning a  [2.x.29]  (containing at least a face) and a direction along which all the projections will be carried out. New points will be computed by first averaging the surrounding points (as in the FlatManifold case), and then taking the closest intersection between the topological shape and the line passing through the resulting point, along the direction used at construction time.  In this way, the user will have a higher control on the projection direction to be enforced to ensure good mesh quality. 

 [2.x.30]  


Of course the latter approach is effective only when the orientation of the surface is rather uniform, so that a single projection direction can be identified. In cases in which the surface direction is approaching the projection direction, it is even possible that the directional projection is not found. To overcome these problems, the  [2.x.31]  class implements a third projection algorithm. The  [2.x.32]  class is built assigning a  [2.x.33]  (containing at least one face) to the constructor, and works exactly like a  [2.x.34]  But, as the name of the class suggests,  [2.x.35]  tries to come up with a suitable estimate of the direction normal to the mesh elements to be refined, and uses it for the projection of the new nodes onto the CAD surface. If we consider a mesh edge in a 2D space, the direction of its axis is a direction along which to split it in order to give rise to two new cells of the same length. We here extended this concept in 3D, and project all new nodes in a direction that approximates the cell normal. 

In the next figure, which is inspired by the geometry considered in this tutorial, we make an attempt to compare the behavior of the three projectors considered. As can be seen on the left, given the original cell (in blue), the new point found with the normal projection is in a position which does not allow for the generation of evenly spaced new elements (in red). The situation will get worse in further refinement steps.  Since the geometry we considered is somehow perpendicular to the horizontal direction, the directional projection (central image) defined with horizontal direction as the projection direction, does a rather good job in getting the new mesh point. Yet, since the surface is almost horizontal at the bottom of the picture, we can expect problems in those regions when further refinement steps are carried out. Finally, the picture on the right shows that a node located on the cell axis will result in two new cells having the same length. Of course the situation in 3D gets a little more complicated than that described in this simple 2D case. Nevertheless, the results of this test confirm that the normal to the mesh direction is the best approach among the three tested, when arbitrarily shaped surfaces are considered, and unless you have a geometry for which a more specific approach is known to be appropriate. 


 [2.x.36]  


[1.x.25][1.x.26] 


In this program, we will consider creating a surface mesh for a real geometry describing the bow of a ship (this geometry is frequently used in CAD and mesh generation comparisons and is freely available). The surface mesh we get from this could then be used to solve a boundary element equation to simulate the flow of water around the ship (in a way similar to  [2.x.37] ) but we will not try to do this here. To already give you an idea of the geometry we consider, here is a picture: 

 [2.x.38]  

In the program, we read both the geometry and a coarse mesh from files, and then employ several of the options discussed above to place new vertices for a sequence of mesh refinement steps. [1.x.27] [1.x.28] 


[1.x.29]  [1.x.30] 




We start with including a bunch of files that we will use in the various parts of the program. Most of them have been discussed in previous tutorials already: 

[1.x.31] 



These are the headers of the opencascade support classes and functions. Notice that these will contain sensible data only if you compiled your deal.II library with support for OpenCASCADE, i.e., specifying  [2.x.39]  and  [2.x.40]  when calling  [2.x.41]  during deal.II configuration. 

[1.x.32] 



Finally, a few C++ standard header files 

[1.x.33] 



We isolate the rest of the program in its own namespace 

[1.x.34] 




[1.x.35]  [1.x.36] 




This is the main class. All it really does is store names for input and output files, and a triangulation. It then provides a function that generates such a triangulation from a coarse mesh, using one of the strategies discussed in the introduction and listed in the enumeration type at the top of the class.    


The member functions of this class are similar to what you can find in most of the other tutorial programs in the setup stage of the grid for the simulations. 







[1.x.37] 




[1.x.38]  [1.x.39] 




The constructor of the TriangulationOnCAD class is very simple. The input arguments are strings for the input and output file names, and the enumeration type that determines which kind of surface projector is used in the mesh refinement cycles (see below for details). 







[1.x.40] 




[1.x.41]  [1.x.42] 








The following function represents the core of this program.  In this function we import the CAD shape upon which we want to generate and refine our triangulation. We assume that the CAD surface is contained in the  [2.x.42]  file (we provide an example IGES file in the input directory called "input/DTMB-5415_bulbous_bow.iges" that represents the bulbous bow of a ship). The presence of several convex and concave high curvature regions makes the geometry we provided a particularly meaningful example.    


After importing the hull bow surface, we extract some of the curves and surfaces composing it, and use them to generate a set of projectors. Such projectors define the rules the Triangulation has to follow to position each new node during cell refinement.    


To initialize the Triangulation, as done in previous tutorial programs, we import a pre-existing grid saved in VTK format. We assume here that the user has generated a coarse mesh externally, which matches the IGES geometry. At the moment of writing this tutorial, the deal.II library does not automatically support generation of such meshes, but there are several tools which can provide you with reasonable initial meshes starting from CAD files. In our example, the imported mesh is composed of a single quadrilateral cell whose vertices have been placed on the CAD shape.    


After importing both the IGES geometry and the initial mesh, we assign the projectors previously discussed to each of the edges and cells which will have to be refined on the CAD surface.    


In this tutorial, we will test the three different CAD surface projectors described in the introduction, and will analyze the results obtained with each of them.  As mentioned, each of these projection strategies has been implemented in a different class, and objects of these types can be assigned to a triangulation using the  [2.x.43]  method.    


The following function then first imports the given CAD file. The function arguments are a string containing the desired file name, and a scale factor. In this example, the scale factor is set to 1e-3, as the original geometry is written in millimeters (which is the typical unit of measure for most IGES files), while we prefer to work in meters.  The output of the function is an object of OpenCASCADE generic topological shape class, namely a  [2.x.44]  

[1.x.43] 



Each CAD geometrical object is defined along with a tolerance, which indicates possible inaccuracy of its placement. For instance, the tolerance  [2.x.45]  of a vertex indicates that it can be located in any point contained in a sphere centered in the nominal position and having radius  [2.x.46]  While projecting a point onto a surface (which will in turn have its tolerance) we must keep in mind that the precision of the projection will be limited by the tolerance with which the surface is built. 




The following method extracts the tolerance of the given shape and makes it a bit bigger to stay our of trouble: 

[1.x.44] 



We now want to extract a set of composite sub-shapes from the generic shape. In particular, each face of the CAD file is composed of a trimming curve of type  [2.x.47]  which is the collection of  [2.x.48]  that compose the boundary of a surface, and a NURBS description of the surface itself. We will use a line projector to associate the boundary of our Triangulation to the wire delimiting the surface.  To extract all compound sub-shapes, like wires, shells, or solids, we resort to a method of the OpenCASCADE namespace.  The input of  [2.x.49]  is a shape and a set of empty  [2.x.50]  of subshapes, which will be filled with all compound shapes found in the given topological shape: 

[1.x.45] 



The next few steps are more familiar, and allow us to import an existing mesh from an external VTK file, and convert it to a deal triangulation. 

[1.x.46] 



We output this initial mesh saving it as the refinement step 0. 

[1.x.47] 



The mesh imported has a single, two-dimensional cell located in three-dimensional space. We now want to ensure that it is refined according to the CAD geometry imported above. This this end, we get an iterator to that cell and assign to it the manifold_id 1 (see  [2.x.51]  "this glossary entry"). We also get an iterator to its four faces, and assign each of them the manifold_id 2: 

[1.x.48] 



Once both the CAD geometry and the initial mesh have been imported and digested, we use the CAD surfaces and curves to define the projectors and assign them to the manifold ids just specified. 




A first projector is defined using the single wire contained in our CAD file.  The ArclengthProjectionLineManifold will make sure that every mesh edge located on the wire is refined with a point that lies on the wire and splits it into two equal arcs lying between the edge vertices. We first check that the wires vector contains at least one element and then create a Manifold object for it.      


Once the projector is created, we then assign it to all the parts of the triangulation with manifold_id = 2: 

[1.x.49] 



The surface projector is created according to what is specified with the  [2.x.52]  option of the constructor. In particular, if the surface_projection_kind value equals  [2.x.53]  we select the  [2.x.54]  The new mesh points will then initially be generated at the barycenter of the cell/edge considered, and then projected on the CAD surface along its normal direction.  The NormalProjectionManifold constructor only needs a shape and a tolerance, and we then assign it to the triangulation for use with all parts that manifold having id 1: 

[1.x.50] 



 [2.x.55]  surface_projection_kind value is  [2.x.56]  we select the  [2.x.57]  class. The new mesh points will then initially be generated at the barycenter of the cell/edge considered, and then projected on the CAD surface along a direction that is specified to the  [2.x.58]  constructor. In this case, the projection is done along the y-axis. 

[1.x.51] 



As a third option, if  [2.x.59]  value is  [2.x.60]  we select the  [2.x.61]  The new mesh points will again initially be generated at the barycenter of the cell/edge considered, and then projected on the CAD surface along a direction that is an estimate of the mesh normal direction. The  [2.x.62]  constructor only requires a shape (containing at least a face) and a tolerance. 

[1.x.52] 



Finally, we use good software cleanliness by ensuring that this really covers all possible options of the  [2.x.63]  statement. If we get any other value, we simply abort the program: 

[1.x.53] 




[1.x.54]  [1.x.55] 




This function globally refines the mesh. In other tutorials, it would typically also distribute degrees of freedom, and resize matrices and vectors. These tasks are not carried out here, since we are not running any simulation on the Triangulation produced.    


While the function looks innocent, this is where most of the work we are interested in for this tutorial program actually happens. In particular, when refining the quads and lines that define the surface of the ship's hull, the Triangulation class will ask the various objects we have assigned to handle individual manifold ids for where the new vertices should lie. 

[1.x.56] 




[1.x.57]  [1.x.58] 




Outputting the results of our computations is a rather mechanical task. All the components of this function have been discussed before: 

[1.x.59] 




[1.x.60]  [1.x.61] 




This is the main function. It should be self explanatory in its briefness: 

[1.x.62] 




[1.x.63]  [1.x.64] 




This is the main function of this program. It is in its basic structure like all previous tutorial programs, but runs the main class through the three possibilities of new vertex placement: 

[1.x.65] 

[1.x.66][1.x.67] 


The program execution produces a series of mesh files  [2.x.64]  that we can visualize with any of the usual visualization programs that can read the VTK file format. 

The following table illustrates the results obtained employing the normal projection strategy. The first two rows of the table show side views of the grids obtained for progressive levels of refinement, overlain on a very fine rendering of the exact geometry. The dark and light red areas simply indicate whether the current mesh or the fine geometry is closer to the observer; the distinction does not carry any particularly deep meaning. The last row of pictures depict front views (mirrored to both sides of the geometry) of the same grids shown in the second row. 


 [2.x.65]  

As can be seen in the pictures---and as we anticipated---the normal refinement strategy is unable to produce nicely shaped elements when applied to surfaces with significant curvature changes. This is particularly apparent at the bulb of the hull where all new points have been placed in the upper part of the bulb and the lower part remains completely unresolved. 

The following table, which is arranged as the previous one, illustrates the results obtained adopting the directional projection approach, in which the projection direction selected was the y-axis (which is indicated with a small yellow arrow at the bottom left of each image). 


 [2.x.66]  

The images confirm that the quality of the mesh obtained with a directional projection is sensibly higher than that obtained projecting along the surface normal. Yet, a number of elements elongated in the y-direction are observed around the bottom of the bulb, where the surface is almost parallel to the direction chosen for the projection. 

The final test shows results using instead the projection normal to the faces: 

 [2.x.67]  

The pictures confirm that the normal to mesh projection approach leads to grids that remain evenly spaced throughtout the refinement steps. At the same time, these meshes represent rather well the original geometry even in the bottom region of the bulb, which is not well recovered employing the directional projector or the normal projector. [1.x.68] [1.x.69]  [2.x.68]  

 [2.x.69] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19] 

 [2.x.4]  

[1.x.20] 




 [2.x.5]  As a prerequisite of this program, you need to have PETSc or Trilinos and the p4est library installed. The installation of deal.II together with these additional libraries is described in the [1.x.21] file. 

[1.x.22] [1.x.23][1.x.24] 


Building on  [2.x.6] , this tutorial shows how to solve linear PDEs with several components in parallel using MPI with PETSc or Trilinos for the linear algebra. For this, we return to the Stokes equations as discussed in  [2.x.7] . The motivation for writing this tutorial is to provide an intermediate step (pun intended) between  [2.x.8]  (parallel Laplace) and  [2.x.9]  (parallel coupled Stokes with Boussinesq for a time dependent problem). 

The learning outcomes for this tutorial are: 

- You are able to solve PDEs with several variables in parallel and can   apply this to different problems. 

- You understand the concept of optimal preconditioners and are able to check   this for a particular problem. 

- You are able to construct manufactured solutions using the free computer   algreba system SymPy (https://sympy.org). 

- You can implement various other tasks for parallel programs: error   computation, writing graphical output, etc. 

- You can visualize vector fields, stream lines, and contours of vector   quantities. 

We are solving for a velocity  [2.x.10]  and pressure  [2.x.11]  that satisfy the Stokes equation, which reads [1.x.25] 




[1.x.26][1.x.27] 


Make sure that you read (even better: try) what is described in "Block Schur complement preconditioner" in the "Possible Extensions" section in  [2.x.12] . Like described there, we are going to solve the block system using a Krylov method and a block preconditioner. 

Our goal here is to construct a very simple (maybe the simplest?) optimal preconditioner for the linear system. A preconditioner is called "optimal" or "of optimal complexity", if the number of iterations of the preconditioned system is independent of the mesh size  [2.x.13] . You can extend that definition to also require indepence of the number of processors used (we will discuss that in the results section), the computational domain and the mesh quality, the test case itself, the polynomial degree of the finite element space, and more. 

Why is a constant number of iterations considered to be "optimal"? Assume the discretized PDE gives a linear system with N unknowns. Because the matrix coming from the FEM discretization is sparse, a matrix-vector product can be done in O(N) time. A preconditioner application can also only be O(N) at best (for example doable with multigrid methods). If the number of iterations required to solve the linear system is independent of  [2.x.14]  (and therefore N), the total cost of solving the system will be O(N). It is not possible to beat this complexity, because even looking at all the entries of the right-hand side already takes O(N) time. For more information see  [2.x.15] , Chapter 2.5 (Multigrid). 

The preconditioner described here is even simpler than the one described in  [2.x.16]  and will typically require more iterations and consequently time to solve. When considering preconditioners, optimality is not the only important metric. But an optimal and expensive preconditioner is typically more desirable than a cheaper, non-optimal one. This is because, eventually, as the mesh size becomes smaller and smaller and linear problems become bigger and bigger, the former will eventually beat the latter. 

[1.x.28][1.x.29] 


We precondition the linear system [1.x.30] 



with the block diagonal preconditioner [1.x.31] 

where  [2.x.17]  is the Schur complement. 

With this choice of  [2.x.18] , assuming that we handle  [2.x.19]  and  [2.x.20]  exactly (which is an "idealized" situation), the preconditioned linear system has three distinct eigenvalues independent of  [2.x.21]  and is therefore "optimal".  See section 6.2.1 (especially p. 292) in  [2.x.22] . For comparison, using the ideal version of the upper block-triangular preconditioner in  [2.x.23]  (also used in  [2.x.24] ) would have all eigenvalues be equal to one. 

We will use approximations of the inverse operations in  [2.x.25]  that are (nearly) independent of  [2.x.26] . In this situation, one can again show, that the eigenvalues are independent of  [2.x.27] . For the Krylov method we choose MINRES, which is attractive for the analysis (iteration count is proven to be independent of  [2.x.28] , see the remainder of the chapter 6.2.1 in the book mentioned above), great from the computational standpoint (simpler and cheaper than GMRES for example), and applicable (matrix and preconditioner are symmetric). 

For the approximations we will use a CG solve with the mass matrix in the pressure space for approximating the action of  [2.x.29] . Note that the mass matrix is spectrally equivalent to  [2.x.30] . We can expect the number of CG iterations to be independent of  [2.x.31] , even with a simple preconditioner like ILU. 

For the approximation of the velocity block  [2.x.32]  we will perform a single AMG V-cycle. In practice this choice is not exactly independent of  [2.x.33] , which can explain the slight increase in iteration numbers. A possible explanation is that the coarsest level will be solved exactly and the number of levels and size of the coarsest matrix is not predictable. 


[1.x.32][1.x.33] 


We will construct a manufactured solution based on the classical Kovasznay problem, see  [2.x.34] . Here is an image of the solution colored by the x velocity including streamlines of the velocity: 

  [2.x.35]  

We have to cheat here, though, because we are not solving the non-linear Navier-Stokes equations, but the linear Stokes system without convective term. Therefore, to recreate the exact same solution, we use the method of manufactured solutions with the solution of the Kovasznay problem. This will effectively move the convective term into the right-hand side  [2.x.36] . 

The right-hand side is computed using the script "reference.py" and we use the exact solution for boundary conditions and error computation. [1.x.34] [1.x.35] 




[1.x.36] 



The following chunk out code is identical to  [2.x.37]  and allows switching between PETSc and Trilinos: 







[1.x.37] 




[1.x.38]  [1.x.39] 




We need a few helper classes to represent our solver strategy described in the introduction. 







[1.x.40] 



This class exposes the action of applying the inverse of a giving matrix via the function  [2.x.38]  Internally, the inverse is not formed explicitly. Instead, a linear solver with CG is performed. This class extends the InverseMatrix class in  [2.x.39]  with an option to specify a preconditioner, and to allow for different vector types in the vmult function. 

[1.x.41] 



The class A template class for a simple block diagonal preconditioner for 2x2 matrices. 

[1.x.42] 




[1.x.43]  [1.x.44] 




The following classes represent the right hand side and the exact solution for the test problem. 







[1.x.45] 




[1.x.46]  [1.x.47]    


The main class is very similar to  [2.x.40] , except that matrices and vectors are now block versions, and we store a  [2.x.41]  for owned and relevant DoFs instead of a single IndexSet. We have exactly two IndexSets, one for all velocity unknowns and one for all pressure unknowns. 

[1.x.48] 



The Kovasnay flow is defined on the domain [-0.5, 1.5]^2, which we create by passing the min and max values to  [2.x.42]  

[1.x.49] 




[1.x.50]  [1.x.51]    


The construction of the block matrices and vectors is new compared to  [2.x.43]  and is different compared to serial codes like  [2.x.44] , because we need to supply the set of rows that belong to our processor. 

[1.x.52] 



Put all dim velocities into block 0 and the pressure into block 1, then reorder the unknowns by block. Finally count how many unknowns we have per block. 

[1.x.53] 



We split up the IndexSet for locally owned and locally relevant DoFs into two IndexSets based on how we want to create the block matrices and vectors. 

[1.x.54] 



Setting up the constraints for boundary conditions and hanging nodes is identical to  [2.x.45] . Rven though we don't have any hanging nodes because we only perform global refinement, it is still a good idea to put this function call in, in case adaptive refinement gets introduced later. 

[1.x.55] 



Now we create the system matrix based on a BlockDynamicSparsityPattern. We know that we won't have coupling between different velocity components (because we use the laplace and not the deformation tensor) and no coupling between pressure with its test functions, so we use a Table to communicate this coupling information to  [2.x.46]  

[1.x.56] 



The preconditioner matrix has a different coupling (we only fill in the 1,1 block with the mass matrix), otherwise this code is identical to the construction of the system_matrix above. 

[1.x.57] 



owned_partitioning, 

[1.x.58] 



Finally, we construct the block vectors with the right sizes. The function call with two  [2.x.47]  will create a ghosted vector. 

[1.x.59] 




[1.x.60]  [1.x.61]    


This function assembles the system matrix, the preconditioner matrix, and the right hand side. The code is pretty standard. 

[1.x.62] 




[1.x.63]  [1.x.64]    


This function solves the linear system with MINRES with a block diagonal preconditioner and AMG for the two diagonal blocks as described in the introduction. The preconditioner applies a v cycle to the 0,0 block and a CG with the mass matrix for the 1,1 block (the Schur complement). 

[1.x.65] 



The InverseMatrix is used to solve for the mass matrix: 

[1.x.66] 



This constructs the block preconditioner based on the preconditioners for the individual blocks defined above. 

[1.x.67] 



With that, we can finally set up the linear solver and solve the system: 

[1.x.68] 



Like in  [2.x.48] , we subtract the mean pressure to allow error computations against our reference solution, which has a mean value of zero. 

[1.x.69] 




[1.x.70]  [1.x.71]    


The remainder of the code that deals with mesh refinement, output, and the main loop is pretty standard. 

[1.x.72] 

[1.x.73][1.x.74] 


As expected from the discussion above, the number of iterations is independent of the number of processors and only very slightly dependent on  [2.x.49] : 

 [2.x.50]  

 [2.x.51]  

While the PETSc results show a constant number of iterations, the iterations increase when using Trilinos. This is likely because of the different settings used for the AMG preconditioner. For performance reasons we do not allow coarsening below a couple thousand unknowns. As the coarse solver is an exact solve (we are using LU by default), a change in number of levels will influence the quality of a V-cycle. Therefore, a V-cycle is closer to an exact solver for smaller problem sizes. 

[1.x.75] [1.x.76][1.x.77] 


[1.x.78][1.x.79] 


Play with the smoothers, smoothing steps, and other properties for the Trilinos AMG to achieve an optimal preconditioner. 

[1.x.80][1.x.81] 


This change requires changing the outer solver to GMRES or BiCGStab, because the system is no longer symmetric. 

You can prescribe the exact flow solution as  [2.x.52]  in the convective term  [2.x.53] . This should give the same solution as the original problem, if you set the right hand side to zero. 

[1.x.82][1.x.83] 


So far, this tutorial program refines the mesh globally in each step. Replacing the code in  [2.x.54]  by something like 

[1.x.84] 

makes it simple to explore adaptive mesh refinement. [1.x.85] [1.x.86]  [2.x.55]  

 [2.x.56] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28] 

[1.x.29] 

 [2.x.4]  

[1.x.30] [1.x.31][1.x.32] 


[1.x.33][1.x.34] 


The purpose of this tutorial is to create an efficient linear solver for the Stokes equation and compare it to alternative approaches.  Here, we will use FGMRES with geometric multigrid as a preconditioner velocity block, and we will show in the results section that this is a fundamentally better approach than the linear solvers used in  [2.x.5]  (including the scheme described in "Possible Extensions").  Fundamentally, this is because only with multigrid it is possible to get  [2.x.6]  solve time, where  [2.x.7]  is the number of unknowns of the linear system. Using the Timer class, we collect some statistics to compare setup times, solve times, and number of iterations. We also compute errors to make sure that what we have implemented is correct. 

Let  [2.x.8]  and  [2.x.9] . The Stokes equations read as follows in non-dimensionalized form: 

[1.x.35] 



Note that we are using the deformation tensor instead of  [2.x.10]  (a detailed description of the difference between the two can be found in  [2.x.11] , but in summary, the deformation tensor is more physical as well as more expensive). 

[1.x.36][1.x.37] 


The weak form of the discrete equations naturally leads to the following linear system for the nodal values of the velocity and pressure fields: [1.x.38] 



Our goal is to compare several solution approaches.  While  [2.x.12]  solves the linear system using a "Schur complement approach" in two separate steps, we instead attack the block system at once using FMGRES with an efficient preconditioner, in the spirit of the approach outlined in the "Results" section of  [2.x.13] . The idea is as follows: if we find a block preconditioner  [2.x.14]  such that the matrix 

[1.x.39] 



is simple, then an iterative solver with that preconditioner will converge in a few iterations. Notice that we are doing right preconditioning here.  Using the Schur complement  [2.x.15] , we find that 

[1.x.40] 



is a good choice. Let  [2.x.16]  be an approximation of  [2.x.17]  and  [2.x.18]  of  [2.x.19] , we see [1.x.41] 



Since  [2.x.20]  is aimed to be a preconditioner only, we shall use the approximations on the right in the equation above. 

As discussed in  [2.x.21] ,  [2.x.22] , where  [2.x.23]  is the pressure mass matrix and is solved approximately by using CG with ILU as a preconditioner, and  [2.x.24]  is obtained by one of multiple methods: solving a linear system with CG and ILU as preconditioner, just using one application of an ILU, solving a linear system with CG and GMG (Geometric Multigrid as described in  [2.x.25] ) as a preconditioner, or just performing a single V-cycle of GMG. 

As a comparison, instead of FGMRES, we also use the direct solver UMFPACK on the whole system to compare our results with.  If you want to use a direct solver (like UMFPACK), the system needs to be invertible. To avoid the one dimensional null space given by the constant pressures, we fix the first pressure unknown  to zero. This is not necessary for the iterative solvers. 


[1.x.42][1.x.43] 


The test problem is a "Manufactured Solution" (see  [2.x.26]  for details), and we choose  [2.x.27]  and  [2.x.28] . We apply Dirichlet boundary conditions for the velocity on the whole boundary of the domain  [2.x.29] . To enforce the boundary conditions we can just use our reference solution. 

If you look up in the deal.II manual what is needed to create a class derived from  [2.x.30] , you will find that this class has numerous  [2.x.31]  functions, including  [2.x.32]   [2.x.33]   [2.x.34]  etc., all of which can be overloaded.  Different parts of deal.II will require different ones of these particular functions. This can be confusing at first, but luckily the only thing you actually have to implement is  [2.x.35]   The other virtual functions in the Function class have default implementations inside that will call your implementation of  [2.x.36]  by default. 

Notice that our reference solution fulfills  [2.x.37] . In addition, the pressure is chosen to have a mean value of zero.  For the "Method of Manufactured Solutions" of  [2.x.38] , we need to find  [2.x.39]  such that: 

[1.x.44] 



Using the reference solution above, we obtain: 

[1.x.45] 



[1.x.46][1.x.47] 


Because we do not enforce the mean pressure to be zero for our numerical solution in the linear system, we need to post process the solution after solving. To do this we use the  [2.x.40]  function to compute the mean value of the pressure to subtract it from the pressure. 


[1.x.48][1.x.49] 


The way we implement geometric multigrid here only executes it on the velocity variables (i.e., the  [2.x.41]  matrix described above) but not the pressure. One could implement this in different ways, including one in which one considers all coarse grid operations as acting on  [2.x.42]  block systems where we only consider the top left block. Alternatively, we can implement things by really only considering a linear system on the velocity part of the overall finite element discretization. The latter is the way we want to use here. 

To implement this, one would need to be able to ask questions such as "May I have just part of a DoFHandler?". This is not possible at the time when this program was written, so in order to answer this request for our needs, we simply create a separate, second DoFHandler for just the velocities. We then build linear systems for the multigrid preconditioner based on only this second DoFHandler, and simply transfer the first block of (overall) vectors into corresponding vectors for the entire second DoFHandler. To make this work, we have to assure that the [1.x.50] in which the (velocity) degrees of freedom are ordered in the two DoFHandler objects is the same. This is in fact the case by first distributing degrees of freedom on both, and then using the same sequence of DoFRenumbering operations on both. 


[1.x.51][1.x.52] 


The main difference between  [2.x.43]  and  [2.x.44]  is that we use block solvers instead of the Schur Complement approach used in  [2.x.45] . Details of this approach can be found under the "Block Schur complement preconditioner" subsection of the "Possible Extensions" section of  [2.x.46] . For the preconditioner of the velocity block, we borrow a class from [1.x.53] called  [2.x.47]  that has the option to solve for the inverse of  [2.x.48]  or just apply one preconditioner sweep for it instead, which provides us with an expensive and cheap approach, respectively. [1.x.54] [1.x.55] 


[1.x.56]  [1.x.57] 







[1.x.58] 



We need to include the following file to do timings: 

[1.x.59] 



This includes the files necessary for us to use geometric Multigrid 

[1.x.60] 



In order to make it easy to switch between the different solvers that are being used, we declare an enum that can be passed as an argument to the constructor of the main class. 

[1.x.61] 




[1.x.62]  [1.x.63]    


The class Solution is used to define the boundary conditions and to compute errors of the numerical solution. Note that we need to define the values and gradients in order to compute L2 and H1 errors. Here we decided to separate the implementations for 2d and 3d using template specialization.    


Note that the first dim components are the velocity components and the last is the pressure. 

[1.x.64] 



Note that for the gradient we need to return a Tensor<1,dim> 

[1.x.65] 



Implementation of  [2.x.49] . See the introduction for more information. 

[1.x.66] 




[1.x.67]  [1.x.68] 




In the following, we will implement a preconditioner that expands on the ideas discussed in the Results section of  [2.x.50] . Specifically, we 1. use an upper block-triangular preconditioner because we want to use right preconditioning. 2. optionally allow using an inner solver for the velocity block instead of a single preconditioner application. 3. do not use InverseMatrix but explicitly call SolverCG. This approach is also used in the ASPECT code (see https://aspect.geodynamics.org) that solves the Stokes equations in the context of simulating convection in the earth mantle, and which has been used to solve problems on many thousands of processors.    


The bool flag  [2.x.51]  in the constructor allows us to either apply the preconditioner for the velocity block once or use an inner iterative solver for a more accurate approximation instead.    


Notice how we keep track of the sum of the inner iterations (preconditioner applications). 

[1.x.69] 



First solve with the approximation for S 

[1.x.70] 



Second, apply the top right block (B^T) 

[1.x.71] 



Finally, either solve with the top left block or just apply one preconditioner sweep 

[1.x.72] 




[1.x.73]  [1.x.74]    


This is the main class of the problem. 

[1.x.75] 



Finite element for the velocity only: 

[1.x.76] 



Finite element for the whole system: 

[1.x.77] 




[1.x.78]  [1.x.79] 




This function sets up the DoFHandler, matrices, vectors, and Multigrid structures (if needed). 

[1.x.80] 



The main DoFHandler only needs active DoFs, so we are not calling distribute_mg_dofs() here 

[1.x.81] 



This block structure separates the dim velocity components from the pressure component (used for reordering). Note that we have 2 instead of dim+1 blocks like in  [2.x.52] , because our FESystem is nested and the dim velocity components appear as one block. 

[1.x.82] 



Velocities start at component 0: 

[1.x.83] 



ILU behaves better if we apply a reordering to reduce fillin. There is no advantage in doing this for the other solvers. 

[1.x.84] 



This ensures that all velocities DoFs are enumerated before the pressure unknowns. This allows us to use blocks for vectors and matrices and allows us to get the same DoF numbering for dof_handler and velocity_dof_handler. 

[1.x.85] 



This distributes the active dofs and multigrid dofs for the velocity space in a separate DoFHandler as described in the introduction. 

[1.x.86] 



The following block of code initializes the MGConstrainedDofs (using the boundary conditions for the velocity), and the sparsity patterns and matrices for each level. The resize() function of MGLevelObject<T> will destroy all existing contained objects. 

[1.x.87] 



The following makes use of a component mask for interpolation of the boundary values for the velocity only, which is further explained in the vector valued dealii  [2.x.53]  tutorial. 

[1.x.88] 



As discussed in the introduction, we need to fix one degree of freedom of the pressure variable to ensure solvability of the problem. We do this here by marking the first pressure dof, which has index n_u as a constrained dof. 

[1.x.89] 




[1.x.90]  [1.x.91] 




In this function, the system matrix is assembled. We assemble the pressure mass matrix in the (1,1) block (if needed) and move it out of this location at the end of this function. 

[1.x.92] 



If true, we will assemble the pressure mass matrix in the (1,1) block: 

[1.x.93] 




[1.x.94]  [1.x.95] 




Here, like in  [2.x.54] , we have a function that assembles the level and interface matrices necessary for the multigrid preconditioner. 

[1.x.96] 



This iterator goes over all cells (not just active) 

[1.x.97] 




[1.x.98]  [1.x.99] 




This function sets up things differently based on if you want to use ILU or GMG as a preconditioner.  Both methods share the same solver (FGMRES) but require a different preconditioner to be initialized. Here we time not only the entire solve function, but we separately time the setup of the preconditioner as well as the solve itself. 

[1.x.100] 



Here we must make sure to solve for the residual with "good enough" accuracy 

[1.x.101] 



This is used to pass whether or not we want to solve for A inside the preconditioner.  One could change this to false to see if there is still convergence and if so does the program then run faster or slower 

[1.x.102] 



Transfer operators between levels 

[1.x.103] 



Setup coarse grid solver 

[1.x.104] 



Multigrid, when used as a preconditioner for CG, needs to be a symmetric operator, so the smoother must be symmetric 

[1.x.105] 



Now, we are ready to set up the V-cycle operator and the multilevel preconditioner. 

[1.x.106] 




[1.x.107]  [1.x.108] 




This function computes the L2 and H1 errors of the solution. For this, we need to make sure the pressure has mean zero. 

[1.x.109] 



Compute the mean pressure  [2.x.55]  and then subtract it from each pressure coefficient. This will result in a pressure with mean value zero. Here we make use of the fact that the pressure is component  [2.x.56]  and that the finite element space is nodal. 

[1.x.110] 




[1.x.111]  [1.x.112] 




This function generates graphical output like it is done in  [2.x.57] . 

[1.x.113] 




[1.x.114]  [1.x.115] 




The last step in the Stokes class is, as usual, the function that generates the initial grid and calls the other functions in the respective order. 

[1.x.116] 




[1.x.117]  [1.x.118] 

[1.x.119] 



options for SolverType: UMFPACK FGMRES_ILU FGMRES_GMG 

[1.x.120] 

[1.x.121][1.x.122] 


[1.x.123][1.x.124] 


We first run the code and confirm that the finite element solution converges with the correct rates as predicted by the error analysis of mixed finite element problems. Given sufficiently smooth exact solutions  [2.x.58]  and  [2.x.59] , the errors of the Taylor-Hood element  [2.x.60]  should be 

[1.x.125] 

see for example Ern/Guermond "Theory and Practice of Finite Elements", Section 4.2.5 p195. This is indeed what we observe, using the  [2.x.61]  element as an example (this is what is done in the code, but is easily changed in  [2.x.62] ): 

 [2.x.63]  

[1.x.126][1.x.127] 


Let us compare the direct solver approach using UMFPACK to the two methods in which we choose  [2.x.64]  and  [2.x.65]  by solving linear systems with  [2.x.66]  using CG. The preconditioner for CG is then either ILU or GMG. The following table summarizes solver iterations, timings, and virtual memory (VM) peak usage: 

 [2.x.67]  

As can be seen from the table: 

1. UMFPACK uses large amounts of memory, especially in 3d. Also, UMFPACK timings do not scale favorably with problem size. 

2. Because we are using inner solvers for  [2.x.68]  and  [2.x.69] , ILU and GMG require the same number of outer iterations. 

3. The number of (inner) iterations for  [2.x.70]  increases for ILU with refinement, leading to worse than linear scaling in solve time. In contrast, the number of inner iterations for  [2.x.71]  stays constant with GMG leading to nearly perfect scaling in solve time. 

4. GMG needs slightly more memory than ILU to store the level and interface matrices. 

[1.x.128][1.x.129] 


[1.x.130][1.x.131] 


Experiment with higher order stable FE pairs and check that you observe the correct convergence rates. 

[1.x.132][1.x.133] 


The introduction also outlined another option to precondition the overall system, namely one in which we do not choose  [2.x.72]  as in the table above, but in which  [2.x.73]  is only a single preconditioner application with GMG or ILU, respectively. 

This is in fact implemented in the code: Currently, the boolean  [2.x.74]  is set to  [2.x.75]  The option mentioned above is obtained by setting it to  [2.x.76]  

What you will find is that the number of FGMRES iterations stays constant under refinement if you use GMG this way. This means that the Multigrid is optimal and independent of  [2.x.77] . [1.x.134] [1.x.135]  [2.x.78]  

 [2.x.79] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33] 

 [2.x.4]  

[1.x.34] 

 [2.x.5]  

[1.x.35] [1.x.36][1.x.37] 


[1.x.38][1.x.39] 


In this tutorial we show how to solve the incompressible Navier Stokes equations (NSE) with Newton's method. The flow we consider here is assumed to be steady. In a domain  [2.x.6] ,  [2.x.7] , with a piecewise smooth boundary  [2.x.8] , and a given force field  [2.x.9] , we seek a velocity field  [2.x.10]  and a pressure field  [2.x.11]  satisfying [1.x.40] 



Unlike the Stokes equations as discussed in  [2.x.12] , the NSE are a nonlinear system of equations because of the convective term  [2.x.13] . The first step of computing a numerical solution is to linearize the system and this will be done using Newton's method. A time-dependent problem is discussed in  [2.x.14] , where the system is linearized using the solution from the last time step and no nonlinear solve is necessary. 

[1.x.41][1.x.42] 


We define a nonlinear function whose root is a solution to the NSE by [1.x.43] 



Assuming the initial guess is good enough to guarantee the convergence of Newton's iteration and denoting  [2.x.15] , Newton's iteration on a vector function can be defined as [1.x.44] 



where  [2.x.16]  is the approximate solution in step  [2.x.17] ,  [2.x.18]  represents the solution from the previous step, and  [2.x.19]  is the Jacobian matrix evaluated at  [2.x.20] . A similar iteration can be found in  [2.x.21] . 

The Newton iteration formula implies the new solution is obtained by adding an update term to the old solution. Instead of evaluating the Jacobian matrix and taking its inverse, we consider the update term as a whole, that is [1.x.45] 



where  [2.x.22] . 

We can find the update term by solving the system [1.x.46] 



Here, the left of the previous equation represents the directional gradient of  [2.x.23]  along  [2.x.24]  at  [2.x.25] . By definition, the directional gradient is given by [1.x.47] 



Therefore, we arrive at the linearized system: [1.x.48] 



where  [2.x.26]  and  [2.x.27]  are the solutions from the previous iteration. Additionally, the right hand side of the second equation is not zero since the discrete solution is not exactly divergence free (divergence free for the continuous solution). The right hand side here acts as a correction which leads the discrete solution of the velocity to be divergence free along Newton's iteration. In this linear system, the only unknowns are the update terms  [2.x.28]  and  [2.x.29] , and we can use a similar strategy to the one used in  [2.x.30]  (and derive the weak form in the same way). 

Now, Newton's iteration can be used to solve for the update terms: 

 [2.x.31]     [2.x.32] Initialization: Initial guess  [2.x.33]  and  [2.x.34] , tolerance  [2.x.35] ; [2.x.36]     [2.x.37] Linear solve to compute update term  [2.x.38]  and        [2.x.39] ; [2.x.40]     [2.x.41] Update the approximation:        [2.x.42]  and        [2.x.43] ; [2.x.44]     [2.x.45] Check residual norm:  [2.x.46] :        [2.x.47]           [2.x.48] If  [2.x.49] , STOP. [2.x.50]           [2.x.51] If  [2.x.52] , back to step 2. [2.x.53]         [2.x.54]  [2.x.55]   [2.x.56]  

[1.x.49][1.x.50] 


The initial guess needs to be close enough to the solution for Newton's method to converge; hence, finding a good starting value is crucial to the nonlinear solver. 

When the viscosity  [2.x.57]  is large, a good initial guess can be obtained by solving the Stokes equation with viscosity  [2.x.58] . While problem dependent, this works for  [2.x.59]  for the test problem considered here. 

However, the convective term  [2.x.60]  will be dominant if the viscosity is small, like  [2.x.61]  in test case 2.  In this situation, we use a continuation method to set up a series of auxiliary NSEs with viscosity approaching the one in the target NSE. Correspondingly, we create a sequence  [2.x.62]  with  [2.x.63] , and accept that the solutions to two NSE with viscosity  [2.x.64]  and  [2.x.65]  are close if  [2.x.66]  is small.  Then we use the solution to the NSE with viscosity  [2.x.67]  as the initial guess of the NSE with  [2.x.68] . This can be thought of as a staircase from the Stokes equations to the NSE we want to solve. 

That is, we first solve a Stokes problem [1.x.51] 



to get the initial guess for [1.x.52] 



which also acts as the initial guess of the continuation method. Here  [2.x.69]  is relatively large so that the solution to the Stokes problem with viscosity  [2.x.70]  can be used as an initial guess for the NSE in Newton's iteration. 

Then the solution to [1.x.53] 



acts as the initial guess for [1.x.54] 



This process is repeated with a sequence of viscosities  [2.x.71]  that is determined experimentally so that the final solution can used as a starting guess for the Newton iteration. 

[1.x.55][1.x.56] 


At each step of Newton's iteration, the problem results in solving a saddle point systems of the form [1.x.57] 



This system matrix has the same block structure as the one in  [2.x.72] . However, the matrix  [2.x.73]  at the top left corner is not symmetric because of the nonlinear term. Instead of solving the above system, we can solve the equivalent system [1.x.58] 



with a parameter  [2.x.74]  and an invertible matrix  [2.x.75] . Here  [2.x.76]  is the Augmented Lagrangian term; see [1] for details. 

Denoting the system matrix of the new system by  [2.x.77]  and the right-hand side by  [2.x.78] , we solve it iteratively with right preconditioning  [2.x.79]  as  [2.x.80] , where [1.x.59] 



with  [2.x.81]  and  [2.x.82]  is the corresponding Schur complement  [2.x.83] . We let  [2.x.84]  where  [2.x.85]  is the pressure mass matrix, then  [2.x.86]  can be approximated by [1.x.60] 



See [1] for details. 

We decompose  [2.x.87]  as [1.x.61] 



Here two inexact solvers will be needed for  [2.x.88]  and  [2.x.89] , respectively (see [1]). Since the pressure mass matrix is symmetric and positive definite, CG with ILU as a preconditioner is appropriate to use for  [2.x.90] . For simplicity, we use the direct solver UMFPACK for  [2.x.91] . The last ingredient is a sparse matrix-vector product with  [2.x.92] . Instead of computing the matrix product in the augmented Lagrangian term in  [2.x.93] , we assemble Grad-Div stabilization  [2.x.94] , as explained in [2]. 

[1.x.62][1.x.63] 


We use the lid driven cavity flow as our test case; see [3] for details. The computational domain is the unit square and the right-hand side is  [2.x.95] . The boundary condition is [1.x.64] 



When solving this problem, the error consists of the nonlinear error (from Newton's iteration) and the discretization error (dependent on mesh size). The nonlinear part decreases with each Newton iteration and the discretization error reduces with mesh refinement. In this example, the solution from the coarse mesh is transferred to successively finer meshes and used as an initial guess. Therefore, the nonlinear error is always brought below the tolerance of Newton's iteration and the discretization error is reduced with each mesh refinement. 

Inside the loop, we involve three solvers: one for  [2.x.96] , one for  [2.x.97]  and one for  [2.x.98] . The first two solvers are invoked in the preconditioner and the outer solver gives us the update term. Overall convergence is controlled by the nonlinear residual; as Newton's method does not require an exact Jacobian, we employ FGMRES with a relative tolerance of only 1e-4 for the outer linear solver. In fact, we use the truncated Newton solve for this system. As described in  [2.x.99] , the inner linear solves are also not required to be done very accurately. Here we use CG with a relative tolerance of 1e-6 for the pressure mass matrix. As expected, we still see convergence of the nonlinear residual down to 1e-14. Also, we use a simple line search algorithm for globalization of the Newton method. 

The cavity reference values for  [2.x.100]  and  [2.x.101]  are from [4] and [5], respectively, where  [2.x.102]  is the Reynolds number and can be located at [8]. Here the viscosity is defined by  [2.x.103] . Even though we can still find a solution for  [2.x.104]  and the references contain results for comparison, we limit our discussion here to  [2.x.105] . This is because the solution is no longer stationary starting around  [2.x.106]  but instead becomes periodic, see [7] for details. 

[1.x.65][1.x.66] 

 [2.x.107]  

   [2.x.108]   An Augmented Lagrangian-Based Approach to the Oseen Problem, M. Benzi and M. Olshanskii, SIAM J. SCI. COMPUT. 2006    [2.x.109]   Efficient augmented Lagrangian-type preconditioning for the Oseen problem using Grad-Div stabilization, Timo Heister and Gerd Rapin    [2.x.110]   http://www.cfd-online.com/Wiki/Lid-driven_cavity_problem    [2.x.111]   High-Re solution for incompressible flow using the Navier-Stokes Equations and a Multigrid Method, U. Ghia, K. N. Ghia, and C. T. Shin    [2.x.112]   Numerical solutions of 2-D steady incompressible driven cavity flow at high Reynolds numbers, E. Erturk, T.C. Corke and C. Gokcol    [2.x.113]  Implicit Weighted ENO Schemes for the Three-Dimensional Incompressible Navier-Stokes Equations, Yang et al, 1998    [2.x.114]  The 2D lid-driven cavity problem revisited, C. Bruneau and M. Saad, 2006    [2.x.115]  https://en.wikipedia.org/wiki/Reynolds_number  [2.x.116]  [1.x.67] [1.x.68] 


[1.x.69]  [1.x.70] 




As usual, we start by including some well-known files: 

[1.x.71] 



To transfer solutions between meshes, this file is included: 

[1.x.72] 



This file includes UMFPACK: the direct solver: 

[1.x.73] 



And the one for ILU preconditioner: 

[1.x.74] 




[1.x.75]  [1.x.76] 




This class manages the matrices and vectors described in the introduction: in particular, we store a BlockVector for the current solution, current Newton update, and the line search update.  We also store two AffineConstraints objects: one which enforces the Dirichlet boundary conditions and one that sets all boundary values to zero. The first constrains the solution vector while the second constraints the updates (i.e., we never update boundary values, so we force the relevant update vector values to be zero). 

[1.x.77] 




[1.x.78]  [1.x.79] 




In this problem we set the velocity along the upper surface of the cavity to be one and zero on the other three walls. The right hand side function is zero so we do not need to set the right hand side function in this tutorial. The number of components of the boundary function is  [2.x.117] . We will ultimately use  [2.x.118]  to set boundary values, which requires the boundary value functions to have the same number of components as the solution, even if all are not used. Put another way: to make this function happy we define boundary values for the pressure even though we will never actually use them. 

[1.x.80] 




[1.x.81]  [1.x.82]    


As discussed in the introduction, the preconditioner in Krylov iterative methods is implemented as a matrix-vector product operator. In practice, the Schur complement preconditioner is decomposed as a product of three matrices (as presented in the first section). The  [2.x.119]  in the first factor involves a solve for the linear system  [2.x.120] . Here we solve this system via a direct solver for simplicity. The computation involved in the second factor is a simple matrix-vector multiplication. The Schur complement  [2.x.121]  can be well approximated by the pressure mass matrix and its inverse can be obtained through an inexact solver. Because the pressure mass matrix is symmetric and positive definite, we can use CG to solve the corresponding linear system. 

[1.x.83] 



We can notice that the initialization of the inverse of the matrix at the top left corner is completed in the constructor. If so, every application of the preconditioner then no longer requires the computation of the matrix factors. 







[1.x.84] 




[1.x.85]  [1.x.86] 

[1.x.87]  [1.x.88]    


The constructor of this class looks very similar to the one in  [2.x.122] . The only difference is the viscosity and the Augmented Lagrangian coefficient  [2.x.123] . 

[1.x.89] 




[1.x.90]  [1.x.91]    


This function initializes the DoFHandler enumerating the degrees of freedom and constraints on the current mesh. 

[1.x.92] 



The first step is to associate DoFs with a given mesh. 

[1.x.93] 



We renumber the components to have all velocity DoFs come before the pressure DoFs to be able to split the solution vector in two blocks which are separately accessed in the block preconditioner. 

[1.x.94] 



In Newton's scheme, we first apply the boundary condition on the solution obtained from the initial step. To make sure the boundary conditions remain satisfied during Newton's iteration, zero boundary conditions are used for the update  [2.x.124] . Therefore we set up two different constraint objects. 

[1.x.95] 




[1.x.96]  [1.x.97]    


On each mesh the SparsityPattern and the size of the linear system are different. This function initializes them after mesh refinement. 

[1.x.98] 




[1.x.99]  [1.x.100]    


This function builds the system matrix and right hand side that we currently work on. The  [2.x.125]  argument is used to determine which set of constraints we apply (nonzero for the initial step and zero for the others). The  [2.x.126]  argument determines whether to assemble the whole system or only the right hand side vector, respectively. 

[1.x.101] 



For the linearized system, we create temporary storage for present velocity and gradient, and present pressure. In practice, they are all obtained through their shape functions at quadrature points. 







[1.x.102] 



The assembly is similar to  [2.x.127] . An additional term with gamma as a coefficient is the Augmented Lagrangian (AL), which is assembled via grad-div stabilization.  As we discussed in the introduction, the bottom right block of the system matrix should be zero. Since the pressure mass matrix is used while creating the preconditioner, we assemble it here and then move it into a separate SparseMatrix at the end (same as in  [2.x.128] ). 

[1.x.103] 



Finally we move pressure mass matrix into a separate matrix: 

[1.x.104] 



Note that settings this pressure block to zero is not identical to not assembling anything in this block, because this operation here will (incorrectly) delete diagonal entries that come in from hanging node constraints for pressure DoFs. This means that our whole system matrix will have rows that are completely zero. Luckily, FGMRES handles these rows without any problem. 

[1.x.105] 




[1.x.106]  [1.x.107]    


In this function, we use FGMRES together with the block preconditioner, which is defined at the beginning of the program, to solve the linear system. What we obtain at this step is the solution vector. If this is the initial step, the solution vector gives us an initial guess for the Navier Stokes equations. For the initial step, nonzero constraints are applied in order to make sure boundary conditions are satisfied. In the following steps, we will solve for the Newton update so zero constraints are used. 

[1.x.108] 




[1.x.109]  [1.x.110]    


After finding a good initial guess on the coarse mesh, we hope to decrease the error through refining the mesh. Here we do adaptive refinement similar to  [2.x.129]  except that we use the Kelly estimator on the velocity only. We also need to transfer the current solution to the next mesh using the SolutionTransfer class. 

[1.x.111] 



First the DoFHandler is set up and constraints are generated. Then we create a temporary BlockVector  [2.x.130] , whose size is according with the solution on the new mesh. 

[1.x.112] 



Transfer solution from coarse to fine mesh and apply boundary value constraints to the new transferred solution. Note that present_solution is still a vector corresponding to the old mesh. 

[1.x.113] 



Finally set up matrix and vectors and set the present_solution to the interpolated data. 

[1.x.114] 




[1.x.115]  [1.x.116]    


This function implements the Newton iteration with given tolerance, maximum number of iterations, and the number of mesh refinements to do.    


The argument  [2.x.131]  tells us whether  [2.x.132]  is necessary, and which part, system matrix or right hand side vector, should be assembled. If we do a line search, the right hand side is already assembled while checking the residual norm in the last iteration. Therefore, we just need to assemble the system matrix at the current iteration. The last argument  [2.x.133]  determines whether or not graphical output should be produced. 

[1.x.117] 



To make sure our solution is getting close to the exact solution, we let the solution be updated with a weight  [2.x.134]  such that the new residual is smaller than the one of last step, which is done in the following loop. This is the same line search algorithm used in  [2.x.135] . 

[1.x.118] 




[1.x.119]  [1.x.120]    


This function will provide us with an initial guess by using a continuation method as we discussed in the introduction. The Reynolds number is increased  [2.x.136] by-step until we reach the target value. By experiment, the solution to Stokes is good enough to be the initial guess of NSE with Reynolds number 1000 so we start there.  To make sure the solution from previous problem is close enough to the next one, the step size must be small enough. 

[1.x.121] 




[1.x.122]  [1.x.123]    


This function is the same as in  [2.x.137]  except that we choose a name for the output file that also contains the Reynolds number (i.e., the inverse of the viscosity in the current context). 

[1.x.124] 




[1.x.125]  [1.x.126]    


In our test case, we do not know the analytical solution. This function outputs the velocity components along  [2.x.138]  and  [2.x.139]  so they can be compared with data from the literature. 

[1.x.127] 




[1.x.128]  [1.x.129]    


This is the last step of this program. In this part, we generate the grid and run the other functions respectively. The max refinement can be set by the argument. 

[1.x.130] 



If the viscosity is smaller than  [2.x.140] , we have to first search for an initial guess via a continuation method. What we should notice is the search is always on the initial mesh, that is the  [2.x.141]  mesh in this program. After that, we just do the same as we did when viscosity is larger than  [2.x.142] : run Newton's iteration, refine the mesh, transfer solutions, and repeat. 

[1.x.131] 



When the viscosity is larger than 1/1000, the solution to Stokes equations is good enough as an initial guess. If so, we do not need to search for the initial guess using a continuation method. Newton's iteration can be started directly. 







[1.x.132] 

[1.x.133][1.x.134] 


Now we use the method we discussed above to solve Navier Stokes equations with viscosity  [2.x.143]  and  [2.x.144] . 

[1.x.135][1.x.136] 


In the first test case the viscosity is set to be  [2.x.145] . As we discussed in the introduction, the initial guess is the solution to the corresponding Stokes problem. In the following table, the residuals at each Newton's iteration on every mesh is shown. The data in the table shows that Newton's iteration converges quadratically. 

 [2.x.146]  








The following figures show the sequence of generated grids. For the case of  [2.x.147] , the initial guess is obtained by solving Stokes on an  [2.x.148]  mesh, and the mesh is refined adaptively. Between meshes, the solution from the coarse mesh is interpolated to the fine mesh to be used as an initial guess. 

 [2.x.149]  

This picture is the graphical streamline result of lid-driven cavity with  [2.x.150] .  [2.x.151]  

Then the solution is compared with a reference solution from [4] and the reference solution data can be found in the file "ref_2d_ghia_u.txt". 

 [2.x.152]  

[1.x.137][1.x.138] 


Newton's iteration requires a good initial guess. However, the nonlinear term dominates when the Reynolds number is large, so that the solution to the Stokes equations may be far away from the exact solution. If the Stokes solution acts as the initial guess, the convergence will be lost. The following picture shows that the nonlinear iteration gets stuck and the residual no longer decreases in further iterations. 

 [2.x.153]  

The initial guess, therefore, has to be obtained via a continuation method which has been discussed in the introduction. Here the step size in the continuation method, that is  [2.x.154] , is 2000 and the initial mesh is of size  [2.x.155] . After obtaining an initial guess, the mesh is refined as in the previous test case. The following picture shows that at each refinement Newton's iteration has quadratic convergence. 52 steps of Newton's iterations are executed for solving this test case. 

 [2.x.156]  

We also show the residual from each step of Newton's iteration on every mesh. The quadratic convergence is clearly visible in the table. 

 [2.x.157]  








The sequence of generated grids looks like this:  [2.x.158]  We compare our solution with reference solution from [5].  [2.x.159]  The following picture presents the graphical result.  [2.x.160]  

Furthermore, the error consists of the nonlinear error, which decreases as we perform Newton iterations, and the discretization error, which depends on the mesh size. That is why we have to refine the mesh and repeat Newton's iteration on the next finer mesh. From the table above, we can see that the residual (nonlinear error) is below 1e-12 on each mesh, but the following picture shows us the difference between solutions on subsequently finer meshes. 

 [2.x.161]  

[1.x.139] 

[1.x.140][1.x.141] 


[1.x.142][1.x.143] 


It is easy to compare the currently implemented linear solver to just using UMFPACK for the whole linear system. You need to remove the nullspace containing the constant pressures and it is done in  [2.x.162] . More interesting is the comparison to other state of the art preconditioners like PCD. It turns out that the preconditioner here is very competitive, as can be seen in the paper [2]. 

The following table shows the timing results between our iterative approach (FGMRES) compared to a direct solver (UMFPACK) for the whole system with viscosity set to 1/400. Even though we use the same direct solver for the velocity block in the iterative solver, it is considerably faster and consumes less memory. This will be even more pronounced in 3d. 

 [2.x.163]  


[1.x.144][1.x.145] 


The code is set up to also run in 3d. Of course the reference values are different, see [6] for example. High resolution computations are not doable with this example as is, because a direct solver for the velocity block does not work well in 3d. Rather, a parallel solver based on algebraic or geometric multigrid is needed. See below. 

[1.x.146][1.x.147] 


For larger computations, especially in 3d, it is necessary to implement MPI parallel solvers and preconditioners. A good starting point would be  [2.x.164] , which uses algebraic multigrid for the velocity block for the Stokes equations. Another option would be to take a look at the list of codes in the [1.x.148], which already contains parallel Navier-Stokes solvers. [1.x.149] [1.x.150]  [2.x.165]  

 [2.x.166] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27] 

 [2.x.4]  

[1.x.28][1.x.29] 

[1.x.30] [1.x.31][1.x.32] 


The [1.x.33] for a function  [2.x.5]  and a potential  [2.x.6]  is a model often used in quantum mechanics and nonlinear optics. If one measures in appropriate quantities (so that  [2.x.7] ), then it reads as follows: 

[1.x.34] 

If there is no potential, i.e.  [2.x.8] , then it can be used to describe the propagation of light in optical fibers. If  [2.x.9] , the equation is also sometimes called the [1.x.35] and can be used to model the time dependent behavior of [1.x.36]. 

For this particular tutorial program, the physical interpretation of the equation is not of much concern to us. Rather, we want to use it as a model that allows us to explain two aspects: 

- It is a [1.x.37] for  [2.x.10] . We have previously seen complex-valued equations in  [2.x.11] ,   but there have opted to split the equations into real and imaginary   parts and consequently ended up solving a system of two real-valued   equations. In contrast, the goal here is to show how to solve   problems in which we keep everything as complex numbers. 

- The equation is a nice model problem to explain how [1.x.38] work. This is because it has terms with   fundamentally different character: on the one hand,  [2.x.12]  is a regular spatial operator in the way we have seen   many times before; on the other hand,  [2.x.13]  has no spatial or temporal derivatives, i.e., it is a purely   local operator. It turns out that we have efficient methods for each   of these terms (in particular, we have analytic solutions for the   latter), and that we may be better off treating these terms   differently and separately. We will explain this in more detail   below. 




[1.x.39][1.x.40] 


At first glance, the equations appear to be parabolic and similar to the heat equation (see  [2.x.14] ) as there is only a single time derivative and two spatial derivatives. But this is misleading. Indeed, that this is not the correct interpretation is more easily seen if we assume for a moment that the potential  [2.x.15]  and  [2.x.16] . Then we have the equation 

[1.x.41] 

If we separate the solution into real and imaginary parts,  [2.x.17] , with  [2.x.18] , then we can split the one equation into its real and imaginary parts in the same way as we did in  [2.x.19] : 

[1.x.42] 

Not surprisingly, the factor  [2.x.20]  in front of the time derivative couples the real and imaginary parts of the equation. If we want to understand this equation further, take the time derivative of one of the equations, say 

[1.x.43] 

(where we have assumed that, at least in some formal sense, we can commute the spatial and temporal derivatives), and then insert the other equation into it: 

[1.x.44] 

This equation is hyperbolic and similar in character to the wave equation. (This will also be obvious if you look at the video in the "Results" section of this program.) Furthermore, we could have arrived at the same equation for  [2.x.21]  as well. Consequently, a better assumption for the NLSE is to think of it as a hyperbolic, wave-propagation equation than as a diffusion equation such as the heat equation. (You may wonder whether it is correct that the operator  [2.x.22]  appears with a positive sign whereas in the wave equation,  [2.x.23]  has a negative sign. This is indeed correct: After multiplying by a test function and integrating by parts, we want to come out with a positive (semi-)definite form. So, from  [2.x.24]  we obtain  [2.x.25] . Likewise, after integrating by parts twice, we obtain from  [2.x.26]  the form  [2.x.27] . In both cases do we get the desired positive sign.) 

The real NLSE, of course, also has the terms  [2.x.28]  and  [2.x.29] . However, these are of lower order in the spatial derivatives, and while they are obviously important, they do not change the character of the equation. 

In any case, the purpose of this discussion is to figure out what time stepping scheme might be appropriate for the equation. The conclusions is that, as a hyperbolic-kind of equation, we need to choose a time step that satisfies a CFL-type condition. If we were to use an explicit method (which we will not), we would have to investigate the eigenvalues of the matrix that corresponds to the spatial operator. If you followed the discussions of the video lectures ( [2.x.30]  then you will remember that the pattern is that one needs to make sure that  [2.x.31]  where  [2.x.32]  is the time step,  [2.x.33]  the mesh width, and  [2.x.34]  are the orders of temporal and spatial derivatives. Whether you take the original equation ( [2.x.35] ) or the reformulation for only the real or imaginary part, the outcome is that we would need to choose  [2.x.36]  if we were to use an explicit time stepping method. This is not feasible for the same reasons as in  [2.x.37]  for the heat equation: It would yield impractically small time steps for even only modestly refined meshes. Rather, we have to use an implicit time stepping method and can then choose a more balanced  [2.x.38] . Indeed, we will use the implicit Crank-Nicolson method as we have already done in  [2.x.39]  before for the regular wave equation. 


[1.x.45][1.x.46] 


 [2.x.40]  

If one thought of the NLSE as an ordinary differential equation in which the right hand side happens to have spatial derivatives, i.e., write it as 

[1.x.47] 

one may be tempted to "formally solve" it by integrating both sides over a time interval  [2.x.41]  and obtain 

[1.x.48] 

Of course, it's not that simple: the  [2.x.42]  in the integrand is still changing over time in accordance with the differential equation, so we cannot just evaluate the integral (or approximate it easily via quadrature) because we don't know  [2.x.43] . But we can write this with separate contributions as follows, and this will allow us to deal with different terms separately: 

[1.x.49] 

The way this equation can now be read is as follows: For each time interval  [2.x.44] , the change  [2.x.45]  in the solution consists of three contributions: 

- The contribution of the Laplace operator. 

- The contribution of the potential  [2.x.46] . 

- The contribution of the "phase" term  [2.x.47] . 

[1.x.50] is now an approximation technique that allows us to treat each of these contributions separately. (If we want: In practice, we will treat the first two together, and the last one separate. But that is a detail, conceptually we could treat all of them differently.) To this end, let us introduce three separate "solutions": 

[1.x.51] 



These three "solutions" can be thought of as satisfying the following differential equations: 

[1.x.52] 

In other words, they are all trajectories  [2.x.48]  that start at  [2.x.49]  and integrate up the effects of exactly one of the three terms. The increments resulting from each of these terms over our time interval are then  [2.x.50] ,  [2.x.51] , and  [2.x.52] . 

It is now reasonable to assume (this is an approximation!) that the change due to all three of the effects in question is well approximated by the sum of the three separate increments: 

[1.x.53] 

This intuition is indeed correct, though the approximation is not exact: the difference between the exact left hand side and the term  [2.x.53]  (i.e., the difference between the [1.x.54] increment for the exact solution  [2.x.54]  when moving from  [2.x.55]  to  [2.x.56] , and the increment composed of the three parts on the right hand side), is proportional to  [2.x.57] . In other words, this approach introduces an error of size  [2.x.58] . Nothing we have done so far has discretized anything in time or space, so the [1.x.55] error is going to be  [2.x.59]  plus whatever error we commit when approximating the integrals (the temporal discretization error) plus whatever error we commit when approximating the spatial dependencies of  [2.x.60]  (the spatial error). 

Before we continue with discussions about operator splitting, let us talk about why one would even want to go this way? The answer is simple: For some of the separate equations for the  [2.x.61] , we may have ways to solve them more efficiently than if we throw everything together and try to solve it at once. For example, and particularly pertinent in the current case: The equation for  [2.x.62] , i.e., 

[1.x.56] 

or equivalently, 

[1.x.57] 

can be solved exactly: the equation is solved by 

[1.x.58] 

This is easy to see if (i) you plug this solution into the differential equation, and (ii) realize that the magnitude  [2.x.63]  is constant, i.e., the term  [2.x.64]  in the exponent is in fact equal to  [2.x.65] . In other words, the solution of the ODE for  [2.x.66]  only changes its [1.x.59], but the [1.x.60] of the complex-valued function  [2.x.67]  remains constant. This makes computing  [2.x.68]  particularly convenient: we don't actually need to solve any ODE, we can write the solution down by hand. Using the operator splitting approach, none of the methods to compute  [2.x.69]  therefore have to deal with the nonlinear term and all of the associated unpleasantries: we can get away with solving only [1.x.61] problems, as long as we allow ourselves the luxury of using an operator splitting approach. 

Secondly, one often uses operator splitting if the different physical effects described by the different terms have different time scales. Imagine, for example, a case where we really did have some sort of diffusion equation. Diffusion acts slowly, but if  [2.x.70]  is large, then the "phase rotation" by the term  [2.x.71]  acts quickly. If we treated everything together, this would imply having to take rather small time steps. But with operator splitting, we can take large time steps  [2.x.72]  for the diffusion, and (assuming we didn't have an analytic solution) use an ODE solver with many small time steps to integrate the "phase rotation" equation for  [2.x.73]  from  [2.x.74]  to  [2.x.75] . In other words, operator splitting allows us to decouple slow and fast time scales and treat them differently, with methods adjusted to each case. 


[1.x.62][1.x.63] 


While the method above allows to compute the three contributions  [2.x.76]  in parallel, if we want, the method can be made slightly more accurate and easy to implement if we don't let the trajectories for the  [2.x.77]  start all at  [2.x.78] , but instead let the trajectory for  [2.x.79]  start at the [1.x.64] of the trajectory for  [2.x.80] , namely  [2.x.81] ; similarly, we will start the trajectory for  [2.x.82]  start at the end point of the trajectory for  [2.x.83] , namely  [2.x.84] . This method is then called "Lie splitting" and has the same order of error as the method above, i.e., the splitting error is  [2.x.85] . 

This variation of operator splitting can be written as follows (carefully compare the initial conditions to the ones above): 

[1.x.65] 

(Obviously, while the formulas above imply that we should solve these problems in this particular order, it is equally valid to first solve for trajectory 3, then 2, then 1, or any other permutation.) 

The integrated forms of these equations are then 

[1.x.66] 

From a practical perspective, this has the advantage that we need to keep around fewer solution vectors: Once  [2.x.86]  has been computed, we don't need  [2.x.87]  any more; once  [2.x.88]  has been computed, we don't need  [2.x.89]  any more. And once  [2.x.90]  has been computed, we can just call it  [2.x.91]  because, if you insert the first into the second, and then into the third equation, you see that the right hand side of  [2.x.92]  now contains the contributions of all three physical effects: 

[1.x.67] 

(Compare this again with the "exact" computation of  [2.x.93] : It only differs in how we approximate  [2.x.94]  in each of the three integrals.) In other words, Lie splitting is a lot simpler to implement that the original method outlined above because data handling is so much simpler. 


[1.x.68][1.x.69] 


As mentioned above, Lie splitting is only  [2.x.95]  accurate. This is acceptable if we were to use a first order time discretization, for example using the explicit or implicit Euler methods to solve the differential equations for  [2.x.96] . This is because these time integration methods introduce an error proportional to  [2.x.97]  themselves, and so the splitting error is proportional to an error that we would introduce anyway, and does not diminish the overall convergence order. 

But we typically want to use something higher order -- say, a [1.x.70] or [1.x.71] method -- since these are often not more expensive than a simple Euler method. It would be a shame if we were to use a time stepping method that is  [2.x.98] , but then lose the accuracy again through the operator splitting. 

This is where the [1.x.72] method comes in. It is easier to explain if we had only two parts, and so let us combine the effects of the Laplace operator and of the potential into one, and the phase rotation into a second effect. (Indeed, this is what we will do in the code since solving the equation with the Laplace equation with or without the potential costs the same -- so we merge these two steps.) The Lie splitting method from above would then do the following: It computes solutions of the following two ODEs, 

[1.x.73] 

and then uses the approximation  [2.x.99] . In other words, we first make one full time step for physical effect one, then one full time step for physical effect two. The solution at the end of the time step is simply the sum of the increments due to each of these physical effects separately. 

In contrast, [1.x.74] (one of the titans of numerical analysis starting in the mid-20th century) figured out that it is more accurate to first do one half-step for one physical effect, then a full time step for the other physical effect, and then another half step for the first. Which one is which does not matter, but because it is so simple to do the phase rotation, we will use this effect for the half steps and then only need to do one spatial solve with the Laplace operator plus potential. This operator splitting method is now  [2.x.100]  accurate. Written in formulas, this yields the following sequence of steps: 

[1.x.75] 

As before, the first and third step can be computed exactly for this particular equation, yielding 

[1.x.76] 



This is then how we are going to implement things in this program: In each time step, we execute three steps, namely 

- Update the solution value at each node by analytically integrating   the phase rotation equation by one half time step; 

- Solving the space-time equation that corresponds to the full step   for  [2.x.101] , namely    [2.x.102] ,   with initial conditions equal to the solution of the first half step   above. 

- Update the solution value at each node by analytically integrating   the phase rotation equation by another half time step. 

This structure will be reflected in an obvious way in the main time loop of the program. 




[1.x.77][1.x.78] 


From the discussion above, it should have become clear that the only partial differential equation we have to solve in each time step is 

[1.x.79] 

This equation is linear. Furthermore, we only have to solve it from  [2.x.103]  to  [2.x.104] , i.e., for exactly one time step. 

To do this, we will apply the second order accurate Crank-Nicolson scheme that we have already used in some of the other time dependent codes (specifically:  [2.x.105]  and  [2.x.106] ). It reads as follows: 

[1.x.80] 

Here, the "previous" solution  [2.x.107]  (or the "initial condition" for this part of the time step) is the output of the first phase rotation half-step; the output of the current step will be denoted by  [2.x.108] .  [2.x.109]  is the length of the time step. (One could argue whether  [2.x.110]  and  [2.x.111]  live at time step  [2.x.112]  or  [2.x.113]  and what their upper indices should be. This is a philosophical discussion without practical impact, and one might think of  [2.x.114]  as something like  [2.x.115] , and  [2.x.116]  as  [2.x.117]  if that helps clarify things -- though, again  [2.x.118]  is not to be understood as "one third time step after  [2.x.119] " but more like "we've already done one third of the work necessary for time step  [2.x.120] ".) 

If we multiply the whole equation with  [2.x.121]  and sort terms with the unknown  [2.x.122]  to the left and those with the known  [2.x.123]  to the right, then we obtain the following (spatial) partial differential equation that needs to be solved in each time step: 

[1.x.81] 






[1.x.82][1.x.83] 


As mentioned above, the previous tutorial program dealing with complex-valued solutions (namely,  [2.x.124] ) separated real and imaginary parts of the solution. It thus reduced everything to real arithmetic. In contrast, we here want to keep things complex-valued. 

The first part of this is that we need to define the discretized solution as  [2.x.125]  where the  [2.x.126]  are the usual shape functions (which are real valued) but the expansion coefficients  [2.x.127]  at time step  [2.x.128]  are now complex-valued. This is easily done in deal.II: We just have to use  [2.x.129]  instead of Vector<double> to store these coefficients. 

Of more interest is how to build and solve the linear system. Obviously, this will only be necessary for the second step of the Strang splitting discussed above, with the time discretization of the previous subsection. We obtain the fully discrete version through straightforward substitution of  [2.x.130]  by  [2.x.131]  and multiplication by a test function: 

[1.x.84] 

or written in a more compact way: 

[1.x.85] 

Here, the matrices are defined in their obvious ways: 

[1.x.86] 

Note that all matrices individually are in fact symmetric, real-valued, and at least positive semidefinite, though the same is obviously not true for the system matrix  [2.x.132]  and the corresponding matrix  [2.x.133]  on the right hand side. 


[1.x.87][1.x.88] 


 [2.x.134]  

The only remaining important question about the solution procedure is how to solve the complex-valued linear system 

[1.x.89] 

with the matrix  [2.x.135]  and a right hand side that is easily computed as the product of a known matrix and the previous part-step's solution. As usual, this comes down to the question of what properties the matrix  [2.x.136]  has. If it is symmetric and positive definite, then we can for example use the Conjugate Gradient method. 

Unfortunately, the matrix's only useful property is that it is complex symmetric, i.e.,  [2.x.137] , as is easy to see by recalling that  [2.x.138]  are all symmetric. It is not, however, [1.x.90], which would require that  [2.x.139]  where the bar indicates complex conjugation. 

Complex symmetry can be exploited for iterative solvers as a quick literature search indicates. We will here not try to become too sophisticated (and indeed leave this to the [1.x.91] section below) and instead simply go with the good old standby for problems without properties: A direct solver. That's not optimal, especially for large problems, but it shall suffice for the purposes of a tutorial program. Fortunately, the SparseDirectUMFPACK class allows solving complex-valued problems. 


[1.x.92][1.x.93] 


Initial conditions for the NLSE are typically chosen to represent particular physical situations. This is beyond the scope of this program, but suffice it to say that these initial conditions are (i) often superpositions of the wave functions of particles located at different points, and that (ii) because  [2.x.140]  corresponds to a particle density function, the integral [1.x.94] corresponds to the number of particles in the system. (Clearly, if one were to be physically correct,  [2.x.141]  better be a constant if the system is closed, or  [2.x.142]  if one has absorbing boundary conditions.) The important point is that one should choose initial conditions so that [1.x.95] makes sense. 

What we will use here, primarily because it makes for good graphics, is the following: [1.x.96] where  [2.x.143]  is the distance from the (fixed) locations  [2.x.144] , and  [2.x.145]  are chosen so that each of the Gaussians that we are adding up adds an integer number of particles to  [2.x.146] . We achieve this by making sure that [1.x.97] is a positive integer. In other words, we need to choose  [2.x.147]  as an integer multiple of [1.x.98] assuming for the moment that  [2.x.148]  -- which is of course not the case, but we'll ignore the small difference in integral. 

Thus, we choose  [2.x.149]  for all, and  [2.x.150] . This  [2.x.151]  is small enough that the difference between the exact (infinite) integral and the integral over  [2.x.152]  should not be too concerning. We choose the four points  [2.x.153]  as  [2.x.154]  -- also far enough away from the boundary of  [2.x.155]  to keep ourselves on the safe side. 

For simplicity, we pose the problem on the square  [2.x.156] . For boundary conditions, we will use time-independent Neumann conditions of the form [1.x.99] This is not a realistic choice of boundary conditions but sufficient for what we want to demonstrate here. We will comment further on this in the [1.x.100] section below. 

Finally, we choose  [2.x.157] , and the potential as [1.x.101] Using a large potential makes sure that the wave function  [2.x.158]  remains small outside the circle of radius 0.7. All of the Gaussians that make up the initial conditions are within this circle, and the solution will mostly oscillate within it, with a small amount of energy radiating into the outside. The use of a large potential also makes sure that the nonphysical boundary condition does not have too large an effect. [1.x.102] [1.x.103] 


[1.x.104]  [1.x.105] The program starts with the usual include files, all of which you should have seen before by now: 

[1.x.106] 



Then the usual placing of all content of this program into a namespace and the importation of the deal.II namespace into the one we will work in: 

[1.x.107] 




[1.x.108]  [1.x.109]    


Then the main class. It looks very much like the corresponding classes in  [2.x.159]  or  [2.x.160] , with the only exception that the matrices and vectors and everything else related to the linear system are now storing elements of type  [2.x.161]  instead of just `double`. 

[1.x.110] 




[1.x.111]  [1.x.112] 




Before we go on filling in the details of the main class, let us define the equation data corresponding to the problem, i.e. initial values, as well as a right hand side class. (We will reuse the initial conditions also for the boundary values, which we simply keep constant.) We do so using classes derived from the Function class template that has been used many times before, so the following should not look surprising. The only point of interest is that we here have a complex-valued problem, so we have to provide the second template argument of the Function class (which would otherwise default to `double`). Furthermore, the return type of the `value()` functions is then of course also complex.    


What precisely these functions return has been discussed at the end of the Introduction section. 

[1.x.113] 




[1.x.114]  [1.x.115] 




We start by specifying the implementation of the constructor of the class. There is nothing of surprise to see here except perhaps that we choose quadratic ( [2.x.162] ) Lagrange elements -- the solution is expected to be smooth, so we choose a higher polynomial degree than the bare minimum. 

[1.x.116] 




[1.x.117]  [1.x.118] 




The next function is the one that sets up the mesh, DoFHandler, and matrices and vectors at the beginning of the program, i.e. before the first time step. The first few lines are pretty much standard if you've read through the tutorial programs at least up to  [2.x.163] : 

[1.x.119] 



Next, we assemble the relevant matrices. The way we have written the Crank-Nicolson discretization of the spatial step of the Strang splitting (i.e., the second of the three partial steps in each time step), we were led to the linear system  [2.x.164] . In other words, there are two matrices in play here -- one for the left and one for the right hand side. We build these matrices separately. (One could avoid building the right hand side matrix and instead just form the *action* of the matrix on  [2.x.165]  in each time step. This may or may not be more efficient, but efficiency is not foremost on our minds for this program.) 

[1.x.120] 




[1.x.121]  [1.x.122] 




Having set up all data structures above, we are now in a position to implement the partial steps that form the Strang splitting scheme. We start with the half-step to advance the phase, and that is used as the first and last part of each time step.    


To this end, recall that for the first half step, we needed to compute  [2.x.166] . Here,  [2.x.167]  and  [2.x.168]  are functions of space and correspond to the output of the previous complete time step and the result of the first of the three part steps, respectively. A corresponding solution must be computed for the third of the part steps, i.e.  [2.x.169] , where  [2.x.170]  is the result of the time step as a whole, and its input  [2.x.171]  is the result of the spatial step of the Strang splitting.    


An important realization is that while  [2.x.172]  may be a finite element function (i.e., is piecewise polynomial), this may not necessarily be the case for the "rotated" function in which we have updated the phase using the exponential factor (recall that the amplitude of that function remains constant as part of that step). In other words, we could *compute*  [2.x.173]  at every point  [2.x.174] , but we can't represent it on a mesh because it is not a piecewise polynomial function. The best we can do in a discrete setting is to compute a projection or interpolation. In other words, we can compute  [2.x.175]  where  [2.x.176]  is a projection or interpolation operator. The situation is particularly simple if we choose the interpolation: Then, all we need to compute is the value of the right hand side *at the node points* and use these as nodal values for the vector  [2.x.177]  of degrees of freedom. This is easily done because evaluating the right hand side at node points for a Lagrange finite element as used here requires us to only look at a single (complex-valued) entry of the node vector. In other words, what we need to do is to compute  [2.x.178]  where  [2.x.179]  loops over all of the entries of our solution vector. This is what the function below does -- in fact, it doesn't even use separate vectors for  [2.x.180]  and  [2.x.181] , but just updates the same vector as appropriate. 

[1.x.123] 



The next step is to solve for the linear system in each time step, i.e., the second half step of the Strang splitting we use. Recall that it had the form  [2.x.182]  where  [2.x.183]  and  [2.x.184]  are the matrices we assembled earlier.    


The way we solve this here is using a direct solver. We first form the right hand side  [2.x.185]  using the  [2.x.186]  function and put the result into the `system_rhs` variable. We then call  [2.x.187]  which takes as argument the matrix  [2.x.188]  and the right hand side vector and returns the solution in the same vector `system_rhs`. The final step is then to put the solution so computed back into the `solution` variable. 

[1.x.124] 




[1.x.125]  [1.x.126] 




The last of the helper functions and classes we ought to discuss are the ones that create graphical output. The result of running the half and full steps for the local and spatial parts of the Strang splitting is that we have updated the `solution` vector  [2.x.189]  to the correct value at the end of each time step. Its entries contain complex numbers for the solution at the nodes of the finite element mesh.    


Complex numbers are not easily visualized. We can output their real and imaginary parts, i.e., the fields  [2.x.190]  and  [2.x.191] , and that is exactly what the DataOut class does when one attaches as complex-valued vector via  [2.x.192]  and then calls  [2.x.193]  That is indeed what we do below. 




But oftentimes we are not particularly interested in real and imaginary parts of the solution vector, but instead in derived quantities such as the magnitude  [2.x.194]  and phase angle  [2.x.195]  of the solution. In the context of quantum systems such as here, the magnitude itself is not so interesting, but instead it is the "amplitude",  [2.x.196]  that is a physical property: it corresponds to the probability density of finding a particle in a particular place of state. The way to put computed quantities into output files for visualization -- as used in numerous previous tutorial programs -- is to use the facilities of the DataPostprocessor and derived classes. Specifically, both the amplitude of a complex number and its phase angles are scalar quantities, and so the DataPostprocessorScalar class is the right tool to base what we want to do on.    


Consequently, what we do here is to implement two classes `ComplexAmplitude` and `ComplexPhase` that compute for each point at which DataOut decides to generate output, the amplitudes  [2.x.197]  and phases  [2.x.198]  of the solution for visualization. There is a fair amount of boiler-plate code below, with the only interesting parts of the first of these two classes being how its `evaluate_vector_field()` function computes the `computed_quantities` object.    


(There is also the rather awkward fact that the [1.x.127] function does not compute what one would naively imagine, namely  [2.x.199] , but returns  [2.x.200]  instead. It's certainly quite confusing to have a standard function mis-named in such a way...) 

[1.x.128] 



The second of these postprocessor classes computes the phase angle of the complex-valued solution at each point. In other words, if we represent  [2.x.201] , then this class computes  [2.x.202] . The function [1.x.129] does this for us, and returns the angle as a real number between  [2.x.203]  and  [2.x.204] .      


For reasons that we will explain in detail in the results section, we do not actually output this value at each location where output is generated. Rather, we take the maximum over all evaluation points of the phase and then fill each evaluation point's output field with this maximum -- in essence, we output the phase angle as a piecewise constant field, where each cell has its own constant value. The reasons for this will become clear once you read through the discussion further down below. 

[1.x.130] 



Having so implemented these post-processors, we create output as we always do. As in many other time-dependent tutorial programs, we attach flags to DataOut that indicate the number of the time step and the current simulation time. 

[1.x.131] 




[1.x.132]  [1.x.133] 




The remaining step is how we set up the overall logic for this program. It's really relatively simple: Set up the data structures; interpolate the initial conditions onto finite element space; then iterate over all time steps, and on each time step perform the three parts of the Strang splitting method. Every tenth time step, we generate graphical output. That's it. 

[1.x.134] 




[1.x.135]  [1.x.136] 




The rest is again boiler plate and exactly as in almost all of the previous tutorial programs: 

[1.x.137] 

[1.x.138][1.x.139] 


Running the code results in screen output like the following: ``` Number of active cells: 4096 Number of degrees of freedom: 16641 

Time step 1 at t=0 Time step 2 at t=0.00390625 Time step 3 at t=0.0078125 Time step 4 at t=0.0117188 [...] ``` Running the program also yields a good number of output files that we will visualize in the following. 


[1.x.140][1.x.141] 


The `output_results()` function of this program generates output files that consist of a number of variables: The solution (split into its real and imaginary parts), the amplitude, and the phase. If we visualize these four fields, we get images like the following after a few time steps (at time  [2.x.205] , to be precise: 

 [2.x.206]  

While the real and imaginary parts of the solution shown above are not particularly interesting (because, from a physical perspective, the global offset of the phase and therefore the balance between real and imaginary components, is meaningless), it is much more interesting to visualize the amplitude  [2.x.207]  and phase  [2.x.208]  of the solution and, in particular, their evolution. This leads to pictures like the following: 

The phase picture shown here clearly has some flaws: 

- First, phase is a "cyclic quantity", but the color scale uses a   fundamentally different color for values close to  [2.x.209]  than   for values close to  [2.x.210] . This is a nuisance -- what we need   is a "cyclic color map" that uses the same colors for the two   extremes of the range of the phase. Such color maps exist,   see [1.x.142] or   [1.x.143], for example. The problem is that the   author's favorite   one of the two big visualization packages, VisIt, does not have any   of these color maps built in. In an act of desperation, I therefore   had to resort to using Paraview given that it has several of the   color maps mentioned in the post above implemented. The picture   below uses the `nic_Edge` map in which both of the extreme values are shown   as black. 

- There is a problem on cells in which the phase wraps around. If   at some evaluation point of the cell the phase value is close to    [2.x.211]  and at another evaluation point it is close to  [2.x.212] , then   what we would really like to happen is for the entire cell to have a   color close to the extremes. But, instead, visualization programs   produce a linear interpolation in which the values within the cell,   i.e., between the evaluation points, is linearly interpolated between   these two values, covering essentially the entire range of possible   phase values and, consequently, cycling through the entire   rainbow of colors from dark red to dark green over the course of   one cell. The solution to this problem is to just output   the phase value on each cell as a piecewise constant. Because   averaging values close to the  [2.x.213]  and  [2.x.214]  is going to   result in an average that has nothing to do with the actual phase   angle, the `ComplexPhase` class just uses the *maximal* phase   angle encountered on each cell. 

With these modifications, the phase plot now looks as follows: 

 [2.x.215]  

Finally, we can generate a movie out of this. (To be precise, the video uses two more global refinement cycles and a time step half the size of what is used in the program above.) The author of these lines made the movie with VisIt, because that's what he's more familiar with, and using a hacked color map that is also cyclic -- though this color map lacks all of the skill employed by the people who wrote the posts mentioned in the links above. It does, however, show the character of the solution as a wave equation if you look at the shaded part of the domain outside the circle of radius 0.7 in which the potential is zero -- you can see how every time one of the bumps (showing the amplitude  [2.x.216] ) bumps into the area where the potential is large: a wave travels outbound from there. Take a look at the video: 

[1.x.144] 



So why did I end up shading the area where the potential  [2.x.217]  is large? In that outside region, the solution is relatively small. It is also relatively smooth. As a consequence, to some approximate degree, the equation in that region simplifies to [1.x.145] or maybe easier to read: [1.x.146] To the degree to which this approximation is valid (which, among other things, eliminates the traveling waves you can see in the video), this equation has a solution [1.x.147] Because  [2.x.218]  is large, this means that the phase *rotates quite rapidly*. If you focus on the semi-transparent outer part of the domain, you can see that. If one colors this region in the same way as the inner part of the domain, this rapidly flashing outer part may be psychedelic, but is also distracting of what's happening on the inside; it's also quite hard to actually see the radiating waves that are easy to see at the beginning of the video. 


[1.x.148] [1.x.149][1.x.150] 


[1.x.151][1.x.152] 


The solver chosen here is just too simple. It is also not efficient. What we do here is give the matrix to a sparse direct solver in every time step and let it find the solution of the linear system. But we know that we could do far better: 

- First, we should make use of the fact that the matrix doesn't   actually change from time step to time step. This is an artifact   of the fact that we here have constant boundary values and that   we don't change the time step size -- two assumptions that might   not be true in actual applications. But at least in cases where this   does happen to be the case, it would make sense to only factorize   the matrix once (i.e., compute  [2.x.219]  and  [2.x.220]  factors once) and then   use these factors for all following time steps until the matrix    [2.x.221]  changes and requires a new factorization. The interface of the   SparseDirectUMFPACK class allows for this. 

- Ultimately, however, sparse direct solvers are only efficient for   relatively small problems, say up to a few 100,000 unknowns. Beyond   this, one needs iterative solvers such as the Conjugate Gradient method (for   symmetric and positive definite problems) or GMRES. We have used many   of these in other tutorial programs. In all cases, they need to be   accompanied by good preconditioners. For the current case, one   could in principle use GMRES -- a method that does not require   any specific properties of the matrix -- but would be better   advised to implement an iterative scheme that exploits the one   structural feature we know is true for this problem: That the matrix   is complex-symmetric (albeit not Hermitian). 


[1.x.153][1.x.154] 


In order to be usable for actual, realistic problems, solvers for the nonlinear Schr&ouml;dinger equation need to utilize boundary conditions that make sense for the problem at hand. We have here restricted ourselves to simple Neumann boundary conditions -- but these do not actually make sense for the problem. Indeed, the equations are generally posed on an infinite domain. But, since we can't compute on infinite domains, we need to truncate it somewhere and instead pose boundary conditions that make sense for this artificially small domain. The approach widely used is to use the [1.x.155] method that corresponds to a particular kind of attenuation. It is, in a different context, also used in  [2.x.222] . 


[1.x.156][1.x.157] 


Finally, we know from experience and many other tutorial programs that it is worthwhile to use adaptively refined meshes, rather than the uniform meshes used here. It would, in fact, not be very difficult to add this here: It just requires periodic remeshing and transfer of the solution from one mesh to the next.  [2.x.223]  will be a good guide for how this could be implemented. [1.x.158] [1.x.159]  [2.x.224]  

 [2.x.225] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15] 

 [2.x.3]  

[1.x.16] 

[1.x.17] [1.x.18][1.x.19] 


Matrix-free operator evaluation enables very efficient implementations of discretization with high-order polynomial bases due to a method called sum factorization. This concept has been introduced in the  [2.x.4]  and  [2.x.5]  tutorial programs. In this tutorial program, we extend those concepts to discontinuous Galerkin (DG) schemes that include face integrals, a class of methods where high orders are particularly widespread. 

The underlying idea of the matrix-free evaluation is the same as for continuous elements: The matrix-vector product that appears in an iterative solver or multigrid smoother is not implemented by a classical sparse matrix kernel, but instead applied implicitly by the evaluation of the underlying integrals on the fly. For tensor product shape functions that are integrated with a tensor product quadrature rule, this evaluation is particularly efficient by using the sum-factorization technique, which decomposes the initially  [2.x.6]  operations for interpolation involving  [2.x.7]  vector entries with associated shape functions at degree  [2.x.8]  in  [2.x.9]  dimensions to  [2.x.10]  quadrature points into  [2.x.11]  one-dimensional operations of cost  [2.x.12]  each. In 3D, this reduces the order of complexity by two powers in  [2.x.13] . When measured as the complexity per degree of freedom, the complexity is  [2.x.14]  in the polynomial degree. Due to the presence of face integrals in DG, and due to the fact that operations on quadrature points involve more memory transfer, which both scale as  [2.x.15] , the observed complexity is often constant for moderate  [2.x.16] . This means that a high order method can be evaluated with the same throughput in terms of degrees of freedom per second as a low-order method. 

More information on the algorithms are available in the preprint  [2.x.17]  [1.x.20] by Martin Kronbichler and Katharina Kormann, arXiv:1711.03590. 

[1.x.21][1.x.22] 


For this tutorial program, we exemplify the matrix-free DG framework for the interior penalty discretization of the Laplacian, i.e., the same scheme as the one used for the  [2.x.18]  tutorial program. The discretization of the Laplacian is given by the following weak form 

[1.x.23] 

where  [2.x.19]  denotes the directed jump of the quantity  [2.x.20]  from the two associated cells  [2.x.21]  and  [2.x.22] , and  [2.x.23]  is the average from both sides. 

The terms in the equation represent the cell integral after integration by parts, the primal consistency term that arises at the element interfaces due to integration by parts and insertion of an average flux, the adjoint consistency term that is added for restoring symmetry of the underlying matrix, and a penalty term with factor  [2.x.24] , whose magnitude is equal the length of the cells in direction normal to face multiplied by  [2.x.25] , see  [2.x.26] . The penalty term is chosen such that an inverse estimate holds and the final weak form is coercive, i.e., positive definite in the discrete setting. The adjoint consistency term and the penalty term involve the jump  [2.x.27]  at the element interfaces, which disappears for the analytic solution  [2.x.28] . Thus, these terms are consistent with the original PDE, ensuring that the method can retain optimal orders of convergence. 

In the implementation below, we implement the weak form above by moving the normal vector  [2.x.29]  from the jump terms to the derivatives to form a [1.x.24] derivative of the form  [2.x.30] . This makes the implementation on quadrature points slightly more efficient because we only need to work with scalar terms rather than tensors, and is mathematically equivalent. 

For boundary conditions, we use the so-called mirror principle that defines [1.x.25] exterior values  [2.x.31]  by extrapolation from the interior solution  [2.x.32]  combined with the given boundary data, setting  [2.x.33]  and  [2.x.34]  on Dirichlet boundaries and  [2.x.35]  and  [2.x.36]  on Neumann boundaries, for given Dirichlet values  [2.x.37]  and Neumann values  [2.x.38] . These expressions are then inserted in the above weak form. Contributions involving the known quantities  [2.x.39]  and  [2.x.40]  are eventually moved to the right hand side, whereas the unknown value  [2.x.41]  is retained on the left hand side and contributes to the matrix terms similarly as interior faces. Upon these manipulations, the same weak form as in  [2.x.42]  is obtained. 

[1.x.26][1.x.27] 


The matrix-free framework of deal.II provides the necessary infrastructure to implement the action of the discretized equation above. As opposed to the  [2.x.43]  that we used in  [2.x.44]  and  [2.x.45] , we now build a code in terms of  [2.x.46]  that takes three function pointers, one for the cell integrals, one for the inner face integrals, and one for the boundary face integrals (in analogy to the design of MeshWorker used in the  [2.x.47]  tutorial program). In each of these three functions, we then implement the respective terms on the quadrature points. For interpolation between the vector entries and the values and gradients on quadrature points, we use the class FEEvaluation for cell contributions and FEFaceEvaluation for face contributions. The basic usage of these functions has been discussed extensively in the  [2.x.48]  tutorial program. 

In  [2.x.49]  all interior faces are visited exactly once, so one must make sure to compute the contributions from both the test functions  [2.x.50]  and  [2.x.51] . Given the fact that the test functions on both sides are indeed independent, the weak form above effectively means that we submit the same contribution to both an FEFaceEvaluation object called `phi_inner` and `phi_outer` for testing with the normal derivative of the test function, and values with opposite sign for testing with the values of the test function, because the latter involves opposite signs due to the jump term. For faces between cells of different refinement level, the integration is done from the refined side, and FEFaceEvaluation automatically performs interpolation to a subface on the coarse side. Thus, a hanging node never appears explicitly in a user implementation of a weak form. 

The fact that each face is visited exactly once also applies to those faces at subdomain boundaries between different processors when parallelized with MPI, where one cell belongs to one processor and one to the other. The setup in  [2.x.52]  splits the faces between the two sides, and eventually only reports the faces actually handled locally in  [2.x.53]  and  [2.x.54]  respectively. Note that, in analogy to the cell integrals discussed in  [2.x.55] , deal.II applies vectorization over several faces to use SIMD, working on something we call a [1.x.28] with a single instruction. The face batches are independent from the cell batches, even though the time at which face integrals are processed is kept close to the time when the cell integrals of the respective cells are processed, in order to increase the data locality. 

Another thing that is new in this program is the fact that we no longer split the vector access like  [2.x.56]  or  [2.x.57]  from the evaluation and integration steps, but call combined functions  [2.x.58]  and  [2.x.59]  respectively. This is useful for face integrals because, depending on what gets evaluated on the faces, not all vector entries of a cell must be touched in the first place. Think for example of the case of the nodal element FE_DGQ with node points on the element surface: If we are interested in the shape function values on a face, only  [2.x.60]  degrees of freedom contribute to them in a non-trivial way (in a more technical way of speaking, only  [2.x.61]  shape functions have a nonzero support on the face and return true for  [2.x.62]  When compared to the  [2.x.63]  degrees of freedom of a cell, this is one power less. 

Now of course we are not interested in only the function values, but also the derivatives on the cell. Fortunately, there is an element in deal.II that extends this property of reduced access also for derivatives on faces, the FE_DGQHermite element. 

[1.x.29][1.x.30] 


The element FE_DGQHermite belongs to the family of FE_DGQ elements, i.e., its shape functions are a tensor product of 1D polynomials and the element is fully discontinuous. As opposed to the nodal character in the usual FE_DGQ element, the FE_DGQHermite element is a mixture of nodal contributions and derivative contributions based on a Hermite-like concept. The underlying polynomial class is  [2.x.64]  and can be summarized as follows: For cubic polynomials, we use two polynomials to represent the function value and first derivative at the left end of the unit interval,  [2.x.65] , and two polynomials to represent the function value and first derivative and the right end of the unit interval,  [2.x.66] . At the opposite ends, both the value and first derivative of the shape functions are zero, ensuring that only two out of the four basis functions contribute to values and derivative on the respective end. However, we deviate from the classical Hermite interpolation in not strictly assigning one degree of freedom for the value and one for the first derivative, but rather allow the first derivative to be a linear combination of the first and the second shape function. This is done to improve the conditioning of the interpolation. Also, when going to degrees beyond three, we add node points in the element interior in a Lagrange-like fashion, combined with double zeros in the points  [2.x.67]  and  [2.x.68] . The position of these extra nodes is determined by the zeros of some Jacobi polynomials as explained in the description of the class  [2.x.69]  

Using this element, we only need to access  [2.x.70]  degrees of freedom for computing both values and derivatives on a face. The check whether the Hermite property is fulfilled is done transparently inside  [2.x.71]  and  [2.x.72]  that check the type of the basis and reduce the access to data if possible. Obviously, this would not be possible if we had separated  [2.x.73]  from  [2.x.74]  because the amount of entries we need to read depends on the type of the derivative (only values, first derivative, etc.) and thus must be given to `read_dof_values()`. 

This optimization is not only useful for computing the face integrals, but also for the MPI ghost layer exchange: In a naive exchange, we would need to send all degrees of freedom of a cell to another processor if the other processor is responsible for computing the face's contribution. Since we know that only some of the degrees of freedom in the evaluation with FEFaceEvaluation are touched, it is natural to only exchange the relevant ones. The  [2.x.75]  function has support for a selected data exchange when combined with  [2.x.76]  To make this happen, we need to tell the loop what kind of evaluation on faces we are going to do, using an argument of type  [2.x.77]  as can be seen in the implementation of  [2.x.78]  below. The way data is exchanged in that case is as follows: The ghost layer data in the vector still pretends to represent all degrees of freedom, such that FEFaceEvaluation can continue to read the values as if the cell were a locally owned one. The data exchange routines take care of the task for packing and unpacking the data into this format. While this sounds pretty complicated, we will show in the results section below that this really pays off by comparing the performance to a baseline code that does not specify the data access on faces. 

[1.x.31][1.x.32] 


In the tradition of the  [2.x.79]  program, we again solve a Poisson problem with a geometric multigrid preconditioner inside a conjugate gradient solver. Instead of computing the diagonal and use the basic PreconditionChebyshev as a smoother, we choose a different strategy in this tutorial program. We implement a block-Jacobi preconditioner, where a block refers to all degrees of freedom on a cell. Rather than building the full cell matrix and applying its LU factorization (or inverse) in the preconditioner &mdash; an operation that would be heavily memory bandwidth bound and thus pretty slow &mdash; we approximate the inverse of the block by a special technique called fast diagonalization method. 

The idea of the method is to take use of the structure of the cell matrix. In case of the Laplacian with constant coefficients discretized on a Cartesian mesh, the cell matrix  [2.x.80]  can be written as 

[1.x.33] 

in 2D and 

[1.x.34] 

in 3D. The matrices  [2.x.81]  and  [2.x.82]  denote the 1D Laplace matrix (including the cell and face term associated to the current cell values  [2.x.83]  and  [2.x.84] ) and  [2.x.85]  and  [2.x.86]  are the mass matrices. Note that this simple tensor product structure is lost once there are non-constant coefficients on the cell or the geometry is not constant any more. We mention that a similar setup could also be used to replace the computed integrals with this final tensor product form of the matrices, which would cut the operations for the operator evaluation into less than half. However, given the fact that this only holds for Cartesian cells and constant coefficients, which is a pretty narrow case, we refrain from pursuing this idea. 

Interestingly, the exact inverse of the matrix  [2.x.87]  can be found through tensor products due to a method introduced by [1.x.35] from 1964, 

[1.x.36] 

where  [2.x.88]  is the matrix of eigenvectors to the generalized eigenvalue problem in the given tensor direction  [2.x.89] : 

[1.x.37] 

and  [2.x.90]  is the diagonal matrix representing the generalized eigenvalues  [2.x.91] . Note that the vectors  [2.x.92]  are such that they simultaneously diagonalize  [2.x.93]  and  [2.x.94] , i.e.  [2.x.95]  and  [2.x.96] . 

The deal.II library implements a class using this concept, called TensorProductMatrixSymmetricSum. 

For the sake of this program, we stick with constant coefficients and Cartesian meshes, even though an approximate version based on tensor products would still be possible for a more general mesh, and the operator evaluation itself is of course generic. Also, we do not bother with adaptive meshes where the multigrid algorithm would need to get access to flux matrices over the edges of different refinement, as explained in  [2.x.97] . One thing we do, however, is to still wrap our block-Jacobi preconditioner inside PreconditionChebyshev. That class relieves us from finding an appropriate relaxation parameter (which would be around 0.7 in 2D and 0.5 in 3D for the block-Jacobi smoother), and often increases smoothing efficiency a bit over plain Jacobi smoothing in that it enables lower the time to solution when setting the degree of the Chebyshev polynomial to one or two. 

Note that the block-Jacobi smoother has an additional benefit: The fast diagonalization method can also be interpreted as a change from the Hermite-like polynomials underlying FE_DGQHermite to a basis where the cell Laplacian is diagonal. Thus, it cancels the effect of the basis, and we get the same iteration counts irrespective of whether we use FE_DGQHermite or FE_DGQ. This is in contrast to using the PreconditionChebyshev class with only the diagonal (a point-Jacobi scheme), where FE_DGQ and FE_DGQHermite do indeed behave differently and FE_DGQ needs 2-5 less iterations than FE_DGQHermite, despite the modification made to the Hermite-like shape functions to ensure a good conditioning. [1.x.38] [1.x.39] 

The include files are essentially the same as in  [2.x.98] , with the exception of the finite element class FE_DGQHermite instead of FE_Q. All functionality for matrix-free computations on face integrals is already contained in `fe_evaluation.h`. 

[1.x.40] 



As in  [2.x.99] , we collect the dimension and polynomial degree as constants here at the top of the program for simplicity. As opposed to  [2.x.100] , we choose a really high order method this time with degree 8 where any implementation not using sum factorization would become prohibitively slow compared to the implementation with MatrixFree which provides an efficiency that is essentially the same as at degrees two or three. Furthermore, all classes in this tutorial program are templated, so it would be easy to select the degree at run time from an input file or a command-line argument by adding instantiations of the appropriate degrees in the `main()` function. 







[1.x.41] 




[1.x.42]  [1.x.43] 




In analogy to  [2.x.101] , we define an analytic solution that we try to reproduce with our discretization. Since the aim of this tutorial is to show matrix-free methods, we choose one of the simplest possibilities, namely a cosine function whose derivatives are simple enough for us to compute analytically. Further down, the wave number 2.4 we select here will be matched with the domain extent in  [2.x.102] -direction that is 2.5, such that we obtain a periodic solution at  [2.x.103]  including  [2.x.104]  or three full wave revolutions in the cosine. The first function defines the solution and its gradient for expressing the analytic solution for the Dirichlet and Neumann boundary conditions, respectively. Furthermore, a class representing the negative Laplacian of the solution is used to represent the right hand side (forcing) function that we use to match the given analytic solution in the discretized version (manufactured solution). 







[1.x.44] 




[1.x.45]  [1.x.46] 




The `LaplaceOperator` class is similar to the respective class in  [2.x.105] . A significant difference is that we do not derive the class from  [2.x.106]  because we want to present some additional features of  [2.x.107]  that are not available in the general-purpose class  [2.x.108]  We derive the class from the Subscriptor class to be able to use the operator within the Chebyshev preconditioner because that preconditioner stores the underlying matrix via a SmartPointer.    


Given that we implement a complete matrix interface by hand, we need to add an `initialize()` function, an `m()` function, a `vmult()` function, and a `Tvmult()` function that were previously provided by  [2.x.109]  Our LaplaceOperator also contains a member function `get_penalty_factor()` that centralizes the selection of the penalty parameter in the symmetric interior penalty method according to  [2.x.110] . 







[1.x.47] 



The `%PreconditionBlockJacobi` class defines our custom preconditioner for this problem. As opposed to  [2.x.111]  which was based on the matrix diagonal, we here compute an approximate inversion of the diagonal blocks in the discontinuous Galerkin method by using the so-called fast diagonalization method discussed in the introduction. 







[1.x.48] 



This free-standing function is used in both the `LaplaceOperator` and `%PreconditionBlockJacobi` classes to adjust the ghost range. This function is necessary because some of the vectors that the `vmult()` functions are supplied with are not initialized properly with  [2.x.112]  that includes the correct layout of ghost entries, but instead comes from the MGTransferMatrixFree class that has no notion on the ghost selection of the matrix-free classes. To avoid index confusion, we must adjust the ghost range before actually doing something with these vectors. Since the vectors are kept around in the multigrid smoother and transfer classes, a vector whose ghost range has once been adjusted will remain in this state throughout the lifetime of the object, so we can use a shortcut at the start of the function to see whether the partitioner object of the distributed vector, which is stored as a shared pointer, is the same as the layout expected by MatrixFree, which is stored in a data structure accessed by  [2.x.113]  where the 0 indicates the DoFHandler number from which this was extracted; we only use a single DoFHandler in MatrixFree, so the only valid number is 0 here. 







[1.x.49] 



The next five functions to clear and initialize the `LaplaceOperator` class, to return the shared pointer holding the MatrixFree data container, as well as the correct initialization of the vector and operator sizes are the same as in  [2.x.114]  or rather  [2.x.115]  

[1.x.50] 



This function implements the action of the LaplaceOperator on a vector `src` and stores the result in the vector `dst`. When compared to  [2.x.116] , there are four new features present in this call.    


The first new feature is the `adjust_ghost_range_if_necessary` function mentioned above that is needed to fit the vectors to the layout expected by FEEvaluation and FEFaceEvaluation in the cell and face functions.    


The second new feature is the fact that we do not implement a `vmult_add()` function as we did in  [2.x.117]  (through the virtual function  [2.x.118]  but directly implement a `vmult()` functionality. Since both cell and face integrals will sum into the destination vector, we must of course zero the vector somewhere. For DG elements, we are given two options &ndash; one is to use  [2.x.119]  instead of  [2.x.120]  in the `apply_cell` function below. This works because the loop layout in MatrixFree is such that cell integrals always touch a given vector entry before the face integrals. However, this really only works for fully discontinuous bases where every cell has its own degrees of freedom, without any sharing with neighboring results. An alternative setup, the one chosen here, is to let the  [2.x.121]  take care of zeroing the vector. This can be thought of as simply calling `dst = 0;` somewhere in the code. The implementation is more involved for supported vectors such as  [2.x.122]  because we aim to not zero the whole vector at once. Doing the zero operation on a small enough pieces of a few thousands of vector entries has the advantage that the vector entries that get zeroed remain in caches before they are accessed again in  [2.x.123]  and  [2.x.124]  Since matrix-free operator evaluation is really fast, just zeroing a large vector can amount to up to a 25% of the operator evaluation time, and we obviously want to avoid this cost. This option of zeroing the vector is also available for  [2.x.125]  and for continuous bases, even though it was not used in the  [2.x.126]  or  [2.x.127]  tutorial programs.    


The third new feature is the way we provide the functions to compute on cells, inner faces, and boundary faces: The class MatrixFree has a function called `loop` that takes three function pointers to the three cases, allowing to separate the implementations of different things. As explained in  [2.x.128] , these function pointers can be  [2.x.129]  objects or member functions of a class. In this case, we use pointers to member functions.    


The final new feature are the last two arguments of type  [2.x.130]  that can be given to  [2.x.131]  This class passes the type of data access for face integrals to the MPI data exchange routines  [2.x.132]  and  [2.x.133]  of the parallel vectors. The purpose is to not send all degrees of freedom of a neighboring element, but to reduce the amount of data to what is really needed for the computations at hand. The data exchange is a real bottleneck in particular for high-degree DG methods, therefore a more restrictive way of exchange is often beneficial. The enum field  [2.x.134]  can take the value `none`, which means that no face integrals at all are done, which would be analogous to  [2.x.135]  the value `values` meaning that only shape function values (but no derivatives) are used on faces, and the value `gradients` when also first derivatives on faces are accessed besides the values. A value `unspecified` means that all degrees of freedom will be exchanged for the faces that are located at the processor boundaries and designated to be worked on at the local processor.    


To see how the data can be reduced, think of the case of the nodal element FE_DGQ with node points on the element surface, where only  [2.x.136]  degrees of freedom contribute to the values on a face for polynomial degree  [2.x.137]  in  [2.x.138]  space dimensions, out of the  [2.x.139]  degrees of freedom of a cell. A similar reduction is also possible for the interior penalty method that evaluates values and first derivatives on the faces. When using a Hermite-like basis in 1D, only up to two basis functions contribute to the value and derivative. The class FE_DGQHermite implements a tensor product of this concept, as discussed in the introduction. Thus, only  [2.x.140]  degrees of freedom must be exchanged for each face, which is a clear win once  [2.x.141]  gets larger than four or five. Note that this reduced exchange of FE_DGQHermite is valid also on meshes with curved boundaries, as the derivatives are taken on the reference element, whereas the geometry only mixes them on the inside. Thus, this is different from the attempt to obtain  [2.x.142]  continuity with continuous Hermite-type shape functions where the non-Cartesian case changes the picture significantly. Obviously, on non-Cartesian meshes the derivatives also include tangential derivatives of shape functions beyond the normal derivative, but those only need the function values on the element surface, too. Should the element not provide any compression, the loop automatically exchanges all entries for the affected cells. 







[1.x.51] 



Since the Laplacian is symmetric, the `Tvmult()` (needed by the multigrid smoother interfaces) operation is simply forwarded to the `vmult()` case. 







[1.x.52] 



The cell operation is very similar to  [2.x.143] . We do not use a coefficient here, though. The second difference is that we replaced the two steps of  [2.x.144]  followed by  [2.x.145]  by a single function call  [2.x.146]  which internally calls the sequence of the two individual methods. Likewise,  [2.x.147]  implements the sequence of  [2.x.148]  followed by  [2.x.149]  In this case, these new functions merely save two lines of code. However, we use them for the analogy with FEFaceEvaluation where they are more important as explained below. 







[1.x.53] 



The face operation implements the terms of the interior penalty method in analogy to  [2.x.150] , as explained in the introduction. We need two evaluator objects for this task, one for handling the solution that comes from the cell on one of the two sides of an interior face, and one for handling the solution from the other side. The evaluators for face integrals are called FEFaceEvaluation and take a boolean argument in the second slot of the constructor to indicate which of the two sides the evaluator should belong two. In FEFaceEvaluation and MatrixFree, we call one of the two sides the `interior` one and the other the `exterior` one. The name `exterior` refers to the fact that the evaluator from both sides will return the same normal vector. For the `interior` side, the normal vector points outwards, whereas it points inwards on the other side, and is opposed to the outer normal vector of that cell. Apart from the new class name, we again get a range of items to work with in analogy to what was discussed in  [2.x.151] , but for the interior faces in this case. Note that the data structure of MatrixFree forms batches of faces that are analogous to the batches of cells for the cell integrals. All faces within a batch involve different cell numbers but have the face number within the reference cell, have the same refinement configuration (no refinement or the same subface), and the same orientation, to keep SIMD operations simple and efficient.    


Note that there is no implied meaning in interior versus exterior except the logic decision of the orientation of the normal, which is pretty random internally. One can in no way rely on a certain pattern of assigning interior versus exterior flags, as the decision is made for the sake of access regularity and uniformity in the MatrixFree setup routines. Since most sane DG methods are conservative, i.e., fluxes look the same from both sides of an interface, the mathematics are unaltered if the interior/exterior flags are switched and normal vectors get the opposite sign. 







[1.x.54] 



On a given batch of faces, we first update the pointers to the current face and then access the vector. As mentioned above, we combine the vector access with the evaluation. In the case of face integrals, the data access into the vector can be reduced for the special case of an FE_DGQHermite basis as explained for the data exchange above: Since only  [2.x.152]  out of the  [2.x.153]  cell degrees of freedom get multiplied by a non-zero value or derivative of a shape function, this structure can be utilized for the evaluation, significantly reducing the data access. The reduction of the data access is not only beneficial because it reduces the data in flight and thus helps caching, but also because the data access to faces is often more irregular than for cell integrals when gathering values from cells that are farther apart in the index list of cells. 

[1.x.55] 



The next two statements compute the penalty parameter for the interior penalty method. As explained in the introduction, we would like to have a scaling like  [2.x.154]  of the length  [2.x.155]  normal to the face. For a general non-Cartesian mesh, this length must be computed by the product of the inverse Jacobian times the normal vector in real coordinates. From this vector of `dim` components, we must finally pick the component that is oriented normal to the reference cell. In the geometry data stored in MatrixFree, a permutation of the components in the Jacobian is applied such that this latter direction is always the last component `dim-1` (this is beneficial because reference-cell derivative sorting can be made agnostic of the direction of the face). This means that we can simply access the last entry `dim-1` and must not look up the local face number in `data.get_face_info(face).interior_face_no` and `data.get_face_info(face).exterior_face_no`. Finally, we must also take the absolute value of these factors as the normal could point into either positive or negative direction. 

[1.x.56] 



In the loop over the quadrature points, we eventually compute all contributions to the interior penalty scheme. According to the formulas in the introduction, the value of the test function gets multiplied by the difference of the jump in the solution times the penalty parameter and the average of the normal derivative in real space. Since the two evaluators for interior and exterior sides get different signs due to the jump, we pass the result with a different sign here. The normal derivative of the test function gets multiplied by the negative jump in the solution between the interior and exterior side. This term, coined adjoint consistency term, must also include the factor of  [2.x.156]  in the code in accordance with its relation to the primal consistency term that gets the factor of one half due to the average in the test function slot. 

[1.x.57] 



Once we are done with the loop over quadrature points, we can do the sum factorization operations for the integration loops on faces and sum the results into the result vector, using the `integrate_scatter` function. The name `scatter` reflects the distribution of the vector data into scattered positions in the vector using the same pattern as in `gather_evaluate`. Like before, the combined integrate + write operation allows us to reduce the data access. 

[1.x.58] 



The boundary face function follows by and large the interior face function. The only difference is the fact that we do not have a separate FEFaceEvaluation object that provides us with exterior values  [2.x.157] , but we must define them from the boundary conditions and interior values  [2.x.158] . As explained in the introduction, we use  [2.x.159]  and  [2.x.160]  on Dirichlet boundaries and  [2.x.161]  and  [2.x.162]  on Neumann boundaries. Since this operation implements the homogeneous part, i.e., the matrix-vector product, we must neglect the boundary functions  [2.x.163]  and  [2.x.164]  here, and added them to the right hand side in  [2.x.165]  Note that due to extension of the solution  [2.x.166]  to the exterior via  [2.x.167] , we can keep all factors  [2.x.168]  the same as in the inner face function, see also the discussion in  [2.x.169] .    


There is one catch at this point: The implementation below uses a boolean variable `is_dirichlet` to switch between the Dirichlet and the Neumann cases. However, we solve a problem where we also want to impose periodic boundary conditions on some boundaries, namely along those in the  [2.x.170]  direction. One might wonder how those conditions should be handled here. The answer is that MatrixFree automatically treats periodic boundaries as what they are technically, namely an inner face where the solution values of two adjacent cells meet and must be treated by proper numerical fluxes. Thus, all the faces on the periodic boundaries will appear in the `apply_face()` function and not in this one. 







[1.x.59] 



Next we turn to the preconditioner initialization. As explained in the introduction, we want to construct an (approximate) inverse of the cell matrices from a product of 1D mass and Laplace matrices. Our first task is to compute the 1D matrices, which we do by first creating a 1D finite element. Instead of anticipating FE_DGQHermite<1> here, we get the finite element's name from DoFHandler, replace the  [2.x.171]  argument (2 or 3) by 1 to create a 1D name, and construct the 1D element by using FETools. 







[1.x.60] 



As for computing the 1D matrices on the unit element, we simply write down what a typical assembly procedure over rows and columns of the matrix as well as the quadrature points would do. We select the same Laplace matrices once and for all using the coefficients 0.5 for interior faces (but possibly scaled differently in different directions as a result of the mesh). Thus, we make a slight mistake at the Dirichlet boundary (where the correct factor would be 1 for the derivative terms and 2 for the penalty term, see  [2.x.172] ) or at the Neumann boundary where the factor should be zero. Since we only use this class as a smoother inside a multigrid scheme, this error is not going to have any significant effect and merely affects smoothing quality. 

[1.x.61] 



The left and right boundary terms assembled by the next two statements appear to have somewhat arbitrary signs, but those are correct as can be verified by looking at  [2.x.173]  and inserting the value -1 and 1 for the normal vector in the 1D case. 

[1.x.62] 



Next, we go through the cells and pass the scaled matrices to TensorProductMatrixSymmetricSum to actually compute the generalized eigenvalue problem for representing the inverse: Since the matrix approximation is constructed as  [2.x.174]  and the weights are constant for each element, we can apply all weights on the Laplace matrix and simply keep the mass matrices unscaled. In the loop over cells, we want to make use of the geometry compression provided by the MatrixFree class and check if the current geometry is the same as on the last cell batch, in which case there is nothing to do. This compression can be accessed by  [2.x.175]  once `reinit()` has been called.      


Once we have accessed the inverse Jacobian through the FEEvaluation access function (we take the one for the zeroth quadrature point as they should be the same on all quadrature points for a Cartesian cell), we check that it is diagonal and then extract the determinant of the original Jacobian, i.e., the inverse of the determinant of the inverse Jacobian, and set the weight as  [2.x.176]  according to the 1D Laplacian times  [2.x.177]  copies of the mass matrix. 

[1.x.63] 



Once we know the factor by which we should scale the Laplace matrix, we apply this weight to the unscaled DG Laplace matrix and send the array to the class TensorProductMatrixSymmetricSum for computing the generalized eigenvalue problem mentioned in the introduction. 







[1.x.64] 



The vmult function for the approximate block-Jacobi preconditioner is very simple in the DG context: We simply need to read the values of the current cell batch, apply the inverse for the given entry in the array of tensor product matrix, and write the result back. In this loop, we overwrite the content in `dst` rather than first setting the entries to zero. This is legitimate for a DG method because every cell has independent degrees of freedom. Furthermore, we manually write out the loop over all cell batches, rather than going through  [2.x.178]  We do this because we know that we are not going to need data exchange over the MPI network here as all computations are done on the cells held locally on each processor. 







[1.x.65] 



The definition of the LaplaceProblem class is very similar to  [2.x.179] . One difference is the fact that we add the element degree as a template argument to the class, which would allow us to more easily include more than one degree in the same program by creating different instances in the `main()` function. The second difference is the selection of the element, FE_DGQHermite, which is specialized for this kind of equations. 







[1.x.66] 



The setup function differs in two aspects from  [2.x.180] . The first is that we do not need to interpolate any constraints for the discontinuous ansatz space, and simply pass a dummy AffineConstraints object into  [2.x.181]  The second change arises because we need to tell MatrixFree to also initialize the data structures for faces. We do this by setting update flags for the inner and boundary faces, respectively. On the boundary faces, we need both the function values, their gradients, JxW values (for integration), the normal vectors, and quadrature points (for the evaluation of the boundary conditions), whereas we only need shape function values, gradients, JxW values, and normal vectors for interior faces. The face data structures in MatrixFree are always built as soon as one of `mapping_update_flags_inner_faces` or `mapping_update_flags_boundary_faces` are different from the default value `update_default` of UpdateFlags. 







[1.x.67] 



The computation of the right hand side is a bit more complicated than in  [2.x.182] . The cell term now consists of the negative Laplacian of the analytical solution, `RightHandSide`, for which we need to first split up the Point of VectorizedArray fields, i.e., a batch of points, into a single point by evaluating all lanes in the VectorizedArray separately. Remember that the number of lanes depends on the hardware; it could be 1 for systems that do not offer vectorization (or where deal.II does not have intrinsics), but it could also be 8 or 16 on AVX-512 of recent Intel architectures. 

[1.x.68] 



Secondly, we also need to apply the Dirichlet and Neumann boundary conditions. This function is the missing part of to the function  [2.x.183]  function once the exterior solution values  [2.x.184]  and  [2.x.185]  on Dirichlet boundaries and  [2.x.186]  and  [2.x.187]  on Neumann boundaries are inserted and expanded in terms of the boundary functions  [2.x.188]  and  [2.x.189] . One thing to remember is that we move the boundary conditions to the right hand side, so the sign is the opposite from what we imposed on the solution part.      


We could have issued both the cell and the boundary part through a  [2.x.190]  part, but we choose to manually write the full loop over all faces to learn how the index layout of face indices is set up in MatrixFree: Both the inner faces and the boundary faces share the index range, and all batches of inner faces have lower numbers than the batches of boundary cells. A single index for both variants allows us to easily use the same data structure FEFaceEvaluation for both cases that attaches to the same data field, just at different positions. The number of inner face batches (where a batch is due to the combination of several faces into one for vectorization) is given by  [2.x.191]  whereas the number of boundary face batches is given by  [2.x.192]  

[1.x.69] 



The MatrixFree class lets us query the boundary_id of the current face batch. Remember that MatrixFree sets up the batches for vectorization such that all faces within a batch have the same properties, which includes their `boundary_id`. Thus, we can query that id here for the current face index `face` and either impose the Dirichlet case (where we add something to the function value) or the Neumann case (where we add something to the normal derivative). 

[1.x.70] 



Since we have manually run the loop over cells rather than using  [2.x.193]  we must not forget to perform the data exchange with MPI - or actually, we would not need that for DG elements here because each cell carries its own degrees of freedom and cell and boundary integrals only evaluate quantities on the locally owned cells. The coupling to neighboring subdomain only comes in by the inner face integrals, which we have not done here. That said, it does not hurt to call this function here, so we do it as a reminder of what happens inside  [2.x.194]  

[1.x.71] 



The `solve()` function is copied almost verbatim from  [2.x.195] . We set up the same multigrid ingredients, namely the level transfer, a smoother, and a coarse grid solver. The only difference is the fact that we do not use the diagonal of the Laplacian for the preconditioner of the Chebyshev iteration used for smoothing, but instead our newly resolved class `%PreconditionBlockJacobi`. The mechanisms are the same, though. 

[1.x.72] 



Since we have solved a problem with analytic solution, we want to verify the correctness of our implementation by computing the L2 error of the numerical result against the analytic solution. 







[1.x.73] 



The `run()` function sets up the initial grid and then runs the multigrid program in the usual way. As a domain, we choose a rectangle with periodic boundary conditions in the  [2.x.196] -direction, a Dirichlet condition on the front face in  [2.x.197]  direction (i.e., the face with index number 2, with boundary id equal to 0), and Neumann conditions on the back face as well as the two faces in  [2.x.198]  direction for the 3D case (with boundary id equal to 1). The extent of the domain is a bit different in the  [2.x.199]  direction (where we want to achieve a periodic solution given the definition of `Solution`) as compared to the  [2.x.200]  and  [2.x.201]  directions. 







[1.x.74] 



There is nothing unexpected in the `main()` function. We call `MPI_Init()` through the `MPI_InitFinalize` class, pass on the two parameters on the dimension and the degree set at the top of the file, and run the Laplace problem. 







[1.x.75] 

[1.x.76][1.x.77] 


[1.x.78][1.x.79] 


Like in  [2.x.202] , we evaluate the multigrid solver in terms of run time.  In two space dimensions with elements of degree 8, a possible output could look as follows: 

[1.x.80] 



Like in  [2.x.203] , the number of CG iterations remains constant with increasing problem size. The iteration counts are a bit higher, which is because we use a lower degree of the Chebyshev polynomial (2 vs 5 in  [2.x.204] ) and because the interior penalty discretization has a somewhat larger spread in eigenvalues. Nonetheless, 13 iterations to reduce the residual by 12 orders of magnitude, or almost a factor of 9 per iteration, indicates an overall very efficient method. In particular, we can solve a system with 21 million degrees of freedom in 5 seconds when using 12 cores, which is a very good efficiency. Of course, in 2D we are well inside the regime of roundoff for a polynomial degree of 8 &ndash; as a matter of fact, around 83k DoFs or 0.025s would have been enough to fully converge this (simple) analytic solution here. 

Not much changes if we run the program in three spatial dimensions, except for the fact that we now use do something more useful with the higher polynomial degree and increasing mesh sizes, as the roundoff errors are only obtained at the finest mesh. Still, it is remarkable that we can solve a 3D Laplace problem with a wave of three periods to roundoff accuracy on a twelve-core machine pretty easily - using about 3.5 GB of memory in total for the second to largest case with 24m DoFs, taking not more than eight seconds. The largest case uses 30GB of memory with 191m DoFs. 

[1.x.81] 



[1.x.82][1.x.83] 


In the introduction and in-code comments, it was mentioned several times that high orders are treated very efficiently with the FEEvaluation and FEFaceEvaluation evaluators. Now, we want to substantiate these claims by looking at the throughput of the 3D multigrid solver for various polynomial degrees. We collect the times as follows: We first run a solver at problem size close to ten million, indicated in the first four table rows, and record the timings. Then, we normalize the throughput by recording the number of million degrees of freedom solved per second (MDoFs/s) to be able to compare the efficiency of the different degrees, which is computed by dividing the number of degrees of freedom by the solver time. 

 [2.x.205]  

We clearly see how the efficiency per DoF initially improves until it reaches a maximum for the polynomial degree  [2.x.206] . This effect is surprising, not only because higher polynomial degrees often yield a vastly better solution, but especially also when having matrix-based schemes in mind where the denser coupling at higher degree leads to a monotonously decreasing throughput (and a drastic one in 3D, with  [2.x.207]  being more than ten times slower than  [2.x.208] !). For higher degrees, the throughput decreases a bit, which is both due to an increase in the number of iterations (going from 12 at  [2.x.209]  to 19 at  [2.x.210] ) and due to the  [2.x.211]  complexity of operator evaluation. Nonetheless, efficiency as the time to solution would be still better for higher polynomial degrees because they have better convergence rates (at least for problems as simple as this one): For  [2.x.212] , we reach roundoff accuracy already with 1 million DoFs (solver time less than a second), whereas for  [2.x.213]  we need 24 million DoFs and 8 seconds. For  [2.x.214] , the error is around  [2.x.215]  with 57m DoFs and thus still far away from roundoff, despite taking 16 seconds. 

Note that the above numbers are a bit pessimistic because they include the time it takes the Chebyshev smoother to compute an eigenvalue estimate, which is around 10 percent of the solver time. If the system is solved several times (as e.g. common in fluid dynamics), this eigenvalue cost is only paid once and faster times become available. 

[1.x.84][1.x.85] 


Finally, we take a look at some of the special ingredients presented in this tutorial program, namely the FE_DGQHermite basis in particular and the specification of  [2.x.216]  In the following table, the third row shows the optimized solver above, the fourth row shows the timings with only the  [2.x.217]  set to `unspecified` rather than the optimal `gradients`, and the last one with replacing FE_DGQHermite by the basic FE_DGQ elements where both the MPI exchange are more expensive and the operations done by  [2.x.218]  and  [2.x.219]  

 [2.x.220]  

The data in the table shows that not using  [2.x.221]  increases costs by around 10% for higher polynomial degrees. For lower degrees, the difference is obviously less pronounced because the volume-to-surface ratio is more beneficial and less data needs to be exchanged. The difference is larger when looking at the matrix-vector product only, rather than the full multigrid solver shown here, with around 20% worse timings just because of the MPI communication. 

For  [2.x.222]  and  [2.x.223] , the Hermite-like basis functions do obviously not really pay off (indeed, for  [2.x.224]  the polynomials are exactly the same as for FE_DGQ) and the results are similar as with the FE_DGQ basis. However, for degrees starting at three, we see an increasing advantage for FE_DGQHermite, showing the effectiveness of these basis functions. 

[1.x.86][1.x.87] 


As mentioned in the introduction, the fast diagonalization method is tied to a Cartesian mesh with constant coefficients. If we wanted to solve variable-coefficient problems, we would need to invest a bit more time in the design of the smoother parameters by selecting proper generalizations (e.g., approximating the inverse on the nearest box-shaped element). 

Another way of extending the program would be to include support for adaptive meshes, for which interface operations at edges of different refinement level become necessary, as discussed in  [2.x.225] . [1.x.88] [1.x.89]  [2.x.226]  

 [2.x.227] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21] 

 [2.x.3]  

[1.x.22] 

 [2.x.4]  


[1.x.23][1.x.24] 


[1.x.25][1.x.26] 




In this tutorial we consider the case of two domains,  [2.x.5]  in  [2.x.6]  and  [2.x.7]  in  [2.x.8] , where  [2.x.9]  is embedded in  [2.x.10]  ( [2.x.11] ). We want to solve a partial differential equation on  [2.x.12] , enforcing some conditions on the solution of the problem *on the embedded domain*  [2.x.13] . 

There are two interesting scenarios: 

- the geometrical dimension `dim` of the embedded domain  [2.x.14]  is the same of the domain  [2.x.15]  (`spacedim`), that is, the spacedim-dimensional measure of  [2.x.16]  is not zero, or 

- the embedded domain  [2.x.17]  has an intrinsic dimension `dim` which is smaller than that of  [2.x.18]  (`spacedim`), thus its spacedim-dimensional measure is zero; for example it is a curve embedded in a two dimensional domain, or a surface embedded in a three-dimensional domain. 

In both cases define the restriction operator  [2.x.19]  as the operator that, given a continuous function on  [2.x.20] , returns its (continuous) restriction on  [2.x.21] , i.e., 

[1.x.27] 

It is well known that the operator  [2.x.22]  can be extended to a continuous operator on  [2.x.23] , mapping functions in  [2.x.24]  to functions in  [2.x.25]  when the intrinsic dimension of  [2.x.26]  is the same of  [2.x.27] . 

The same is true, with a less regular range space (namely  [2.x.28] ), when the dimension of  [2.x.29]  is one less with respect to  [2.x.30] , and  [2.x.31]  does not have a boundary. In this second case, the operator  [2.x.32]  is also known as the *trace* operator, and it is well defined for Lipschitz co-dimension one curves and surfaces  [2.x.33]  embedded in  [2.x.34]  (read  [1.x.28] for further details on the trace operator). 

The co-dimension two case is a little more complicated, and in general it is not possible to construct a continuous trace operator, not even from  [2.x.35]  to  [2.x.36] , when the dimension of  [2.x.37]  is zero or one respectively in two and three dimensions. 

In this tutorial program we're not interested in further details on  [2.x.38] : we take the extension  [2.x.39]  for granted, assuming that the dimension of the embedded domain (`dim`) is always smaller by one or equal with respect to the dimension of the embedding domain  [2.x.40]  (`spacedim`). 

We are going to solve the following differential problem: given a sufficiently regular function  [2.x.41]  on  [2.x.42] , find the solution  [2.x.43]  to 

[1.x.29] 



This is a constrained problem, where we are looking for a harmonic function  [2.x.44]  that satisfies homogeneous boundary conditions on  [2.x.45] , subject to the constraint  [2.x.46]  using a Lagrange multiplier. 

This problem has a physical interpretation: harmonic functions, i.e., functions that satisfy the Laplace equation, can be thought of as the displacements of a membrane whose boundary values are prescribed. The current situation then corresponds to finding the shape of a membrane for which not only the displacement at the boundary, but also on  [2.x.47]  is prescribed. For example, if  [2.x.48]  is a closed curve in 2d space, then that would model a soap film that is held in place by a wire loop along  [2.x.49]  as well as a second loop along  [2.x.50] . In cases where  [2.x.51]  is a whole area, you can think of this as a membrane that is stretched over an obstacle where  [2.x.52]  is the contact area. (If the contact area is not known we have a different problem -- called the "obstacle problem" -- which is modeled in  [2.x.53] .) 

As a first example we study the zero Dirichlet boundary condition on  [2.x.54] . The same equations apply if we apply zero Neumann boundary conditions on  [2.x.55]  or a mix of the two. 

The variational formulation can be derived by introducing two infinite dimensional spaces  [2.x.56]  and  [2.x.57] , respectively for the solution  [2.x.58]  and for the Lagrange multiplier  [2.x.59] . 

Multiplying the first equation by  [2.x.60]  and the second by  [2.x.61] , integrating by parts when possible, and exploiting the boundary conditions on  [2.x.62] , we obtain the following variational problem: 

Given a sufficiently regular function  [2.x.63]  on  [2.x.64] , find the solution  [2.x.65]  to [1.x.30] 



where  [2.x.66]  and  [2.x.67]  represent, respectively,  [2.x.68]  scalar products in  [2.x.69]  and in  [2.x.70] . 

Inspection of the variational formulation tells us that the space  [2.x.71]  can be taken to be  [2.x.72] . The space  [2.x.73] , in the co-dimension zero case, should be taken as  [2.x.74] , while in the co-dimension one case should be taken as  [2.x.75] . 

The function  [2.x.76]  should therefore be either in  [2.x.77]  (for the co-dimension zero case) or  [2.x.78]  (for the co-dimension one case). This leaves us with a Lagrange multiplier  [2.x.79]  in  [2.x.80] , which is either  [2.x.81]  or  [2.x.82] . 

There are two options for the discretization of the problem above. One could choose matching discretizations, where the Triangulation for  [2.x.83]  is aligned with the Triangulation for  [2.x.84] , or one could choose to discretize the two domains in a completely independent way. 

The first option is clearly more indicated for the simple problem we proposed above: it is sufficient to use a single Triangulation for  [2.x.85]  and then impose certain constraints depending  [2.x.86] . An example of this approach is studied in  [2.x.87] , where the solution has to stay above an obstacle and this is achieved imposing constraints on  [2.x.88] . 

To solve more complex problems, for example one where the domain  [2.x.89]  is time dependent, the second option could be a more viable solution. Handling non aligned meshes is complex by itself: to illustrate how is done we study a simple problem. 

The technique we describe here is presented in the literature using one of many names: the [1.x.31], the [1.x.32], the [1.x.33], and others. The main principle is that the discretization of the two grids and of the two finite element spaces are kept completely independent. This technique is particularly efficient for the simulation of fluid-structure interaction problems, where the configuration of the embedded structure is part of the problem itself, and one solves a (possibly non-linear) elastic problem to determine the (time dependent) configuration of  [2.x.90] , and a (possibly non-linear) flow problem in  [2.x.91] , plus coupling conditions on the interface between the fluid and the solid. 

In this tutorial program we keep things a little simpler, and we assume that the configuration of the embedded domain is given in one of two possible ways: 

- as a deformation mapping  [2.x.92] , defined on a continuous finite dimensional space on  [2.x.93]  and representing, for any point  [2.x.94] , its coordinate  [2.x.95]  in  [2.x.96] ; 

- as a displacement mapping  [2.x.97]  for  [2.x.98] , representing for any point  [2.x.99]  the displacement vector applied in order to deform  [2.x.100]  to its actual configuration  [2.x.101] . 

We define the embedded reference domain  [2.x.102]  `embedded_grid`: on this triangulation we construct a finite dimensional space (`embedded_configuration_dh`) to describe either the deformation or the displacement through a FiniteElement system of FE_Q objects (`embedded_configuration_fe`). This finite dimensional space is used only to interpolate a user supplied function (`embedded_configuration_function`) representing either  [2.x.103]  (if the parameter `use_displacement` is set to  [2.x.104]  or  [2.x.105]  (if the parameter `use_displacement` is set to  [2.x.106]  

The Lagrange multiplier  [2.x.107]  and the user supplied function  [2.x.108]  are defined through another finite dimensional space `embedded_dh`, and through another FiniteElement `embedded_fe`, using the same reference domain. In order to take into account the deformation of the domain, either a MappingFEField or a MappingQEulerian object are initialized with the `embedded_configuration` vector. 

In the embedding space, a standard finite dimensional space `space_dh` is constructed on the embedding grid `space_grid`, using the FiniteElement `space_fe`, following almost verbatim the approach taken in  [2.x.109] . 

We represent the discretizations of the spaces  [2.x.110]  and  [2.x.111]  with [1.x.34] and [1.x.35] respectively, where  [2.x.112]  is the dimension of `space_dh`, and  [2.x.113]  the dimension of `embedded_dh`. 

Once all the finite dimensional spaces are defined, the variational formulation of the problem above leaves us with the following finite dimensional system of equations: 

[1.x.36] 

where 

[1.x.37] 



While the matrix  [2.x.114]  is the standard stiffness matrix for the Poisson problem on  [2.x.115] , and the vector  [2.x.116]  is a standard right-hand-side vector for a finite element problem with forcing term  [2.x.117]  on  [2.x.118] , (see, for example,  [2.x.119] ), the matrix  [2.x.120]  or its transpose  [2.x.121]  are non-standard since they couple information on two non-matching grids. 

In particular, the integral that appears in the computation of a single entry of  [2.x.122] , is computed on  [2.x.123] . As usual in finite elements we split this integral into contributions from all cells of the triangulation used to discretize  [2.x.124] , we transform the integral on  [2.x.125]  to an integral on the reference element  [2.x.126] , where  [2.x.127]  is the mapping from  [2.x.128]  to  [2.x.129] , and compute the integral on  [2.x.130]  using a quadrature formula: 

[1.x.38] 

Computing this sum is non-trivial because we have to evaluate  [2.x.131] . In general, if  [2.x.132]  and  [2.x.133]  are not aligned, the point  [2.x.134]  is completely arbitrary with respect to  [2.x.135] , and unless we figure out a way to interpolate all basis functions of  [2.x.136]  on an arbitrary point on  [2.x.137] , we cannot compute the integral needed for an entry of the matrix  [2.x.138] . 

To evaluate  [2.x.139]  the following steps needs to be taken (as shown in the picture below): 

- For a given cell  [2.x.140]  in  [2.x.141]  compute the real point  [2.x.142] , where  [2.x.143]  is one of the quadrature points used for the integral on  [2.x.144] . 

- Find the cell of  [2.x.145]  in which  [2.x.146]  lies. We shall call this element  [2.x.147] . 

- To evaluate the basis function use the inverse of the mapping  [2.x.148]  that transforms the reference element  [2.x.149]  into the element  [2.x.150] :  [2.x.151] . 

 [2.x.152]  

The three steps above can be computed by calling, in turn, 

-  [2.x.153]  followed by 

-  [2.x.154]  We then 

- construct a custom Quadrature formula, containing the point in the reference  cell and then 

- construct an FEValues object, with the given quadrature formula, and  initialized with the cell obtained in the first step. 

This is what the deal.II function  [2.x.155]  does when evaluating a finite element field (not just a single shape function) at an arbitrary point; but this would be inefficient in this case. 

A better solution is to use a convenient wrapper to perform the first three steps on a collection of points:  [2.x.156]  If one is actually interested in computing the full coupling matrix, then it is possible to call the method  [2.x.157]  that performs the above steps in an efficient way, reusing all possible data structures, and gathering expensive steps together. This is the function we'll be using later in this tutorial. 

We solve the final saddle point problem by an iterative solver, applied to the Schur complement  [2.x.158]  (whose construction is described, for example, in  [2.x.159] ), and we construct  [2.x.160]  using LinearOperator classes. 


[1.x.39][1.x.40] 


The problem we solve here is identical to  [2.x.161] , with the difference that we impose some constraints on an embedded domain  [2.x.162] . The tutorial is written in a dimension independent way, and in the results section we show how to vary both `dim` and `spacedim`. 

The tutorial is compiled for `dim` equal to one and `spacedim` equal to two. If you want to run the program in embedding dimension `spacedim` equal to three, you will most likely want to change the reference domain for  [2.x.163]  to be, for example, something you read from file, or a closed sphere that you later deform to something more interesting. 

In the default scenario,  [2.x.164]  has co-dimension one, and this tutorial program implements the Fictitious Boundary Method. As it turns out, the same techniques are used in the Variational Immersed Finite Element Method, and the coupling operator  [2.x.165]  defined above is the same in almost all of these non-matching methods. 

The embedded domain is assumed to be included in  [2.x.166] , which we take as the unit square  [2.x.167] . The definition of the fictitious domain  [2.x.168]  can be modified through the parameter file, and can be given as a mapping from the reference interval  [2.x.169]  to a curve in  [2.x.170] . 

If the curve is closed, then the results will be similar to running the same problem on a grid whose boundary is  [2.x.171] . The program will happily run also with a non-closed  [2.x.172] , although in those cases the mathematical formulation of the problem is more difficult, since  [2.x.173]  will have a boundary by itself that has co-dimension two with respect to the domain  [2.x.174] . 


[1.x.41][1.x.42] 


 [2.x.175]   [2.x.176]  Glowinski, R., T.-W. Pan, T.I. Hesla, and D.D. Joseph. 1999. “A Distributed   Lagrange Multiplier/fictitious Domain Method for Particulate Flows.”   International Journal of Multiphase Flow 25 (5). Pergamon: 755–94. 

 [2.x.177]  Boffi, D., L. Gastaldi, L. Heltai, and C.S. Peskin. 2008. “On the   Hyper-Elastic Formulation of the Immersed Boundary Method.” Computer Methods   in Applied Mechanics and Engineering 197 (25–28). 

 [2.x.178]  Heltai, L., and F. Costanzo. 2012. “Variational Implementation of Immersed   Finite Element Methods.” Computer Methods in Applied Mechanics and Engineering   229–232.  [2.x.179]  [1.x.43] [1.x.44] 


[1.x.45]  [1.x.46] Most of these have been introduced elsewhere, we'll comment only on the new ones. 







[1.x.47] 



The parameter acceptor class is the first novelty of this tutorial program: in general parameter files are used to steer the execution of a program at run time. While even a simple approach saves compile time, as the same executable can be run with different parameter settings, it can become difficult to handle hundreds of parameters simultaneously while maintaining compatibility between different programs. This is where the class ParameterAcceptor proves useful. 




This class is used to define a public interface for classes that want to use a single global ParameterHandler to handle parameters. The class provides a static ParameterHandler member, namely  [2.x.180]  and implements the "Command design pattern" (see, for example, E. Gamma, R. Helm, R. Johnson, J. Vlissides, Design Patterns: Elements of Reusable Object-Oriented Software, Addison-Wesley Professional, 1994. https://goo.gl/FNYByc). 




ParameterAcceptor provides a global subscription mechanism. Whenever an object of a class derived from ParameterAcceptor is constructed, a pointer to that object-of-derived-type is registered, together with a section entry in the parameter file. Such registry is traversed upon invocation of the single function  [2.x.181]  which in turn makes sure that all classes stored in the global registry declare the parameters they will be using, and after having declared them, it reads the content of `file.prm` to parse the actual parameters. 




If you call the method  [2.x.182]  for each of the parameters you want to use in your code, there is nothing else you need to do. If you are using an already existing class that provides the two functions `declare_parameters` and `parse_parameters`, you can still use ParameterAcceptor, by encapsulating the existing class into a ParameterAcceptorProxy class. 




In this example, we'll use both strategies, using ParameterAcceptorProxy for deal.II classes, and deriving our own parameter classes directly from ParameterAcceptor. 

[1.x.48] 



The other new include file is the one that contains the  [2.x.183]  class. The structure of deal.II, as many modern numerical libraries, is organized following a Directed Acyclic Graph (DAG). A DAG is a directed graph with topological ordering: each node structurally represents an object, and is connected to non-root nodes by one (or more) oriented edges, from the parent to the child. The most significant example of this structure is the Triangulation and its  [2.x.184]  structure. From a Triangulation (the main node), we can access each cell (children nodes of the triangulation). From the cells themselves we can access over all vertices of the cell. In this simple example, the DAG structure can be represented as three node types (the triangulation, the cell iterator, and the vertex) connected by oriented edges from the triangulation to the cell iterators, and from the cell iterator to the vertices. This has several advantages, but it intrinsically creates “asymmetries”, making certain operations fast and their inverse very slow: finding the vertices of a cell has low computational cost, and can be done by simply traversing the DAG, while finding all the cells that share a vertex requires a non-trivial computation unless a new DAG data structure is added that represents the inverse search. 




Since inverse operations are usually not needed in a finite element code, these are implemented in GridTools without the use of extra data structures related to the Triangulation which would make them much faster. One such data structure, for example, is a map from the vertices of a Triangulation to all cells that share those vertices, which would reduce the computations needed to answer to the previous question. 




Some methods, for example  [2.x.185]  make heavy usage of these non-standard operations. If you need to call these methods more than once, it becomes convenient to store those data structures somewhere.  [2.x.186]  does exactly this, giving you access to previously computed objects, or computing them on the fly (and then storing them inside the class for later use), and making sure that whenever the Triangulation is updated, also the relevant data structures are recomputed. 

[1.x.49] 



In this example, we will be using a reference domain to describe an embedded Triangulation, deformed through a finite element vector field. 




The next two include files contain the definition of two classes that can be used in these cases. MappingQEulerian allows one to describe a domain through a *displacement* field, based on a FESystem[FE_Q(p)^spacedim] finite element space. The second is a little more generic, and allows you to use arbitrary vector FiniteElement spaces, as long as they provide a *continuous* description of your domain. In this case, the description is done through the actual *deformation* field, rather than a *displacement* field. 




Which one is used depends on how the user wants to specify the reference domain, and/or the actual configuration. We'll provide both options, and experiment a little in the results section of this tutorial program. 

[1.x.50] 



The parsed function class is another new entry. It allows one to create a Function object, starting from a string in a parameter file which is parsed into an object that you can use anywhere deal.II accepts a Function (for example, for interpolation, boundary conditions, etc.). 

[1.x.51] 



This is the last new entry for this tutorial program. The namespace NonMatching contains a few methods that are useful when performing computations on non-matching grids, or on curves that are not aligned with the underlying mesh. 




We'll discuss its use in detail later on in the `setup_coupling` method. 

[1.x.52] 




[1.x.53]  [1.x.54]    


In the DistributedLagrangeProblem, we need two parameters describing the dimensions of the domain  [2.x.187]  (`dim`) and of the domain  [2.x.188]  (`spacedim`).    


These will be used to initialize a Triangulation<dim,spacedim> (for  [2.x.189] ) and a Triangulation<spacedim,spacedim> (for  [2.x.190] ).    


A novelty with respect to other tutorial programs is the heavy use of  [2.x.191]  These behave like classical pointers, with the advantage of doing automatic house-keeping: the contained object is automatically destroyed as soon as the unique_ptr goes out of scope, even if it is inside a container or there's an exception. Moreover it does not allow for duplicate pointers, which prevents ownership problems. We do this, because we want to be able to i) construct the problem, ii) read the parameters, and iii) initialize all objects according to what is specified in a parameter file.    


We construct the parameters of our problem in the internal class `Parameters`, derived from ParameterAcceptor. The `DistributedLagrangeProblem` class takes a const reference to a `Parameters` object, so that it is not possible to modify the parameters from within the DistributedLagrangeProblem class itself.    


We could have initialized the parameters first, and then pass the parameters to the DistributedLagrangeProblem assuming all entries are set to the desired values, but this has two disadvantages: 

   




- We should not make assumptions on how the user initializes a class that is not under our direct control. If the user fails to initialize the class, we should notice and throw an exception; 

   




- Not all objects that need to read parameters from a parameter file may be available when we construct the Parameters; this is often the case for complex programs, with multiple physics, or where we reuse existing code in some external classes. We simulate this by keeping some "complex" objects, like ParsedFunction objects, inside the `DistributedLagrangeProblem` instead of inside the `Parameters`.    


Here we assume that upon construction, the classes that build up our problem are not usable yet. Parsing the parameter file is what ensures we have all ingredients to build up our classes, and we design them so that if parsing fails, or is not executed, the run is aborted. 







[1.x.55] 



The `Parameters` class is derived from ParameterAcceptor. This allows us to use the  [2.x.192]  method in its constructor.      


The members of this function are all non-const, but the `DistributedLagrangeProblem` class takes a const reference to a `Parameters` object: this ensures that parameters are not modified from within the `DistributedLagrangeProblem` class. 

[1.x.56] 



The parameters now described can all be set externally using a parameter file: if no parameter file is present when running the executable, the program will create a "parameters.prm" file with the default values defined here, and then abort to give the user a chance to modify the parameters.prm file. 




Initial refinement for the embedding grid, corresponding to the domain  [2.x.193] . 

[1.x.57] 



The interaction between the embedded grid  [2.x.194]  and the embedding grid  [2.x.195]  is handled through the computation of  [2.x.196] , which involves all cells of  [2.x.197]  overlapping with parts of  [2.x.198] : a higher refinement of such cells might improve quality of our computations. For this reason we define `delta_refinement`: if it is greater than zero, then we mark each cell of the space grid that contains a vertex of the embedded grid and its neighbors, execute the refinement, and repeat this process `delta_refinement` times. 

[1.x.58] 



Starting refinement of the embedded grid, corresponding to the domain  [2.x.199] . 

[1.x.59] 



The list of boundary ids where we impose homogeneous Dirichlet boundary conditions. On the remaining boundary ids (if any), we impose homogeneous Neumann boundary conditions. As a default problem we have zero Dirichlet boundary conditions on  [2.x.200]  

[1.x.60] 



FiniteElement degree of the embedding space:  [2.x.201]  

[1.x.61] 



FiniteElement degree of the embedded space:  [2.x.202]  

[1.x.62] 



FiniteElement degree of the space used to describe the deformation of the embedded domain 

[1.x.63] 



Order of the quadrature formula used to integrate the coupling 

[1.x.64] 



If set to true, then the embedded configuration function is interpreted as a displacement function 

[1.x.65] 



Level of verbosity to use in the output 

[1.x.66] 



A flag to keep track if we were initialized or not 

[1.x.67] 



Entry point for the DistributedLagrangeProblem 

[1.x.68] 



Object containing the actual parameters 

[1.x.69] 



The following functions are similar to all other tutorial programs, with the exception that we now need to set up things for two different families of objects, namely the ones related to the *embedding* grids, and the ones related to the *embedded* one. 







[1.x.70] 



The only unconventional function we have here is the `setup_coupling()` method, used to generate the sparsity patter for the coupling matrix  [2.x.203] . 







[1.x.71] 



first we gather all the objects related to the embedding space geometry 







[1.x.72] 



Then the ones related to the embedded grid, with the DoFHandler associated to the Lagrange multiplier `lambda` 







[1.x.73] 



And finally, everything that is needed to *deform* the embedded triangulation 

[1.x.74] 



The ParameterAcceptorProxy class is a "transparent" wrapper derived from both ParameterAcceptor and the type passed as its template parameter. At construction, the arguments are split into two parts: the first argument is an  [2.x.204]  forwarded to the ParameterAcceptor class, and containing the name of the section that should be used for this class, while all the remaining arguments are forwarded to the constructor of the templated type, in this case, to the  [2.x.205]  constructor.      


This class allows you to use existing classes in conjunction with the ParameterAcceptor registration mechanism, provided that those classes have the members `declare_parameters()` and `parse_parameters()`.      


This is the case here, making it fairly easy to exploit the  [2.x.206]  class: instead of requiring users to create new Function objects in their code for the RHS, boundary functions, etc., (like it is done in most of the other tutorials), here we allow the user to use deal.II interface to muParser (http://muparser.beltoforion.de), where the specification of the function is not done at compile time, but at run time, using a string that is parsed into an actual Function object.      


In this case, the `embedded_configuration_function` is a vector valued Function that can be interpreted as either a *deformation* or a *displacement* according to the boolean value of `parameters.use_displacement`. The number of components is specified later on in the construction. 







[1.x.75] 



We do the same thing to specify the value of the function  [2.x.207] , which is what we want our solution to be in the embedded space. In this case the Function is a scalar one. 

[1.x.76] 



Similarly to what we have done with the  [2.x.208]  class, we repeat the same for the ReductionControl class, allowing us to specify all possible stopping criteria for the Schur complement iterative solver we'll use later on. 

[1.x.77] 



Next we gather all SparsityPattern, SparseMatrix, and Vector objects we'll need 

[1.x.78] 



The TimerOutput class is used to provide some statistics on the performance of our program. 

[1.x.79] 




[1.x.80]  [1.x.81]    


At construction time, we initialize also the ParameterAcceptor class, with the section name we want our problem to use when parsing the parameter file.    


Parameter files can be organized into section/subsection/etc.: this has the advantage that defined objects share parameters when sharing the same section/subsection/etc. ParameterAcceptor allows to specify the section name using Unix conventions on paths. If the section name starts with a slash ("/"), then the section is interpreted as an *absolute path*, ParameterAcceptor enters a subsection for each directory in the path, using the last name it encountered as the landing subsection for the current class.    


For example, if you construct your class using `ParameterAcceptor("/first/second/third/My Class")`, the parameters will be organized as follows:    


 [2.x.209]     


Internally, the *current path* stored in ParameterAcceptor is now considered to be "/first/second/third/", i.e. when you specify an absolute path, ParameterAcceptor *changes* the current section to the current path, i.e. to the path of the section name until the *last* "/".    


You can now construct another class derived from ParameterAcceptor using a relative path (e.g., `ParameterAcceptor("My Other Class")`) instead of the absolute one (e.g. `ParameterAcceptor("/first/second/third/My Other Class")`), obtaining:  [2.x.210]     


If the section name *ends* with a slash then subsequent classes will interpret this as a full path: for example, similar to the one above, if we have two classes, one initialized with `ParameterAcceptor("/first/second/third/My Class/")` and the other with `ParameterAcceptor("My Other Class")`, then the resulting parameter file will look like:    


 [2.x.211]     


We are going to exploit this, by making our `Parameters` the *parent* of all subsequently constructed classes. Since most of the other classes are members of `DistributedLagrangeProblem` this allows, for example, to construct two `DistributedLagrangeProblem` for two different dimensions, without having conflicts in the parameters for the two problems. 

[1.x.85] 



The  [2.x.212]  function does a few things: 

     




- enters the subsection specified at construction time to ParameterAcceptor 

     




- calls the  [2.x.213]  function 

     




- calls any signal you may have attached to  [2.x.214]  

     




- leaves the subsection      


In turn,  [2.x.215]  

     




- declares an entry in the parameter handler for the given variable; 

     




- takes the current value of the variable 

     




- transforms it to a string, used as the default value for the parameter file 

     




- attaches an *action* to  [2.x.216]  that monitors when a file is parsed, or when an entry is set, and when this happens, it updates the value of the variable passed to `add_parameter()` by setting it to whatever was specified in the input file (of course, after the input file has been parsed and the text representation converted to the type of the variable). 

[1.x.86] 



Once the parameter file has been parsed, then the parameters are good to go. Set the internal variable `initialized` to true. 

[1.x.87] 



The constructor is pretty standard, with the exception of the `ParameterAcceptorProxy` objects, as explained earlier. 

[1.x.88] 



Here is a way to set default values for a ParameterAcceptor class that was constructed using ParameterAcceptorProxy.      


In this case, we set the default deformation of the embedded grid to be a circle with radius  [2.x.217]  and center  [2.x.218] , we set the default value for the embedded_value_function to be the constant one, and specify some sensible values for the SolverControl object.      


It is fundamental for  [2.x.219]  to be embedded: from the definition of  [2.x.220]  is clear that, if  [2.x.221] , certain rows of the matrix  [2.x.222]  will be zero. This would be a problem, as the Schur complement method requires  [2.x.223]  to have full column rank. 

[1.x.89] 




[1.x.90]  [1.x.91]    


The function  [2.x.224]  is used to set up the finite element spaces. Notice how  [2.x.225]  is used to create objects wrapped inside  [2.x.226]  objects. 

[1.x.92] 



Initializing  [2.x.227] : constructing the Triangulation and wrapping it into a  [2.x.228]  object 

[1.x.93] 



Next, we actually create the triangulation using  [2.x.229]  The last argument is set to true: this activates colorization (i.e., assigning different boundary indicators to different parts of the boundary), which we use to assign the Dirichlet and Neumann conditions. 

[1.x.94] 



Once we constructed a Triangulation, we refine it globally according to the specifications in the parameter file, and construct a  [2.x.230]  with it. 

[1.x.95] 



The same is done with the embedded grid. Since the embedded grid is deformed, we first need to setup the deformation mapping. We do so in the following few lines: 

[1.x.96] 



Once we have defined a finite dimensional space for the deformation, we interpolate the `embedded_configuration_function` defined in the parameter file: 

[1.x.97] 



Now we can interpret it according to what the user has specified in the parameter file: as a displacement, in which case we construct a mapping that *displaces* the position of each support point of our configuration finite element space by the specified amount on the corresponding configuration vector, or as an absolution position.      


In the first case, the class MappingQEulerian offers its services, while in the second one, we'll use the class MappingFEField. They are in fact very similar. MappingQEulerian will only work for systems of FE_Q finite element spaces, where the displacement vector is stored in the first `spacedim` components of the FESystem, and the degree given as a parameter at construction time, must match the degree of the first `spacedim` components.      


The class MappingFEField is slightly more general, in that it allows you to select arbitrary FiniteElement types when constructing your approximation. Naturally some choices may (or may not) make sense, according to the type of FiniteElement you choose. MappingFEField implements the pure iso-parametric concept, and can be used, for example, to implement iso-geometric analysis codes in deal.II, by combining it with the FE_Bernstein finite element class. In this example, we'll use the two interchangeably, by taking into account the fact that one configuration will be a `displacement`, while the other will be an absolute `deformation` field. 







[1.x.98] 



In this tutorial program we not only refine  [2.x.231]  globally, but also allow a local refinement depending on the position of  [2.x.232] , according to the value of `parameters.delta_refinement`, that we use to decide how many rounds of local refinement we should do on  [2.x.233] , corresponding to the position of  [2.x.234] .      


With the mapping in place, it is now possible to query what is the location of all support points associated with the `embedded_dh`, by calling the method  [2.x.235]       


This method has two variants. One that does *not* take a Mapping, and one that takes a Mapping. If you use the second type, like we are doing in this case, the support points are computed through the specified mapping, which can manipulate them accordingly.      


This is precisely what the `embedded_mapping` is there for. 

[1.x.99] 



Once we have the support points of the embedded finite element space, we would like to identify what cells of the embedding space contain what support point, to get a chance at refining the embedding grid where it is necessary, i.e., where the embedded grid is. This can be done manually, by looping over each support point, and then calling the method  [2.x.236]  for each cell of the embedding space, until we find one that returns points in the unit reference cell, or it can be done in a more intelligent way.      


The  [2.x.237]  is a possible option that performs the above task in a cheaper way, by first identifying the closest vertex of the embedding Triangulation to the target point, and then by calling  [2.x.238]  only for those cells that share the found vertex.      


In fact, there are algorithms in the GridTools namespace that exploit a  [2.x.239]  object, and possibly a KDTree object to speed up these operations as much as possible.      


The simplest way to exploit the maximum speed is by calling a specialized method,  [2.x.240]  that will store a lot of useful information and data structures during the first point search, and then reuse all of this for subsequent points.      


 [2.x.241]  returns a tuple where the first element is a vector of cells containing the input points, in this case support_points. For refinement, this is the only information we need, and this is exactly what happens now.      


When we need to assemble a coupling matrix, however, we'll also need the reference location of each point to evaluate the basis functions of the embedding space. The other elements of the tuple returned by  [2.x.242]  allow you to reconstruct, for each point, what cell contains it, and what is the location in the reference cell of the given point. Since this information is better grouped into cells, then this is what the algorithm returns: a tuple, containing a vector of all cells that have at least one point in them, together with a list of all reference points and their corresponding index in the original vector.      


In the following loop, we will be ignoring all returned objects except the first, identifying all cells contain at least one support point of the embedded space. This allows for a simple adaptive refinement strategy: refining these cells and their neighbors.      


Notice that we need to do some sanity checks, in the sense that we want to have an embedding grid which is well refined around the embedded grid, but where two consecutive support points lie either in the same cell, or in neighbor embedding cells.      


This is only possible if we ensure that the smallest cell size of the embedding grid is nonetheless bigger than the largest cell size of the embedded grid. Since users can modify both levels of refinements, as well as the amount of local refinement they want around the embedded grid, we make sure that the resulting meshes satisfy our requirements, and if this is not the case, we bail out with an exception. 

[1.x.100] 



In order to construct a well posed coupling interpolation operator  [2.x.243] , there are some constraints on the relative dimension of the grids between the embedding and the embedded domains. The coupling operator  [2.x.244]  and the spaces  [2.x.245]  and  [2.x.246]  have to satisfy an inf-sup condition in order for the problem to have a solution. It turns out that the non-matching  [2.x.247]  projection satisfies such inf-sup, provided that the spaces  [2.x.248]  and  [2.x.249]  are compatible between each other (for example, provided that they are chosen to be the ones described in the introduction).      


However, the *discrete* inf-sup condition must also hold. No complications arise here, but it turns out that the discrete inf-sup constant deteriorates when the non-matching grids have local diameters that are too far away from each other. In particular, it turns out that if you choose an embedding grid which is *finer* with respect to the embedded grid, the inf-sup constant deteriorates much more than if you let the embedded grid be finer.      


In order to avoid issues, in this tutorial we will throw an exception if the parameters chosen by the user are such that the maximal diameter of the embedded grid is greater than the minimal diameter of the embedding grid.      


This choice guarantees that almost every cell of the embedded grid spans no more than two cells of the embedding grid, with some rare exceptions, that are negligible in terms of the resulting inf-sup. 

[1.x.101] 



 [2.x.250]  has been refined and we can now set up its DoFs 

[1.x.102] 



We now set up the DoFs of  [2.x.251]  and  [2.x.252] : since they are fundamentally independent (except for the fact that  [2.x.253] 's mesh is more refined "around"  [2.x.254] ) the procedure is standard. 

[1.x.103] 



By definition the stiffness matrix involves only  [2.x.255] 's DoFs 

[1.x.104] 



By definition the rhs of the system we're solving involves only a zero vector and  [2.x.256] , which is computed using only  [2.x.257] 's DoFs 

[1.x.105] 



Creating the coupling sparsity pattern is a complex operation, but it can be easily done using the  [2.x.258]  which requires the two DoFHandler objects, the quadrature points for the coupling, a DynamicSparsityPattern (which then needs to be copied into the sparsity one, as usual), the component mask for the embedding and embedded Triangulation (which we leave empty) and the mappings for both the embedding and the embedded Triangulation. 

[1.x.106] 




[1.x.107]  [1.x.108]    


The following function creates the matrices: as noted before computing the stiffness matrix and the rhs is a standard procedure. 

[1.x.109] 



Embedding stiffness matrix  [2.x.259] , and the right hand side  [2.x.260] . 

[1.x.110] 



To compute the coupling matrix we use the  [2.x.261]  tool, which works similarly to  [2.x.262]  

[1.x.111] 




[1.x.112]  [1.x.113]    


All parts have been assembled: we solve the system using the Schur complement method 

[1.x.114] 



Start by creating the inverse stiffness matrix 

[1.x.115] 



Initializing the operators, as described in the introduction 

[1.x.116] 



Using the Schur complement method 

[1.x.117] 



The following function simply generates standard result output on two separate files, one for each mesh. 

[1.x.118] 



The only difference between the two output routines is that in the second case, we want to output the data on the current configuration, and not on the reference one. This is possible by passing the actual embedded_mapping to the  [2.x.263]  function. The mapping will take care of outputting the result on the actual deformed configuration. 







[1.x.119] 



Similar to all other tutorial programs, the `run()` function simply calls all other methods in the correct order. Nothing special to note, except that we check if parsing was done before we actually attempt to run our program. 

[1.x.120] 



Differently to what happens in other tutorial programs, here we use ParameterAcceptor style of initialization, i.e., all objects are first constructed, and then a single call to the static method  [2.x.264]  is issued to fill all parameters of the classes that are derived from ParameterAcceptor.        


We check if the user has specified a parameter file name to use when the program was launched. If so, try to read that parameter file, otherwise, try to read the file "parameters.prm".        


If the parameter file that was specified (implicitly or explicitly) does not exist,  [2.x.265]  will create one for you, and exit the program. 







[1.x.121] 

[1.x.122][1.x.123] 


The directory in which this program is run does not contain a parameter file by default. On the other hand, this program wants to read its parameters from a file called parameters.prm -- and so, when you execute it the first time, you will get an exception that no such file can be found: 

[1.x.124] 



However, as the error message already states, the code that triggers the exception will also generate a parameters.prm file that simply contains the default values for all parameters this program cares about. By inspection of the parameter file, we see the following: 

[1.x.125] 



If you now run the program, you will get a file called `used_parameters.prm`, containing a shorter version of the above parameters (without comments and documentation), documenting all parameters that were used to run your program: 

[1.x.126] 



The rationale behind creating first `parameters.prm` file (the first time the program is run) and then a `used_parameters.prm` (every other times you run the program), is because you may want to leave most parameters to their default values, and only modify a handful of them. 

For example, you could use the following (perfectly valid) parameter file with this tutorial program: 

[1.x.127] 



and you would obtain exactly the same results as in test case 1 below. 

[1.x.128][1.x.129] 


For the default problem the value of  [2.x.266]  on  [2.x.267]  is set to the constant  [2.x.268] : this is like imposing a constant Dirichlet boundary condition on  [2.x.269] , seen as boundary of the portion of  [2.x.270]  inside  [2.x.271] . Similarly on  [2.x.272]  we have zero Dirichlet boundary conditions. 


 [2.x.273]  

The output of the program will look like the following: 

[1.x.130] 



You may notice that, in terms of CPU time, assembling the coupling system is twice as expensive as assembling the standard Poisson system, even though the matrix is smaller. This is due to the non-matching nature of the discretization. Whether this is acceptable or not, depends on the applications. 

If the problem was set in a three-dimensional setting, and the immersed mesh was time dependent, it would be much more expensive to recreate the mesh at each step rather than use the technique we present here. Moreover, you may be able to create a very fast and optimized solver on a uniformly refined square or cubic grid, and embed the domain where you want to perform your computation using the technique presented here. This would require you to only have a surface representatio of your domain (a much cheaper and easier mesh to produce). 

To play around a little bit, we are going to complicate a little the fictitious domain as well as the boundary conditions we impose on it. 

[1.x.131][1.x.132] 


If we use the following parameter file: 

[1.x.133] 



We get a "flowery" looking domain, where we impose a linear boundary condition  [2.x.274] . This test shows that the method is actually quite accurate in recovering an exactly linear function from its boundary conditions, and even though the meshes are not aligned, we obtain a pretty good result. 

Replacing  [2.x.275]  with  [2.x.276] , i.e., modifying the parameter file such that we have 

[1.x.134] 

produces the saddle on the right. 

 [2.x.277]  

[1.x.135] [1.x.136][1.x.137] 


[1.x.138][1.x.139] 


While the current tutorial program is written for `spacedim` equal to two, there are only minor changes you have to do in order for the program to run in different combinations of dimensions. 

If you want to run with `spacedim` equal to three and `dim` equal to two, then you will almost certainly want to perform the following changes: 

- use a different reference domain for the embedded grid, maybe reading it from   a file. It is not possible to construct a smooth closed surface with one   single parametrization of a square domain, therefore you'll most likely want   to use a reference domain that is topologically equivalent to a the boundary   of a sphere. 

- use a displacement instead of the deformation to map  [2.x.278]  into  [2.x.279]  

[1.x.140][1.x.141] 


We have seen in other tutorials (for example in  [2.x.280]  and  [2.x.281] ) how to read grids from input files. A nice generalization for this tutorial program would be to allow the user to select a grid to read from the parameter file itself, instead of hardcoding the mesh type in the tutorial program itself. 

[1.x.142][1.x.143] 


At the moment, we have no preconditioner on the Schur complement. This is ok for two dimensional problems, where a few hundred iterations bring the residual down to the machine precision, but it's not going to work in three dimensions. 

It is not obvious what a good preconditioner would be here. The physical problem we are solving with the Schur complement, is to associate to the Dirichlet data  [2.x.282] , the value of the Lagrange multiplier  [2.x.283] .  [2.x.284]  can be interpreted as the *jump* in the normal gradient that needs to be imposed on  [2.x.285]  across  [2.x.286] , in order to obtain the Dirichlet data  [2.x.287] . 

So  [2.x.288]  is some sort of Neumann to Dirichlet map, and we would like to have a good approximation for the Dirichlet to Neumann map. A possibility would be to use a Boundary Element approximation of the problem on  [2.x.289] , and construct a rough approximation of the hyper-singular operator for the Poisson problem associated to  [2.x.290] , which is precisely a Dirichlet to Neumann map. 

[1.x.144][1.x.145] 


The simple code proposed here can serve as a starting point for more complex problems which, to be solved, need to be run on parallel code, possibly using distributed meshes (see  [2.x.291] ,  [2.x.292] , and the documentation for  [2.x.293]  and  [2.x.294]  

When using non-matching grids in parallel a problem arises: to compute the matrix  [2.x.295]  a process needs information about both meshes on the same portion of real space but, when working with distributed meshes, this information may not be available, because the locally owned part of the  [2.x.296]  triangulation stored on a given processor may not be physically co-located with the locally owned part of the  [2.x.297]  triangulation stored on the same processor. 

Various strategies can be implemented to tackle this problem: 

- distribute the two meshes so that this constraint is satisfied; 

- use communication for the parts of real space where the constraint is not   satisfied; 

- use a distributed triangulation for the embedding space, and a shared   triangulation for the emdedded configuration. 

The latter strategy is clearly the easiest to implement, as most of the functions used in this tutorial program will work unchanged also in the parallel case. Of course one could use the reversal strategy (that is, have a distributed embedded Triangulation and a shared embedding Triangulation). 

However, this strategy is most likely going to be more expensive, since by definition the embedding grid is larger than the embedded grid, and it makes more sense to distribute the largest of the two grids, maintaining the smallest one shared among all processors. [1.x.146] [1.x.147]  [2.x.298]  

 [2.x.299] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38][1.x.39] 

 [2.x.3]  

[1.x.40] 

[1.x.41] [1.x.42][1.x.43] 


This tutorial program presents an implementation of the "weak Galerkin" finite element method for the Poisson equation. In some sense, the motivation for considering this method starts from the same point as in  [2.x.4] : We would like to consider discontinuous shape functions, but then need to address the fact that the resulting problem has a much larger number of degrees of freedom compared to the usual continuous Galerkin method (because, for example, each vertex carries as many degrees of freedom as there are adjacent cells). We also have to address the fact that, unlike in the continuous Galerkin method, [1.x.44] degree of freedom on one cell couples with all of the degrees of freedom on each of its face neighbor cells. Consequently, the matrix one gets from the "traditional" discontinuous Galerkin methods are both large and relatively dense. 

Both the hybridized discontinuous Galerkin method (HDG) in  [2.x.5]  and the weak Galerkin (WG) method in this tutorial address the issue of coupling by introducing additional degrees of freedom whose shape functions only live on a face between cells (i.e., on the "skeleton" of the mesh), and which therefore "insulate" the degrees of freedom on the adjacent cells from each other: cell degrees of freedom only couple with other cell degrees of freedom on the same cell, as well as face degrees of freedom, but not with cell degrees of freedom on neighboring cells. Consequently, the coupling of shape functions for these cell degrees of freedom indeed couple on exactly one cell and the degrees of freedom defined on its faces. 

For a given equation, say the second order Poisson equation, the difference between the HDG and the WG method is how precisely one formulates the problem that connects all of these different shape functions. (Indeed, for some WG and HDG formulation, it is possible to show that they are equivalent.) The HDG does things by reformulating second order problems in terms of a system of first order equations and then conceptually considers the face degrees of freedom to be "fluxes" of this first order system. In contrast, the WG method keeps things in second order form and considers the face degrees of freedom as of the same type as the primary solution variable, just restricted to the lower-dimensional faces. For the purposes of the equation, one then needs to somehow "extend" these shape functions into the interior of the cell when defining what it means to apply a differential operator to them. Compared to the HDG, the method has the advantage that it does not lead to a proliferation of unknowns due to rewriting the equation as a first-order system, but it is also not quite as easy to implement. However, as we will see in the following, this additional effort is not prohibitive. 


[1.x.45][1.x.46] 


Weak Galerkin Finite Element Methods (WGFEMs) use discrete weak functions to approximate scalar unknowns, and discrete weak gradients to approximate classical gradients. The method was originally introduced by Junping Wang and Xiu Ye in the paper [1.x.47][1.x.48]. Compared to the continuous Galerkin method, the weak Galerkin method satisfies important physical properties, namely local mass conservation and bulk normal flux continuity. It results in a SPD linear system, and optimal convergence rates can be obtained with mesh refinement. 


[1.x.49][1.x.50] 

This program solves the Poisson equation using the weak Galerkin finite element method: 

[1.x.51] 

where  [2.x.6]  is a bounded domain. In the context of the flow of a fluid through a porous medium,  [2.x.7]  is the pressure,  [2.x.8]  is a permeability tensor,  [2.x.9]  is the source term, and  [2.x.10]  represent Dirichlet and Neumann boundary conditions. We can introduce a flux,  [2.x.11] , that corresponds to the Darcy velocity (in the way we did in  [2.x.12] ) and this variable will be important in the considerations below. 

In this program, we will consider a test case where the exact pressure is  [2.x.13]  on the unit square domain, with homogeneous Dirichelet boundary conditions and  [2.x.14]  the identity matrix. Then we will calculate  [2.x.15]  errors of pressure, velocity, and flux. 


[1.x.52][1.x.53] 


The Poisson equation above has a solution  [2.x.16]  that needs to satisfy the weak formulation of the problem, [1.x.54] 

for all test functions  [2.x.17] , where [1.x.55] 

and [1.x.56] 

Here, we have integrated by parts in the bilinear form, and we are evaluating the gradient of  [2.x.18]  in the interior and the values of  [2.x.19]  on the boundary of the domain. All of this is well defined because we assume that the solution is in  [2.x.20]  for which taking the gradient and evaluating boundary values are valid operations. 

The idea of the weak Galerkin method is now to approximate the exact  [2.x.21]  solution with a [1.x.57]  [2.x.22] . This function may only be discontinuous along interfaces between cells, and because we will want to evaluate this function also along interfaces, we have to prescribe not only what values it is supposed to have in the cell interiors but also its values along interfaces. We do this by saying that  [2.x.23]  is actually a tuple,  [2.x.24] , though it's really just a single function that is either equal to  [2.x.25]  or  [2.x.26] , depending on whether it is evaluated at a point  [2.x.27]  that lies in the cell interior or on cell interfaces. 

We would then like to simply stick this approximation into the bilinear form above. This works for the case where we have to evaluate the test function  [2.x.28]  on the boundary (where we would simply take its interface part  [2.x.29] ) but we have to be careful with the gradient because that is only defined in cell interiors. Consequently, the weak Galerkin scheme for the Poisson equation is defined by [1.x.58] 

for all discrete test functions  [2.x.30] , where [1.x.59] 

and [1.x.60] 

The key point is that here, we have replaced the gradient  [2.x.31]  by the [1.x.61] operator  [2.x.32]  that makes sense for our peculiarly defined approximation  [2.x.33] . 

The question is then how that operator works. For this, let us first say how we think of the discrete approximation  [2.x.34]  of the pressure. As mentioned above, the "function"  [2.x.35]  actually consists of two parts: the values  [2.x.36]  in the interior of cells, and  [2.x.37]  on the interfaces. We have to define discrete (finite-dimensional) function spaces for both of these; in this program, we will use FE_DGQ for  [2.x.38]  as the space in the interior of cells (defined on each cell, but in general discontinuous along interfaces), and FE_FaceQ for  [2.x.39]  as the space on the interfaces. 

Then let us consider just a single cell (because the integrals above are all defined cell-wise, and because the weak discrete gradient is defined cell-by-cell). The restriction of  [2.x.40]  to cell  [2.x.41] ,  [2.x.42]  then consists of the pair  [2.x.43] . In essence, we can think of  [2.x.44]  of some function defined on  [2.x.45]  that approximates the gradient; in particular, if  [2.x.46]  was the restriction of a differentiable function (to the interior and boundary of  [2.x.47]  -- which would make it continuous between the interior and boundary), then  [2.x.48]  would simply be the exact gradient  [2.x.49] . But, since  [2.x.50]  is not continuous between interior and boundary of  [2.x.51] , we need a more general definition; furthermore, we can not deal with arbitrary functions, and so require that  [2.x.52]  is also in a finite element space (which, since the gradient is a vector, has to be vector-valued, and because the weak gradient is defined on each cell separately, will also be discontinuous between cells). 

The way this is done is to define this weak gradient operator  [2.x.53]  (where  [2.x.54]  is the vector-valued Raviart-Thomas space of order  [2.x.55]  on cell  [2.x.56] ) in the following way: [1.x.62] 

for all test functions  [2.x.57] . This is, in essence, simply an application of the integration-by-parts formula. In other words, for a given  [2.x.58] , we need to think of  [2.x.59]  as that Raviart-Thomas function of degree  [2.x.60]  for which the left hand side and right hand side are equal for all test functions. 

A key point to make is then the following: While the usual gradient  [2.x.61]  is a *local* operator that computes derivatives based simply on the value of a function at a point and its (infinitesimal) neighborhood, the weak discrete gradient  [2.x.62]  does not have this property: It depends on the values of the function it is applied to on the entire cell, including the cell's boundary. Both are, however, linear operators as is clear from the definition of  [2.x.63]  above, and that will allow us to represent  [2.x.64]  via a matrix in the discussion below. 

 [2.x.65]  It may be worth pointing out that while the weak discrete   gradient is an element of the Raviart-Thomas space  [2.x.66]  on each   cell  [2.x.67] , it is discontinuous between cells. On the other hand, the   Raviart-Thomas space  [2.x.68]  defined on the entire   mesh and implemented by the FE_RaviartThomas class represents   functions that have continuous normal components at interfaces   between cells. This means that [1.x.63],  [2.x.69]    is not in  [2.x.70] , even though it is on every cell  [2.x.71]  in  [2.x.72] .   Rather, it is in a "broken" Raviart-Thomas space that below we will   represent by the symbol  [2.x.73] . (The term "broken" here refers to   the process of "breaking something apart", and not to the synonym to   the expression "not functional".) One might therefore (rightfully) argue that   the notation used in the weak Galerkin literature is a bit misleading,   but as so often it all depends on the context in which a certain   notation is used -- in the current context, references to the   Raviart-Thomas space or element are always understood to be to the   "broken" spaces. 

 [2.x.74]  deal.II happens to have an implementation of this broken Raviart-Thomas   space: The FE_DGRT class. As a consequence, in this tutorial we will simply   always use the FE_DGRT class, even though in all of those places where   we have to compute cell-local matrices and vectors, it makes no difference. 


[1.x.64][1.x.65] 


Since  [2.x.75]  is an element of a finite element space, we can expand it in a basis as we always do, i.e., we can write [1.x.66] 

Here, since  [2.x.76]  has two components (the interior and the interface components), the same must hold true for the basis functions  [2.x.77] , which we can write as  [2.x.78] . If you've followed the descriptions in  [2.x.79] ,  [2.x.80] , and the  [2.x.81]  "documentation module on vector-valued problems", it will be no surprise that for some values of  [2.x.82] ,  [2.x.83]  will be zero, whereas for other values of  [2.x.84] ,  [2.x.85]  will be zero -- i.e., shape functions will be of either one or the other kind. That is not important, here, however. What is important is that we need to wonder how we can represent  [2.x.86]  because that is clearly what will appear in the problem when we want to implement the bilinear form [1.x.67] 



The key point is that  [2.x.87]  is known to be a member of the "broken" Raviart-Thomas space  [2.x.88] . What this means is that we can represent (on each cell  [2.x.89]  separately) [1.x.68] 

where the functions  [2.x.90] , and where  [2.x.91]  is a matrix of dimension 

[1.x.69] 

(That the weak discrete gradient can be represented as a matrix should not come as a surprise: It is a linear operator from one finite dimensional space to another finite dimensional space. If one chooses bases for both of these spaces, then [1.x.70] can of course be written as a matrix mapping the vector of expansion coefficients with regards to the basis of the domain space of the operator, to the vector of expansion coefficients with regards to the basis in the image space.) 

Using this expansion, we can easily use the definition of the weak discrete gradient above to define what the matrix is going to be: [1.x.71] 

for all test functions  [2.x.92] . 

This clearly leads to a linear system of the form [1.x.72] 

with [1.x.73] 

and consequently [1.x.74] 

(In this last step, we have assumed that the indices  [2.x.93]  only range over those degrees of freedom active on cell  [2.x.94] , thereby ensuring that the mass matrix on the space  [2.x.95]  is invertible.) Equivalently, using the symmetry of the matrix  [2.x.96] , we have that [1.x.75] 

Also worth pointing out is that the matrices  [2.x.97]  and  [2.x.98]  are of course not square but rectangular. 


[1.x.76][1.x.77] 


Having explained how the weak discrete gradient is defined, we can now come back to the question of how the linear system for the equation in question should be assembled. Specifically, using the definition of the bilinear form  [2.x.99]  shown above, we then need to compute the elements of the local contribution to the global matrix, [1.x.78] 

As explained above, we can expand  [2.x.100]  in terms of the Raviart-Thomas basis on each cell, and similarly for  [2.x.101] : [1.x.79] 

By re-arranging sums, this yields the following expression: [1.x.80] 

So, if we have the matrix  [2.x.102]  for each cell  [2.x.103] , then we can easily compute the contribution  [2.x.104]  for cell  [2.x.105]  to the matrix  [2.x.106]  as follows: [1.x.81] 

Here, [1.x.82] 

which is really just the mass matrix on cell  [2.x.107]  using the Raviart-Thomas basis and weighting by the permeability tensor  [2.x.108] . The derivation here then shows that the weak Galerkin method really just requires us to compute these  [2.x.109]  and  [2.x.110]  matrices on each cell  [2.x.111] , and then  [2.x.112] , which is easily computed. The code to be shown below does exactly this. 

Having so computed the contribution  [2.x.113]  of cell  [2.x.114]  to the global matrix, all we have to do is to "distribute" these local contributions into the global matrix. How this is done is first shown in  [2.x.115]  and  [2.x.116] . In the current program, this will be facilitated by calling  [2.x.117]  

A linear system of course also needs a right hand side. There is no difficulty associated with computing the right hand side here other than the fact that we only need to use the cell-interior part  [2.x.118]  for each shape function  [2.x.119] . 


[1.x.83][1.x.84][1.x.85] 


The discussions in the previous sections have given us a linear system that we can solve for the numerical pressure  [2.x.120] . We can use this to compute an approximation to the variable  [2.x.121]  that corresponds to the velocity with which the medium flows in a porous medium if this is the model we are trying to solve. This kind of step -- computing a derived quantity from the solution of the discrete problem -- is typically called "post-processing". 

Here, instead of using the exact gradient of  [2.x.122] , let us instead use the discrete weak gradient of  [2.x.123]  to calculate the velocity on each element. As discussed above, on each element the gradient of the numerical pressure  [2.x.124]  can be approximated by discrete weak gradients   [2.x.125] : [1.x.86] 



On cell  [2.x.126] , the numerical velocity  [2.x.127]  can be written as 

[1.x.87] 

where  [2.x.128]  is the expansion matrix from above, and  [2.x.129]  is the basis function of the  [2.x.130]  space on a cell. 

Unfortunately,  [2.x.131]  may not be in the  [2.x.132]  space (unless, of course, if  [2.x.133]  is constant times the identity matrix). So, in order to represent it in a finite element program, we need to project it back into a finite dimensional space we can work with. Here, we will use the  [2.x.134] -projection to project it back to the (broken)  [2.x.135]  space. 

We define the projection as  [2.x.136]  on each cell  [2.x.137] . For any  [2.x.138] ,  [2.x.139]  So, rather than the formula shown above, the numerical velocity on cell  [2.x.140]  instead becomes [1.x.88] 

and we have the following system to solve for the coefficients  [2.x.141] : [1.x.89] 

In the implementation below, the matrix with elements  [2.x.142]  is called  [2.x.143] , whereas the matrix with elements  [2.x.144]  is called  [2.x.145] . 

Then the elementwise velocity is [1.x.90] 

where  [2.x.146]  is called `cell_velocity` in the code. 

Using this velocity obtained by "postprocessing" the solution, we can define the  [2.x.147] -errors of pressure, velocity, and flux by the following formulas: 

[1.x.91] 

where  [2.x.148]  is the area of the element,  [2.x.149]  are faces of the element,  [2.x.150]  are unit normal vectors of each face. The last of these norms measures the accuracy of the normal component of the velocity vectors over the interfaces between the cells of the mesh. The scaling factor  [2.x.151]  is chosen so as to scale out the difference in the length (or area) of the collection of interfaces as the mesh size changes. 

The first of these errors above is easily computed using  [2.x.152]  The others require a bit more work and are implemented in the code below. [1.x.92] [1.x.93] 


[1.x.94]  [1.x.95] This program is based on  [2.x.153] ,  [2.x.154]  and  [2.x.155] , so most of the following header files are familiar. We need the following, of which only the one that imports the FE_DGRaviartThomas class (namely, `deal.II/fe/fe_dg_vector.h`) is really new; the FE_DGRaviartThomas implements the "broken" Raviart-Thomas space discussed in the introduction: 

[1.x.96] 



Our first step, as always, is to put everything related to this tutorial program into its own namespace: 

[1.x.97] 




[1.x.98]  [1.x.99] 




This is the main class of this program. We will solve for the numerical pressure in the interior and on faces using the weak Galerkin (WG) method, and calculate the  [2.x.156]  error of pressure. In the post-processing step, we will also calculate  [2.x.157] -errors of the velocity and flux.    


The structure of the class is not fundamentally different from that of previous tutorial programs, so there is little need to comment on the details with one exception: The class has a member variable `fe_dgrt` that corresponds to the "broken" Raviart-Thomas space mentioned in the introduction. There is a matching `dof_handler_dgrt` that represents a global enumeration of a finite element field created from this element, and a vector `darcy_velocity` that holds nodal values for this field. We will use these three variables after solving for the pressure to compute a postprocessed velocity field for which we can then evaluate the error and which we can output for visualization. 

[1.x.100] 




[1.x.101]  [1.x.102] 




Next, we define the coefficient matrix  [2.x.158]  (here, the identity matrix), Dirichlet boundary conditions, the right-hand side  [2.x.159] , and the exact solution that corresponds to these choices for  [2.x.160]  and  [2.x.161] , namely  [2.x.162] . 

[1.x.103] 



The class that implements the exact pressure solution has an oddity in that we implement it as a vector-valued one with two components. (We say that it has two components in the constructor where we call the constructor of the base Function class.) In the `value()` function, we do not test for the value of the `component` argument, which implies that we return the same value for both components of the vector-valued function. We do this because we describe the finite element in use in this program as a vector-valued system that contains the interior and the interface pressures, and when we compute errors, we will want to use the same pressure solution to test both of these components. 

[1.x.104] 




[1.x.105]  [1.x.106] 





[1.x.107]  [1.x.108] 




In this constructor, we create a finite element space for vector valued functions, which will here include the ones used for the interior and interface pressures,  [2.x.163]  and  [2.x.164] . 

[1.x.109] 




[1.x.110]  [1.x.111] 




We generate a mesh on the unit square domain and refine it. 

[1.x.112] 




[1.x.113]  [1.x.114] 




After we have created the mesh above, we distribute degrees of freedom and resize matrices and vectors. The only piece of interest in this function is how we interpolate the boundary values for the pressure. Since the pressure consists of interior and interface components, we need to make sure that we only interpolate onto that component of the vector-valued solution space that corresponds to the interface pressures (as these are the only ones that are defined on the boundary of the domain). We do this via a component mask object for only the interface pressures. 

[1.x.115] 



In the bilinear form, there is no integration term over faces between two neighboring cells, so we can just use  [2.x.165]  to calculate the sparse matrix. 

[1.x.116] 




[1.x.117]  [1.x.118] 




This function is more interesting. As detailed in the introduction, the assembly of the linear system requires us to evaluate the weak gradient of the shape functions, which is an element in the Raviart-Thomas space. As a consequence, we need to define a Raviart-Thomas finite element object, and have FEValues objects that evaluate it at quadrature points. We then need to compute the matrix  [2.x.166]  on every cell  [2.x.167] , for which we need the matrices  [2.x.168]  and  [2.x.169]  mentioned in the introduction.    


A point that may not be obvious is that in all previous tutorial programs, we have always called  [2.x.170]  with a cell iterator from a DoFHandler. This is so that one can call functions such as  [2.x.171]  that extract the values of a finite element function (represented by a vector of DoF values) on the quadrature points of a cell. For this operation to work, one needs to know which vector elements correspond to the degrees of freedom on a given cell -- i.e., exactly the kind of information and operation provided by the DoFHandler class.    


We could create a DoFHandler object for the "broken" Raviart-Thomas space (using the FE_DGRT class), but we really don't want to here: At least in the current function, we have no need for any globally defined degrees of freedom associated with this broken space, but really only need to reference the shape functions of such a space on the current cell. As a consequence, we use the fact that one can call  [2.x.172]  also with cell iterators into Triangulation objects (rather than DoFHandler objects). In this case, FEValues can of course only provide us with information that only references information about cells, rather than degrees of freedom enumerated on these cells. So we can't use  [2.x.173]  but we can use  [2.x.174]  to obtain the values of shape functions at quadrature points on the current cell. It is this kind of functionality we will make use of below. The variable that will give us this information about the Raviart-Thomas functions below is then the `fe_values_rt` (and corresponding `fe_face_values_rt`) object.    


Given this introduction, the following declarations should be pretty obvious: 

[1.x.119] 



Next, let us declare the various cell matrices discussed in the introduction: 

[1.x.120] 



We need  [2.x.175]  to access the  [2.x.176]  and  [2.x.177]  component of the shape functions. 

[1.x.121] 



This finally gets us in position to loop over all cells. On each cell, we will first calculate the various cell matrices used to construct the local matrix -- as they depend on the cell in question, they need to be re-computed on each cell. We need shape functions for the Raviart-Thomas space as well, for which we need to create first an iterator to the cell of the triangulation, which we can obtain by assignment from the cell pointing into the DoFHandler. 

[1.x.122] 



The first cell matrix we will compute is the mass matrix for the Raviart-Thomas space.  Hence, we need to loop over all the quadrature points for the velocity FEValues object. 

[1.x.123] 



Next we take the inverse of this matrix by using  [2.x.178]  It will be used to calculate the coefficient matrix  [2.x.179]  later. It is worth recalling later that `cell_matrix_M` actually contains the *inverse* of  [2.x.180]  after this call. 

[1.x.124] 



From the introduction, we know that the right hand side  [2.x.181]  of the equation that defines  [2.x.182]  is the difference between a face integral and a cell integral. Here, we approximate the negative of the contribution in the interior. Each component of this matrix is the integral of a product between a basis function of the polynomial space and the divergence of a basis function of the Raviart-Thomas space. These basis functions are defined in the interior. 

[1.x.125] 



Next, we approximate the integral on faces by quadrature. Each component is the integral of a product between a basis function of the polynomial space and the dot product of a basis function of the Raviart-Thomas space and the normal vector. So we loop over all the faces of the element and obtain the normal vector. 

[1.x.126] 



 [2.x.183]  is then the matrix product between the transpose of  [2.x.184]  and the inverse of the mass matrix (where this inverse is stored in  [2.x.185]  

[1.x.127] 



Finally we can compute the local matrix  [2.x.186] .  Element  [2.x.187]  is given by  [2.x.188] . We have calculated the coefficients  [2.x.189]  in the previous step, and so obtain the following after suitably re-arranging the loops: 

[1.x.128] 



Next, we calculate the right hand side,  [2.x.190] : 

[1.x.129] 



The last step is to distribute components of the local matrix into the system matrix and transfer components of the cell right hand side into the system right hand side: 

[1.x.130] 




[1.x.131]  [1.x.132] 




This step is rather trivial and the same as in many previous tutorial programs: 

[1.x.133] 




[1.x.134]  [1.x.135] 




In this function, compute the velocity field from the pressure solution previously computed. The velocity is defined as  [2.x.191] , which requires us to compute many of the same terms as in the assembly of the system matrix. There are also the matrices  [2.x.192]  we need to assemble (see the introduction) but they really just follow the same kind of pattern.    


Computing the same matrices here as we have already done in the `assemble_system()` function is of course wasteful in terms of CPU time. Likewise, we copy some of the code from there to this function, and this is also generally a poor idea. A better implementation might provide for a function that encapsulates this duplicated code. One could also think of using the classic trade-off between computing efficiency and memory efficiency to only compute the  [2.x.193]  matrices once per cell during the assembly, storing them somewhere on the side, and re-using them here. (This is what  [2.x.194]  does, for example, where the `assemble_system()` function takes an argument that determines whether the local matrices are recomputed, and a similar approach 

-- maybe with storing local matrices elsewhere -- could be adapted for the current program.) 

[1.x.136] 



In the introduction, we explained how to calculate the numerical velocity on the cell. We need the pressure solution values on each cell, coefficients of the Gram matrix and coefficients of the  [2.x.195]  projection. We have already calculated the global solution, so we will extract the cell solution from the global solution. The coefficients of the Gram matrix have been calculated when we assembled the system matrix for the pressures. We will do the same way here. For the coefficients of the projection, we do matrix multiplication, i.e., the inverse of the Gram matrix times the matrix with  [2.x.196]  as components. Then, we multiply all these coefficients and call them beta. The numerical velocity is the product of beta and the basis functions of the Raviart-Thomas space. 

[1.x.137] 



The component of this  [2.x.197]  is the integral of  [2.x.198] .  [2.x.199]  is the Gram matrix. 

[1.x.138] 



To compute the matrix  [2.x.200]  mentioned in the introduction, we then need to evaluate  [2.x.201]  as explained in the introduction: 

[1.x.139] 



Then we also need, again, to compute the matrix  [2.x.202]  that is used to evaluate the weak discrete gradient. This is the exact same code as used in the assembly of the system matrix, so we just copy it from there: 

[1.x.140] 



Finally, we need to extract the pressure unknowns that correspond to the current cell: 

[1.x.141] 



We are now in a position to compute the local velocity unknowns (with respect to the Raviart-Thomas space we are projecting the term  [2.x.203]  into): 

[1.x.142] 



We compute Darcy velocity. This is same as cell_velocity but used to graph Darcy velocity. 

[1.x.143] 




[1.x.144]  [1.x.145] 




This part is to calculate the  [2.x.204]  error of the pressure.  We define a vector that holds the norm of the error on each cell. Next, we use  [2.x.205]  to compute the error in the  [2.x.206]  norm on each cell. However, we really only care about the error in the interior component of the solution vector (we can't even evaluate the interface pressures at the quadrature points because these are all located in the interior of cells) and consequently have to use a weight function that ensures that the interface component of the solution variable is ignored. This is done by using the ComponentSelectFunction whose arguments indicate which component we want to select (component zero, i.e., the interior pressures) and how many components there are in total (two). 

[1.x.146] 




[1.x.147]  [1.x.148] 




In this function, we evaluate  [2.x.207]  errors for the velocity on each cell, and  [2.x.208]  errors for the flux on faces. The function relies on the `compute_postprocessed_velocity()` function having previous computed, which computes the velocity field based on the pressure solution that has previously been computed.    


We are going to evaluate velocities on each cell and calculate the difference between numerical and exact velocities. 

[1.x.149] 



Having previously computed the postprocessed velocity, we here only have to extract the corresponding values on each cell and face and compare it to the exact values. 

[1.x.150] 



First compute the  [2.x.209]  error between the postprocessed velocity field and the exact one: 

[1.x.151] 



For reconstructing the flux we need the size of cells and faces. Since fluxes are calculated on faces, we have the loop over all four faces of each cell. To calculate the face velocity, we extract values at the quadrature points from the `darcy_velocity` which we have computed previously. Then, we calculate the squared velocity error in normal direction. Finally, we calculate the  [2.x.210]  flux error on the cell by appropriately scaling with face and cell areas and add it to the global error. 

[1.x.152] 



After adding up errors over all cells and faces, we take the square root and get the  [2.x.211]  errors of velocity and flux. These we output to screen. 

[1.x.153] 




[1.x.154]  [1.x.155] 




We have two sets of results to output: the interior solution and the skeleton solution. We use  [2.x.212]  to visualize interior results. The graphical output for the skeleton results is done by using the DataOutFaces class.    


In both of the output files, both the interior and the face variables are stored. For the interface output, the output file simply contains the interpolation of the interior pressures onto the faces, but because it is undefined which of the two interior pressure variables you get from the two adjacent cells, it is best to ignore the interior pressure in the interface output file. Conversely, for the cell interior output file, it is of course impossible to show any interface pressures  [2.x.213] , because these are only available on interfaces and not cell interiors. Consequently, you will see them shown as an invalid value (such as an infinity).    


For the cell interior output, we also want to output the velocity variables. This is a bit tricky since it lives on the same mesh but uses a different DoFHandler object (the pressure variables live on the `dof_handler` object, the Darcy velocity on the `dof_handler_dgrt` object). Fortunately, there are variations of the  [2.x.214]  function that allow specifying which DoFHandler a vector corresponds to, and consequently we can visualize the data from both DoFHandler objects within the same file. 

[1.x.156] 



First attach the pressure solution to the DataOut object: 

[1.x.157] 



Then do the same with the Darcy velocity field, and continue with writing everything out into a file. 

[1.x.158] 




[1.x.159]  [1.x.160] 




This is the final function of the main class. It calls the other functions of our class. 

[1.x.161] 




[1.x.162]  [1.x.163] 




This is the main function. We can change the dimension here to run in 3d. 

[1.x.164] 

[1.x.165][1.x.166] 


We run the program with a right hand side that will produce the solution  [2.x.215]  and with homogeneous Dirichlet boundary conditions in the domain  [2.x.216] . In addition, we choose the coefficient matrix in the differential operator  [2.x.217]  as the identity matrix. We test this setup using  [2.x.218] ,  [2.x.219]  and  [2.x.220]  element combinations, which one can select by using the appropriate constructor argument for the `WGDarcyEquation` object in `main()`. We will then visualize pressure values in interiors of cells and on faces. We want to see that the pressure maximum is around 1 and the minimum is around 0. With mesh refinement, the convergence rates of pressure, velocity and flux should then be around 1 for  [2.x.221]  , 2 for  [2.x.222] , and 3 for  [2.x.223] . 


[1.x.167][1.x.168][1.x.169] 


The following figures show interior pressures and face pressures using the  [2.x.224]  element. The mesh is refined 2 times (top) and 4 times (bottom), respectively. (This number can be adjusted in the `make_grid()` function.) When the mesh is coarse, one can see the face pressures  [2.x.225]  neatly between the values of the interior pressures  [2.x.226]  on the two adjacent cells. 

 [2.x.227]  

From the figures, we can see that with the mesh refinement, the maximum and minimum pressure values are approaching the values we expect. Since the mesh is a rectangular mesh and numbers of cells in each direction is even, we have symmetric solutions. From the 3d figures on the right, we can see that on  [2.x.228] , the pressure is a constant in the interior of the cell, as expected. 

[1.x.170][1.x.171][1.x.172] 


We run the code with differently refined meshes (chosen in the `make_grid()` function) and get the following convergence rates of pressure, velocity, and flux (as defined in the introduction). 

 [2.x.229]  

We can see that the convergence rates of  [2.x.230]  are around 1. This, of course, matches our theoretical expectations. 


[1.x.173][1.x.174][1.x.175] 


We can repeat the experiment from above using the next higher polynomial degree: The following figures are interior pressures and face pressures implemented using  [2.x.231] . The mesh is refined 4 times.  Compared to the previous figures using  [2.x.232] , on each cell, the solution is no longer constant on each cell, as we now use bilinear polynomials to do the approximation. Consequently, there are 4 pressure values in one interior, 2 pressure values on each face. 

 [2.x.233]  

Compared to the corresponding image for the  [2.x.234]  combination, the solution is now substantially more accurate and, in particular so close to being continuous at the interfaces that we can no longer distinguish the interface pressures  [2.x.235]  from the interior pressures  [2.x.236]  on the adjacent cells. 

[1.x.176][1.x.177][1.x.178] 


The following are the convergence rates of pressure, velocity, and flux we obtain from using the  [2.x.237]  element combination: 

 [2.x.238]  

The convergence rates of  [2.x.239]  are around 2, as expected. 




[1.x.179][1.x.180][1.x.181] 


Let us go one polynomial degree higher. The following are interior pressures and face pressures implemented using  [2.x.240] , with mesh size  [2.x.241]  (i.e., 5 global mesh refinement steps). In the program, we use `data_out_face.build_patches(fe.degree)` when generating graphical output (see the documentation of  [2.x.242]  which here implies that we divide each 2d cell interior into 4 subcells in order to provide a better visualization of the quadratic polynomials.  [2.x.243]  


[1.x.182][1.x.183][1.x.184] 


As before, we can generate convergence data for the  [2.x.244]  errors of pressure, velocity, and flux using the  [2.x.245]  combination: 

 [2.x.246]  

Once more, the convergence rates of  [2.x.247]  is as expected, with values around 3. [1.x.185] [1.x.186]  [2.x.248]  

 [2.x.249] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33][1.x.34][1.x.35][1.x.36][1.x.37][1.x.38] 

 [2.x.4]  

[1.x.39]  [2.x.5]  




 [2.x.6]  As a prerequisite of this program, you need to have HDF5, complex PETSc, and the p4est libraries installed. The installation of deal.II together with these additional libraries is described in the [1.x.40] file. 

[1.x.41][1.x.42] 

A phononic crystal is a periodic nanostructure that modifies the motion of mechanical vibrations or [phonons](https://en.wikipedia.org/wiki/Phonon). Phononic structures can be used to disperse, route and confine mechanical vibrations. These structures have potential applications in [quantum information](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391) and have been used to study [macroscopic quantum phenomena](https://science.sciencemag.org/content/358/6360/203). Phononic crystals are usually fabricated in [cleanrooms](https://en.wikipedia.org/wiki/Cleanroom). 

In this tutorial we show how to a design a [phononic superlattice cavity](https://doi.org/10.1103/PhysRevA.94.033813) which is a particular type of phononic crystal that can be used to confine mechanical vibrations. A phononic superlattice cavity is formed by two [Distributed Bragg Reflector](https://en.wikipedia.org/wiki/Distributed_Bragg_reflector), mirrors and a  [2.x.7]  cavity where  [2.x.8]  is the acoustic wavelength. Acoustic DBRs are  periodic structures where a set of bilayer stacks with contrasting physical properties (sound velocity index) is repeated  [2.x.9]  times. Superlattice cavities are usually grown on a [Gallium Arsenide](https://en.wikipedia.org/wiki/Gallium_arsenide) wafer by [Molecular Beam Epitaxy](https://en.wikipedia.org/wiki/Molecular-beam_epitaxy). The bilayers correspond to GaAs/AlAs mirror pairs. As shown below, the thickness of the mirror layers (brown and green) is  [2.x.10]  and the thickness of the cavity (blue) is  [2.x.11] . 

 [2.x.12]  

In this tutorial we calculate the [band gap](https://en.wikipedia.org/wiki/Band_gap) and the mechanical resonance of a phononic superlattice cavity but the code presented here can be easily used to design and calculate other types of [phononic crystals](https://science.sciencemag.org/content/358/6360/203). 

The device is a waveguide in which the wave goes from left to right. The simulations of this tutorial are done in 2D, but the code is dimension independent and can be easily used with 3D simulations. The waveguide width is equal to the  [2.x.13]  dimension of the domain and the waveguide length is equal to the  [2.x.14]  dimension of the domain. There are two regimes that depend on the waveguide width: 

- Single mode: In this case the width of the structure is much   smaller than the wavelength.   This case can be solved either with FEM (the approach that we take here) or with   a simple semi-analytical   [1D transfer matrix formalism](https://en.wikipedia.org/wiki/Transfer_matrix). 

- Multimode: In this case the width of the structure is larger than the wavelength.   This case can be solved using FEM   or with a [scattering matrix formalism](https://doi.org/10.1103/PhysRevA.94.033813).   Although we do not study this case in this tutorial, it is very easy to reach the multimode   regime by increasing the parameter waveguide width (`dimension_y` in the jupyter notebook). 

The simulations of this tutorial are performed in the frequency domain. To calculate the transmission spectrum, we use a [procedure](https://meep.readthedocs.io/en/latest/Python_Tutorials/Resonant_Modes_and_Transmission_in_a_Waveguide_Cavity/) that is commonly used in time domain [FDTD](https://en.wikipedia.org/wiki/Finite-difference_time-domain_method) simulations. A pulse at a certain frequency is generated on the left side of the structure and the transmitted energy is measured on the right side of the structure. The simulation is run twice. First, we run the simulation with the phononic structure and measure the transmitted energy: 

 [2.x.15]  

Then, we run the simulation without the phononic structure and measure the transmitted energy. We use the simulation without the structure for the calibration: 

 [2.x.16]  

The transmission coefficient corresponds to the energy of the first simulation divided by the calibration energy. We repeat this procedure for each frequency step. 


[1.x.43][1.x.44] 

What we want to simulate here is the transmission of elastic waves. Consequently, the right description of the problem uses the elastic equations, which in the time domain are given by [1.x.45] where the stiffness tensor  [2.x.17]  depends on the spatial coordinates and the strain is the symmetrized gradient of the displacement, given by [1.x.46] 

[A perfectly matched layer (PML)](https://en.wikipedia.org/wiki/Perfectly_matched_layer) can be used to truncate the solution at the boundaries. A PML is a transformation that results in a complex coordinate stretching. 

Instead of a time domain approach, this tutorial program converts the equations above into the frequency domain by performing a Fourier transform with regard to the time variable. The elastic equations in the frequency domain then read as follows [1.x.47] 

where the coefficients  [2.x.18]  account for the absorption. There are 3  [2.x.19]  coefficients in 3D and 2 in 2D. The imaginary par of  [2.x.20]  is equal to zero outside the PML. The PMLs are reflectionless only for the exact wave equations. When the set of equations is discretized the PML is no longer reflectionless. The reflections can be made arbitrarily small as long as the medium is slowly varying, see [the adiabatic theorem](https://doi.org/10.1103/PhysRevE.66.066608). In the code a quadratic turn-on of the PML has been used. A linear and cubic turn-on is also [known to work](https://doi.org/10.1364/OE.16.011376). These equations can be expanded into [1.x.48] [1.x.49] where summation over repeated indices (here  [2.x.21] , as well as  [2.x.22]  and  [2.x.23] ) is as always implied. Note that the strain is no longer symmetric after applying the complex coordinate stretching of the PML. This set of equations can be written as [1.x.50] 

The same as the strain, the stress tensor is not symmetric inside the PML ( [2.x.24] ). Indeed the fields inside the PML are not physical. It is useful to introduce the tensors  [2.x.25]  and  [2.x.26] . [1.x.51] 

We can multiply by  [2.x.27]  and integrate over the domain  [2.x.28]  and integrate by parts. [1.x.52] 

It is this set of equations we want to solve for a set of frequencies  [2.x.29]  in order to compute the transmission coefficient as function of frequency. The linear system becomes [1.x.53] 



[1.x.54][1.x.55] 

In this tutorial we use a python [jupyter notebook](https://github.com/dealii/dealii/blob/phononic-cavity/examples/ [2.x.30] / [2.x.31] .ipynb) to set up the parameters and run the simulation. First we create a HDF5 file where we store the parameters and the results of the simulation. 

Each of the simulations (displacement and calibration) is stored in a separate HDF5 group: 

[1.x.56] 

[1.x.57] [1.x.58] 


[1.x.59]  [1.x.60] 




Most of the include files we need for this program have already been discussed in previous programs, in particular in  [2.x.32] . 

[1.x.61] 



The following header provides the Tensor class that we use to represent the material properties. 

[1.x.62] 



The following header is necessary for the HDF5 interface of deal.II. 

[1.x.63] 



This header is required for the function  [2.x.33]  that we use to evaluate the result of the simulation. 

[1.x.64] 



We need this header for the function  [2.x.34]  that we use in the function  [2.x.35]  

[1.x.65] 




[1.x.66]  [1.x.67] The following classes are used to store the parameters of the simulation. 





[1.x.68]  [1.x.69] This class is used to define the force pulse on the left side of the structure: 

[1.x.70] 



The variable `data` is the  [2.x.36]  in which all the simulation results will be stored. Note that the variables  [2.x.37]   [2.x.38]   [2.x.39]  and  [2.x.40]  point to the same group of the HDF5 file. When a  [2.x.41]  is copied, it will point to the same group of the HDF5 file. 

[1.x.71] 



The simulation parameters are stored in `data` as HDF5 attributes. The following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor. 

[1.x.72] 



In this particular simulation the force has only a  [2.x.42]  component,  [2.x.43] . 

[1.x.73] 




[1.x.74]  [1.x.75] This class is used to define the shape of the Perfectly Matches Layer (PML) to absorb waves traveling towards the boundary: 

[1.x.76] 



 [2.x.44]  in which all the simulation results will be stored. 

[1.x.77] 



The same as before, the following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor. 

[1.x.78] 




[1.x.79]  [1.x.80] This class is used to define the mass density. 

[1.x.81] 



 [2.x.45]  in which all the simulation results will be stored. 

[1.x.82] 



The same as before, the following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor. 

[1.x.83] 




[1.x.84]  [1.x.85] This class contains all the parameters that will be used in the simulation. 

[1.x.86] 



 [2.x.46]  in which all the simulation results will be stored. 

[1.x.87] 



The same as before, the following attributes are defined in the jupyter notebook, stored in `data` as HDF5 attributes and then read by the constructor. 

[1.x.88] 




[1.x.89]  [1.x.90] The calculation of the mass and stiffness matrices is very expensive. These matrices are the same for all the frequency steps. The right hand side vector is also the same for all the frequency steps. We use this class to store these objects and re-use them at each frequency step. Note that here we don't store the assembled mass and stiffness matrices and right hand sides, but instead the data for a single cell. `QuadratureCache` class is very similar to the `PointHistory` class that has been used in  [2.x.47] . 

[1.x.91] 



We store the mass and stiffness matrices in the variables mass_coefficient and stiffness_coefficient. We store as well the right_hand_side and JxW values which are going to be the same for all the frequency steps. 

[1.x.92] 




[1.x.93]  [1.x.94] 




This function returns the stiffness tensor of the material. For the sake of simplicity we consider the stiffness to be isotropic and homogeneous; only the density  [2.x.48]  depends on the position. As we have previously shown in  [2.x.49] , if the stiffness is isotropic and homogeneous, the stiffness coefficients  [2.x.50]  can be expressed as a function of the two coefficients  [2.x.51]  and  [2.x.52] . The coefficient tensor reduces to [1.x.95] 

[1.x.96] 




[1.x.97]  [1.x.98] 




Next let's declare the main class of this program. Its structure is very similar to the  [2.x.53]  tutorial program. The main differences are: 

- The sweep over the frequency values. 

- We save the stiffness and mass matrices in `quadrature_cache` and use them for each frequency step. 

- We store the measured energy by the probe for each frequency step in the HDF5 file. 

[1.x.99] 



This is called before every frequency step to set up a pristine state for the cache variables. 

[1.x.100] 



This function loops over the frequency vector and runs the simulation for each frequency step. 

[1.x.101] 



The parameters are stored in this variable. 

[1.x.102] 



We store the mass and stiffness matrices for each cell this vector. 

[1.x.103] 



This vector contains the range of frequencies that we are going to simulate. 

[1.x.104] 



This vector contains the coordinates  [2.x.54]  of the points of the measurement probe. 

[1.x.105] 



HDF5 datasets to store the frequency and `probe_positions` vectors. 

[1.x.106] 



HDF5 dataset that stores the values of the energy measured by the probe. 

[1.x.107] 




[1.x.108]  [1.x.109] 





[1.x.110]  [1.x.111] 




The constructor reads all the parameters from the  [2.x.55]  `data` using the  [2.x.56]  function. 

[1.x.112] 



This function defines the spatial shape of the force vector pulse which takes the form of a Gaussian function 

[1.x.113] 

where  [2.x.57]  is the maximum amplitude that takes the force and  [2.x.58]  and  [2.x.59]  are the standard deviations for the  [2.x.60]  and  [2.x.61]  components. Note that the pulse has been cropped to  [2.x.62]  and  [2.x.63] . 

[1.x.114] 




[1.x.115]  [1.x.116] 




As before, the constructor reads all the parameters from the  [2.x.64]  `data` using the  [2.x.65]  function. As we have discussed, a quadratic turn-on of the PML has been defined in the jupyter notebook. It is possible to use a linear, cubic or another power degree by changing the parameter `pml_coeff_degree`. The parameters `pml_x` and `pml_y` can be used to turn on and off the `x` and `y` PMLs. 

[1.x.117] 



The PML coefficient for the `x` component takes the form  [2.x.66]  

[1.x.118] 




[1.x.119]  [1.x.120] 




This class is used to define the mass density. As we have explaine before, a phononic superlattice cavity is formed by two [Distributed Reflector](https://en.wikipedia.org/wiki/Band_gap), mirrors and a  [2.x.67]  cavity where  [2.x.68]  is the acoustic wavelength. Acoustic DBRs are periodic structures where a set of bilayer stacks with contrasting physical properties (sound velocity index) is repeated  [2.x.69]  times. The change of in the wave velocity is generated by alternating layers with different density. 

[1.x.121] 



In order to increase the precision we use [subpixel smoothing](https://meep.readthedocs.io/en/latest/Subpixel_Smoothing/). 

[1.x.122] 



The speed of sound is defined by [1.x.123] where  [2.x.70]  is the effective elastic constant and  [2.x.71]  the density. Here we consider the case in which the waveguide width is much smaller than the wavelength. In this case it can be shown that for the two dimensional case [1.x.124] and for the three dimensional case  [2.x.72]  is equal to the Young's modulus. [1.x.125] 

[1.x.126] 



The density  [2.x.73]  takes the following form <img alt="Phononic superlattice cavity" src="https://www.dealii.org/images/steps/developer/ [2.x.74] .04.svg" height="200" /> where the brown color represents material_a and the green color represents material_b. 

[1.x.127] 



Here we define the [subpixel smoothing](https://meep.readthedocs.io/en/latest/Subpixel_Smoothing/) which improves the precision of the simulation. 

[1.x.128] 



then the cavity 

[1.x.129] 



the material_a layers 

[1.x.130] 



the material_b layers 

[1.x.131] 



and finally the default is material_a. 

[1.x.132] 




[1.x.133]  [1.x.134] 




The constructor reads all the parameters from the  [2.x.75]  `data` using the  [2.x.76]  function. 

[1.x.135] 




[1.x.136]  [1.x.137] 




We need to reserve enough space for the mass and stiffness matrices and the right hand side vector. 

[1.x.138] 




[1.x.139]  [1.x.140] 





[1.x.141]  [1.x.142] 




This is very similar to the constructor of  [2.x.77] . In addition we create the HDF5 datasets `frequency_dataset`, `position_dataset` and `displacement`. Note the use of the `template` keyword for the creation of the HDF5 datasets. It is a C++ requirement to use the `template` keyword in order to treat `create_dataset` as a dependent template name. 

[1.x.143] 




[1.x.144]  [1.x.145] 




There is nothing new in this function, the only difference with  [2.x.78]  is that we don't have to apply boundary conditions because we use the PMLs to truncate the domain. 

[1.x.146] 




[1.x.147]  [1.x.148] 




This function is also very similar to  [2.x.79] , though there are notable differences. We assemble the system for each frequency/omega step. In the first step we set `calculate_quadrature_data = True` and we calculate the mass and stiffness matrices and the right hand side vector. In the subsequent steps we will use that data to accelerate the calculation. 

[1.x.149] 



Here we store the value of the right hand side, rho and the PML. 

[1.x.150] 



We calculate the stiffness tensor for the  [2.x.80]  and  [2.x.81]  that have been defined in the jupyter notebook. Note that contrary to  [2.x.82]  the stiffness is constant among for the whole domain. 

[1.x.151] 



We use the same method of  [2.x.83]  for vector-valued problems. 

[1.x.152] 



We have to calculate the values of the right hand side, rho and the PML only if we are going to calculate the mass and the stiffness matrices. Otherwise we can skip this calculation which considerably reduces the total calculation time. 

[1.x.153] 



We have done this in  [2.x.84] . Get a pointer to the quadrature cache data local to the present cell, and, as a defensive measure, make sure that this pointer is within the bounds of the global array: 

[1.x.154] 



The quadrature_data variable is used to store the mass and stiffness matrices, the right hand side vector and the value of `JxW`. 

[1.x.155] 



Below we declare the force vector and the parameters of the PML  [2.x.85]  and  [2.x.86] . 

[1.x.156] 



The following block is calculated only in the first frequency step. 

[1.x.157] 



Store the value of `JxW`. 

[1.x.158] 



Convert vectors to tensors and calculate xi 

[1.x.159] 



Here we calculate the  [2.x.87]  and  [2.x.88]  tensors. 

[1.x.160] 



calculate the values of the mass matrix. 

[1.x.161] 



Loop over the  [2.x.89]  indices of the stiffness tensor. 

[1.x.162] 



Here we calculate the stiffness matrix. Note that the stiffness matrix is not symmetric because of the PMLs. We use the gradient function (see the [documentation](https://www.dealii.org/current/doxygen/deal.II/group__vector__valued.html)) which is a  [2.x.90] . The matrix  [2.x.91]  consists of entries [1.x.163] Note the position of the indices  [2.x.92]  and  [2.x.93]  and the notation that we use in this tutorial:  [2.x.94] . As the stiffness tensor is not symmetric, it is very easy to make a mistake. 

[1.x.164] 



We save the value of the stiffness matrix in quadrature_data 

[1.x.165] 



and the value of the right hand side in quadrature_data. 

[1.x.166] 



We loop again over the degrees of freedom of the cells to calculate the system matrix. These loops are really quick because we have already calculated the stiffness and mass matrices, only the value of  [2.x.95]  changes. 

[1.x.167] 




[1.x.168]  [1.x.169] 




This is even more simple than in  [2.x.96] . We use the parallel direct solver MUMPS which requires less options than an iterative solver. The drawback is that it does not scale very well. It is not straightforward to solve the Helmholtz equation with an iterative solver. The shifted Laplacian multigrid method is a well known approach to precondition this system, but this is beyond the scope of this tutorial. 

[1.x.170] 




[1.x.171]  [1.x.172] 




We use this function to calculate the values of the position vector. 

[1.x.173] 



Because of the way the operator + and - are overloaded to subtract two points, the following has to be done: `Point_b<dim> + (-Point_a<dim>)` 

[1.x.174] 




[1.x.175]  [1.x.176] 




This function stores in the HDF5 file the measured energy by the probe. 

[1.x.177] 



We store the displacement in the  [2.x.97]  direction; the displacement in the  [2.x.98]  direction is negligible. 

[1.x.178] 



The vector coordinates contains the coordinates in the HDF5 file of the points of the probe that are located in locally owned cells. The vector displacement_data contains the value of the displacement at these points. 

[1.x.179] 



First we have to find out if the point is in a locally owned cell. 

[1.x.180] 



Then we can store the values of the displacement in the points of the probe in `displacement_data`. 

[1.x.181] 



We write the displacement data in the HDF5 file. The call  [2.x.99]  is MPI collective which means that all the processes have to participate. 

[1.x.182] 



Therefore even if the process has no data to write it has to participate in the collective call. For this we can use  [2.x.100]  Note that we have to specify the data type, in this case  [2.x.101]  

[1.x.183] 



If the variable `save_vtu_files` in the input file equals `True` then all the data will be saved as vtu. The procedure to write `vtu` files has been described in  [2.x.102] . 

[1.x.184] 



And on the cells that we are not interested in, set the respective value to a bogus value in order to make sure that if we were somehow wrong about our assumption we would find out by looking at the graphical output: 

[1.x.185] 




[1.x.186]  [1.x.187] 




This function writes the datasets that have not already been written. 

[1.x.188] 



The vectors `frequency` and `position` are the same for all the processes. Therefore any of the processes can write the corresponding `datasets`. Because the call  [2.x.103]  is MPI collective, the rest of the processes will have to call  [2.x.104]  

[1.x.189] 




[1.x.190]  [1.x.191] 




We use this function at the beginning of our computations to set up initial values of the cache variables. This function has been described in  [2.x.105] . There are no differences with the function of  [2.x.106] . 

[1.x.192] 




[1.x.193]  [1.x.194] 




For clarity we divide the function `run` of  [2.x.107]  into the functions `run` and `frequency_sweep`. In the function `frequency_sweep` we place the iteration over the frequency vector. 

[1.x.195] 



Write the simulation parameters only once 

[1.x.196] 



We calculate the frequency and omega values for this particular step. 

[1.x.197] 



In the first frequency step we calculate the mass and stiffness matrices and the right hand side. In the subsequent frequency steps we will use those values. This improves considerably the calculation time. 

[1.x.198] 




[1.x.199]  [1.x.200] 




This function is very similar to the one in  [2.x.108] . 

[1.x.201] 




[1.x.202]  [1.x.203] 




The main function is very similar to the one in  [2.x.109] . 

[1.x.204] 



Each of the simulations (displacement and calibration) is stored in a separate HDF5 group: 

[1.x.205] 



For each of these two group names, we now create the group and put attributes into these groups. Specifically, these are: 

- The dimensions of the waveguide (in  [2.x.110]  and  [2.x.111]  directions) 

- The position of the probe (in  [2.x.112]  and  [2.x.113]  directions) 

- The number of points in the probe 

- The global refinement level 

- The cavity resonance frequency 

- The number of mirror pairs 

- The material properties 

- The force parameters 

- The PML parameters 

- The frequency parameters 







[1.x.206] 



Displacement simulation. The parameters are read from the displacement HDF5 group and the results are saved in the same HDF5 group. 

[1.x.207] 



Calibration simulation. The parameters are read from the calibration HDF5 group and the results are saved in the same HDF5 group. 

[1.x.208] 

[1.x.209][1.x.210] 


[1.x.211][1.x.212] 


The results are analyzed in the [jupyter notebook](https://github.com/dealii/dealii/blob/phononic-cavity/examples/ [2.x.114] / [2.x.115] .ipynb) with the following code 

[1.x.213] 



A phononic cavity is characterized by the [resonance frequency](https://en.wikipedia.org/wiki/Resonance) and the [the quality factor](https://en.wikipedia.org/wiki/Q_factor). The quality factor is equal to the ratio between the stored energy in the resonator and the energy dissipated energy per cycle, which is approximately equivalent to the ratio between the resonance frequency and the [full width at half maximum (FWHM)](https://en.wikipedia.org/wiki/Full_width_at_half_maximum). The FWHM is equal to the bandwidth over which the power of vibration is greater than half the power at the resonant frequency. [1.x.214] 

The square of the amplitude of the mechanical resonance  [2.x.116]  as a function of the frequency has a gaussian shape [1.x.215] where  [2.x.117]  is the resonance frequency and  [2.x.118]  is the dissipation rate. We used the previous equation in the jupyter notebook to fit the mechanical resonance. 

Given the values we have chosen for the parameters, one could estimate the resonance frequency analytically. Indeed, this is then confirmed by what we get in this program: the phononic superlattice cavity exhibits a mechanical resonance at 20GHz and a quality factor of 5046. The following images show the transmission amplitude and phase as a function of frequency in the vicinity of the resonance frequency: 

 [2.x.119]   [2.x.120]  

The images above suggest that the periodic structure has its intended effect: It really only lets waves of a very specific frequency pass through, whereas all other waves are reflected. This is of course precisely what one builds these sorts of devices for. But it is not quite this easy. In practice, there is really only a "band gap", i.e., the device blocks waves other than the desired one at 20GHz only within a certain frequency range. Indeed, to find out how large this "gap" is within which waves are blocked, we can extend the frequency range to 16 GHz through the appropriate parameters in the input file. We then obtain the following image: 

 [2.x.121]  

What this image suggests is that in the range of around 18 to around 22 GHz, really only the waves with a frequency of 20 GHz are allowed to pass through, but beyond this range, there are plenty of other frequencies that can pass through the device. 

[1.x.216][1.x.217] 


We can inspect the mode profile with Paraview or VisIt. As we have discussed, at resonance all the mechanical energy is transmitted and the amplitude of motion is amplified inside the cavity. It can be observed that the PMLs are quite effective to truncate the solution. The following image shows the mode profile at resonance: 

 [2.x.122]  

On the other hand,  out of resonance all the mechanical energy is reflected. The following image shows the profile at 19.75 GHz. Note the interference between the force pulse and the reflected wave at the position  [2.x.123] . 

 [2.x.124]  

[1.x.218][1.x.219] 


Phononic superlattice cavities find application in [quantum optomechanics](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1391). Here we have presented the simulation of a 2D superlattice cavity, but this code can be used as well to simulate "real world" 3D devices such as [micropillar superlattice cavities](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.060101), which are promising candidates to study macroscopic quantum phenomena. The 20GHz mode of a micropillar superlattice cavity is essentially a mechanical harmonic oscillator that is very well isolated from the environment. If the device is cooled down to 20mK in a dilution fridge, the mode would then become a macroscopic quantum harmonic oscillator. 


[1.x.220][1.x.221] 


Instead of setting the parameters in the C++ file we could set the parameters using a python script and save them in the HDF5 file that we will use for the simulations. Then the deal.II program will read the parameters from the HDF5 file. 

[1.x.222] 



In order to read the HDF5 parameters we have to use the  [2.x.125]  flag. 

[1.x.223] 

[1.x.224] [1.x.225]  [2.x.126]  

 [2.x.127] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31][1.x.32][1.x.33] 

 [2.x.3]  

[1.x.34] 

 [2.x.4]  

[1.x.35] [1.x.36][1.x.37] 


This program solves an advection-diffusion problem using a geometric multigrid (GMG) preconditioner. The basics of this preconditioner are discussed in  [2.x.5] ; here we discuss the necessary changes needed for a non-symmetric PDE. Additionally, we introduce the idea of block smoothing (as compared to point smoothing in  [2.x.6] ), and examine the effects of DoF renumbering for additive and multiplicative smoothers. 

[1.x.38][1.x.39] 

The advection-diffusion equation is given by 

[1.x.40] 

where  [2.x.7] ,  [2.x.8]  is the [1.x.41], and  [2.x.9]  is a source. A few notes: 

1. If  [2.x.10] , this is the Laplace equation solved in  [2.x.11]  (and many other places). 

2. If  [2.x.12]  then this is the stationary advection equation solved in  [2.x.13] . 

3. One can define a dimensionless number for this problem, called the [1.x.42]:  [2.x.14] , where  [2.x.15]  is the length scale of the domain. It characterizes the kind of equation we are considering: If  [2.x.16] , we say the problem is [1.x.43], else if  [2.x.17]  we will say the problem is [1.x.44]. 

For the discussion in this tutorial we will be concerned with advection-dominated flow. This is the complicated case: We know that for diffusion-dominated problems, the standard Galerkin method works just fine, and we also know that simple multigrid methods such as those defined in  [2.x.18]  are very efficient. On the other hand, for advection-dominated problems, the standard Galerkin approach leads to oscillatory and unstable discretizations, and simple solvers are often not very efficient. This tutorial program is therefore intended to address both of these issues. 


[1.x.45][1.x.46] 


Using the standard Galerkin finite element method, for suitable test functions  [2.x.19] , a discrete weak form of the PDE would read 

[1.x.47] 

where 

[1.x.48] 



Unfortunately, one typically gets oscillatory solutions with this approach. Indeed, the following error estimate can be shown for this formulation: 

[1.x.49] 

The infimum on the right can be estimated as follows if the exact solution is sufficiently smooth: 

[1.x.50] 

where  [2.x.20]  is the polynomial degree of the finite elements used. As a consequence, we obtain the estimate 

[1.x.51] 

In other words, the numerical solution will converge. On the other hand, given the definition of  [2.x.21]  above, we have to expect poor numerical solutions with a large error when  [2.x.22] , i.e., if the problem has only a small amount of diffusion. 

To combat this, we will consider the new weak form 

[1.x.52] 

where the sum is done over all cells  [2.x.23]  with the inner product taken for each cell, and  [2.x.24]  is a cell-wise constant stabilization parameter defined in  [2.x.25] . 

Essentially, adding in the discrete strong form residual enhances the coercivity of the bilinear form  [2.x.26]  which increases the stability of the discrete solution. This method is commonly referred to as [1.x.53] or [1.x.54] (streamline upwind/Petrov-Galerkin). 


[1.x.55][1.x.56] 


One of the goals of this tutorial is to expand from using a simple (point-wise) Gauss-Seidel (SOR) smoother that is used in  [2.x.27]  (class PreconditionSOR) on each level of the multigrid hierarchy. The term "point-wise" is traditionally used in solvers to indicate that one solves at one "grid point" at a time; for scalar problems, this means to use a solver that updates one unknown of the linear system at a time, keeping all of the others fixed; one would then iterate over all unknowns in the problem and, once done, start over again from the first unknown until these "sweeps" converge. Jacobi, Gauss-Seidel, and SOR iterations can all be interpreted in this way. In the context of multigrid, one does not think of these methods as "solvers", but as "smoothers". As such, one is not interested in actually solving the linear system. It is enough to remove the high-frequency part of the residual for the multigrid method to work, because that allows restricting the solution to a coarser mesh.  Therefore, one only does a few, fixed number of "sweeps" over all unknowns. In the code in this tutorial this is controlled by the "Smoothing steps" parameter. 

But these methods are known to converge rather slowly when used as solvers. While as multigrid smoothers, they are surprisingly good, they can also be improved upon. In particular, we consider "cell-based" smoothers here as well. These methods solve for all unknowns on a cell at once, keeping all other unknowns fixed; they then move on to the next cell, and so on and so forth. One can think of them as "block" versions of Jacobi, Gauss-Seidel, or SOR, but because degrees of freedom are shared among multiple cells, these blocks overlap and the methods are in fact best be explained within the framework of additive and multiplicative Schwarz methods. 

In contrast to  [2.x.28] , our test problem contains an advective term. Especially with a small diffusion constant  [2.x.29] , information is transported along streamlines in the given advection direction. This means that smoothers are likely to be more effective if they allow information to travel in downstream direction within a single smoother application. If we want to solve one unknown (or block of unknowns) at a time in the order in which these unknowns (or blocks) are enumerated, then this information propagation property requires reordering degrees of freedom or cells (for the cell-based smoothers) accordingly so that the ones further upstream are treated earlier (have lower indices) and those further downstream are treated later (have larger indices). The influence of the ordering will be visible in the results section. 

Let us now briefly define the smoothers used in this tutorial. For a more detailed introduction, we refer to  [2.x.30]  and the books  [2.x.31]  and  [2.x.32] . A Schwarz preconditioner requires a decomposition 

[1.x.57] 

of our finite element space  [2.x.33] . Each subproblem  [2.x.34]  also has a Ritz projection  [2.x.35]  based on the bilinear form  [2.x.36] . This projection induces a local operator  [2.x.37]  for each subproblem  [2.x.38] . If  [2.x.39]  is the orthogonal projector onto  [2.x.40] , one can show  [2.x.41] . 

With this we can define an [1.x.58] for the operator  [2.x.42]  as 

[1.x.59] 

In other words, we project our solution into each subproblem, apply the inverse of the subproblem  [2.x.43] , and sum the contributions up over all  [2.x.44] . 

Note that one can interpret the point-wise (one unknown at a time) Jacobi method as an additive Schwarz method by defining a subproblem  [2.x.45]  for each degree of freedom. Then,  [2.x.46]  becomes a multiplication with the inverse of a diagonal entry of  [2.x.47] . 

For the "Block Jacobi" method used in this tutorial, we define a subproblem  [2.x.48]  for each cell of the mesh on the current level. Note that we use a continuous finite element, so these blocks are overlapping, as degrees of freedom on an interface between two cells belong to both subproblems. The logic for the Schwarz operator operating on the subproblems (in deal.II they are called "blocks") is implemented in the class RelaxationBlock. The "Block Jacobi" method is implemented in the class RelaxationBlockJacobi. Many aspects of the class (for example how the blocks are defined and how to invert the local subproblems  [2.x.49] ) can be configured in the smoother data, see  [2.x.50]  and  [2.x.51]  for details. 

So far, we discussed additive smoothers where the updates can be applied independently and there is no information flowing within a single smoother application. A [1.x.60] addresses this and is defined by 

[1.x.61] 

In contrast to above, the updates on the subproblems  [2.x.52]  are applied sequentially. This means that the update obtained when inverting the subproblem  [2.x.53]  is immediately used in  [2.x.54] . This becomes visible when writing out the project: 

[1.x.62] 



When defining the sub-spaces  [2.x.55]  as whole blocks of degrees of freedom, this method is implemented in the class RelaxationBlockSOR and used when you select "Block SOR" in this tutorial. The class RelaxationBlockSOR is also derived from RelaxationBlock. As such, both additive and multiplicative Schwarz methods are implemented in a unified framework. 

Finally, let us note that the standard Gauss-Seidel (or SOR) method can be seen as a multiplicative Schwarz method with a subproblem for each DoF. 


[1.x.63][1.x.64] 


We will be considering the following test problem:  [2.x.56] , i.e., a square with a circle of radius 0.3 centered at the origin removed. In addition, we use  [2.x.57] ,  [2.x.58] ,  [2.x.59] , and Dirichlet boundary values 

[1.x.65] 



The following figures depict the solutions with (left) and without (right) streamline diffusion. Without streamline diffusion we see large oscillations around the boundary layer, demonstrating the instability of the standard Galerkin finite element method for this problem. 

 [2.x.60]  [1.x.66] [1.x.67] 


[1.x.68]  [1.x.69] 




Typical files needed for standard deal.II: 

[1.x.70] 



Include all relevant multilevel files: 

[1.x.71] 



C++: 

[1.x.72] 



We will be using  [2.x.61]  functionality for assembling matrices: 

[1.x.73] 




[1.x.74]  [1.x.75] 




As always, we will be putting everything related to this program into a namespace of its own. 




Since we will be using the MeshWorker framework, the first step is to define the following structures needed by the assemble_cell() function used by  [2.x.62]  `ScratchData` contains an FEValues object which is needed for assembling a cell's local contribution, while `CopyData` contains the output from a cell's local contribution and necessary information to copy that to the global system. (Their purpose is also explained in the documentation of the WorkStream class.) 

[1.x.76] 




[1.x.77]  [1.x.78] 




The second step is to define the classes that deal with run-time parameters to be read from an input file.    


We will use ParameterHandler to pass in parameters at runtime. The structure `Settings` parses and stores the parameters to be queried throughout the program. 

[1.x.79] 




[1.x.80]  [1.x.81]    


The ordering in which cells and degrees of freedom are traversed will play a role in the speed of convergence for multiplicative methods. Here we define functions which return a specific ordering of cells to be used by the block smoothers.    


For each type of cell ordering, we define a function for the active mesh and one for a level mesh (i.e., for the cells at one level of a multigrid hierarchy). While the only reordering necessary for solving the system will be on the level meshes, we include the active reordering for visualization purposes in output_results().    


For the two downstream ordering functions, we first create an array with all of the relevant cells that we then sort in downstream direction using a "comparator" object. The output of the functions is then simply an array of the indices of the cells in the just computed order. 

[1.x.82] 



The functions that produce a random ordering are similar in spirit in that they first put information about all cells into an array. But then, instead of sorting them, they shuffle the elements randomly using the facilities C++ offers to generate random numbers. The way this is done is by iterating over all elements of the array, drawing a random number for another element before that, and then exchanging these elements. The result is a random shuffle of the elements of the array. 

[1.x.83] 




[1.x.84]  [1.x.85] 




The problem solved in this tutorial is an adaptation of Ex. 3.1.3 found on pg. 118 of [1.x.86]. The main difference being that we add a hole in the center of our domain with zero Dirichlet boundary conditions.    


For a complete description, we need classes that implement the zero right-hand side first (we could of course have just used  [2.x.63]  

[1.x.87] 



We also have Dirichlet boundary conditions. On a connected portion of the outer, square boundary we set the value to 1, and we set the value to 0 everywhere else (including the inner, circular boundary): 

[1.x.88] 



Set boundary to 1 if  [2.x.64] , or if  [2.x.65]  and  [2.x.66] . 

[1.x.89] 




[1.x.90]  [1.x.91] 




The streamline diffusion method has a stabilization constant that we need to be able to compute. The choice of how this parameter is computed is taken from [1.x.92]. 

[1.x.93] 




[1.x.94]  [1.x.95] 




This is the main class of the program, and should look very similar to  [2.x.67] . The major difference is that, since we are defining our multigrid smoother at runtime, we choose to define a function `create_smoother()` and a class object `mg_smoother` which is a  [2.x.68]  to a smoother that is derived from MGSmoother. Note that for smoothers derived from RelaxationBlock, we must include a `smoother_data` object for each level. This will contain information about the cell ordering and the method of inverting cell matrices. 







[1.x.96] 




[1.x.97]  [1.x.98] 




Here we first set up the DoFHandler, AffineConstraints, and SparsityPattern objects for both active and multigrid level meshes.    


We could renumber the active DoFs with the DoFRenumbering class, but the smoothers only act on multigrid levels and as such, this would not matter for the computations. Instead, we will renumber the DoFs on each multigrid level below. 

[1.x.99] 



Having enumerated the global degrees of freedom as well as (in the last line above) the level degrees of freedom, let us renumber the level degrees of freedom to get a better smoother as explained in the introduction.  The first block below renumbers DoFs on each level in downstream or upstream direction if needed. This is only necessary for point smoothers (SOR and Jacobi) as the block smoothers operate on cells (see `create_smoother()`). The blocks below then also implement random numbering. 

[1.x.100] 



The rest of the function just sets up data structures. The last lines of the code below is unlike the other GMG tutorials, as it sets up both the interface in and out matrices. We need this since our problem is non-symmetric. 

[1.x.101] 




[1.x.102]  [1.x.103] 




Here we define the assembly of the linear system on each cell to be used by the mesh_loop() function below. This one function assembles the cell matrix for either an active or a level cell (whatever it is passed as its first argument), and only assembles a right-hand side if called with an active cell. 







[1.x.104] 



If we are using streamline diffusion we must add its contribution to both the cell matrix and the cell right-hand side. If we are not using streamline diffusion, setting  [2.x.69]  negates this contribution below and we are left with the standard, Galerkin finite element assembly. 

[1.x.105] 



The assembly of the local matrix has two parts. First the Galerkin contribution: 

[1.x.106] 



and then the streamline diffusion contribution: 

[1.x.107] 



The same applies to the right hand side. First the Galerkin contribution: 

[1.x.108] 



and then the streamline diffusion contribution: 

[1.x.109] 




[1.x.110]  [1.x.111] 




Here we employ  [2.x.70]  to go over cells and assemble the system_matrix, system_rhs, and all mg_matrices for us. 







[1.x.112] 



Unlike the constraints for the active level, we choose to create constraint objects for each multigrid level local to this function since they are never needed elsewhere in the program. 

[1.x.113] 



If  [2.x.71]  is an `interface_out` dof pair, then  [2.x.72]  is an `interface_in` dof pair. Note: For `interface_in`, we load the transpose of the interface entries, i.e., the entry for dof pair  [2.x.73]  is stored in `interface_in(i,j)`. This is an optimization for the symmetric case which allows only one matrix to be used when setting the edge_matrices in solve(). Here, however, since our problem is non-symmetric, we must store both `interface_in` and `interface_out` matrices. 

[1.x.114] 




[1.x.115]  [1.x.116] 




Next, we set up the smoother based on the settings in the `.prm` file. The two options that are of significance is the number of pre- and post-smoothing steps on each level of the multigrid v-cycle and the relaxation parameter. 




Since multiplicative methods tend to be more powerful than additive method, fewer smoothing steps are required to see convergence independent of mesh size. The same holds for block smoothers over point smoothers. This is reflected in the choice for the number of smoothing steps for each type of smoother below. 




The relaxation parameter for point smoothers is chosen based on trial and error, and reflects values necessary to keep the iteration counts in the GMRES solve constant (or as close as possible) as we refine the mesh. The two values given for both "Jacobi" and "SOR" in the `.prm` files are for degree 1 and degree 3 finite elements. If the user wants to change to another degree, they may need to adjust these numbers. For block smoothers, this parameter has a more straightforward interpretation, namely that for additive methods in 2D, a DoF can have a repeated contribution from up to 4 cells, therefore we must relax these methods by 0.25 to compensate. This is not an issue for multiplicative methods as each cell's inverse application carries new information to all its DoFs. 




Finally, as mentioned above, the point smoothers only operate on DoFs, and the block smoothers on cells, so only the block smoothers need to be given information regarding cell orderings. DoF ordering for point smoothers has already been taken care of in `setup_system()`. 







[1.x.117] 




[1.x.118]  [1.x.119] 




Before we can solve the system, we must first set up the multigrid preconditioner. This requires the setup of the transfer between levels, the coarse matrix solver, and the smoother. This setup follows almost identically to  [2.x.74] , the main difference being the various smoothers defined above and the fact that we need different interface edge matrices for in and out since our problem is non-symmetric. (In reality, for this tutorial these interface matrices are empty since we are only using global refinement, and thus have no refinement edges. However, we have still included both here since if one made the simple switch to an adaptively refined method, the program would still run correctly.) 




The last thing to note is that since our problem is non-symmetric, we must use an appropriate Krylov subspace method. We choose here to use GMRES since it offers the guarantee of residual reduction in each iteration. The major disavantage of GMRES is that, for each iteration, the number of stored temporary vectors increases by one, and one also needs to compute a scalar product with all previously stored vectors. This is rather expensive. This requirement is relaxed by using the restarted GMRES method which puts a cap on the number of vectors we are required to store at any one time (here we restart after 50 temporary vectors, or 48 iterations). This then has the disadvantage that we lose information we have gathered throughout the iteration and therefore we could see slower convergence. As a consequence, where to restart is a question of balancing memory consumption, CPU effort, and convergence speed. However, the goal of this tutorial is to have very low iteration counts by using a powerful GMG preconditioner, so we have picked the restart length such that all of the results shown below converge prior to restart happening, and thus we have a standard GMRES method. If the user is interested, another suitable method offered in deal.II would be BiCGStab. 







[1.x.120] 




[1.x.121]  [1.x.122] 




The final function of interest generates graphical output. Here we output the solution and cell ordering in a .vtu format. 




At the top of the function, we generate an index for each cell to visualize the ordering used by the smoothers. Note that we do this only for the active cells instead of the levels, where the smoothers are actually used. For the point smoothers we renumber DoFs instead of cells, so this is only an approximation of what happens in reality. Finally, the random ordering is not the random ordering we actually use (see `create_smoother()` for that).    


The (integer) ordering of cells is then copied into a (floating point) vector for graphical output. 

[1.x.123] 



The remainder of the function is then straightforward, given previous tutorial programs: 

[1.x.124] 




[1.x.125]  [1.x.126] 




As in most tutorials, this function creates/refines the mesh and calls the various functions defined above to set up, assemble, solve, and output the results. 




In cycle zero, we generate the mesh for the on the square  [2.x.75]  with a hole of radius 3/10 units centered at the origin. For objects with `manifold_id` equal to one (namely, the faces adjacent to the hole), we assign a spherical manifold. 







[1.x.127] 




[1.x.128]  [1.x.129] 




Finally, the main function is like most tutorials. The only interesting bit is that we require the user to pass a `.prm` file as a sole command line argument. If no parameter file is given, the program will output the contents of a sample parameter file with all default values to the screen that the user can then copy and paste into their own `.prm` file. 







[1.x.130] 

[1.x.131][1.x.132] 


[1.x.133][1.x.134] 


The major advantage for GMG is that it is an  [2.x.76]  method, that is, the complexity of the problem increases linearly with the problem size. To show then that the linear solver presented in this tutorial is in fact  [2.x.77] , all one needs to do is show that the iteration counts for the GMRES solve stay roughly constant as we refine the mesh. 

Each of the following tables gives the GMRES iteration counts to reduce the initial residual by a factor of  [2.x.78] . We selected a sufficient number of smoothing steps (based on the method) to get iteration numbers independent of mesh size. As can be seen from the tables below, the method is indeed  [2.x.79] . 

[1.x.135][1.x.136] 


The point-wise smoothers ("Jacobi" and "SOR") get applied in the order the DoFs are numbered on each level. We can influence this using the DoFRenumbering namespace. The block smoothers are applied based on the ordering we set in `setup_smoother()`. We can visualize this numbering. The following pictures show the cell numbering of the active cells in downstream, random, and upstream numbering (left to right): 

 [2.x.80]  

Let us start with the additive smoothers. The following table shows the number of iterations necessary to obtain convergence from GMRES: 

 [2.x.81]  

We see that renumbering the DoFs/cells has no effect on convergence speed. This is because these smoothers compute operations on each DoF (point-smoother) or cell (block-smoother) independently and add up the results. Since we can define these smoothers as an application of a sum of matrices, and matrix addition is commutative, the order at which we sum the different components will not affect the end result. 

On the other hand, the situation is different for multiplicative smoothers: 

 [2.x.82]  

Here, we can speed up convergence by renumbering the DoFs/cells in the advection direction, and similarly, we can slow down convergence if we do the renumbering in the opposite direction. This is because advection-dominated problems have a directional flow of information (in the advection direction) which, given the right renumbering of DoFs/cells, multiplicative methods are able to capture. 

This feature of multiplicative methods is, however, dependent on the value of  [2.x.83] . As we increase  [2.x.84]  and the problem becomes more diffusion-dominated, we have a more uniform propagation of information over the mesh and there is a diminished advantage for renumbering in the advection direction. On the opposite end, in the extreme case of  [2.x.85]  (advection-only), we have a 1st-order PDE and multiplicative methods with the right renumbering become effective solvers: A correct downstream numbering may lead to methods that require only a single iteration because information can be propagated from the inflow boundary downstream, with no information transport in the opposite direction. (Note, however, that in the case of  [2.x.86] , special care must be taken for the boundary conditions in this case). 


[1.x.137][1.x.138] 


We will limit the results to runs using the downstream renumbering. Here is a cross comparison of all four smoothers for both  [2.x.87]  and  [2.x.88]  elements: 

 [2.x.89]  

We see that for  [2.x.90] , both multiplicative smoothers require a smaller combination of smoothing steps and iteration counts than either additive smoother. However, when we increase the degree to a  [2.x.91]  element, there is a clear advantage for the block smoothers in terms of the number of smoothing steps and iterations required to solve. Specifically, the block SOR smoother gives constant iteration counts over the degree, and the block Jacobi smoother only sees about a 38% increase in iterations compared to 75% and 183% for Jacobi and SOR respectively. 

[1.x.139][1.x.140] 


Iteration counts do not tell the full story in the optimality of a one smoother over another. Obviously we must examine the cost of an iteration. Block smoothers here are at a disadvantage as they are having to construct and invert a cell matrix for each cell. Here is a comparison of solve times for a  [2.x.92]  element with 74,496 DoFs: 

 [2.x.93]  

The smoother that requires the most iterations (Jacobi) actually takes the shortest time (roughly 2/3 the time of the next fastest method). This is because all that is required to apply a Jacobi smoothing step is multiplication by a diagonal matrix which is very cheap. On the other hand, while SOR requires over 3x more iterations (each with 3x more smoothing steps) than block SOR, the times are roughly equivalent, implying that a smoothing step of block SOR is roughly 9x slower than a smoothing step of SOR. Lastly, block Jacobi is almost 6x more expensive than block SOR, which intuitively makes sense from the fact that 1 step of each method has the same cost (inverting the cell matrices and either adding or multiply them together), and block Jacobi has 3 times the number of smoothing steps per iteration with 2 times the iterations. 


[1.x.141][1.x.142] 


There are a few more important points to mention: 

 [2.x.94]   [2.x.95]  For a mesh distributed in parallel, multiplicative methods cannot be executed over the entire domain. This is because they operate one cell at a time, and downstream cells can only be handled once upstream cells have already been done. This is fine on a single processor: The processor just goes through the list of cells one after the other. However, in parallel, it would imply that some processors are idle because upstream processors have not finished doing the work on cells upstream from the ones owned by the current processor. Once the upstream processors are done, the downstream ones can start, but by that time the upstream processors have no work left. In other words, most of the time during these smoother steps, most processors are in fact idle. This is not how one obtains good parallel scalability! 

One can use a hybrid method where a multiplicative smoother is applied on each subdomain, but as you increase the number of subdomains, the method approaches the behavior of an additive method. This is a major disadvantage to these methods.  [2.x.96]  

 [2.x.97]  Current research into block smoothers suggest that soon we will be able to compute the inverse of the cell matrices much cheaper than what is currently being done inside deal.II. This research is based on the fast diagonalization method (dating back to the 1960s) and has been used in the spectral community for around 20 years (see, e.g., [1.x.143]). There are currently efforts to generalize these methods to DG and make them more robust. Also, it seems that one should be able to take advantage of matrix-free implementations and the fact that, in the interior of the domain, cell matrices tend to look very similar, allowing fewer matrix inverse computations.  [2.x.98]   [2.x.99]  

Combining 1. and 2. gives a good reason for expecting that a method like block Jacobi could become very powerful in the future, even though currently for these examples it is quite slow. 


[1.x.144][1.x.145] 


[1.x.146][1.x.147] 


Change the number of smoothing steps and the smoother relaxation parameter (set in  [2.x.100]  inside  [2.x.101] , only necessary for point smoothers) so that we maintain a constant number of iterations for a  [2.x.102]  element. 

[1.x.148][1.x.149] 


Increase/decrease the parameter "Epsilon" in the `.prm` files of the multiplicative methods and observe for which values renumbering no longer influences convergence speed. 

[1.x.150][1.x.151] 


The code is set up to work correctly with an adaptively refined mesh (the interface matrices are created and set). Devise a suitable refinement criterium or try the KellyErrorEstimator class. [1.x.152] [1.x.153]  [2.x.103]  

 [2.x.104] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15] 

 [2.x.4]  

[1.x.16] 


[1.x.17][1.x.18] 


This example shows how to implement a matrix-free method on the GPU using CUDA for the Helmholtz equation with variable coefficients on a hypercube. The linear system will be solved using the conjugate gradient method and is parallelized  with MPI. 

In the last few years, heterogeneous computing in general and GPUs in particular have gained a lot of popularity. This is because GPUs offer better computing capabilities and memory bandwidth than CPUs for a given power budget. Among the architectures available in early 2019, GPUs are about 2x-3x as power efficient than server CPUs with wide [1.x.19] for PDE-related tasks. GPUs are also the most popular architecture for machine learning. On the other hand, GPUs are not easy to program. This program explores the deal.II capabilities to see how efficiently such a program can be implemented. 

While we have tried for the interface of the matrix-free classes for the CPU and the GPU to be as close as possible, there are a few differences. When using the matrix-free framework on a GPU, one must write some CUDA code. However, the amount is fairly small and the use of CUDA is limited to a few keywords. 


[1.x.20][1.x.21] 


In this example, we consider the Helmholtz problem [1.x.22] 

where  [2.x.5]  is a variable coefficient. 

We choose as domain  [2.x.6]  and  [2.x.7] . Since the coefficient is symmetric around the origin but the domain is not, we will end up with a non-symmetric solution. 

If you've made it this far into the tutorial, you will know how the weak formulation of this problem looks like and how, in principle, one assembles linear systems for it. Of course, in this program we will in fact not actually form the matrix, but rather only represent its action when one multiplies with it. 


[1.x.23][1.x.24] 


GPUs (we will use the term "device" from now on to refer to the GPU) have their own memory that is separate from the memory accessible to the CPU (we will use the term "host" from now on). A normal calculation on the device can be divided in three separate steps: 

 -# the data is moved from the host to the device, 

 -# the computation is done on the device, 

 -# the result is moved back from the device to the host 

The data movements can either be done explicitly by the user code or done automatically using UVM (Unified Virtual Memory). In deal.II, only the first method is supported. While it means an extra burden for the user, this allows for better control of data movement and more importantly it avoids to mistakenly run important kernels on the host instead of the device. 

The data movement in deal.II is done using  [2.x.8]  These vectors can be seen as buffers on the host that are used to either store data received from the device or to send data to the device. There are two types of vectors that can be used on the device: 

-  [2.x.9]  which is similar to the more common Vector<Number>, and 

-  [2.x.10]   [2.x.11]  which is a regular  [2.x.12]  where we have specified which memory space to use. 

If no memory space is specified, the default is  [2.x.13]  

Next, we show how to move data to/from the device using  [2.x.14]  

[1.x.25] 

Both of the vector classes used here only work on a single machine, i.e., one memory space on a host and one on a device. 

But there are cases where one wants to run a parallel computation between multiple MPI processes on a number of machines, each of which is equipped with GPUs. In that case, one wants to use  [2.x.15]  which is similar but the `import()` stage may involve MPI communication: 

[1.x.26] 

The `relevant_rw_vector` is an object that stores a subset of all elements of the vector. Typically, these are the  [2.x.16]  "locally relevant DoFs", which implies that they overlap between different MPI processes. Consequently, the elements stored in that vector on one machine may not coincide with the ones stored by the GPU on that machine, requiring MPI communication to import them. 

In all of these cases, while importing a vector, values can either be inserted (using  [2.x.17]  or added to prior content of the vector (using  [2.x.18]  


[1.x.27][1.x.28] 


The code necessary to evaluate the matrix-free operator on the device is very similar to the one on the host. However, there are a few differences, the main ones being that the `local_apply()` function in  [2.x.19]  and the loop over quadrature points both need to be encapsulated in their own functors. [1.x.29] [1.x.30] 

First include the necessary files from the deal.II library known from the previous tutorials. 

[1.x.31] 



The following ones include the data structures for the implementation of matrix-free methods on GPU: 

[1.x.32] 



As usual, we enclose everything into a namespace of its own: 

[1.x.33] 




[1.x.34]  [1.x.35] 




Next, we define a class that implements the varying coefficients we want to use in the Helmholtz operator. Later, we want to pass an object of this type to a  [2.x.20]  object that expects the class to have an `operator()` that fills the values provided in the constructor for a given cell. This operator needs to run on the device, so it needs to be marked as `__device__` for the compiler. 

[1.x.36] 



Since  [2.x.21]  doesn't know about the size of its arrays, we need to store the number of quadrature points and the numbers of degrees of freedom in this class to do necessary index conversions. 

[1.x.37] 



The following function implements this coefficient. Recall from the introduction that we have defined it as  [2.x.22]  

[1.x.38] 




[1.x.39]  [1.x.40] 




The class `HelmholtzOperatorQuad` implements the evaluation of the Helmholtz operator at each quadrature point. It uses a similar mechanism as the MatrixFree framework introduced in  [2.x.23] . In contrast to there, the actual quadrature point index is treated implicitly by converting the current thread index. As before, the functions of this class need to run on the device, so need to be marked as `__device__` for the compiler. 

[1.x.41] 



The Helmholtz problem we want to solve here reads in weak form as follows: [1.x.42] 

If you have seen  [2.x.24] , then it will be obvious that the two terms on the left-hand side correspond to the two function calls here: 

[1.x.43] 




[1.x.44]  [1.x.45] 




Finally, we need to define a class that implements the whole operator evaluation that corresponds to a matrix-vector product in matrix-based approaches. 

[1.x.46] 



Again, the  [2.x.25]  object doesn't know about the number of degrees of freedom and the number of quadrature points so we need to store these for index calculations in the call operator. 

[1.x.47] 



This is the call operator that performs the Helmholtz operator evaluation on a given cell similar to the MatrixFree framework on the CPU. In particular, we need access to both values and gradients of the source vector and we write value and gradient information to the destination vector. 

[1.x.48] 




[1.x.49]  [1.x.50] 




The `HelmholtzOperator` class acts as wrapper for `LocalHelmholtzOperator` defining an interface that can be used with linear solvers like SolverCG. In particular, like every class that implements the interface of a linear operator, it needs to have a `vmult()` function that performs the action of the linear operator on a source vector. 

[1.x.51] 



The following is the implementation of the constructor of this class. In the first part, we initialize the `mf_data` member variable that is going to provide us with the necessary information when evaluating the operator.    


In the second half, we need to store the value of the coefficient for each quadrature point in every active, locally owned cell. We can ask the parallel triangulation for the number of active, locally owned cells but only have a DoFHandler object at hand. Since  [2.x.26]  returns a Triangulation object, not a  [2.x.27]  object, we have to downcast the return value. This is safe to do here because we know that the triangulation is a  [2.x.28]  object in fact. 

[1.x.52] 



The key step then is to use all of the previous classes to loop over all cells to perform the matrix-vector product. We implement this in the next function.    


When applying the Helmholtz operator, we have to be careful to handle boundary conditions correctly. Since the local operator doesn't know about constraints, we have to copy the correct values from the source to the destination vector afterwards. 

[1.x.53] 




[1.x.54]  [1.x.55] 




This is the main class of this program. It defines the usual framework we use for tutorial programs. The only point worth commenting on is the `solve()` function and the choice of vector types. 

[1.x.56] 



Since all the operations in the `solve()` function are executed on the graphics card, it is necessary for the vectors used to store their values on the GPU as well.  [2.x.29]  can be told which memory space to use. There is also  [2.x.30]  that always uses GPU memory storage but doesn't work with MPI. It might be worth noticing that the communication between different MPI processes can be improved if the MPI implementation is CUDA-aware and the configure flag `DEAL_II_MPI_WITH_CUDA_SUPPORT` is enabled. (The value of this flag needs to be set at the time you call `cmake` when installing deal.II.)      


In addition, we also keep a solution vector with CPU storage such that we can view and display the solution as usual. 

[1.x.57] 



The implementation of all the remaining functions of this class apart from  [2.x.31]  doesn't contain anything new and we won't further comment much on the overall approach. 

[1.x.58] 



Unlike programs such as  [2.x.32]  or  [2.x.33] , we will not have to assemble the whole linear system but only the right hand side vector. This looks in essence like we did in  [2.x.34] , for example, but we have to pay attention to using the right constraints object when copying local contributions into the global vector. In particular, we need to make sure the entries that correspond to boundary nodes are properly zeroed out. This is necessary for CG to converge.  (Another solution would be to modify the `vmult()` function above in such a way that we pretend the source vector has zero entries by just not taking them into account in matrix-vector products. But the approach used here is simpler.)    


At the end of the function, we can't directly copy the values from the host to the device but need to use an intermediate object of type  [2.x.35]  to construct the correct communication pattern necessary. 

[1.x.59] 



This solve() function finally contains the calls to the new classes previously discussed. Here we don't use any preconditioner, i.e., precondition by the identity matrix, to focus just on the peculiarities of the  [2.x.36]  framework. Of course, in a real application the choice of a suitable preconditioner is crucial but we have at least the same restrictions as in  [2.x.37]  since matrix entries are computed on the fly and not stored.    


After solving the linear system in the first part of the function, we copy the solution from the device to the host to be able to view its values and display it in `output_results()`. This transfer works the same as at the end of the previous function. 

[1.x.60] 



The output results function is as usual since we have already copied the values back from the GPU to the CPU.    


While we're already doing something with the function, we might as well compute the  [2.x.38]  norm of the solution. We do this by calling  [2.x.39]  That function is meant to compute the error by evaluating the difference between the numerical solution (given by a vector of values for the degrees of freedom) and an object representing the exact solution. But we can easily compute the  [2.x.40]  norm of the solution by passing in a zero function instead. That is, instead of evaluating the error  [2.x.41] , we are just evaluating  [2.x.42]  instead. 

[1.x.61] 



There is nothing surprising in the `run()` function either. We simply compute the solution on a series of (globally) refined meshes. 

[1.x.62] 




[1.x.63]  [1.x.64] 




Finally for the `main()` function.  By default, all the MPI ranks will try to access the device with number 0, which we assume to be the GPU device associated with the CPU on which a particular MPI rank runs. This works, but if we are running with MPI support it may be that multiple MPI processes are running on the same machine (for example, one per CPU core) and then they would all want to access the same GPU on that machine. If there is only one GPU in the machine, there is nothing we can do about it: All MPI ranks on that machine need to share it. But if there are more than one GPU, then it is better to address different graphic cards for different processes. The choice below is based on the MPI process id by assigning GPUs round robin to GPU ranks. (To work correctly, this scheme assumes that the MPI ranks on one machine are consecutive. If that were not the case, then the rank-GPU association may just not be optimal.) To make this work, MPI needs to be initialized before using this function. 

[1.x.65] 

[1.x.66][1.x.67] 


Since the main purpose of this tutorial is to demonstrate how to use the  [2.x.43]  interface, not to compute anything useful in itself, we just show the expected output here: 

[1.x.68] 



One can make two observations here: First, the norm of the numerical solution converges, presumably to the norm of the exact (but unknown) solution. And second, the number of iterations roughly doubles with each refinement of the mesh. (This is in keeping with the expectation that the number of CG iterations grows with the square root of the condition number of the matrix; and that we know that the condition number of the matrix of a second-order differential operation grows like  [2.x.44] .) This is of course rather inefficient, as an optimal solver would have a number of iterations that is independent of the size of the problem. But having such a solver would require using a better preconditioner than the identity matrix we have used here. 


[1.x.69] [1.x.70][1.x.71] 


Currently, this program uses no preconditioner at all. This is mainly since constructing an efficient matrix-free preconditioner is non-trivial.  However, simple choices just requiring the diagonal of the corresponding matrix are good candidates and these can be computed in a matrix-free way as well. Alternatively, and maybe even better, one could extend the tutorial to use multigrid with Chebyshev smoothers similar to  [2.x.45] . [1.x.72] [1.x.73]  [2.x.46]  

 [2.x.47] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21] 



 [2.x.3]  

[1.x.22] 

[1.x.23] [1.x.24][1.x.25] 


This tutorial program presents an advanced manifold class, TransfiniteInterpolationManifold, and how to work around its main disadvantage, the relatively high cost. 

[1.x.26][1.x.27] 


[1.x.28][1.x.29] 


In many applications, the finite element mesh must be able to represent a relatively complex geometry. In the  [2.x.4] ,  [2.x.5] , and  [2.x.6]  tutorial programs, some techniques to generate grids available within the deal.II library have been introduced. Given a base mesh, deal.II is then able to create a finer mesh by subdividing the cells into children, either uniformly or only in selected parts of the computational domain. Besides the basic meshing capabilities collected in the GridGenerator namespace, deal.II also comes with interfaces to read in meshes generated by (quad- and hex-only) mesh generators using the functions of namespace GridIn, as for example demonstrated in  [2.x.7] . A fundamental limitation of externally generated meshes is that the information provided by the generated cells in the mesh only consists of the position of the vertices and their connectivity, without the context of the underlying geometry that used to be available in the mesh generator that originally created this mesh. This becomes problematic once the mesh is refined within deal.II and additional points need to be placed. The  [2.x.8]  tutorial program shows how to overcome this limitation by using CAD surfaces in terms of the OpenCASCADE library, and  [2.x.9]  by providing the same kind of information programmatically from within the source code. 

Within deal.II, the placement of new points during mesh refinement or for the definition of higher order mappings is controlled by manifold objects, see the  [2.x.10]  "manifold module" for details. To give an example, consider the following situation of a two-dimensional annulus (with pictures taken from the manifold module). If we start with an initial mesh of 10 cells and refine the mesh three times globally without attaching any manifolds, we would obtain the following mesh: 

 [2.x.11]  

The picture looks like this because, by default, deal.II only knows where to put the vertices of child cells by averaging the locations of the vertices of the parent cell. This yields a polygonal domain whose faces are the edges of the original (coarse mesh) cells. Obviously, we must attach a curved description to the boundary faces of the triangulation to reproduce the circular shape upon mesh refinement, like in the following picture: 

 [2.x.12]  

This is better: At least the inner and outer boundaries are now approaching real circles if we continue to refine the mesh. However, the mesh in this picture is still not optimal for an annulus in the sense that the [1.x.30] lines from one cell to the next have kinks at certain vertices, and one would rather like to use the following mesh: 

 [2.x.13]  

In this last (optimal) case, which is also the default produced by  [2.x.14]  the curved manifold description (in this case a polar manifold description) is applied not only to the boundary faces, but to the whole domain. Whenever the triangulation requests a new point, e.g., the mid point of the edges or the cells when it refines a cell into four children, it will place them along the respective mid points in the polar coordinate system. By contrast, the case above where only the boundary was subject to the polar manifold, only mid points along the boundary would be placed along the curved description, whereas mid points in the interior would be computed by suitable averages of the surrounding points in the Cartesian coordinate system (see the  [2.x.15]  "manifold module" for more details). 

At this point, one might assume that curved volume descriptions are the way to go. This is generally not wrong, though it is sometimes not so easy to describe how exactly this should work. Here are a couple of examples: 

- Imagine that the mesh above had actually been a disk, not just a ring.   In that case the polar manifold degenerates at the origin and   would not produce reasonable new points. In fact, defining a   manifold description for things that are supposed "to look round"   but might have points at or close to the origin is surprisingly very   difficult. 

- A similar thing happens at the origin   of the three-dimensional ball when one tries to attach a spherical manifold to   the whole volume &ndash; in this case, the computation of new manifold points   would abort with an exception. 

- CAD geometries often only describe the boundary of the domain, in a   similar way to how we only attached a manifold to the boundary in   the second picture above. Similarly,  [2.x.16]  only uses the CAD   geometry to generate a surface mesh (maybe because that is what is   needed to solve the problem in question), but if one wanted to solve   a problem in the water or the air around the ship described there,   we would need to have a volume mesh. The question is then how   exactly we should describe what is supposed to happen in the   interior of the domain. 

These simple examples make it clear that for many interesting cases we must step back from the desire to have an analytic curved description for the full volume: There will need to be [1.x.31] kind of information that leads to curvature also in the interior, but it must be possible to do this without actually writing down an explicit formula that describes the kind of geometry. 

So what happens if we don't do anything at all in the interior and only describe the surface as a manifold? Sometimes, as in the ring shown above, the result is not terrible. But sometimes it is. Consider the case of a torus (e.g. generated with  [2.x.17]  with a TorusManifold object attached to the surface only, no additional manifolds on the interior cells and faces, and with six cells in toroidal direction before refinement. If the mesh is refined once, we would obtain the following mesh, shown with the upper half of the mesh clipped away: 

 [2.x.18]  

This is clearly sub-optimal. Indeed, if we had started with fewer than the six cells shown above in toroidal direction, the mapping actually inverts in some regions because the new points placed along interior cells intersect with the boundary as they are not following the circular shape along the toroidal direction. The simple case of a torus can still be fixed because we know that the toroidal direction follows a cylindrical coordinate system, so attaching a TorusManifold to the surface combined with CylindricalManifold with appropriate periodicity in toroidal direction applied to all interior entities would produce a high-quality mesh as follows, now shown with two top cells hidden: 

 [2.x.19]  

This mesh is pretty good, but obviously it is linked to a good description of the volume, which we lack in other cases. Actually, there is an imperfection also in this case, as we can see some unnatural kinks of two adjacent cells in the interior of the domain which are hidden by the top two boundary cells, as opposed to the following setup (the default manifolds applied by  [2.x.20]  and using the TransfiniteInterpolationManifold): 

 [2.x.21]  

[1.x.32][1.x.33] 


In order to find a better strategy, let us look at the two-dimensional disk again (that is also the base entity rotated along the toroidal direction in the torus). As we learned above, we can only apply the curved polar description to the boundary (or a rim of cells sufficiently far away from the origin) but must eventually transition to a straight description towards the disk's center. If we use a flat manifold in the interior of the cells (i.e., one in which new vertices are created by averaging of the adjacent existing ones) and a polar manifold only for the boundary of the disk, we get the following mesh upon four global refinements: 

 [2.x.22]  

That's not a terrible mesh. At the same time, if you know that the original coarse mesh consisted of a single square in the middle, with four caps around it, then it's not hard to see every refinement step that happened to this mesh to get the picture above. 

While the triangulation class of deal.II tries to propagate information from the boundary into the interior when creating new points, the reach of this algorithm is limited: 

 [2.x.23]  

The picture above highlights those cells on the disk that are touching the boundary and where boundary information could in principle be taken into account when only looking at a single cell at the time. Clearly, the area where some curvature can be taken into account gets more limited as the mesh is refined, thus creating the seemingly irregular spots in the mesh: When computing the center of any one of the boundary cells in the leftmost picture, the ideal position is the mid point between the outer circle and the cell in the middle. This is exactly what is used for the first refinement step in the Triangulation class. However, for the second refinement all interior edges as well as the interior cell layers can only add points according to a flat manifold description. 

At this point, we realize what would be needed to create a better mesh: For [1.x.34] new points in [1.x.35] child cell that is created within the red shaded layer on the leftmost picture, we want to compute the interpolation with respect to the curvature in the area covered by the respective coarse cell. This is achieved by adding the class TransfiniteInterpolationManifold to the highlighted cells of the coarse grid in the leftmost panel of the figure above. This class adheres to the general manifold interfaces, i.e., given any set of points within its domain of definition, it can compute weighted averages conforming to the manifold (using a formula that will be given in a minute). These weighted averages are used whenever the mesh is refined, or when a higher order mapping (such as MappingQGeneric or MappingC1) is evaluated on a given cell subject to this manifold. Using this manifold on the shaded cells of the coarse grid of the disk (i.e., not only in the outer-most layer of cells) produces the following mesh upon four global steps of refinement: 

 [2.x.24]  

There are still some kinks in the lines of this mesh, but they are restricted to the faces between coarse mesh cells, whereas the rest of the mesh is about as smooth as one would like. Indeed, given a straight-sided central cell, this representation is the best possible one as all mesh cells follow a smooth transition from the straight sides in the square block in the interior to the circular shape on the boundary. (One could possibly do a bit better by allowing some curvature also in the central square block, that eventually vanishes as the center is approached.) 


[1.x.36][1.x.37] 


In the simple case of a disk with one curved and three straight edges, we can explicitly write down how to achieve the blending of the shapes. For this, it is useful to map the physical cell, like the top one, back to the reference coordinate system  [2.x.25]  where we compute averages between certain points. If we were to use a simple bilinear map spanned by four vertices  [2.x.26] , the image of a point  [2.x.27]  would be 

[1.x.38] 



For the case of the curved surface, we want to modify this formula. For the top cell of the coarse mesh of the disk, we can assume that the points  [2.x.28]  and  [2.x.29]  sit along the straight line at the lower end and the points  [2.x.30]  and  [2.x.31]  are connected by a quarter circle along the top. We would then map a point  [2.x.32]  as 

[1.x.39] 

where  [2.x.33]  is a curve that describes the  [2.x.34]  coordinates of the quarter circle in terms of an arclength parameter  [2.x.35] . This represents a linear interpolation between the straight lower edge and the curved upper edge of the cell, and is the basis for the picture shown above. 

This formula is easily generalized to the case where all four edges are described by a curve rather than a straight line. We call the four functions, parameterized by a single coordinate  [2.x.36]  or  [2.x.37]  in the horizontal and vertical directions,  [2.x.38]  for the left, right, lower, and upper edge of a quadrilateral, respectively. The interpolation then reads 

[1.x.40] 



This formula assumes that the boundary curves match and coincide with the vertices  [2.x.39] , e.g.  [2.x.40]  or  [2.x.41] . The subtraction of the bilinear interpolation in the second line of the formula makes sure that the prescribed curves are followed exactly on the boundary: Along each of the four edges, we need to subtract the contribution of the two adjacent edges evaluated in the corners, which is then simply a vertex position. It is easy to check that the formula for the circle above is reproduced if three of the four curves  [2.x.42]  are straight and thus coincide with the bilinear interpolation. 

This formula, called transfinite interpolation, was introduced in 1973 by [1.x.41]. Even though transfinite interpolation essentially only represents a linear blending of the bounding curves, the interpolation exactly follows the boundary curves for each real number  [2.x.43]  or  [2.x.44] , i.e., it interpolates in an infinite number of points, which was the original motivation to label this variant of interpolation a transfinite one by Gordon and Hall. Another interpretation is that the transfinite interpolation interpolates from the left and right and the top and bottom linearly, from which we need to subtract the bilinear interpolation to ensure a unit weight in the interior of the domain. 

The transfinite interpolation is easily generalized to three spatial dimensions. In that case, the interpolation allows to blend 6 different surface descriptions for any of the quads of a three-dimensional cell and 12 edge descriptions for the lines of a cell. Again, to ensure a consistent map, it is necessary to subtract the contribution of edges and add the contribution of vertices again to make the curves follow the prescribed surface or edge description. In the three-dimensional case, it is also possible to use a transfinite interpolation from a curved edge both into the adjacent faces and the adjacent cells. 

The interpolation of the transfinite interpolation in deal.II is general in the sense that it can deal with arbitrary curves. It will evaluate the curves in terms of their original coordinates of the  [2.x.45] -dimensional space but with one (or two, in the case of edges in 3D) coordinate held fixed at  [2.x.46]  or  [2.x.47]  to ensure that any other manifold class, including CAD files if desired, can be applied out of the box. Transfinite interpolation is a standard ingredient in mesh generators, so the main strength of the integration of this feature within the deal.II library is to enable it during adaptive refinement and coarsening of the mesh, and for creating higher-degree mappings that use manifolds to insert additional points beyond the mesh vertices. 

As a final remark on transfinite interpolation, we mention that the mesh refinement strategies in deal.II in absence of a volume manifold description are also based on the weights of the transfinite interpolation and optimal in that sense. The difference is that the default algorithm sees only one cell at a time, and so will apply the optimal algorithm only on those cells touching the curved manifolds. In contrast, using the transfinite mapping on entire [1.x.42] of cells (originating from one coarser cell) allows to use the transfinite interpolation method in a way that propagates information from the boundary to cells far away. 


[1.x.43][1.x.44] 


A mesh with a transfinite manifold description is typically set up in two steps. The first step is to create a coarse mesh (or read it in from a file) and to attach a curved manifold to some of the mesh entities. For the above example of the disk, we attach a polar manifold to the faces along the outer circle (this is done automatically by  [2.x.48]  Before we start refining the mesh, we then assign a TransfiniteInterpolationManifold to all interior cells and edges of the mesh, which of course needs to be based on some manifold id that we have assigned to those entities (everything except the circle on the boundary). It does not matter whether we also assign a TransfiniteInterpolationManifold to the inner square of the disk or not because the transfinite interpolation on a coarse cell with straight edges (or flat faces in 3d) simply yields subdivided children with straight edges (flat faces). 

Later, when the mesh is refined or when a higher-order mapping is set up based on this mesh, the cells will query the underlying manifold object for new points. This process takes a set of surrounding points, for example the four vertices of a two-dimensional cell, and a set of weights to each of these points, for definition a new point. For the mid point of a cell, each of the four vertices would get weight 0.25. For the transfinite interpolation manifold, the process of building weighted sums requires some serious work. By construction, we want to combine the points in terms of the reference coordinates  [2.x.49]  and  [2.x.50]  (or  [2.x.51]  in 3D) of the surrounding points. However, the interface of the manifold classes in deal.II does not get the reference coordinates of the surrounding points (as they are not stored globally) but rather the physical coordinates only. Thus, the first step the transfinite interpolation manifold has to do is to invert the mapping and find the reference coordinates within one of the coarse cells of the transfinite interpolation (e.g. one of the four shaded coarse-grid cells of the disk mesh above). This inversion is done by a Newton iteration (or rather, finite-difference based Newton scheme combined with Broyden's method) and queries the transfinite interpolation according to the formula above several times. Each of these queries in turn might call an expensive manifold, e.g. a spherical description of a ball, and be expensive on its own. Since the Manifold interface class of deal.II only provides a set of points, the transfinite interpolation initially does not even know to which coarse grid cell the set of surrounding points belong to and needs to search among several cells based on some heuristics. In terms of [1.x.45], one could describe the implementation of the transfinite interpolation as an [1.x.46]-based implementation: Each cell of the initial coarse grid of the triangulation represents a chart with its own reference space, and the surrounding manifolds provide a way to transform from the chart space (i.e., the reference cell) to the physical space. The collection of the charts of the coarse grid cells is an atlas, and as usual, the first thing one does when looking up something in an atlas is to find the right chart. 

Once the reference coordinates of the surrounding points have been found, a new point in the reference coordinate system is computed by a simple weighted sum. Finally, the reference point is inserted into the formula for the transfinite interpolation, which gives the desired new point. 

In a number of cases, the curved manifold is not only used during mesh refinement, but also to ensure a curved representation of boundaries within the cells of the computational domain. This is a necessity to guarantee high-order convergence for high-order polynomials on complex geometries anyway, but sometimes an accurate geometry is also desired with linear shape functions. This is often done by polynomial descriptions of the cells and called the isoparametric concept if the polynomial degree to represent the curved mesh elements is the same as the degree of the polynomials for the numerical solution. If the degree of the geometry is higher or lower than the solution, one calls that a super- or sub-parametric geometry representation, respectively. In deal.II, the standard class for polynomial representation is MappingQGeneric. If, for example, this class is used with polynomial degree  [2.x.52]  in 3D, a total of 125 (i.e.,  [2.x.53] ) points are needed for the interpolation. Among these points, 8 are the cell's vertices and already available from the mesh, but the other 117 need to be provided by the manifold. In case the transfinite interpolation manifold is used, we can imagine that going through the pull-back into reference coordinates of some yet to be determined coarse cell, followed by subsequent push-forward on each of the 117 points, is a lot of work and can be very time consuming. 

What makes things worse is that the structure of many programs is such that the mapping is queried several times independently for the same cell. Its primary use is in the assembly of the linear system, i.e., the computation of the system matrix and the right hand side, via the `mapping` argument of the FEValues object. However, also the interpolation of boundary values, the computation of numerical errors, writing the output, and evaluation of error estimators must involve the same mapping to ensure a consistent interpretation of the solution vectors. Thus, even a linear stationary problem that is solved once will evaluate the points of the mapping several times. For the cubic case in 3D mentioned above, this means computing 117 points per cell by an expensive algorithm many times. The situation is more pressing for nonlinear or time-dependent problems where those operations are done over and over again. 

As the manifold description via a transfinite interpolation can easily be hundreds of times more expensive than a similar query on a flat manifold, it makes sense to compute the additional points only once and use them in all subsequent calls. The deal.II library provides the class MappingQCache for exactly this purpose. The cache is typically not overly big compared to the memory consumed by a system matrix, as will become clear when looking at the results of this tutorial program. The usage of MappingQCache is simple: Once the mesh has been set up (or changed during refinement), we call  [2.x.54]  with the desired triangulation as well as a desired mapping as arguments. The initialization then goes through all cells of the mesh and queries the given mapping for its additional points. Those get stored for an identifier of the cell so that they can later be returned whenever the mapping computes some quantities related to the cell (like the Jacobians of the map between the reference and physical coordinates). 

As a final note, we mention that the TransfiniteInterpolationManifold also makes the refinement of the mesh more expensive. In this case, the MappingQCache does not help because it would compute points that can subsequently not be re-used; there currently does not exist a more efficient mechanism in deal.II. However, the mesh refinement contains many other expensive steps as well, so it is not as big as an issue compared to the rest of the computation. It also only happens at most once per time step or nonlinear iteration. 

[1.x.47][1.x.48] 


In this tutorial program, the usage of TransfiniteInterpolationManifold is exemplified in combination with MappingQCache. The test case is relatively simple and takes up the solution stages involved in many typical programs, e.g., the  [2.x.55]  tutorial program. As a geometry, we select one prototype use of TransfiniteInterpolationManifold, namely a setup involving a spherical ball that is in turn surrounded by a cube. Such a setup would be used, for example, for a spherical inclusion embedded in a background medium, and if that inclusion has different material properties that require that the interface between the two materials needs to be tracked by element interfaces. A visualization of the grid is given here: 

 [2.x.56]  

For this case, we want to attach a spherical description to the surface inside the domain and use the transfinite interpolation to smoothly switch to the straight lines of the outer cube and the cube at the center of the ball. 

Within the program, we will follow a typical flow in finite element programs, starting from the setup of DoFHandler and sparsity patterns, the assembly of a linear system for solving the Poisson equation with a jumping coefficient, its solution with a simple iterative method, computation of some numerical error with  [2.x.57]  as well as an error estimator. We record timings for each section and run the code twice. In the first run, we hand a MappingQGeneric object to each stage of the program separately, where points get re-computed over and over again. In the second run, we use MappingQCache instead. [1.x.49] [1.x.50] 


[1.x.51]  [1.x.52] 




The include files for this tutorial are essentially the same as in  [2.x.58] . Importantly, the TransfiniteInterpolationManifold class we will be using is provided by `deal.II/grid/manifold_lib.h`. 







[1.x.53] 



The only new include file is the one for the MappingQCache class. 

[1.x.54] 




[1.x.55]  [1.x.56] 




In this tutorial program, we want to solve the Poisson equation with a coefficient that jumps along a sphere of radius 0.5, and using a constant right hand side of value  [2.x.59] . (This setup is similar to  [2.x.60]  and  [2.x.61] , but the concrete values for the coefficient and the right hand side are different.) Due to the jump in the coefficient, the analytical solution must have a kink where the coefficient switches from one value to the other. To keep things simple, we select an analytical solution that is quadratic in all components, i.e.,  [2.x.62]  in the ball of radius 0.5 and  [2.x.63]  in the outer part of the domain. This analytical solution is compatible with the right hand side in case the coefficient is 0.5 in the inner ball and 5 outside. It is also continuous along the circle of radius 0.5. 

[1.x.57] 




[1.x.58]  [1.x.59]    


The implementation of the Poisson problem is very similar to what we used in the  [2.x.64]  tutorial program. The two main differences are that we pass a mapping object to the various steps in the program in order to switch between two mapping representations as explained in the introduction, and the `timer` object (of type TimerOutput) that will be used for measuring the run times in the various cases. (The concept of mapping objects was first introduced in  [2.x.65]  and  [2.x.66] , in case you want to look up the use of these classes.) 

[1.x.60] 



In the constructor, we set up the timer object to record wall times but be quiet during the normal execution. We will query it for timing details in the  [2.x.67]  function. Furthermore, we select a relatively high polynomial degree of three for the finite element in use. 

[1.x.61] 




[1.x.62]  [1.x.63]    


The next function presents the typical usage of TransfiniteInterpolationManifold. The first step is to create the desired grid, which can be done by composition of two grids from GridGenerator. The inner ball mesh is simple enough: We run  [2.x.68]  centered at the origin with radius 0.5 (third function argument). The second mesh is more interesting and constructed as follows: We want to have a mesh that is spherical in the interior but flat on the outer surface. Furthermore, the mesh topology of the inner ball should be compatible with the outer grid in the sense that their vertices coincide so as to allow the two grid to be merged. The grid coming out of  [2.x.69]  fulfills the requirements on the inner side in case it is created with  [2.x.70]  coarse cells (6 coarse cells in 3D which we are going to use) &ndash; this is the same number of cells as there are boundary faces for the ball. For the outer surface, we use the fact that the 6 faces on the surface of the shell without a manifold attached would degenerate to the surface of a cube. What we are still missing is the radius of the outer shell boundary. Since we desire a cube of extent  [2.x.71]  and the 6-cell shell puts its 8 outer vertices at the 8 opposing diagonals, we must translate the points  [2.x.72]  into a radius: Clearly, the radius must be  [2.x.73]  in  [2.x.74]  dimensions, i.e.,  [2.x.75]  for the three-dimensional case we want to consider.    


Thus, we have a plan: After creating the inner triangulation for the ball and the one for the outer shell, we merge those two grids but remove all manifolds that the functions in GridGenerator may have set from the resulting triangulation, to ensure that we have full control over manifolds. In particular, we want additional points added on the boundary during refinement to follow a flat manifold description. To start the process of adding more appropriate manifold ids, we assign the manifold id 0 to all mesh entities (cells, faces, lines), which will later be associated with the TransfiniteInterpolationManifold. Then, we must identify the faces and lines that are along the sphere of radius 0.5 and mark them with a different manifold id, so as to then assign a SphericalManifold to those. We will choose the manifold id of 1. Since we have thrown away all manifolds that pre-existed after calling  [2.x.76]  we manually go through the cells of the mesh and all their faces. We have found a face on the sphere if all four vertices have a radius of 0.5, or, as we write in the program, have  [2.x.77] . Note that we call `cell->face(f)->set_all_manifold_ids(1)` to set the manifold id both on the faces and the surrounding lines. Furthermore, we want to distinguish the cells inside the ball and outside the ball by a material id for visualization, corresponding to the picture in the introduction. 

[1.x.64] 



With all cells, faces and lines marked appropriately, we can attach the Manifold objects to those numbers. The entities with manifold id 1 will get a spherical manifold, whereas the other entities, which have the manifold id 0, will be assigned the TransfiniteInterpolationManifold. As mentioned in the introduction, we must explicitly initialize the manifold with the current mesh using a call to  [2.x.78]  in order to pick up the coarse mesh cells and the manifolds attached to the boundaries of those cells. We also note that the manifold objects we create locally in this function are allowed to go out of scope (as they do at the end of the function scope), because the Triangulation object internally copies them.      


With all manifolds attached, we will finally go about and refine the mesh a few times to create a sufficiently large test case. 

[1.x.65] 




[1.x.66]  [1.x.67]    


The following function is well-known from other tutorials in that it enumerates the degrees of freedom, creates a constraint object and sets up a sparse matrix for the linear system. The only thing worth mentioning is the fact that the function receives a reference to a mapping object that we then pass to the  [2.x.79]  function to ensure that our boundary values are evaluated on the high-order mesh used for assembly. In the present example, it does not really matter because the outer surfaces are flat, but for curved outer cells this leads to more accurate approximation of the boundary values. 

[1.x.68] 




[1.x.69]  [1.x.70]    


The function that assembles the linear system is also well known from the previous tutorial programs. One thing to note is that we set the number of quadrature points to the polynomial degree plus two, not the degree plus one as in most other tutorials. This is because we expect some extra accuracy as the mapping also involves a degree one more than the polynomials for the solution.    


The only somewhat unusual code in the assembly is the way we compute the cell matrix. Rather than using three nested loop over the quadrature point index, the row, and the column of the matrix, we first collect the derivatives of the shape function, multiplied by the square root of the product of the coefficient and the integration factor `JxW` in a separate matrix `partial_matrix`. To compute the cell matrix, we then execute `cell_matrix = partial_matrix * transpose(partial_matrix)` in the line `partial_matrix.mTmult(cell_matrix, partial_matrix);`. To understand why this works, we realize that the matrix-matrix multiplication performs a summation over the columns of `partial_matrix`. If we denote the coefficient by  [2.x.80] , the entries in the temporary matrix are  [2.x.81] . If we take the product of the [1.x.71]th row with the [1.x.72]th column of that matrix, we compute a nested sum involving  [2.x.82] , which is exactly the terms needed for the bilinear form of the Laplace equation.    


The reason for choosing this somewhat unusual scheme is due to the heavy work involved in computing the cell matrix for a relatively high polynomial degree in 3D. As we want to highlight the cost of the mapping in this tutorial program, we better do the assembly in an optimized way in order to not chase bottlenecks that have been solved by the community already. Matrix-matrix multiplication is one of the best optimized kernels in the HPC context, and the  [2.x.83]  function will call into those optimized BLAS functions. If the user has provided a good BLAS library when configuring deal.II (like OpenBLAS or Intel's MKL), the computation of the cell matrix will execute close to the processor's peak arithmetic performance. As a side note, we mention that despite an optimized matrix-matrix multiplication, the current strategy is sub-optimal in terms of complexity as the work to be done is proportional to  [2.x.84]  operations for degree  [2.x.85]  (this also applies to the usual evaluation with FEValues). One could compute the cell matrix with  [2.x.86]  operations by utilizing the tensor product structure of the shape functions, as is done by the matrix-free framework in deal.II. We refer to  [2.x.87]  and the documentation of the tensor-product-aware evaluators FEEvaluation for details on how an even more efficient cell matrix computation could be realized. 

[1.x.73] 




[1.x.74]  [1.x.75]    


For solving the linear system, we pick a simple Jacobi-preconditioned conjugate gradient solver, similar to the settings in the early tutorials. 

[1.x.76] 




[1.x.77]  [1.x.78]    


In the next function we do various post-processing steps with the solution, all of which involve the mapping in one way or the other.    


The first operation we do is to write the solution as well as the material ids to a VTU file. This is similar to what was done in many other tutorial programs. The new ingredient presented in this tutorial program is that we want to ensure that the data written to the file used for visualization is actually a faithful representation of what is used internally by deal.II. That is because most of the visualization data formats only represent cells by their vertex coordinates, but have no way of representing the curved boundaries that are used in deal.II when using higher order mappings -- in other words, what you see in the visualization tool is not actually what you are computing on. (The same, incidentally, is true when using higher order shape functions: Most visualization tools only render bilinear/trilinear representations. This is discussed in detail in  [2.x.88]     


So we need to ensure that a high-order representation is written to the file. We need to consider two particular topics. Firstly, we tell the DataOut object via the  [2.x.89]  that we intend to interpret the subdivisions of the elements as a high-order Lagrange polynomial rather than a collection of bilinear patches. Recent visualization programs, like ParaView version 5.5 or newer, can then render a high-order solution (see a [1.x.79] for more details). Secondly, we need to make sure that the mapping is passed to the  [2.x.90]  method. Finally, the DataOut class only prints curved faces for [1.x.80] cells by default, so we need to ensure that also inner cells are printed in a curved representation via the mapping. 

[1.x.81] 



The next operation in the postprocessing function is to compute the  [2.x.91]  and  [2.x.92]  errors against the analytical solution. As the analytical solution is a quadratic polynomial, we expect a very accurate result at this point. If we were solving on a simple mesh with planar faces and a coefficient whose jumps are aligned with the faces between cells, then we would expect the numerical result to coincide with the analytical solution up to roundoff accuracy. However, since we are using deformed cells following a sphere, which are only tracked by polynomials of degree 4 (one more than the degree for the finite elements), we will see that there is an error around  [2.x.93] . We could get more accuracy by increasing the polynomial degree or refining the mesh. 

[1.x.82] 



The final post-processing operation we do here is to compute an error estimate with the KellyErrorEstimator. We use the exact same settings as in the  [2.x.94]  tutorial program, except for the fact that we also hand in the mapping to ensure that errors are evaluated along the curved element, consistent with the remainder of the program. However, we do not really use the result here to drive a mesh adaptation step (that would refine the mesh around the material interface along the sphere), as the focus here is on the cost of this operation. 

[1.x.83] 




[1.x.84]  [1.x.85]    


Finally, we define the `run()` function that controls how we want to execute this program (which is called by the main() function in the usual way). We start by calling the `create_grid()` function that sets up our geometry with the appropriate manifolds. We then run two instances of a solver chain, starting from the setup of the equations, the assembly of the linear system, its solution with a simple iterative solver, and the postprocessing discussed above. The two instances differ in the way they use the mapping. The first uses a conventional MappingQGeneric mapping object which we initialize to a degree one more than we use for the finite element &ndash; after all, we expect the geometry representation to be the bottleneck as the analytic solution is only a quadratic polynomial. (In reality, things are interlinked to quite some extent because the evaluation of the polynomials in real coordinates involves the mapping of a higher-degree polynomials, which represent some smooth rational functions. As a consequence, higher-degree polynomials still pay off, so it does not make sense to increase the degree of the mapping further.) Once the first pass is completed, we let the timer print a summary of the compute times of the individual stages. 

[1.x.86] 



For the second instance, we instead set up the MappingQCache class. Its use is very simple: After constructing it (with the degree, given that we want it to show the correct degree functionality in other contexts), we fill the cache via the  [2.x.95]  function. At this stage, we specify which mapping we want to use (obviously, the same MappingQGeneric as previously in order to repeat the same computations) for the cache, and then run through the same functions again, now handing in the modified mapping. In the end, we again print the accumulated wall times since the reset to see how the times compare to the original setting. 

[1.x.87] 



[1.x.88][1.x.89] 


[1.x.90][1.x.91] 


If we run the three-dimensional version of this program with polynomials of degree three, we get the following program output: 

[1.x.92] 



Before discussing the timings, we look at the memory consumption for the MappingQCache object: Our program prints that it utilizes 23 MB of memory. If we relate this number to the memory consumption of a single (solution or right hand side) vector, which is 1.5 MB (namely, 181,609 elements times 8 bytes per entry in double precision), or to the memory consumed by the system matrix and the sparsity pattern (which is 274 MB), we realize that it is not an overly heavy data structure, given its benefits. 

With respect to the timers, we see a clear improvement in the overall run time of the program by a factor of 2.7. If we disregard the iterative solver, which is the same in both cases (and not optimal, given the simple preconditioner we use, and the fact that sparse matrix-vector products waste operations for cubic polynomials), the advantage is a factor of almost 5. This is pretty impressive for a linear stationary problem, and cost savings would indeed be much more prominent for time-dependent and nonlinear problems where assembly is called several times. If we look into the individual components, we get a clearer picture of what is going on and why the cache is so efficient: In the MappingQGeneric case, essentially every operation that involves a mapping take at least 5 seconds to run. The norm computation runs two  [2.x.96]  functions, which each take almost 5 seconds. (The computation of constraints is cheaper because it only evaluates the mapping in cells at the boundary for the interpolation of boundary conditions.) If we compare these 5 seconds to the time it takes to fill the MappingQCache, which is 5.2 seconds (for all cells, not just the active ones), it becomes obvious that the computation of the mapping support points dominates over everything else in the MappingQGeneric case. Perhaps the most striking result is the time for the error estimator, labeled "Compute error estimator", where the MappingQGeneric implementation takes 17.3 seconds and the MappingQCache variant less than 0.5 seconds. The reason why the former is so expensive (three times more expensive than the assembly, for instance) is that the error estimation involves evaluation of quantities over faces, where each face in the mesh requests additional points of the mapping that in turn go through the very expensive TransfiniteInterpolationManifold class. As there are six faces per cell, this happens much more often than in assembly. Again, MappingQCache nicely eliminates the repeated evaluation, aggregating all the expensive steps involving the manifold in a single initialization call that gets repeatedly used. [1.x.93] [1.x.94]  [2.x.97]  

 [2.x.98] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] ,  [2.x.4] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27] 



 [2.x.5]  

[1.x.28] 

[1.x.29] [1.x.30][1.x.31] 


This tutorial program solves the Euler equations of fluid dynamics using an explicit time integrator with the matrix-free framework applied to a high-order discontinuous Galerkin discretization in space. For details about the Euler system and an alternative implicit approach, we also refer to the  [2.x.6]  tutorial program. You might also want to look at  [2.x.7]  for an alternative approach to solving these equations. 


[1.x.32][1.x.33] 


The Euler equations are a conservation law, describing the motion of a compressible inviscid gas, [1.x.34] where the  [2.x.8]  components of the solution vector are  [2.x.9] . Here,  [2.x.10]  denotes the fluid density,  [2.x.11]  the fluid velocity, and  [2.x.12]  the energy density of the gas. The velocity is not directly solved for, but rather the variable  [2.x.13] , the linear momentum (since this is the conserved quantity). 

The Euler flux function, a  [2.x.14]  matrix, is defined as [1.x.35] with  [2.x.15]  the  [2.x.16]  identity matrix and  [2.x.17]  the outer product; its components denote the mass, momentum, and energy fluxes, respectively. The right hand side forcing is given by [1.x.36] where the vector  [2.x.18]  denotes the direction and magnitude of gravity. It could, however, also denote any other external force per unit mass that is acting on the fluid. (Think, for example, of the electrostatic forces exerted by an external electric field on charged particles.) 

The three blocks of equations, the second involving  [2.x.19]  components, describe the conservation of mass, momentum, and energy. The pressure is not a solution variable but needs to be expressed through a "closure relationship" by the other variables; we here choose the relationship appropriate for a gas with molecules composed of two atoms, which at moderate temperatures is given by  [2.x.20]  with the constant  [2.x.21] . 


[1.x.37][1.x.38] 


For spatial discretization, we use a high-order discontinuous Galerkin (DG) discretization, using a solution expansion of the form [1.x.39] Here,  [2.x.22]  denotes the  [2.x.23] th basis function, written in vector form with separate shape functions for the different components and letting  [2.x.24]  go through the density, momentum, and energy variables, respectively. In this form, the space dependence is contained in the shape functions and the time dependence in the unknown coefficients  [2.x.25] . As opposed to the continuous finite element method where some shape functions span across element boundaries, the shape functions are local to a single element in DG methods, with a discontinuity from one element to the next. The connection of the solution from one cell to its neighbors is instead imposed by the numerical fluxes specified below. This allows for some additional flexibility, for example to introduce directionality in the numerical method by, e.g., upwinding. 

DG methods are popular methods for solving problems of transport character because they combine low dispersion errors with controllable dissipation on barely resolved scales. This makes them particularly attractive for simulation in the field of fluid dynamics where a wide range of active scales needs to be represented and inadequately resolved features are prone to disturb the important well-resolved features. Furthermore, high-order DG methods are well-suited for modern hardware with the right implementation. At the same time, DG methods are no silver bullet. In particular when the solution develops discontinuities (shocks), as is typical for the Euler equations in some flow regimes, high-order DG methods tend to oscillatory solutions, like all high-order methods when not using flux- or slope-limiters. This is a consequence of [1.x.40] that states that any total variation limited (TVD) scheme that is linear (like a basic DG discretization) can at most be first-order accurate. Put differently, since DG methods aim for higher order accuracy, they cannot be TVD on solutions that develop shocks. Even though some communities claim that the numerical flux in DG methods can control dissipation, this is of limited value unless [1.x.41] shocks in a problem align with cell boundaries. Any shock that passes through the interior of cells will again produce oscillatory components due to the high-order polynomials. In the finite element and DG communities, there exist a number of different approaches to deal with shocks, for example the introduction of artificial diffusion on troubled cells (using a troubled-cell indicator based e.g. on a modal decomposition of the solution), a switch to dissipative low-order finite volume methods on a subgrid, or the addition of some limiting procedures. Given the ample possibilities in this context, combined with the considerable implementation effort, we here refrain from the regime of the Euler equations with pronounced shocks, and rather concentrate on the regime of subsonic flows with wave-like phenomena. For a method that works well with shocks (but is more expensive per unknown), we refer to the  [2.x.26]  tutorial program. 

For the derivation of the DG formulation, we multiply the Euler equations with test functions  [2.x.27]  and integrate over an individual cell  [2.x.28] , which gives [1.x.42] 

We then integrate the second term by parts, moving the divergence from the solution slot to the test function slot, and producing an integral over the element boundary: [1.x.43] In the surface integral, we have replaced the term  [2.x.29]  by the term  [2.x.30] , the numerical flux. The role of the numerical flux is to connect the solution on neighboring elements and weakly impose continuity of the solution. This ensures that the global coupling of the PDE is reflected in the discretization, despite independent basis functions on the cells. The connectivity to the neighbor is included by defining the numerical flux as a function  [2.x.31]  of the solution from both sides of an interior face,  [2.x.32]  and  [2.x.33] . A basic property we require is that the numerical flux needs to be [1.x.44]. That is, we want all information (i.e., mass, momentum, and energy) that leaves a cell over a face to enter the neighboring cell in its entirety and vice versa. This can be expressed as  [2.x.34] , meaning that the numerical flux evaluates to the same result from either side. Combined with the fact that the numerical flux is multiplied by the unit outer normal vector on the face under consideration, which points in opposite direction from the two sides, we see that the conservation is fulfilled. An alternative point of view of the numerical flux is as a single-valued intermediate state that links the solution weakly from both sides. 

There is a large number of numerical flux functions available, also called Riemann solvers. For the Euler equations, there exist so-called exact Riemann solvers -- meaning that the states from both sides are combined in a way that is consistent with the Euler equations along a discontinuity -- and approximate Riemann solvers, which violate some physical properties and rely on other mechanisms to render the scheme accurate overall. Approxiate Riemann solvers have the advantage of beging cheaper to compute. Most flux functions have their origin in the finite volume community, which are similar to DG methods with polynomial degree 0 within the cells (called volumes). As the volume integral of the Euler operator  [2.x.35]  would disappear for constant solution and test functions, the numerical flux must fully represent the physical operator, explaining why there has been a large body of research in that community. For DG methods, consistency is guaranteed by higher order polynomials within the cells, making the numerical flux less of an issue and usually affecting only the convergence rate, e.g., whether the solution converges as  [2.x.36] ,  [2.x.37]  or  [2.x.38]  in the  [2.x.39]  norm for polynomials of degree  [2.x.40] . The numerical flux can thus be seen as a mechanism to select more advantageous dissipation/dispersion properties or regarding the extremal eigenvalue of the discretized and linearized operator, which affect the maximal admissible time step size in explicit time integrators. 

In this tutorial program, we implement two variants of fluxes that can be controlled via a switch in the program (of course, it would be easy to make them a run time parameter controlled via an input file). The first flux is the local Lax--Friedrichs flux [1.x.45] 

In the original definition of the Lax--Friedrichs flux, a factor  [2.x.41]  is used (corresponding to the maximal speed at which information is moving on the two sides of the interface), stating that the difference between the two states,  [2.x.42]  is penalized by the largest eigenvalue in the Euler flux, which is  [2.x.43] , where  [2.x.44]  is the speed of sound. In the implementation below, we modify the penalty term somewhat, given that the penalty is of approximate nature anyway. We use 

[1.x.46] 

The additional factor  [2.x.45]  reduces the penalty strength (which results in a reduced negative real part of the eigenvalues, and thus increases the admissible time step size). Using the squares within the sums allows us to reduce the number of expensive square root operations, which is 4 for the original Lax--Friedrichs definition, to a single one. This simplification leads to at most a factor of 2 in the reduction of the parameter  [2.x.46] , since  [2.x.47] , with the last inequality following from Young's inequality. 

The second numerical flux is one proposed by Harten, Lax and van Leer, called the HLL flux. It takes the different directions of propagation of the Euler equations into account, depending on the speed of sound. It utilizes some intermediate states  [2.x.48]  and  [2.x.49]  to define the two branches  [2.x.50]  and  [2.x.51] . From these branches, one then defines the flux [1.x.47] Regarding the definition of the intermediate state  [2.x.52]  and  [2.x.53] , several variants have been proposed. The variant originally proposed uses a density-averaged definition of the velocity,  [2.x.54] . Since we consider the Euler equations without shocks, we simply use arithmetic means,  [2.x.55]  and  [2.x.56] , with  [2.x.57] , in this tutorial program, and leave other variants to a possible extension. We also note that the HLL flux has been extended in the literature to the so-called HLLC flux, where C stands for the ability to represent contact discontinuities. 

At the boundaries with no neighboring state  [2.x.58]  available, it is common practice to deduce suitable exterior values from the boundary conditions (see the general literature on DG methods for details). In this tutorial program, we consider three types of boundary conditions, namely [1.x.48] where all components are prescribed, [1.x.49] [1.x.50], where we do not prescribe exterior solutions as the flow field is leaving the domain and use the interior values instead; we still need to prescribe the energy as there is one incoming characteristic left in the Euler flux, [1.x.51] and [1.x.52] which describe a no-penetration configuration: [1.x.53] 

The polynomial expansion of the solution is finally inserted to the weak form and test functions are replaced by the basis functions. This gives a discrete in space, continuous in time nonlinear system with a finite number of unknown coefficient values  [2.x.59] ,  [2.x.60] . Regarding the choice of the polynomial degree in the DG method, there is no consensus in literature as of 2019 as to what polynomial degrees are most efficient and the decision is problem-dependent. Higher order polynomials ensure better convergence rates and are thus superior for moderate to high accuracy requirements for [1.x.54] solutions. At the same time, the volume-to-surface ratio of where degrees of freedom are located, increases with higher degrees, and this makes the effect of the numerical flux weaker, typically reducing dissipation. However, in most of the cases the solution is not smooth, at least not compared to the resolution that can be afforded. This is true for example in incompressible fluid dynamics, compressible fluid dynamics, and the related topic of wave propagation. In this pre-asymptotic regime, the error is approximately proportional to the numerical resolution, and other factors such as dispersion errors or the dissipative behavior become more important. Very high order methods are often ruled out because they come with more restrictive CFL conditions measured against the number of unknowns, and they are also not as flexible when it comes to representing complex geometries. Therefore, polynomial degrees between two and six are most popular in practice, see e.g. the efficiency evaluation in  [2.x.61]  and references cited therein. 

[1.x.55][1.x.56] 


To discretize in time, we slightly rearrange the weak form and sum over all cells: [1.x.57] where  [2.x.62]  runs through all basis functions with from 1 to  [2.x.63] . 

We now denote by  [2.x.64]  the mass matrix with entries  [2.x.65] , and by [1.x.58] the operator evaluating the right-hand side of the Euler operator, given a function  [2.x.66]  associated with a global vector of unknowns and the finite element in use. This function  [2.x.67]  is explicitly time-dependent as the numerical flux evaluated at the boundary will involve time-dependent data  [2.x.68] ,  [2.x.69] , and  [2.x.70]  on some parts of the boundary, depending on the assignment of boundary conditions. With this notation, we can write the discrete in space, continuous in time system compactly as [1.x.59] where we have taken the liberty to also denote the global solution vector by  [2.x.71]  (in addition to the the corresponding finite element function). Equivalently, the system above has the form [1.x.60] 

For hyperbolic systems discretized by high-order discontinuous Galerkin methods, explicit time integration of this system is very popular. This is due to the fact that the mass matrix  [2.x.72]  is block-diagonal (with each block corresponding to only variables of the same kind defined on the same cell) and thus easily inverted. In each time step -- or stage of a Runge--Kutta scheme -- one only needs to evaluate the differential operator once using the given data and subsequently apply the inverse of the mass matrix. For implicit time stepping, on the other hand, one would first have to linearize the equations and then iteratively solve the linear system, which involves several residual evaluations and at least a dozen applications of the linearized operator, as has been demonstrated in the  [2.x.73]  tutorial program. 

Of course, the simplicity of explicit time stepping comes with a price, namely conditional stability due to the so-called Courant--Friedrichs--Lewy (CFL) condition. It states that the time step cannot be larger than the fastest propagation of information by the discretized differential operator. In more modern terms, the speed of propagation corresponds to the largest eigenvalue in the discretized operator, and in turn depends on the mesh size, the polynomial degree  [2.x.74]  and the physics of the Euler operator, i.e., the eigenvalues of the linearization of  [2.x.75]  with respect to  [2.x.76] . In this program, we set the time step as follows: [1.x.61] 

with the maximum taken over all quadrature points and all cells. The dimensionless number  [2.x.77]  denotes the Courant number and can be chosen up to a maximally stable number  [2.x.78] , whose value depends on the selected time stepping method and its stability properties. The power  [2.x.79]  used for the polynomial scaling is heuristic and represents the closest fit for polynomial degrees between 1 and 8, see e.g.  [2.x.80] . In the limit of higher degrees,  [2.x.81] , a scaling of  [2.x.82]  is more accurate, related to the inverse estimates typically used for interior penalty methods. Regarding the [1.x.62] mesh sizes  [2.x.83]  and  [2.x.84]  used in the formula, we note that the convective transport is directional. Thus an appropriate scaling is to use the element length in the direction of the velocity  [2.x.85] . The code below derives this scaling from the inverse of the Jacobian from the reference to real cell, i.e., we approximate  [2.x.86] . The acoustic waves, instead, are isotropic in character, which is why we use the smallest feature size, represented by the smallest singular value of  [2.x.87] , for the acoustic scaling  [2.x.88] . Finally, we need to add the convective and acoustic limits, as the Euler equations can transport information with speed  [2.x.89] . 

In this tutorial program, we use a specific variant of [1.x.63], which in general use the following update procedure from the state  [2.x.90]  at time  [2.x.91]  to the new time  [2.x.92]  with  [2.x.93] : [1.x.64] The vectors  [2.x.94] ,  [2.x.95] , in an  [2.x.96] -stage scheme are evaluations of the operator at some intermediate state and used to define the end-of-step value  [2.x.97]  via some linear combination. The scalar coefficients in this scheme,  [2.x.98] ,  [2.x.99] , and  [2.x.100] , are defined such that certain conditions are satisfied for higher order schemes, the most basic one being  [2.x.101] . The parameters are typically collected in the form of a so-called [1.x.65] that collects all of the coefficients that define the scheme. For a five-stage scheme, it would look like this: [1.x.66] 

In this tutorial program, we use a subset of explicit Runge--Kutta methods, so-called low-storage Runge--Kutta methods (LSRK), which assume additional structure in the coefficients. In the variant used by reference  [2.x.102] , the assumption is to use Butcher tableaus of the form [1.x.67] With such a definition, the update to  [2.x.103]  shares the storage with the information for the intermediate values  [2.x.104] . Starting with  [2.x.105]  and  [2.x.106] , the update in each of the  [2.x.107]  stages simplifies to [1.x.68] Besides the vector  [2.x.108]  that is successively updated, this scheme only needs two auxiliary vectors, namely the vector  [2.x.109]  to hold the evaluation of the differential operator, and the vector  [2.x.110]  that holds the right-hand side for the differential operator application. In subsequent stages  [2.x.111] , the values  [2.x.112]  and  [2.x.113]  can use the same storage. 

The main advantages of low-storage variants are the reduced memory consumption on the one hand (if a very large number of unknowns must be fit in memory, holding all  [2.x.114]  to compute subsequent updates can be a limit already for  [2.x.115]  in between five and eight -- recall that we are using an explicit scheme, so we do not need to store any matrices that are typically much larger than a few vectors), and the reduced memory access on the other. In this program, we are particularly interested in the latter aspect. Since cost of operator evaluation is only a small multiple of the cost of simply streaming the input and output vector from memory with the optimized matrix-free methods of deal.II, we must consider the cost of vector updates, and low-storage variants can deliver up to twice the throughput of conventional explicit Runge--Kutta methods for this reason, see e.g. the analysis in  [2.x.116] . 

Besides three variants for third, fourth and fifth order accuracy from the reference  [2.x.117] , we also use a fourth-order accurate variant with seven stages that was optimized for acoustics setups from  [2.x.118] . Acoustic problems are one of the interesting aspects of the subsonic regime of the Euler equations where compressibility leads to the transmission of sound waves; often, one uses further simplifications of the linearized Euler equations around a background state or the acoustic wave equation around a fixed frame. 


[1.x.69][1.x.70] 


The major ingredients used in this program are the fast matrix-free techniques we use to evaluate the operator  [2.x.119]  and the inverse mass matrix  [2.x.120] . Actually, the term [1.x.71] is a slight misnomer, because we are working with a nonlinear operator and do not linearize the operator that in turn could be represented by a matrix. However, fast evaluation of integrals has become popular as a replacement of sparse matrix-vector products, as shown in  [2.x.121]  and  [2.x.122] , and we have coined this infrastructure [1.x.72] in deal.II for this reason. Furthermore, the inverse mass matrix is indeed applied in a matrix-free way, as detailed below. 

The matrix-free infrastructure allows us to quickly evaluate the integrals in the weak forms. The ingredients are the fast interpolation from solution coefficients into values and derivatives at quadrature points, point-wise operations at quadrature points (where we implement the differential operator as derived above), as well as multiplication by all test functions and summation over quadrature points. The first and third component make use of sum factorization and have been extensively discussed in the  [2.x.123]  tutorial program for the cell integrals and  [2.x.124]  for the face integrals. The only difference is that we now deal with a system of  [2.x.125]  components, rather than the scalar systems in previous tutorial programs. In the code, all that changes is a template argument of the FEEvaluation and FEFaceEvaluation classes, the one to set the number of components. The access to the vector is the same as before, all handled transparently by the evaluator. We also note that the variant with a single evaluator chosen in the code below is not the only choice -- we could also have used separate evalators for the separate components  [2.x.126] ,  [2.x.127] , and  [2.x.128] ; given that we treat all components similarly (also reflected in the way we state the equation as a vector system), this would be more complicated here. As before, the FEEvaluation class provides explicit vectorization by combining the operations on several cells (and faces), involving data types called VectorizedArray. Since the arithmetic operations are overloaded for this type, we do not have to bother with it all that much, except for the evaluation of functions through the Function interface, where we need to provide particular [1.x.73] evaluations for several quadrature point locations at once. 

A more substantial change in this program is the operation at quadrature points: Here, the multi-component evaluators provide us with return types not discussed before. Whereas  [2.x.129]  would return a scalar (more precisely, a VectorizedArray type due to vectorization across cells) for the Laplacian of  [2.x.130] , it now returns a type that is `Tensor<1,dim+2,VectorizedArray<Number>>`. Likewise, the gradient type is now `Tensor<1,dim+2,Tensor<1,dim,VectorizedArray<Number>>>`, where the outer tensor collects the `dim+2` components of the Euler system, and the inner tensor the partial derivatives in the various directions. For example, the flux  [2.x.131]  of the Euler system is of this type. In order to reduce the amount of code we have to write for spelling out these types, we use the C++ `auto` keyword where possible. 

From an implementation point of view, the nonlinearity is not a big difficulty: It is introduced naturally as we express the terms of the Euler weak form, for example in the form of the momentum term  [2.x.132] . To obtain this expression, we first deduce the velocity  [2.x.133]  from the momentum variable  [2.x.134] . Given that  [2.x.135]  is represented as a  [2.x.136] -degree polynomial, as is  [2.x.137] , the velocity  [2.x.138]  is a rational expression in terms of the reference coordinates  [2.x.139] . As we perform the multiplication  [2.x.140] , we obtain an expression that is the ratio of two polynomials, with polynomial degree  [2.x.141]  in the numerator and polynomial degree  [2.x.142]  in the denominator. Combined with the gradient of the test function, the integrand is of degree  [2.x.143]  in the numerator and  [2.x.144]  in the denominator already for affine cells, i.e., for parallelograms/ parallelepipeds. For curved cells, additional polynomial and rational expressions appear when multiplying the integrand by the determinant of the Jacobian of the mapping. At this point, one usually needs to give up on insisting on exact integration, and take whatever accuracy the Gaussian (more precisely, Gauss--Legrende) quadrature provides. The situation is then similar to the one for the Laplace equation, where the integrand contains rational expressions on non-affince cells and is also only integrated approximately. As these formulas only integrate polynomials exactly, we have to live with the [1.x.74] in the form of an integration error. 

While inaccurate integration is usually tolerable for elliptic problems, for hyperbolic problems inexact integration causes some headache in the form of an effect called [1.x.75]. The term comes from signal processing and expresses the situation of inappropriate, too coarse sampling. In terms of quadrature, the inappropriate sampling means that we use too few quadrature points compared to what would be required to accurately sample the variable-coefficient integrand. It has been shown in the DG literature that aliasing errors can introduce unphysical oscillations in the numerical solution for [1.x.76] resolved simulations. The fact that aliasing mostly affects coarse resolutions -- whereas finer meshes with the same scheme work fine -- is not surprising because well-resolved simulations tend to be smooth on length-scales of a cell (i.e., they have small coefficients in the higher polynomial degrees that are missed by too few quadrature points, whereas the main solution contribution in the lower polynomial degrees is still well-captured -- this is simply a consequence of Taylor's theorem). To address this topic, various approaches have been proposed in the DG literature. One technique is filtering which damps the solution components pertaining to higher polynomial degrees. As the chosen nodal basis is not hierarchical, this would mean to transform from the nodal basis into a hierarchical one (e.g., a modal one based on Legendre polynomials) where the contributions within a cell are split by polynomial degrees. In that basis, one could then multiply the solution coefficients associated with higher degrees by a small number, keep the lower ones intact (to not destroy consistency), and then transform back to the nodal basis. However, filters reduce the accuracy of the method. Another, in some sense simpler, strategy is to use more quadrature points to capture non-linear terms more accurately. Using more than  [2.x.145]  quadrature points per coordinate directions is sometimes called over-integration or consistent integration. The latter name is most common in the context of the incompressible Navier-Stokes equations, where the  [2.x.146]  nonlinearity results in polynomial integrands of degree  [2.x.147]  (when also considering the test function), which can be integrated exactly with  [2.x.148]  quadrature points per direction as long as the element geometry is affine. In the context of the Euler equations with non-polynomial integrands, the choice is less clear. Depending on the variation in the various variables both  [2.x.149]  or  [2.x.150]  points (integrating exactly polynomials of degree  [2.x.151]  or  [2.x.152] , respectively) are common. 

To reflect this variability in the choice of quadrature in the program, we keep the number of quadrature points a variable to be specified just as the polynomial degree, and note that one would make different choices depending also on the flow configuration. The default choice is  [2.x.153]  points -- a bit more than the minimum possible of  [2.x.154]  points. The FEEvaluation and FEFaceEvaluation classes allow to seamlessly change the number of points by a template parameter, such that the program does not get more complicated because of that. 


[1.x.77][1.x.78] 


The last ingredient is the evaluation of the inverse mass matrix  [2.x.155] . In DG methods with explicit time integration, mass matrices are block-diagonal and thus easily inverted -- one only needs to invert the diagonal blocks. However, given the fact that matrix-free evaluation of integrals is closer in cost to the access of the vectors only, even the application of a block-diagonal matrix (e.g. via an array of LU factors) would be several times more expensive than evaluation of  [2.x.156]  simply because just storing and loading matrices of size `dofs_per_cell` times `dofs_per_cell` for higher order finite elements repeatedly is expensive. As this is clearly undesirable, part of the community has moved to bases where the mass matrix is diagonal, for example the [1.x.79]-orthogonal Legendre basis using hierarchical polynomials or Lagrange polynomials on the points of the Gaussian quadrature (which is just another way of utilizing Legendre information). While the diagonal property breaks down for deformed elements, the error made by taking a diagonal mass matrix and ignoring the rest (a variant of mass lumping, though not the one with an additional integration error as utilized in  [2.x.157] ) has been shown to not alter discretization accuracy. The Lagrange basis in the points of Gaussian quadrature is sometimes also referred to as a collocation setup, as the nodal points of the polynomials coincide (= are "co-located") with the points of quadrature, obviating some interpolation operations. Given the fact that we want to use more quadrature points for nonlinear terms in  [2.x.158] , however, the collocation property is lost. (More precisely, it is still used in FEEvaluation and FEFaceEvaluation after a change of basis, see the matrix-free paper  [2.x.159] .) 

In this tutorial program, we use the collocation idea for the application of the inverse mass matrix, but with a slight twist. Rather than using the collocation via Lagrange polynomials in the points of Gaussian quadrature, we prefer a conventional Lagrange basis in Gauss-Lobatto points as those make the evaluation of face integrals cheap. This is because for Gauss-Lobatto points, some of the node points are located on the faces of the cell and it is not difficult to show that on any given face, the only shape functions with non-zero values are exactly the ones whose node points are in fact located on that face. One could of course also use the Gauss-Lobatto quadrature (with some additional integration error) as was done in  [2.x.160] , but we do not want to sacrifice accuracy as these quadrature formulas are generally of lower order than the general Gauss quadrature formulas. Instead, we use an idea described in the reference  [2.x.161]  where it was proposed to change the basis for the sake of applying the inverse mass matrix. Let us denote by  [2.x.162]  the matrix of shape functions evaluated at quadrature points, with shape functions in the row of the matrix and quadrature points in columns. Then, the mass matrix on a cell  [2.x.163]  is given by [1.x.80] Here,  [2.x.164]  is the diagonal matrix with the determinant of the Jacobian times the quadrature weight (JxW) as entries. The matrix  [2.x.165]  is constructed as the Kronecker product (tensor product) of one-dimensional matrices, e.g. in 3D as [1.x.81] which is the result of the basis functions being a tensor product of one-dimensional shape functions and the quadrature formula being the tensor product of 1D quadrature formulas. For the case that the number of polynomials equals the number of quadrature points, all matrices in  [2.x.166]  are square, and also the ingredients to  [2.x.167]  in the Kronecker product are square. Thus, one can invert each matrix to form the overall inverse, [1.x.82] This formula is of exactly the same structure as the steps in the forward evaluation of integrals with sum factorization techniques (i.e., the FEEvaluation and MatrixFree framework of deal.II). Hence, we can utilize the same code paths with a different interpolation matrix,  [2.x.168]  rather than  [2.x.169] . 

The class  [2.x.170]  implements this operation: It changes from the basis contained in the finite element (in this case, FE_DGQ) to the Lagrange basis in Gaussian quadrature points. Here, the inverse of a diagonal mass matrix can be evaluated, which is simply the inverse of the `JxW` factors (i.e., the quadrature weight times the determinant of the Jacobian from reference to real coordinates). Once this is done, we can change back to the standard nodal Gauss-Lobatto basis. 

The advantage of this particular way of applying the inverse mass matrix is a cost similar to the forward application of a mass matrix, which is cheaper than the evaluation of the spatial operator  [2.x.171]  with over-integration and face integrals. (We will demonstrate this with detailed timing information in the [1.x.83].) In fact, it is so cheap that it is limited by the bandwidth of reading the source vector, reading the diagonal, and writing into the destination vector on most modern architectures. The hardware used for the result section allows to do the computations at least twice as fast as the streaming of the vectors from memory. 


[1.x.84][1.x.85] 


In this tutorial program, we implement two test cases. The first case is a convergence test limited to two space dimensions. It runs a so-called isentropic vortex which is transported via a background flow field. The second case uses a more exciting setup: We start with a cylinder immersed in a channel, using the  [2.x.172]  function. Here, we impose a subsonic initial field at Mach number of  [2.x.173]  with a constant velocity in  [2.x.174]  direction. At the top and bottom walls as well as at the cylinder, we impose a no-penetration (i.e., tangential flow) condition. This setup forces the flow to re-orient as compared to the initial condition, which results in a big sound wave propagating away from the cylinder. In upstream direction, the wave travels more slowly (as it has to move against the oncoming gas), including a discontinuity in density and pressure. In downstream direction, the transport is faster as sound propagation and fluid flow go in the same direction, which smears out the discontinuity somewhat. Once the sound wave hits the upper and lower walls, the sound is reflected back, creating some nice shapes as illustrated in the [1.x.86] below. [1.x.87] [1.x.88] 

The include files are similar to the previous matrix-free tutorial programs  [2.x.175] ,  [2.x.176] , and  [2.x.177]  

[1.x.89] 



The following file includes the CellwiseInverseMassMatrix data structure that we will use for the mass matrix inversion, the only new include file for this tutorial program: 

[1.x.90] 



Similarly to the other matrix-free tutorial programs, we collect all parameters that control the execution of the program at the top of the file. Besides the dimension and polynomial degree we want to run with, we also specify a number of points in the Gaussian quadrature formula we want to use for the nonlinear terms in the Euler equations. Furthermore, we specify the time interval for the time-dependent problem, and implement two different test cases. The first one is an analytical solution in 2D, whereas the second is a channel flow around a cylinder as described in the introduction. Depending on the test case, we also change the final time up to which we run the simulation, and a variable `output_tick` that specifies in which intervals we want to write output (assuming that the tick is larger than the time step size). 

[1.x.91] 



Next off are some details of the time integrator, namely a Courant number that scales the time step size in terms of the formula  [2.x.178] , as well as a selection of a few low-storage Runge--Kutta methods. We specify the Courant number per stage of the Runge--Kutta scheme, as this gives a more realistic expression of the numerical cost for schemes of various numbers of stages. 

[1.x.92] 



Eventually, we select a detail of the spatial discretization, namely the numerical flux (Riemann solver) at the faces between cells. For this program, we have implemented a modified variant of the Lax--Friedrichs flux and the Harten--Lax--van Leer (HLL) flux. 

[1.x.93] 




[1.x.94]  [1.x.95] 




We now define a class with the exact solution for the test case 0 and one with a background flow field for test case 1 of the channel. Given that the Euler equations are a problem with  [2.x.179]  equations in  [2.x.180]  dimensions, we need to tell the Function base class about the correct number of components. 

[1.x.96] 



As far as the actual function implemented is concerned, the analytical test case is an isentropic vortex case (see e.g. the book by Hesthaven and Warburton, Example 6.1 in Section 6.6 on page 209) which fulfills the Euler equations with zero force term on the right hand side. Given that definition, we return either the density, the momentum, or the energy depending on which component is requested. Note that the original definition of the density involves the  [2.x.181] -th power of some expression. Since  [2.x.182]  has pretty slow implementations on some systems, we replace it by logarithm followed by exponentiation (of base 2), which is mathematically equivalent but usually much better optimized. This formula might lose accuracy in the last digits for very small numbers compared to  [2.x.183]  but we are happy with it anyway, since small numbers map to data close to 1.    


For the channel test case, we simply select a density of 1, a velocity of 0.4 in  [2.x.184]  direction and zero in the other directions, and an energy that corresponds to a speed of sound of 1.3 measured against the background velocity field, computed from the relation  [2.x.185] . 

[1.x.97] 




[1.x.98]  [1.x.99] 




The next few lines implement a few low-storage variants of Runge--Kutta methods. These methods have specific Butcher tableaux with coefficients  [2.x.186]  and  [2.x.187]  as shown in the introduction. As usual in Runge--Kutta method, we can deduce time steps,  [2.x.188]  from those coefficients. The main advantage of this kind of scheme is the fact that only two vectors are needed per stage, namely the accumulated part of the solution  [2.x.189]  (that will hold the solution  [2.x.190]  at the new time  [2.x.191]  after the last stage), the update vector  [2.x.192]  that gets evaluated during the stages, plus one vector  [2.x.193]  to hold the evaluation of the operator. Such a Runge--Kutta setup reduces the memory storage and memory access. As the memory bandwidth is often the performance-limiting factor on modern hardware when the evaluation of the differential operator is well-optimized, performance can be improved over standard time integrators. This is true also when taking into account that a conventional Runge--Kutta scheme might allow for slightly larger time steps as more free parameters allow for better stability properties.    


In this tutorial programs, we concentrate on a few variants of low-storage schemes defined in the article by Kennedy, Carpenter, and Lewis (2000), as well as one variant described by Tselios and Simos (2007). There is a large series of other schemes available, which could be addressed by additional sets of coefficients or slightly different update formulas.    


We define a single class for the four integrators, distinguished by the enum described above. To each scheme, we then fill the vectors for the  [2.x.194]  and  [2.x.195]  to the given variables in the class. 

[1.x.100] 



First comes the three-stage scheme of order three by Kennedy et al. (2000). While its stability region is significantly smaller than for the other schemes, it only involves three stages, so it is very competitive in terms of the work per stage. 

[1.x.101] 



The next scheme is a five-stage scheme of order four, again defined in the paper by Kennedy et al. (2000). 

[1.x.102] 



The following scheme of seven stages and order four has been explicitly derived for acoustics problems. It is a balance of accuracy for imaginary eigenvalues among fourth order schemes, combined with a large stability region. Since DG schemes are dissipative among the highest frequencies, this does not necessarily translate to the highest possible time step per stage. In the context of the present tutorial program, the numerical flux plays a crucial role in the dissipation and thus also the maximal stable time step size. For the modified Lax--Friedrichs flux, this scheme is similar to the `stage_5_order_4` scheme in terms of step size per stage if only stability is considered, but somewhat less efficient for the HLL flux. 

[1.x.103] 



The last scheme included here is the nine-stage scheme of order five from Kennedy et al. (2000). It is the most accurate among the schemes used here, but the higher order of accuracy sacrifices some stability, so the step length normalized per stage is less than for the fourth order schemes. 

[1.x.104] 



The main function of the time integrator is to go through the stages, evaluate the operator, prepare the  [2.x.196]  vector for the next evaluation, and update the solution vector  [2.x.197] . We hand off the work to the `pde_operator` involved in order to be able to merge the vector operations of the Runge--Kutta setup with the evaluation of the differential operator for better performance, so all we do here is to delegate the vectors and coefficients.      


We separately call the operator for the first stage because we need slightly modified arguments there: We evaluate the solution from the old solution  [2.x.198]  rather than a  [2.x.199]  vector, so the first argument is `solution`. We here let the stage vector  [2.x.200]  also hold the temporary result of the evaluation, as it is not used otherwise. For all subsequent stages, we use the vector `vec_ki` as the second vector argument to store the result of the operator evaluation. Finally, when we are at the last stage, we must skip the computation of the vector  [2.x.201]  as there is no coefficient  [2.x.202]  available (nor will it be used). 

[1.x.105] 




[1.x.106]  [1.x.107] 




In the following functions, we implement the various problem-specific operators pertaining to the Euler equations. Each function acts on the vector of conserved variables  [2.x.203]  that we hold in the solution vectors, and computes various derived quantities.    


First out is the computation of the velocity, that we derive from the momentum variable  [2.x.204]  by division by  [2.x.205] . One thing to note here is that we decorate all those functions with the keyword `DEAL_II_ALWAYS_INLINE`. This is a special macro that maps to a compiler-specific keyword that tells the compiler to never create a function call for any of those functions, and instead move the implementation [1.x.108] to where they are called. This is critical for performance because we call into some of those functions millions or billions of times: For example, we both use the velocity for the computation of the flux further down, but also for the computation of the pressure, and both of these places are evaluated at every quadrature point of every cell. Making sure these functions are inlined ensures not only that the processor does not have to execute a jump instruction into the function (and the corresponding return jump), but also that the compiler can re-use intermediate information from one function's context in code that comes after the place where the function was called. (We note that compilers are generally quite good at figuring out which functions to inline by themselves. Here is a place where compilers may or may not have figured it out by themselves but where we know for sure that inlining is a win.)    


Another trick we apply is a separate variable for the inverse density  [2.x.206] . This enables the compiler to only perform a single division for the flux, despite the division being used at several places. As divisions are around ten to twenty times as expensive as multiplications or additions, avoiding redundant divisions is crucial for performance. We note that taking the inverse first and later multiplying with it is not equivalent to a division in floating point arithmetic due to roundoff effects, so the compiler is not allowed to exchange one way by the other with standard optimization flags. However, it is also not particularly difficult to write the code in the right way.    


To summarize, the chosen strategy of always inlining and careful definition of expensive arithmetic operations allows us to write compact code without passing all intermediate results around, despite making sure that the code maps to excellent machine code. 

[1.x.109] 



The next function computes the pressure from the vector of conserved variables, using the formula  [2.x.207] . As explained above, we use the velocity from the `euler_velocity()` function. Note that we need to specify the first template argument `dim` here because the compiler is not able to deduce it from the arguments of the tensor, whereas the second argument (number type) can be automatically deduced. 

[1.x.110] 



Here is the definition of the Euler flux function, i.e., the definition of the actual equation. Given the velocity and pressure (that the compiler optimization will make sure are done only once), this is straight-forward given the equation stated in the introduction. 

[1.x.111] 



This next function is a helper to simplify the implementation of the numerical flux, implementing the action of a tensor of tensors (with non-standard outer dimension of size `dim + 2`, so the standard overloads provided by deal.II's tensor classes do not apply here) with another tensor of the same inner dimension, i.e., a matrix-vector product. 

[1.x.112] 



This function implements the numerical flux (Riemann solver). It gets the state from the two sides of an interface and the normal vector, oriented from the side of the solution  [2.x.208]  towards the solution  [2.x.209] . In finite volume methods which rely on piece-wise constant data, the numerical flux is the central ingredient as it is the only place where the physical information is entered. In DG methods, the numerical flux is less central due to the polynomials within the elements and the physical flux used there. As a result of higher-degree interpolation with consistent values from both sides in the limit of a continuous solution, the numerical flux can be seen as a control of the jump of the solution from both sides to weakly impose continuity. It is important to realize that a numerical flux alone cannot stabilize a high-order DG method in the presence of shocks, and thus any DG method must be combined with further shock-capturing techniques to handle those cases. In this tutorial, we focus on wave-like solutions of the Euler equations in the subsonic regime without strong discontinuities where our basic scheme is sufficient.    


Nonetheless, the numerical flux is decisive in terms of the numerical dissipation of the overall scheme and influences the admissible time step size with explicit Runge--Kutta methods. We consider two choices, a modified Lax--Friedrichs scheme and the widely used Harten--Lax--van Leer (HLL) flux. For both variants, we first need to get the velocities and pressures from both sides of the interface and evaluate the physical Euler flux.    


For the local Lax--Friedrichs flux, the definition is  [2.x.210] , where the factor  [2.x.211]  gives the maximal wave speed and  [2.x.212]  is the speed of sound. Here, we choose two modifications of that expression for reasons of computational efficiency, given the small impact of the flux on the solution. For the above definition of the factor  [2.x.213] , we would need to take four square roots, two for the two velocity norms and two for the speed of sound on either side. The first modification is hence to rather use  [2.x.214]  as an estimate of the maximal speed (which is at most a factor of 2 away from the actual maximum, as shown in the introduction). This allows us to pull the square root out of the maximum and get away with a single square root computation. The second modification is to further relax on the parameter  [2.x.215] ---the smaller it is, the smaller the dissipation factor (which is multiplied by the jump in  [2.x.216] , which might result in a smaller or bigger dissipation in the end). This allows us to fit the spectrum into the stability region of the explicit Runge--Kutta integrator with bigger time steps. However, we cannot make dissipation too small because otherwise imaginary eigenvalues grow larger. Finally, the current conservative formulation is not energy-stable in the limit of  [2.x.217]  as it is not skew-symmetric, and would need additional measures such as split-form DG schemes in that case.    


For the HLL flux, we follow the formula from literature, introducing an additional weighting of the two states from Lax--Friedrichs by a parameter  [2.x.218] . It is derived from the physical transport directions of the Euler equations in terms of the current direction of velocity and sound speed. For the velocity, we here choose a simple arithmetic average which is sufficient for DG scenarios and moderate jumps in material parameters.    


Since the numerical flux is multiplied by the normal vector in the weak form, we multiply by the result by the normal vector for all terms in the equation. In these multiplications, the `operator*` defined above enables a compact notation similar to the mathematical definition.    


In this and the following functions, we use variable suffixes `_m` and `_p` to indicate quantities derived from  [2.x.219]  and  [2.x.220] , i.e., values "here" and "there" relative to the current cell when looking at a neighbor cell. 

[1.x.113] 



This and the next function are helper functions to provide compact evaluation calls as multiple points get batched together via a VectorizedArray argument (see the  [2.x.221]  tutorial for details). This function is used for the subsonic outflow boundary conditions where we need to set the energy component to a prescribed value. The next one requests the solution on all components and is used for inflow boundaries where all components of the solution are set. 

[1.x.114] 




[1.x.115]  [1.x.116] 




This class implements the evaluators for the Euler problem, in analogy to the `LaplaceOperator` class of  [2.x.222]  or  [2.x.223] . Since the present operator is non-linear and does not require a matrix interface (to be handed over to preconditioners), we skip the various `vmult` functions otherwise present in matrix-free operators and only implement an `apply` function as well as the combination of `apply` with the required vector updates for the low-storage Runge--Kutta time integrator mentioned above (called `perform_stage`). Furthermore, we have added three additional functions involving matrix-free routines, namely one to compute an estimate of the time step scaling (that is combined with the Courant number for the actual time step size) based on the velocity and speed of sound in the elements, one for the projection of solutions (specializing  [2.x.224]  for the DG case), and one to compute the errors against a possible analytical solution or norms against some background state.    


The rest of the class is similar to other matrix-free tutorials. As discussed in the introduction, we provide a few functions to allow a user to pass in various forms of boundary conditions on different parts of the domain boundary marked by  [2.x.225]  variables, as well as possible body forces. 

[1.x.117] 



For the initialization of the Euler operator, we set up the MatrixFree variable contained in the class. This can be done given a mapping to describe possible curved boundaries as well as a DoFHandler object describing the degrees of freedom. Since we use a discontinuous Galerkin discretization in this tutorial program where no constraints are imposed strongly on the solution field, we do not need to pass in an AffineConstraints object and rather use a dummy for the construction. With respect to quadrature, we want to select two different ways of computing the underlying integrals: The first is a flexible one, based on a template parameter `n_points_1d` (that will be assigned the `n_q_points_1d` value specified at the top of this file). More accurate integration is necessary to avoid the aliasing problem due to the variable coefficients in the Euler operator. The second less accurate quadrature formula is a tight one based on `fe_degree+1` and needed for the inverse mass matrix. While that formula provides an exact inverse only on affine element shapes and not on deformed elements, it enables the fast inversion of the mass matrix by tensor product techniques, necessary to ensure optimal computational efficiency overall. 

[1.x.118] 



The subsequent four member functions are the ones that must be called from outside to specify the various types of boundaries. For an inflow boundary, we must specify all components in terms of density  [2.x.226] , momentum  [2.x.227]  and energy  [2.x.228] . Given this information, we then store the function alongside the respective boundary id in a map member variable of this class. Likewise, we proceed for the subsonic outflow boundaries (where we request a function as well, which we use to retrieve the energy) and for wall (no-penetration) boundaries where we impose zero normal velocity (no function necessary, so we only request the boundary id). For the present DG code where boundary conditions are solely applied as part of the weak form (during time integration), the call to set the boundary conditions can appear both before or after the `reinit()` call to this class. This is different from continuous finite element codes where the boundary conditions determine the content of the AffineConstraints object that is sent into MatrixFree for initialization, thus requiring to be set before the initialization of the matrix-free data structures.    


The checks added in each of the four function are used to ensure that boundary conditions are mutually exclusive on the various parts of the boundary, i.e., that a user does not accidentally designate a boundary as both an inflow and say a subsonic outflow boundary. 

[1.x.119] 




[1.x.120]  [1.x.121] 




Now we proceed to the local evaluators for the Euler problem. The evaluators are relatively simple and follow what has been presented in  [2.x.229] ,  [2.x.230] , or  [2.x.231] . The first notable difference is the fact that we use an FEEvaluation with a non-standard number of quadrature points. Whereas we previously always set the number of quadrature points to equal the polynomial degree plus one (ensuring exact integration on affine element shapes), we now set the number quadrature points as a separate variable (e.g. the polynomial degree plus two or three halves of the polynomial degree) to more accurately handle nonlinear terms. Since the evaluator is fed with the appropriate loop lengths via the template argument and keeps the number of quadrature points in the whole cell in the variable  [2.x.232]  we now automatically operate on the more accurate formula without further changes.    


The second difference is due to the fact that we are now evaluating a multi-component system, as opposed to the scalar systems considered previously. The matrix-free framework provides several ways to handle the multi-component case. The variant shown here utilizes an FEEvaluation object with multiple components embedded into it, specified by the fourth template argument `dim + 2` for the components in the Euler system. As a consequence, the return type of  [2.x.233]  is not a scalar any more (that would return a VectorizedArray type, collecting data from several elements), but a Tensor of `dim+2` components. The functionality is otherwise similar to the scalar case; it is handled by a template specialization of a base class, called FEEvaluationAccess. An alternative variant would have been to use several FEEvaluation objects, a scalar one for the density, a vector-valued one with `dim` components for the momentum, and another scalar evaluator for the energy. To ensure that those components point to the correct part of the solution, the constructor of FEEvaluation takes three optional integer arguments after the required MatrixFree field, namely the number of the DoFHandler for multi-DoFHandler systems (taking the first by default), the number of the quadrature point in case there are multiple Quadrature objects (see more below), and as a third argument the component within a vector system. As we have a single vector for all components, we would go with the third argument, and set it to `0` for the density, `1` for the vector-valued momentum, and `dim+1` for the energy slot. FEEvaluation then picks the appropriate subrange of the solution vector during  [2.x.234]  and  [2.x.235]  or the more compact  [2.x.236]  and  [2.x.237]  calls.    


When it comes to the evaluation of the body force vector, we distinguish between two cases for efficiency reasons: In case we have a constant function (derived from  [2.x.238]  we can precompute the value outside the loop over quadrature points and simply use the value everywhere. For a more general function, we instead need to call the `evaluate_function()` method we provided above; this path is more expensive because we need to access the memory associated with the quadrature point data.    


The rest follows the other tutorial programs. Since we have implemented all physics for the Euler equations in the separate `euler_flux()` function, all we have to do here is to call this function given the current solution evaluated at quadrature points, returned by `phi.get_value(q)`, and tell the FEEvaluation object to queue the flux for testing it by the gradients of the shape functions (which is a Tensor of outer `dim+2` components, each holding a tensor of `dim` components for the  [2.x.239]  component of the Euler flux). One final thing worth mentioning is the order in which we queue the data for testing by the value of the test function, `phi.submit_value()`, in case we are given an external function: We must do this after calling `phi.get_value(q)`, because `get_value()` (reading the solution) and `submit_value()` (queuing the value for multiplication by the test function and summation over quadrature points) access the same underlying data field. Here it would be easy to achieve also without temporary variable `w_q` since there is no mixing between values and gradients. For more complicated setups, one has to first copy out e.g. both the value and gradient at a quadrature point and then queue results again by  [2.x.240]  and  [2.x.241]     


As a final note, we mention that we do not use the first MatrixFree argument of this function, which is a call-back from  [2.x.242]  The interfaces imposes the present list of arguments, but since we are in a member function where the MatrixFree object is already available as the `data` variable, we stick with that to avoid confusion. 

[1.x.122] 



The next function concerns the computation of integrals on interior faces, where we need evaluators from both cells adjacent to the face. We associate the variable `phi_m` with the solution component  [2.x.243]  and the variable `phi_p` with the solution component  [2.x.244] . We distinguish the two sides in the constructor of FEFaceEvaluation by the second argument, with `true` for the interior side and `false` for the exterior side, with interior and exterior denoting the orientation with respect to the normal vector.    


Note that the calls  [2.x.245]  and  [2.x.246]  combine the access to the vectors and the sum factorization parts. This combined operation not only saves a line of code, but also contains an important optimization: Given that we use a nodal basis in terms of the Lagrange polynomials in the points of the Gauss-Lobatto quadrature formula, only  [2.x.247]  out of the  [2.x.248]  basis functions evaluate to non-zero on each face. Thus, the evaluator only accesses the necessary data in the vector and skips the parts which are multiplied by zero. If we had first read the vector, we would have needed to load all data from the vector, as the call in isolation would not know what data is required in subsequent operations. If the subsequent  [2.x.249]  call requests values and derivatives, indeed all  [2.x.250]  vector entries for each component are needed, as the normal derivative is nonzero for all basis functions.    


The arguments to the evaluators as well as the procedure is similar to the cell evaluation. We again use the more accurate (over-)integration scheme due to the nonlinear terms, specified as the third template argument in the list. At the quadrature points, we then go to our free-standing function for the numerical flux. It receives the solution evaluated at quadrature points from both sides (i.e.,  [2.x.251]  and  [2.x.252] ), as well as the normal vector onto the minus side. As explained above, the numerical flux is already multiplied by the normal vector from the minus side. We need to switch the sign because the boundary term comes with a minus sign in the weak form derived in the introduction. The flux is then queued for testing both on the minus sign and on the plus sign, with switched sign as the normal vector from the plus side is exactly opposed to the one from the minus side. 

[1.x.123] 



For faces located at the boundary, we need to impose the appropriate boundary conditions. In this tutorial program, we implement four cases as mentioned above. (A fifth case, for supersonic outflow conditions is discussed in the "Results" section below.) The discontinuous Galerkin method imposes boundary conditions not as constraints, but only weakly. Thus, the various conditions are imposed by finding an appropriate [1.x.124] quantity  [2.x.253]  that is then handed to the numerical flux function also used for the interior faces. In essence, we "pretend" a state on the outside of the domain in such a way that if that were reality, the solution of the PDE would satisfy the boundary conditions we want.    


For wall boundaries, we need to impose a no-normal-flux condition on the momentum variable, whereas we use a Neumann condition for the density and energy with  [2.x.254]  and  [2.x.255] . To achieve the no-normal flux condition, we set the exterior values to the interior values and subtract two times the velocity in wall-normal direction, i.e., in the direction of the normal vector.    


For inflow boundaries, we simply set the given Dirichlet data  [2.x.256]  as a boundary value. An alternative would have been to use  [2.x.257] , the so-called mirror principle.    


The imposition of outflow is essentially a Neumann condition, i.e., setting  [2.x.258] . For the case of subsonic outflow, we still need to impose a value for the energy, which we derive from the respective function. A special step is needed for the case of [1.x.125], i.e., the case where there is a momentum flux into the domain on the Neumann portion. According to the literature (a fact that can be derived by appropriate energy arguments), we must switch to another variant of the flux on inflow parts, see Gravemeier, Comerford, Yoshihara, Ismail, Wall, "A novel formulation for Neumann inflow conditions in biomechanics", Int. J. Numer. Meth. Biomed. Eng., vol. 28 (2012). Here, the momentum term needs to be added once again, which corresponds to removing the flux contribution on the momentum variables. We do this in a post-processing step, and only for the case when we both are at an outflow boundary and the dot product between the normal vector and the momentum (or, equivalently, velocity) is negative. As we work on data of several quadrature points at once for SIMD vectorizations, we here need to explicitly loop over the array entries of the SIMD array.    


In the implementation below, we check for the various types of boundaries at the level of quadrature points. Of course, we could also have moved the decision out of the quadrature point loop and treat entire faces as of the same kind, which avoids some map/set lookups in the inner loop over quadrature points. However, the loss of efficiency is hardly noticeable, so we opt for the simpler code here. Also note that the final `else` clause will catch the case when some part of the boundary was not assigned any boundary condition via  [2.x.259]  

[1.x.126] 



The next function implements the inverse mass matrix operation. The algorithms and rationale have been discussed extensively in the introduction, so we here limit ourselves to the technicalities of the  [2.x.260]  class. It does similar operations as the forward evaluation of the mass matrix, except with a different interpolation matrix, representing the inverse  [2.x.261]  factors. These represent a change of basis from the specified basis (in this case, the Lagrange basis in the points of the Gauss--Lobatto quadrature formula) to the Lagrange basis in the points of the Gauss quadrature formula. In the latter basis, we can apply the inverse of the point-wise `JxW` factor, i.e., the quadrature weight times the determinant of the Jacobian of the mapping from reference to real coordinates. Once this is done, the basis is changed back to the nodal Gauss-Lobatto basis again. All of these operations are done by the `apply()` function below. What we need to provide is the local fields to operate on (which we extract from the global vector by an FEEvaluation object) and write the results back to the destination vector of the mass matrix operation.    


One thing to note is that we added two integer arguments (that are optional) to the constructor of FEEvaluation, the first being 0 (selecting among the DoFHandler in multi-DoFHandler systems; here, we only have one) and the second being 1 to make the quadrature formula selection. As we use the quadrature formula 0 for the over-integration of nonlinear terms, we use the formula 1 with the default  [2.x.262]  (or `fe_degree+1` in terms of the variable name) points for the mass matrix. This leads to square contributions to the mass matrix and ensures exact integration, as explained in the introduction. 

[1.x.127] 




[1.x.128]  [1.x.129] 




We now come to the function which implements the evaluation of the Euler operator as a whole, i.e.,  [2.x.263] , calling into the local evaluators presented above. The steps should be clear from the previous code. One thing to note is that we need to adjust the time in the functions we have associated with the various parts of the boundary, in order to be consistent with the equation in case the boundary data is time-dependent. Then, we call  [2.x.264]  to perform the cell and face integrals, including the necessary ghost data exchange in the `src` vector. The seventh argument to the function, `true`, specifies that we want to zero the `dst` vector as part of the loop, before we start accumulating integrals into it. This variant is preferred over explicitly calling `dst = 0.;` before the loop as the zeroing operation is done on a subrange of the vector in parts that are written by the integrals nearby. This enhances data locality and allows for caching, saving one roundtrip of vector data to main memory and enhancing performance. The last two arguments to the loop determine which data is exchanged: Since we only access the values of the shape functions one faces, typical of first-order hyperbolic problems, and since we have a nodal basis with nodes at the reference element surface, we only need to exchange those parts. This again saves precious memory bandwidth.    


Once the spatial operator  [2.x.265]  is applied, we need to make a second round and apply the inverse mass matrix. Here, we call  [2.x.266]  since only cell integrals appear. The cell loop is cheaper than the full loop as access only goes to the degrees of freedom associated with the locally owned cells, which is simply the locally owned degrees of freedom for DG discretizations. Thus, no ghost exchange is needed here.    


Around all these functions, we put timer scopes to record the computational time for statistics about the contributions of the various parts. 

[1.x.130] 



Let us move to the function that does an entire stage of a Runge--Kutta update. It calls  [2.x.267]  followed by some updates to the vectors, namely `next_ri = solution + factor_ai * k_i` and `solution += factor_solution * k_i`. Rather than performing these steps through the vector interfaces, we here present an alternative strategy that is faster on cache-based architectures. As the memory consumed by the vectors is often much larger than what fits into caches, the data has to effectively come from the slow RAM memory. The situation can be improved by loop fusion, i.e., performing both the updates to `next_ki` and `solution` within a single sweep. In that case, we would read the two vectors `rhs` and `solution` and write into `next_ki` and `solution`, compared to at least 4 reads and two writes in the baseline case. Here, we go one step further and perform the loop immediately when the mass matrix inversion has finished on a part of the vector.  [2.x.268]  provides a mechanism to attach an  [2.x.269]  both before the loop over cells first touches a vector entry (which we do not use here, but is e.g. used for zeroing the vector) and a second  [2.x.270]  to be called after the loop last touches an entry. The callback is in form of a range over the given vector (in terms of the local index numbering in the MPI universe) that can be addressed by `local_element()` functions.    


For this second callback, we create a lambda that works on a range and write the respective update on this range. Ideally, we would add the `DEAL_II_OPENMP_SIMD_PRAGMA` before the local loop to suggest to the compiler to SIMD parallelize this loop (which means in practice that we ensure that there is no overlap, also called aliasing, between the index ranges of the pointers we use inside the loops). It turns out that at the time of this writing, GCC 7.2 fails to compile an OpenMP pragma inside a lambda function, so we comment this pragma out below. If your compiler is newer, you should be able to uncomment these lines again.    


Note that we select a different code path for the last Runge--Kutta stage when we do not need to update the `next_ri` vector. This strategy gives a considerable speedup. Whereas the inverse mass matrix and vector updates take more than 60% of the computational time with default vector updates on a 40-core machine, the percentage is around 35% with the more optimized variant. In other words, this is a speedup of around a third. 

[1.x.131] 



Having discussed the implementation of the functions that deal with advancing the solution by one time step, let us now move to functions that implement other, ancillary operations. Specifically, these are functions that compute projections, evaluate errors, and compute the speed of information transport on a cell.    


The first of these functions is essentially equivalent to  [2.x.271]  just much faster because it is specialized for DG elements where there is no need to set up and solve a linear system, as each element has independent basis functions. The reason why we show the code here, besides a small speedup of this non-critical operation, is that it shows additional functionality provided by  [2.x.272]     


The projection operation works as follows: If we denote the matrix of shape functions evaluated at quadrature points by  [2.x.273] , the projection on cell  [2.x.274]  is an operation of the form  [2.x.275] , where  [2.x.276]  is the diagonal matrix containing the determinant of the Jacobian times the quadrature weight (JxW),  [2.x.277]  is the cell-wise mass matrix, and  [2.x.278]  is the evaluation of the field to be projected onto quadrature points. (In reality the matrix  [2.x.279]  has additional structure through the tensor product, as explained in the introduction.) This system can now equivalently be written as  [2.x.280] . Now, the term  [2.x.281]  and then  [2.x.282]  cancel, resulting in the final expression  [2.x.283] . This operation is implemented by  [2.x.284]  The name is derived from the fact that this projection is simply the multiplication by  [2.x.285] , a basis change from the nodal basis in the points of the Gaussian quadrature to the given finite element basis. Note that we call  [2.x.286]  to write the result into the vector, overwriting previous content, rather than accumulating the results as typical in integration tasks -- we can do this because every vector entry has contributions from only a single cell for discontinuous Galerkin discretizations. 

[1.x.132] 



The next function again repeats functionality also provided by the deal.II library, namely  [2.x.287]  We here show the explicit code to highlight how the vectorization across several cells works and how to accumulate results via that interface: Recall that each [1.x.133] of the vectorized array holds data from a different cell. By the loop over all cell batches that are owned by the current MPI process, we could then fill a VectorizedArray of results; to obtain a global sum, we would need to further go on and sum across the entries in the SIMD array. However, such a procedure is not stable as the SIMD array could in fact not hold valid data for all its lanes. This happens when the number of locally owned cells is not a multiple of the SIMD width. To avoid invalid data, we must explicitly skip those invalid lanes when accessing the data. While one could imagine that we could make it work by simply setting the empty lanes to zero (and thus, not contribute to a sum), the situation is more complicated than that: What if we were to compute a velocity out of the momentum? Then, we would need to divide by the density, which is zero -- the result would consequently be NaN and contaminate the result. This trap is avoided by accumulating the results from the valid SIMD range as we loop through the cell batches, using the function  [2.x.288]  to give us the number of lanes with valid data. It equals  [2.x.289]  on most cells, but can be less on the last cell batch if the number of cells has a remainder compared to the SIMD width. 

[1.x.134] 



This final function of the EulerOperator class is used to estimate the transport speed, scaled by the mesh size, that is relevant for setting the time step size in the explicit time integrator. In the Euler equations, there are two speeds of transport, namely the convective velocity  [2.x.290]  and the propagation of sound waves with sound speed  [2.x.291]  relative to the medium moving at velocity  [2.x.292] .    


In the formula for the time step size, we are interested not by these absolute speeds, but by the amount of time it takes for information to cross a single cell. For information transported along with the medium,  [2.x.293]  is scaled by the mesh size, so an estimate of the maximal velocity can be obtained by computing  [2.x.294] , where  [2.x.295]  is the Jacobian of the transformation from real to the reference domain. Note that  [2.x.296]  returns the inverse and transpose Jacobian, representing the metric term from real to reference coordinates, so we do not need to transpose it again. We store this limit in the variable `convective_limit` in the code below.    


The sound propagation is isotropic, so we need to take mesh sizes in any direction into account. The appropriate mesh size scaling is then given by the minimal singular value of  [2.x.297]  or, equivalently, the maximal singular value of  [2.x.298] . Note that one could approximate this quantity by the minimal distance between vertices of a cell when ignoring curved cells. To get the maximal singular value of the Jacobian, the general strategy would be some LAPACK function. Since all we need here is an estimate, we can avoid the hassle of decomposing a tensor of VectorizedArray numbers into several matrices and go into an (expensive) eigenvalue function without vectorization, and instead use a few iterations (five in the code below) of the power method applied to  [2.x.299] . The speed of convergence of this method depends on the ratio of the largest to the next largest eigenvalue and the initial guess, which is the vector of all ones. This might suggest that we get slow convergence on cells close to a cube shape where all lengths are almost the same. However, this slow convergence means that the result will sit between the two largest singular values, which both are close to the maximal value anyway. In all other cases, convergence will be quick. Thus, we can merely hardcode 5 iterations here and be confident that the result is good. 

[1.x.135] 



Similarly to the previous function, we must make sure to accumulate speed only on the valid cells of a cell batch. 

[1.x.136] 




[1.x.137]  [1.x.138] 




This class combines the EulerOperator class with the time integrator and the usual global data structures such as FiniteElement and DoFHandler, to actually run the simulations of the Euler problem.    


The member variables are a triangulation, a finite element, a mapping (to create high-order curved surfaces, see e.g.  [2.x.300] ), and a DoFHandler to describe the degrees of freedom. In addition, we keep an instance of the EulerOperator described above around, which will do all heavy lifting in terms of integrals, and some parameters for time integration like the current time or the time step size.    


Furthermore, we use a PostProcessor instance to write some additional information to the output file, in similarity to what was done in  [2.x.301] . The interface of the DataPostprocessor class is intuitive, requiring us to provide information about what needs to be evaluated (typically only the values of the solution, except for the Schlieren plot that we only enable in 2D where it makes sense), and the names of what gets evaluated. Note that it would also be possible to extract most information by calculator tools within visualization programs such as ParaView, but it is so much more convenient to do it already when writing the output. 

[1.x.139] 



For the main evaluation of the field variables, we first check that the lengths of the arrays equal the expected values (the lengths `2*dim+4` or `2*dim+5` are derived from the sizes of the names we specify in the get_names() function below). Then we loop over all evaluation points and fill the respective information: First we fill the primal solution variables of density  [2.x.302] , momentum  [2.x.303]  and energy  [2.x.304] , then we compute the derived velocity  [2.x.305] , the pressure  [2.x.306] , the speed of sound  [2.x.307] , as well as the Schlieren plot showing  [2.x.308]  in case it is enabled. (See  [2.x.309]  for another example where we create a Schlieren plot.) 

[1.x.140] 



For the interpretation of quantities, we have scalar density, energy, pressure, speed of sound, and the Schlieren plot, and vectors for the momentum and the velocity. 

[1.x.141] 



With respect to the necessary update flags, we only need the values for all quantities but the Schlieren plot, which is based on the density gradient. 

[1.x.142] 



The constructor for this class is unsurprising: We set up a parallel triangulation based on the `MPI_COMM_WORLD` communicator, a vector finite element with `dim+2` components for density, momentum, and energy, a high-order mapping of the same degree as the underlying finite element, and initialize the time and time step to zero. 

[1.x.143] 



As a mesh, this tutorial program implements two options, depending on the global variable `testcase`: For the analytical variant (`testcase==0`), the domain is  [2.x.310] , with Dirichlet boundary conditions (inflow) all around the domain. For `testcase==1`, we set the domain to a cylinder in a rectangular box, derived from the flow past cylinder testcase for incompressible viscous flow by Sch&auml;fer and Turek (1996). Here, we have a larger variety of boundaries. The inflow part at the left of the channel is given the inflow type, for which we choose a constant inflow profile, whereas we set a subsonic outflow at the right. For the boundary around the cylinder (boundary id equal to 2) as well as the channel walls (boundary id equal to 3) we use the wall boundary type, which is no-normal flow. Furthermore, for the 3D cylinder we also add a gravity force in vertical direction. Having the base mesh in place (including the manifolds set by  [2.x.311]  we can then perform the specified number of global refinements, create the unknown numbering from the DoFHandler, and hand the DoFHandler and Mapping objects to the initialization of the EulerOperator. 

[1.x.144] 



In the following, we output some statistics about the problem. Because we often end up with quite large numbers of cells or degrees of freedom, we would like to print them with a comma to separate each set of three digits. This can be done via "locales", although the way this works is not particularly intuitive.  [2.x.312]  explains this in slightly more detail. 

[1.x.145] 



For output, we first let the Euler operator compute the errors of the numerical results. More precisely, we compute the error against the analytical result for the analytical solution case, whereas we compute the deviation against the background field with constant density and energy and constant velocity in  [2.x.313]  direction for the second test case.    


The next step is to create output. This is similar to what is done in  [2.x.314] : We let the postprocessor defined above control most of the output, except for the primal field that we write directly. For the analytical solution test case, we also perform another projection of the analytical solution and print the difference between that field and the numerical solution. Once we have defined all quantities to be written, we build the patches for output. Similarly to  [2.x.315] , we create a high-order VTK output by setting the appropriate flag, which enables us to visualize fields of high polynomial degrees. Finally, we call the  [2.x.316]  function to write the result to the given file name. This function uses special MPI parallel write facilities, which are typically more optimized for parallel file systems than the standard library's  [2.x.317]  variants used in most other tutorial programs. A particularly nice feature of the `write_vtu_in_parallel()` function is the fact that it can combine output from all MPI ranks into a single file, making it unnecessary to have a central record of all such files (namely, the "pvtu" file).    


For parallel programs, it is often instructive to look at the partitioning of cells among processors. To this end, one can pass a vector of numbers to  [2.x.318]  that contains as many entries as the current processor has active cells; these numbers should then be the rank of the processor that owns each of these cells. Such a vector could, for example, be obtained from  [2.x.319]  On the other hand, on each MPI process, DataOut will only read those entries that correspond to locally owned cells, and these of course all have the same value: namely, the rank of the current process. What is in the remaining entries of the vector doesn't actually matter, and so we can just get away with a cheap trick: We just fill *all* values of the vector we give to  [2.x.320]  with the rank of the current MPI process. The key is that on each process, only the entries corresponding to the locally owned cells will be read, ignoring the (wrong) values in other entries. The fact that every process submits a vector in which the correct subset of entries is correct is all that is necessary. 

[1.x.146] 



The  [2.x.321]  function puts all pieces together. It starts off by calling the function that creates the mesh and sets up data structures, and then initializing the time integrator and the two temporary vectors of the low-storage integrator. We call these vectors `rk_register_1` and `rk_register_2`, and use the first vector to represent the quantity  [2.x.322]  and the second one for  [2.x.323]  in the formulas for the Runge--Kutta scheme outlined in the introduction. Before we start the time loop, we compute the time step size by the  [2.x.324]  function. For reasons of comparison, we compare the result obtained there with the minimal mesh size and print them to screen. For velocities and speeds of sound close to unity as in this tutorial program, the predicted effective mesh size will be close, but they could vary if scaling were different. 

[1.x.147] 



Now we are ready to start the time loop, which we run until the time has reached the desired end time. Every 5 time steps, we compute a new estimate for the time step -- since the solution is nonlinear, it is most effective to adapt the value during the course of the simulation. In case the Courant number was chosen too aggressively, the simulation will typically blow up with time step NaN, so that is easy to detect here. One thing to note is that roundoff errors might propagate to the leading digits due to an interaction of slightly different time step selections that in turn lead to slightly different solutions. To decrease this sensitivity, it is common practice to round or truncate the time step size to a few digits, e.g. 3 in this case. In case the current time is near the prescribed 'tick' value for output (e.g. 0.02), we also write the output. After the end of the time loop, we summarize the computation by printing some statistics, which is mostly done by the  [2.x.325]  function. 

[1.x.148] 



The main() function is not surprising and follows what was done in all previous MPI programs: As we run an MPI program, we need to call `MPI_Init()` and `MPI_Finalize()`, which we do through the  [2.x.326]  data structure. Note that we run the program only with MPI, and set the thread count to 1. 

[1.x.149] 

[1.x.150][1.x.151] 


[1.x.152][1.x.153] 


Running the program with the default settings on a machine with 40 processes produces the following output: 

[1.x.154] 



The program output shows that all errors are small. This is due to the fact that we use a relatively fine mesh of  [2.x.327]  cells with polynomials of degree 5 for a solution that is smooth. An interesting pattern shows for the time step size: whereas it is 0.0069 up to time 5, it increases to 0.0096 for later times. The step size increases once the vortex with some motion on top of the speed of sound (and thus faster propagation) leaves the computational domain between times 5 and 6.5. After that point, the flow is simply uniform in the same direction, and the maximum velocity of the gas is reduced compared to the previous state where the uniform velocity was overlaid by the vortex. Our time step formula recognizes this effect. 

The final block of output shows detailed information about the timing of individual parts of the programs; it breaks this down by showing the time taken by the fastest and the slowest processor, and the average time -- this is often useful in very large computations to find whether there are processors that are consistently overheated (and consequently are throttling their clock speed) or consistently slow for other reasons. The summary shows that 1283 time steps have been performed in 1.02 seconds (looking at the average time among all MPI processes), while the output of 11 files has taken additional 0.96 seconds. Broken down per time step and into the five Runge--Kutta stages, the compute time per evaluation is 0.16 milliseconds. This high performance is typical of matrix-free evaluators and a reason why explicit time integration is very competitive against implicit solvers, especially for large-scale simulations. The breakdown of computational times at the end of the program run shows that the evaluation of integrals in  [2.x.328]  contributes with around 0.92 seconds and the application of the inverse mass matrix with 0.06 seconds. Furthermore, the estimation of the transport speed for the time step size computation contributes with another 0.05 seconds of compute time. 

If we use three more levels of global refinement and 9.4 million DoFs in total, the final statistics are as follows (for the modified Lax--Friedrichs flux,  [2.x.329] , and the same system of 40 cores of dual-socket Intel Xeon Gold 6230): 

[1.x.155] 



Per time step, the solver now takes 0.02 seconds, about 25 times as long as for the small problem with 147k unknowns. Given that the problem involves 64 times as many unknowns, the increase in computing time is not surprising. Since we also do 8 times as many time steps, the compute time should in theory increase by a factor of 512. The actual increase is 205 s / 1.02 s = 202. This is because the small problem size cannot fully utilize the 40 cores due to communication overhead. This becomes clear if we look into the details of the operations done per time step. The evaluation of the differential operator  [2.x.330]  with nearest neighbor communication goes from 0.92 seconds to 127 seconds, i.e., it increases with a factor of 138. On the other hand, the cost for application of the inverse mass matrix and the vector updates, which do not need to communicate between the MPI processes at all, has increased by a factor of 1195. The increase is more than the theoretical factor of 512 because the operation is limited by the bandwidth from RAM memory for the larger size while for the smaller size, all vectors fit into the caches of the CPU. The numbers show that the mass matrix evaluation and vector update part consume almost 40% of the time spent by the Runge--Kutta stages -- despite using a low-storage Runge--Kutta integrator and merging of vector operations! And despite using over-integration for the  [2.x.331]  operator. For simpler differential operators and more expensive time integrators, the proportion spent in the mass matrix and vector update part can also reach 70%. If we compute a throughput number in terms of DoFs processed per second and Runge--Kutta stage, we obtain [1.x.156] This throughput number is very high, given that simply copying one vector to another one runs at only around 10,000 MDoFs/s. 

If we go to the next-larger size with 37.7 million DoFs, the overall simulation time is 2196 seconds, with 1978 seconds spent in the time stepping. The increase in run time is a factor of 9.3 for the L_h operator (1179 versus 127 seconds) and a factor of 10.3 for the inverse mass matrix and vector updates (797 vs 77.5 seconds). The reason for this non-optimal increase in run time can be traced back to cache effects on the given hardware (with 40 MB of L2 cache and 55 MB of L3 cache): While not all of the relevant data fits into caches for 9.4 million DoFs (one vector takes 75 MB and we have three vectors plus some additional data in MatrixFree), there is capacity for one and a half vector nonetheless. Given that modern caches are more sophisticated than the naive least-recently-used strategy (where we would have little re-use as the data is used in a streaming-like fashion), we can assume that a sizeable fraction of data can indeed be delivered from caches for the 9.4 million DoFs case. For the larger case, even with optimal caching less than 10 percent of data would fit into caches, with an associated loss in performance. 


[1.x.157][1.x.158] 


For the modified Lax--Friedrichs flux and measuring the error in the momentum variable, we obtain the following convergence table (the rates are very similar for the density and energy variables): 

 [2.x.332]  

If we switch to the Harten-Lax-van Leer flux, the results are as follows:  [2.x.333]  

The tables show that we get optimal  [2.x.334]  convergence rates for both numerical fluxes. The errors are slightly smaller for the Lax--Friedrichs flux for  [2.x.335] , but the picture is reversed for  [2.x.336] ; in any case, the differences on this testcase are relatively small. 

For  [2.x.337] , we reach the roundoff accuracy of  [2.x.338]  with both fluxes on the finest grids. Also note that the errors are absolute with a domain length of  [2.x.339] , so relative errors are below  [2.x.340] . The HLL flux is somewhat better for the highest degree, which is due to a slight inaccuracy of the Lax--Friedrichs flux: The Lax--Friedrichs flux sets a Dirichlet condition on the solution that leaves the domain, which results in a small artificial reflection, which is accentuated for the Lax--Friedrichs flux. Apart from that, we see that the influence of the numerical flux is minor, as the polynomial part inside elements is the main driver of the accucary. The limited influence of the flux also has consequences when trying to approach more challenging setups with the higher-order DG setup: Taking for example the parameters and grid of  [2.x.341] , we get oscillations (which in turn make density negative and make the solution explode) with both fluxes once the high-mass part comes near the boundary, as opposed to the low-order finite volume case ( [2.x.342] ). Thus, any case that leads to shocks in the solution necessitates some form of limiting or artificial dissipation. For another alternative, see the  [2.x.343]  tutorial program. 


[1.x.165][1.x.166] 


For the test case of the flow around a cylinder in a channel, we need to change the first code line to 

[1.x.167] 

This test case starts with a background field of a constant velocity of Mach number 0.31 and a constant initial density; the flow will have to go around an obstacle in the form of a cylinder. Since we impose a no-penetration condition on the cylinder walls, the flow that initially impinges head-on onto to cylinder has to rearrange, which creates a big sound wave. The following pictures show the pressure at times 0.1, 0.25, 0.5, and 1.0 (top left to bottom right) for the 2D case with 5 levels of global refinement, using 102,400 cells with polynomial degree of 5 and 14.7 million degrees of freedom over all 4 solution variables. We clearly see the discontinuity that propagates slowly in the upstream direction and more quickly in downstream direction in the first snapshot at time 0.1. At time 0.25, the sound wave has reached the top and bottom walls and reflected back to the interior. From the different distances of the reflected waves from lower and upper walls we can see the slight asymmetry of the Sch&auml;fer-Turek test case represented by  [2.x.344]  with somewhat more space above the cylinder compared to below. At later times, the picture is more chaotic with many sound waves all over the place. 

 [2.x.345]  

The next picture shows an elevation plot of the pressure at time 1.0 looking from the channel inlet towards the outlet at the same resolution -- here, we can see the large number of reflections. In the figure, two types of waves are visible. The larger-amplitude waves correspond to various reflections that happened as the initial discontinuity hit the walls, whereas the small-amplitude waves of size similar to the elements correspond to numerical artifacts. They have their origin in the finite resolution of the scheme and appear as the discontinuity travels through elements with high-order polynomials. This effect can be cured by increasing resolution. Apart from this effect, the rich wave structure is the result of the transport accuracy of the high-order DG method. 

 [2.x.346]  

With 2 levels of global refinement with 1,600 cells, the mesh and its partitioning on 40 MPI processes looks as follows: 

 [2.x.347]  

When we run the code with 4 levels of global refinements on 40 cores, we get the following output: 

[1.x.168] 



The norms shown here for the various quantities are the deviations  [2.x.348] ,  [2.x.349] , and  [2.x.350]  against the background field (namely, the initial condition). The distribution of run time is overall similar as in the previous test case. The only slight difference is the larger proportion of time spent in  [2.x.351]  as compared to the inverse mass matrix and vector updates. This is because the geometry is deformed and the matrix-free framework needs to load additional arrays for the geometry from memory that are compressed in the affine mesh case. 

Increasing the number of global refinements to 5, the output becomes: 

[1.x.169] 



The effect on performance is similar to the analytical test case -- in theory, computation times should increase by a factor of 8, but we actually see an increase by a factor of 11 for the time steps (219.5 seconds versus 2450 seconds). This can be traced back to caches, with the small case mostly fitting in caches. An interesting effect, typical of programs with a mix of local communication (integrals  [2.x.352] ) and global communication (computation of transport speed) with some load imbalance, can be observed by looking at the MPI ranks that encounter the minimal and maximal time of different phases, respectively. Rank 0 reports the fastest throughput for the "rk time stepping total" part. At the same time, it appears to be slowest for the "compute transport speed" part, almost a factor of 2 slower than the average and almost a factor of 4 compared to the faster rank. Since the latter involves global communication, we can attribute the slowness in this part to the fact that the local Runge--Kutta stages have advanced more quickly on this rank and need to wait until the other processors catch up. At this point, one can wonder about the reason for this imbalance: The number of cells is almost the same on all MPI processes. However, the matrix-free framework is faster on affine and Cartesian cells located towards the outlet of the channel, to which the lower MPI ranks are assigned. On the other hand, rank 32, which reports the highest run time for the Runga--Kutta stages, owns the curved cells near the cylinder, for which no data compression is possible. To improve throughput, we could assign different weights to different cell types when partitioning the  [2.x.353]  object, or even measure the run time for a few time steps and try to rebalance then. 

The throughput per Runge--Kutta stage can be computed to 2085 MDoFs/s for the 14.7 million DoFs test case over the 346,000 Runge--Kutta stages, slightly slower than the Cartesian mesh throughput of 2360 MDoFs/s reported above. 

Finally, if we add one additional refinement, we record the following output: 

[1.x.170] 



The "rk time stepping total" part corresponds to a throughput of 2010 MDoFs/s. The overall run time to perform 139k time steps is 20k seconds (5.7 hours) or 7 time steps per second -- not so bad for having nearly 60 million unknowns. More throughput can be achieved by adding more cores to the computation. 


[1.x.171][1.x.172] 


Switching the channel test case to 3D with 3 global refinements, the output is 

[1.x.173] 



The physics are similar to the 2D case, with a slight motion in the z direction due to the gravitational force. The throughput per Runge--Kutta stage in this case is [1.x.174] 

The throughput is lower than in 2D because the computation of the  [2.x.354]  term is more expensive. This is due to over-integration with `degree+2` points and the larger fraction of face integrals (worse volume-to-surface ratio) with more expensive flux computations. If we only consider the inverse mass matrix and vector update part, we record a throughput of 4857 MDoFs/s for the 2D case of the isentropic vortex with 37.7 million unknowns, whereas the 3D case runs with 4535 MDoFs/s. The performance is similar because both cases are in fact limited by the memory bandwidth. 

If we go to four levels of global refinement, we need to increase the number of processes to fit everything in memory -- the computation needs around 350 GB of RAM memory in this case. Also, the time it takes to complete 35k time steps becomes more tolerable by adding additional resources. We therefore use 6 nodes with 40 cores each, resulting in a computation with 240 MPI processes: 

[1.x.175] 

This simulation had nearly 2 billion unknowns -- quite a large computation indeed, and still only needed around 1.5 seconds per time step. 


[1.x.176][1.x.177] 


The code presented here straight-forwardly extends to adaptive meshes, given appropriate indicators for setting the refinement flags. Large-scale adaptivity of a similar solver in the context of the acoustic wave equation has been achieved by the [1.x.178]. However, in the present context, the benefits of adaptivity are often limited to early times and effects close to the origin of sound waves, as the waves eventually reflect and diffract. This leads to steep gradients all over the place, similar to turbulent flow, and a more or less globally refined mesh. 

Another topic that we did not discuss in the results section is a comparison of different time integration schemes. The program provides four variants of low-storage Runga--Kutta integrators that each have slightly different accuracy and stability behavior. Among the schemes implemented here, the higher-order ones provide additional accuracy but come with slightly lower efficiency in terms of step size per stage before they violate the CFL condition. An interesting extension would be to compare the low-storage variants proposed here with standard Runge--Kutta integrators or to use vector operations that are run separate from the mass matrix operation and compare performance. 


[1.x.179][1.x.180] 


As mentioned in the introduction, the modified Lax--Friedrichs flux and the HLL flux employed in this program are only two variants of a large body of numerical fluxes available in the literature on the Euler equations. One example is the HLLC flux (Harten-Lax-van Leer-Contact) flux which adds the effect of rarefaction waves missing in the HLL flux, or the Roe flux. As mentioned in the introduction, the effect of numerical fluxes on high-order DG schemes is debatable (unlike for the case of low-order discretizations). 

A related improvement to increase the stability of the solver is to also consider the spatial integral terms. A shortcoming in the rather naive implementation used above is the fact that the energy conservation of the original Euler equations (in the absence of shocks) only holds up to a discretization error. If the solution is under-resolved, the discretization error can give rise to an increase in the numerical energy and eventually render the discretization unstable. This is because of the inexact numerical integration of the terms in the Euler equations, which both contain rational nonlinearities and higher-degree content from curved cells. A way out of this dilemma are so-called skew-symmetric formulations, see  [2.x.355]  for a simple variant. Skew symmetry means that switching the role of the solution  [2.x.356]  and test functions  [2.x.357]  in the weak form produces the exact negative of the original quantity, apart from some boundary terms. In the discrete setting, the challenge is to keep this skew symmetry also when the integrals are only computed approximately (in the continuous case, skew-symmetry is a consequence of integration by parts). Skew-symmetric numerical schemes balance spatial derivatives in the conservative form  [2.x.358]  with contributions in the convective form  [2.x.359]  for some  [2.x.360] . The precise terms depend on the equation and the integration formula, and can in some cases by understood by special skew-symmetric finite difference schemes. 

To get started, interested readers could take a look at https://github.com/kronbichler/advection_miniapp, where a skew-symmetric DG formulation is implemented with deal.II for a simple advection equation. 

[1.x.181][1.x.182] 


As mentioned in the introduction, the solution to the Euler equations develops shocks as the Mach number increases, which require additional mechanisms to stabilize the scheme, e.g. in the form of limiters. The main challenge besides actually implementing the limiter or artificial viscosity approach would be to load-balance the computations, as the additional computations involved for limiting the oscillations in troubled cells would make them more expensive than the plain DG cells without limiting. Furthermore, additional numerical fluxes that better cope with the discontinuities would also be an option. 

One ingredient also necessary for supersonic flows are appropriate boundary conditions. As opposed to the subsonic outflow boundaries discussed in the introduction and implemented in the program, all characteristics are outgoing for supersonic outflow boundaries, so we do not want to prescribe any external data, [1.x.183] 

In the code, we would simply add the additional statement 

[1.x.184] 

in the `local_apply_boundary_face()` function. 

[1.x.185][1.x.186] 


When the interest with an Euler solution is mostly in the propagation of sound waves, it often makes sense to linearize the Euler equations around a background state, i.e., a given density, velocity and energy (or pressure) field, and only compute the change against these fields. This is the setting of the wide field of aeroacoustics. Even though the resolution requirements are sometimes considerably reduced, implementation gets somewhat more complicated as the linearization gives rise to additional terms. From a code perspective, in the operator evaluation we also need to equip the code with the state to linearize against. This information can be provided either by analytical functions (that are evaluated in terms of the position of the quadrature points) or by a vector similar to the solution. Based on that vector, we would create an additional FEEvaluation object to read from it and provide the values of the field at quadrature points. If the background velocity is zero and the density is constant, the linearized Euler equations further simplify and can equivalently be written in the form of the acoustic wave equation. 

A challenge in the context of sound propagation is often the definition of boundary conditions, as the computational domain needs to be of finite size, whereas the actual simulation often spans an infinite (or at least much larger) physical domain. Conventional Dirichlet or Neumann boundary conditions give rise to reflections of the sound waves that eventually propagate back to the region of interest and spoil the solution. Therefore, various variants of non-reflecting boundary conditions or sponge layers, often in the form of [1.x.187] -- where the solution is damped without reflection 

-- are common. 


[1.x.188][1.x.189] 


The solver presented in this tutorial program can also be extended to the compressible Navier--Stokes equations by adding viscous terms, as described in  [2.x.361] . To keep as much of the performance obtained here despite the additional cost of elliptic terms, e.g. via an interior penalty method, one can switch the basis from FE_DGQ to FE_DGQHermite like in the  [2.x.362]  tutorial program. [1.x.190] [1.x.191]  [2.x.363]  

 [2.x.364] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28] 

 [2.x.3]  

[1.x.29] 

[1.x.30][1.x.31] 


[1.x.32][1.x.33] 


Particles play an important part in numerical models for a large  number of applications. Particles are routinely used  as massless tracers to visualize the dynamic of a transient flow. They  can also play an intrinsic role as part of a more complex finite element  model, as is the case for the Particle-In-Cell (PIC) method  [2.x.4]   or they can even be used to simulate the motion of granular matter, as in  the Discrete Element Method (DEM)  [2.x.5] . In the case  of DEM, the resulting model is not related to the finite element method anymore,  but just leads to a system of ordinary differential equation which describes  the motion of the particles and the dynamic of their collisions. All of  these models can be built using deal.II's particle handling capabilities. 

In the present step, we use particles as massless tracers to illustrate the dynamic of a vortical flow. Since the particles are massless tracers, the position of each particle  [2.x.6]  is described by the following ordinary differential equation (ODE): [1.x.34] 

where  [2.x.7]  is the position of particle  [2.x.8]  and  [2.x.9]  the flow velocity at its position. In the present step, this ODE is solved using the explicit Euler method. The resulting scheme is: [1.x.35] 

where  [2.x.10]  and  [2.x.11]  are the position of particle  [2.x.12]  at time  [2.x.13]  and  [2.x.14] , respectively and where  [2.x.15]  is the time step. In the present step, the velocity at the location of particles is obtained in two different fashions: 

- By evaluating the velocity function at the location of the particles; 

- By evaluating the velocity function on a background triangulation and, using a  finite element support, interpolating at the position of the particle. 

The first approach is not practical, since the velocity profile is generally not known analytically. The second approach, based on interpolating a solution at the position of the particles, mimics exactly what would be done in a realistic computational fluid dynamic simulation, and this follows the way we have also evaluated the finite element solution at particle locations in  [2.x.16] . In this step, we illustrate both strategies. 

We note that much greater accuracy could be obtained by using a fourth order Runge-Kutta method or another appropriate scheme for the time integration of the motion of the particles.  Implementing a more advanced time-integration scheme would be a straightforward extension of this step. 

[1.x.36][1.x.37] 


In deal.II,  [2.x.17]  are very simple and flexible entities that can be used to build PIC, DEM or any type of particle-based models. Particles have a location in real space, a location in the reference space of the element in which they are located and a unique ID. In the majority of cases, simulations that include particles require a significant number of them. Thus, it becomes interesting to handle all particles through an entity which agglomerates all particles. In deal.II, this is achieved through the use of the  [2.x.18]  class. 

By default, particles do not have a diameter, a mass or any other physical properties which we would generally expect of physical particles. However, through a ParticleHandler, particles have access to a  [2.x.19]  This PropertyPool is an array which can be used to store an arbitrary number of properties associated with the particles. Consequently, users can build their own particle solver and attribute the desired properties to the particles (e.g., mass, charge, diameter, temperature, etc.). In the present tutorial, this is used to store the value of the fluid velocity and the process id to which the particles belong. 

[1.x.38][1.x.39] 


Although the present step is not computationally intensive, simulations that include many particles can be computationally demanding and require parallelization. The present step showcases the distributed parallel capabilities of deal.II for particles. In general, there are three main challenges that specifically arise in parallel distributed simulations that include particles: 

- Generating the particles on the distributed triangulation; 

- Exchanging the particles that leave local domains between the processors; 

- Load balancing the simulation so that every processor has a similar computational load. These challenges and their solution in deal.II have been discussed in more detail in  [2.x.20] , but we will summarize them below. 

There are of course also questions on simply setting up a code that uses particles. These have largely already been addressed in  [2.x.21] . Some more advanced techniques will also be discussed in  [2.x.22] . 

[1.x.40][1.x.41] 


Generating distributed particles in a scalable way is not straightforward since the processor to which they belong must first be identified before the cell in which they are located is found.  deal.II provides numerous capabilities to generate particles through the  [2.x.23]  namespace.  Some of these particle generators create particles only on the locally owned subdomain. For example,  [2.x.24]  creates particles at the same reference locations within each cell of the local subdomain and  [2.x.25]  uses a globally defined probability density function to determine how many and where to generate particles locally. 

In other situations, such as the present step, particles must be generated at specific locations on cells that may be owned only by a subset of the processors. In  most of these situations, the insertion of the particles is done for a very limited number of time-steps and, consequently, does not constitute a large portion of the computational cost. For these occasions, deal.II provides convenient  [2.x.26]  that can globally insert the particles even if the particle is not located in a cell owned by the parallel process on which the call to create the particle is initiated. The generators first locate on which subdomain the particles are situated, identify in which cell they are located and exchange the necessary information among the processors to ensure that the particle is generated with the right properties. Consequently, this type of particle generation can be communication intensive. The  [2.x.27]  and the  [2.x.28]  generate particles using a triangulation and the points of an associated DoFHandler or quadrature respectively. The triangulation that is used to generate the particles can be the same triangulation that is used for the background mesh, in which case these functions are very similar to the  [2.x.29]  function described in the previous paragraph. However, the triangulation used to generate particles can also be different (non-matching) from the triangulation of the background grid, which is useful to generate particles in particular shapes (as in this example), or to transfer information between two different computational grids (as in  [2.x.30] ).  Furthermore, the  [2.x.31]  class provides the  [2.x.32]  function which enables the global insertion of particles from a vector of arbitrary points and a global vector of bounding boxes. In the present step, we use the  [2.x.33]  function on a non-matching triangulation to insert particles located at positions in the shape of a disk. 

[1.x.42][1.x.43] 


As particles move around in parallel distributed computations they may leave the locally owned subdomain and need to be transferred to their new owner processes. This situation can arise in two very different ways: First, if the previous owning process knows the new owner of the particles that were lost (for example because the particles moved from the locally owned cell of one processor into an adjacent ghost cells of a distributed triangulation) then the transfer can be handled efficiently as a point-to-point communication between each process and the new owners. This transfer happens automatically whenever particles are sorted into their new cells. Secondly, the previous owner may not know to which process the particle has moved. In this case the particle is discarded by default, as a global search for the owner can be expensive.  [2.x.34]  shows how such a discarded particle can still be collected, interpreted, and potentially reinserted by the user. In the present example we prevent the second case by imposing a CFL criterion on the timestep to ensure particles will at most move into the ghost layer of the local process and can therefore be send to neighboring processes automatically. 

[1.x.44][1.x.45] 


The last challenge that arises in parallel distributed computations using particles is to balance the computational load between work that is done on the grid, for example solving the finite-element problem, and the work that is done on the particles, for example advecting the particles or computing the forces between particles or between particles and grid. By default, for example in  [2.x.35] , deal.II distributes the background mesh as evenly as possible between the available processes, that is it balances the number of cells on each process. However, if some cells own many more particles than other cells, or if the particles of one cell are much more computationally expensive than the particles in other cells, then this problem no longer scales efficiently (for a discussion of what we consider "scalable" programs, see  [2.x.36]  "this glossary entry"). Thus, we have to apply a form of "load balancing", which means we estimate the computational load that is associated with each cell and its particles. Repartitioning the mesh then accounts for this combined computational load instead of the simplified assumption of the number of cells  [2.x.37] . 

In this section we only discussed the particle-specific challenges in distributed computation. Parallel challenges that particles share with finite-element solutions (parallel output, data transfer during mesh refinement) can be addressed with the solutions found for finite-element problems already discussed in other examples. 

[1.x.46][1.x.47] 


In the present step, we use particles as massless tracers to illustrate the dynamics of a particular vortical flow: the Rayleigh--Kothe vortex. This flow pattern is generally used as a complex test case for interface tracking methods (e.g., volume-of-fluid and level set approaches) since it leads to strong rotation and elongation of the fluid  [2.x.38] . 

The stream function  [2.x.39]  of this Rayleigh-Kothe vortex is defined as: 

[1.x.48] where  [2.x.40]  is half the period of the flow. The velocity profile in 2D ( [2.x.41] ) is : [1.x.49] 



The velocity profile is illustrated in the following animation: 

[1.x.50] 



It can be seen that this velocity reverses periodically due to the term  [2.x.42]  and that material will end up at its starting position after every period of length  [2.x.43] . We will run this tutorial program for exactly one period and compare the final particle location to the initial location to illustrate this flow property. This example uses the testcase to produce two models that handle the particles slightly differently. The first model prescribes the exact analytical velocity solution as the velocity for each particle. Therefore in this model there is no error in the assigned velocity to the particles, and any deviation of particle positions from the analytical position at a given time results from the error in solving the equation of motion for the particle inexactly, using a time stepping method. In the second model the analytical velocity field is first interpolated to a finite-element vector space (to simulate the case that the velocity was obtained from solving a finite-element problem, in the same way as the ODE for each particle in  [2.x.44]  depends on a finite element solution). This finite-element "solution" is then evaluated at the locations of the particles to solve their equation of motion. The difference between the two cases allows to assess whether the chosen finite-element space is sufficiently accurate to advect the particles with the optimal convergence rate of the chosen particle advection scheme, a question that is important in practice to determine the accuracy of the combined algorithm (see e.g.  [2.x.45] ). [1.x.51] [1.x.52] 


[1.x.53]  [1.x.54] 







[1.x.55] 



From the following include file we import the ParticleHandler class that allows you to manage a collection of particles (objects of type  [2.x.46]  representing a collection of points with some attached properties (e.g., an id) floating on a  [2.x.47]  The methods and classes in the namespace Particles allows one to easily implement Particle-In-Cell methods and particle tracing on distributed triangulations: 

[1.x.56] 



We import the particles generator which allow us to insert the particles. In the present step, the particle are globally inserted using a non-matching hyper-shell triangulation: 

[1.x.57] 



Since the particles do not form a triangulation, they have their own specific DataOut class which will enable us to write them to commonly used parallel vtu format (or any number of other file formats): 

[1.x.58] 




[1.x.59]  [1.x.60] 




Similarly to what is done in  [2.x.48] , we set up a class that holds all the parameters of our problem and derive it from the ParameterAcceptor class to simplify the management and creation of parameter files.    


The ParameterAcceptor paradigm requires all parameters to be writable by the ParameterAcceptor methods. In order to avoid bugs that would be very difficult to track down (such as writing things like `if (time = 0)` instead of `if(time == 0)`), we declare all the parameters in an external class, which is initialized before the actual `ParticleTracking` class, and pass it to the main class as a `const` reference.    


The constructor of the class is responsible for the connection between the members of this class and the corresponding entries in the ParameterHandler. Thanks to the use of the  [2.x.49]  method, this connection is trivial, but requires all members of this class to be writable. 

[1.x.61] 



This class consists largely of member variables that describe the details of the particle tracking simulation and its discretization. The following parameters are about where output should written to, the spatial discretization of the velocity (the default is  [2.x.50] ), the time step and the output frequency (how many time steps should elapse before we generate graphical output again): 

[1.x.62] 



We allow every grid to be refined independently. In this tutorial, no physics is resolved on the fluid grid, and its velocity is calculated analytically. 

[1.x.63] 



There remains the task of declaring what run-time parameters we can accept in input files. Since we have a very limited number of parameters, all parameters are declared in the same section. 

[1.x.64] 




[1.x.65]  [1.x.66] 




The velocity profile is provided as a Function object. This function is hard-coded within the example. 

[1.x.67] 



The velocity profile for the Rayleigh-Kothe vertex is time-dependent. Consequently, the current time in the simulation (t) must be gathered from the Function object. 

[1.x.68] 




[1.x.69]  [1.x.70] 




We are now ready to introduce the main class of our tutorial program. 

[1.x.71] 



This function is responsible for the initial generation of the particles on top of the background grid. 

[1.x.72] 



When the velocity profile is interpolated to the position of the particles, it must first be stored using degrees of freedom. Consequently, as is the case for other parallel case (e.g.  [2.x.51] ) we initialize the degrees of freedom on the background grid. 

[1.x.73] 



In one of the test cases, the function is mapped to the background grid and a finite element interpolation is used to calculate the velocity at the particle location. This function calculates the value of the function at the support point of the triangulation. 

[1.x.74] 



The next two functions are responsible for carrying out step of explicit Euler time integration for the cases where the velocity field is interpolated at the positions of the particles or calculated analytically, respectively. 

[1.x.75] 



The `cell_weight()` function indicates to the triangulation how much computational work is expected to happen on this cell, and consequently how the domain needs to be partitioned so that every MPI rank receives a roughly equal amount of work (potentially not an equal number of cells). While the function is called from the outside, it is connected to the corresponding signal from inside this class, therefore it can be `private`. 

[1.x.76] 



The following two functions are responsible for outputting the simulation results for the particles and for the velocity profile on the background mesh, respectively. 

[1.x.77] 



The private members of this class are similar to other parallel deal.II examples. The parameters are stored as a `const` member. It is important to note that we keep the `Vortex` class as a member since its time must be modified as the simulation proceeds. 







[1.x.78] 




[1.x.79]  [1.x.80] 





[1.x.81]  [1.x.82] 




The constructors and destructors are rather trivial. They are very similar to what is done in  [2.x.52] . We set the processors we want to work on to all machines available (`MPI_COMM_WORLD`) and initialize the  [2.x.53]  variable to only allow processor zero to output anything to the standard output. 







[1.x.83] 




[1.x.84]  [1.x.85] 




This function is the key component that allow us to dynamically balance the computational load for this example. The function attributes a weight to every cell that represents the computational work on this cell. Here the majority of work is expected to happen on the particles, therefore the return value of this function (representing "work for this cell") is calculated based on the number of particles in the current cell. The function is connected to the cell_weight() signal inside the triangulation, and will be called once per cell, whenever the triangulation repartitions the domain between ranks (the connection is created inside the generate_particles() function of this class). 

[1.x.86] 



We do not assign any weight to cells we do not own (i.e., artificial or ghost cells) 

[1.x.87] 



This determines how important particle work is compared to cell work (by default every cell has a weight of 1000). We set the weight per particle much higher to indicate that the particle load is the only one that is important to distribute the cells in this example. The optimal value of this number depends on the application and can range from 0 (cheap particle operations, expensive cell operations) to much larger than 1000 (expensive particle operations, cheap cell operations, like presumed in this example). 

[1.x.88] 



This example does not use adaptive refinement, therefore every cell should have the status `CELL_PERSIST`. However this function can also be used to distribute load during refinement, therefore we consider refined or coarsened cells as well. 

[1.x.89] 




[1.x.90]  [1.x.91] 




This function generates the tracer particles and the background triangulation on which these particles evolve. 

[1.x.92] 



We create a hyper cube triangulation which we globally refine. This triangulation covers the full trajectory of the particles. 

[1.x.93] 



In order to consider the particles when repartitioning the triangulation the algorithm needs to know three things:      


1. How much weight to assign to each cell (how many particles are in there); 2. How to pack the particles before shipping data around; 3. How to unpack the particles after repartitioning.      


We attach the correct functions to the signals inside  [2.x.54]  These signal will be called every time the repartition() function is called. These connections only need to be created once, so we might as well have set them up in the constructor of this class, but for the purpose of this example we want to group the particle related instructions. 

[1.x.94] 



This initializes the background triangulation where the particles are living and the number of properties of the particles. 

[1.x.95] 



We create a particle triangulation which is solely used to generate the points which will be used to insert the particles. This triangulation is a hyper shell which is offset from the center of the simulation domain. This will be used to generate a disk filled with particles which will allow an easy monitoring of the motion due to the vortex. 

[1.x.96] 



We generate the necessary bounding boxes for the particles generator. These bounding boxes are required to quickly identify in which process's subdomain the inserted particle lies, and which cell owns it. 

[1.x.97] 



We generate an empty vector of properties. We will attribute the properties to the particles once they are generated. 

[1.x.98] 



We generate the particles at the position of a single point quadrature. Consequently, one particle will be generated at the centroid of each cell. 

[1.x.99] 




[1.x.100]  [1.x.101] 




This function sets up the background degrees of freedom used for the velocity interpolation and allocates the field vector where the entire solution of the velocity field is stored. 

[1.x.102] 



This function takes care of interpolating the vortex velocity field to the field vector. This is achieved rather easily by using the  [2.x.55]  function. 

[1.x.103] 




[1.x.104]  [1.x.105] 




We integrate the particle trajectories using an analytically defined velocity field. This demonstrates a relatively trivial usage of the particles. 

[1.x.106] 



Looping over all particles in the domain using a particle iterator 

[1.x.107] 



We calculate the velocity of the particles using their current location. 

[1.x.108] 



This updates the position of the particles and sets the old position equal to the new position of the particle. 

[1.x.109] 



We store the processor id (a scalar) and the particle velocity (a vector) in the particle properties. In this example, this is done purely for visualization purposes. 

[1.x.110] 



In contrast to the previous function in this function we integrate the particle trajectories by interpolating the value of the velocity field at the degrees of freedom to the position of the particles. 

[1.x.111] 



We loop over all the local particles. Although this could be achieved directly by looping over all the cells, this would force us to loop over numerous cells which do not contain particles. Rather, we loop over all the particles, but, we get the reference of the cell in which the particle lies and then loop over all particles within that cell. This enables us to gather the values of the velocity out of the `velocity_field` vector once and use them for all particles that lie within the cell. 

[1.x.112] 



Next, compute the velocity at the particle locations by evaluating the finite element solution at the position of the particles. This is essentially an optimized version of the particle advection functionality in step 19, but instead of creating quadrature objects and FEValues objects for each cell, we do the evaluation by hand, which is somewhat more efficient and only matters for this tutorial, because the particle work is the dominant cost of the whole program. 

[1.x.113] 



Again, we store the particle velocity and the processor id in the particle properties for visualization purposes. 

[1.x.114] 




[1.x.115]  [1.x.116] 




The next two functions take care of writing both the particles and the background mesh to vtu with a pvtu record. This ensures that the simulation results can be visualized when the simulation is launched in parallel. 

[1.x.117] 



Attach the solution data to data_out object 

[1.x.118] 




[1.x.119]  [1.x.120] This function orchestrates the entire simulation. It is very similar to the other time dependent tutorial programs -- take  [2.x.56]  or  [2.x.57]  as an example. Note that we use the DiscreteTime class to monitor the time, the time-step and the  [2.x.58] number. This function is relatively straightforward. 







[1.x.121] 



We set the initial property of the particles by doing an explicit Euler iteration with a time-step of 0 both in the case of the analytical and the interpolated approach. 

[1.x.122] 



The particles are advected by looping over time. 

[1.x.123] 



After the particles have been moved, it is necessary to identify in which cell they now reside. This is achieved by calling  [2.x.59]  

[1.x.124] 




[1.x.125]  [1.x.126] 




The remainder of the code, the `main()` function, is standard. We note that we run the particle tracking with the analytical velocity and the interpolated velocity and produce both results 

[1.x.127] 

[1.x.128][1.x.129] 


The directory in which this program is run contains an example parameter file by default. If you do not specify a parameter file as an argument on the command line, the program will try to read the file "parameters.prm" by default, and will execute the code. 

On any number of cores, the simulation output will look like: 

[1.x.130] 



We note that, by default, the simulation runs the particle tracking with an analytical velocity for 2000 iterations, then restarts from the beginning and runs the particle tracking with velocity interpolation for the same duration. The results are written every 10th iteration. 

[1.x.131][1.x.132] 


The following animation displays the trajectory of the particles as they are advected by the flow field. We see that after the complete duration of the flow, the particle go back to their initial configuration as is expected. 

[1.x.133] 



[1.x.134][1.x.135] 


The following animation shows the impact of dynamic load balancing. We clearly see that the subdomains adapt themselves to balance the number of particles per subdomain. However, a perfect load balancing is not reached, in part due to the coarseness of the background mesh. 

[1.x.136] 




[1.x.137][1.x.138] 


This program highlights some of the main capabilities for handling particles in deal.II, notably their capacity to be used in distributed parallel simulations. However, this step could be extended in numerous manners: 

- High-order time integration (for example using a Runge-Kutta 4 method) could be used to increase the accuracy or allow for larger time-step sizes with the same accuracy. 

- The full equation of motion (with inertia) could be solved for the particles. In this case the particles would need to have additional properties such as their mass, as in  [2.x.60] , and if one wanted to also consider interactions with the fluid, their diameter. 

- Coupling to a flow solver. This step could be straightforwardly coupled to any parallel program in which the Stokes ( [2.x.61] ,  [2.x.62] ) or the Navier-Stokes equations are solved (e.g.,  [2.x.63] ). 

- Computing the difference in final particle positions between the two models would allow to quantify the influence of the interpolation error on particle motion. [1.x.139] [1.x.140]  [2.x.64]  

 [2.x.65] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28][1.x.29][1.x.30][1.x.31] 

[1.x.32] 

 [2.x.4]  [2.x.5] Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This document describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. [2.x.6]  

 [2.x.7]  This tutorial step implements a first-order accurate [1.x.33] based on a first-order [1.x.34] for solving Euler's equations of gas dynamics  [2.x.8] . As such it is presented primarily for educational purposes. For actual research computations you might want to consider exploring a corresponding [1.x.35] that uses [1.x.36] techniques, and strong stability-preserving (SSP) time integration, see  [2.x.9]  ([1.x.37]). 

 [2.x.10]  

[1.x.38] [1.x.39][1.x.40] 


This tutorial presents a first-order scheme for solving compressible Euler's equations that is based on three ingredients: a [1.x.41]-type discretization of Euler's equations in the context of finite elements; a graph-viscosity stabilization based on a [1.x.42] upper bound of the local wave speed; and explicit time-stepping. As such, the ideas and techniques presented in this tutorial step are drastically different from those used in  [2.x.11] , which focuses on the use of automatic differentiation. From a programming perspective this tutorial will focus on a number of techniques found in large-scale computations: hybrid thread-MPI parallelization; efficient local numbering of degrees of freedom; concurrent post-processing and write-out of results using worker threads; as well as checkpointing and restart. 

It should be noted that first-order schemes in the context of hyperbolic conservation laws require prohibitively many degrees of freedom to resolve certain key features of the simulated fluid, and thus, typically only serve as elementary building blocks in higher-order schemes  [2.x.12] . However, we hope that the reader still finds the tutorial step to be a good starting point (in particular with respect to the programming techniques) before jumping into full research codes such as the second-order scheme discussed in  [2.x.13] . 


[1.x.43] [1.x.44][1.x.45] 


The compressible Euler's equations of gas dynamics are written in conservative form as follows: 

[1.x.46] 

where  [2.x.14] , and  [2.x.15] , and  [2.x.16]  is the space dimension. We say that  [2.x.17]  is the state and  [2.x.18]  is the flux of the system. In the case of Euler's equations the state is given by  [2.x.19] : where  [2.x.20]  denotes the density,  [2.x.21]  is the momentum, and  [2.x.22]  is the total energy of the system. The flux of the system  [2.x.23]  is defined as 

[1.x.47] 

where  [2.x.24]  is the identity matrix and  [2.x.25]  denotes the tensor product. Here, we have introduced the pressure  [2.x.26]  that, in general, is defined by a closed-form equation of state. In this tutorial we limit the discussion to the class of polytropic ideal gases for which the pressure is given by 

[1.x.48] 

where the factor  [2.x.27]  denotes the [1.x.49]. 


[1.x.50][1.x.51] 


Hyperbolic conservation laws, such as 

[1.x.52] 

pose a significant challenge with respect to solution theory. An evident observation is that rewriting the equation in variational form and testing with the solution itself does not lead to an energy estimate because the pairing  [2.x.28]  (understood as the  [2.x.29]  inner product or duality pairing) is not guaranteed to be non-negative. Notions such as energy-stability or  [2.x.30] -stability are (in general) meaningless in this context. 

Historically, the most fruitful step taken in order to deepen the understanding of hyperbolic conservation laws was to assume that the solution is formally defined as  [2.x.31]  where  [2.x.32]  is the solution of the parabolic regularization 

[1.x.53] 

Such solutions, which are understood as the solution recovered in the zero-viscosity limit, are often referred to as [1.x.54]. (This is, because physically  [2.x.33]  can be understood as related to the viscosity of the fluid, i.e., a quantity that indicates the amount of friction neighboring gas particles moving at different speeds exert on each other. The Euler equations themselves are derived under the assumption of no friction, but can physically be expected to describe the limiting case of vanishing friction or viscosity.) Global existence and uniqueness of such solutions is an open issue. However, we know at least that if such viscosity solutions exists they have to satisfy the constraint  [2.x.34]  for all  [2.x.35]  and  [2.x.36]  where 

[1.x.55] 

Here,  [2.x.37]  denotes the specific entropy 

[1.x.56] 

We will refer to  [2.x.38]  as the invariant set of Euler's equations. In other words, a state  [2.x.39]  obeys positivity of the density, positivity of the internal energy, and a local minimum principle on the specific entropy. This condition is a simplified version of a class of pointwise stability constraints satisfied by the exact (viscosity) solution. By pointwise we mean that the constraint has to be satisfied at every point of the domain, not just in an averaged (integral, or high order moments) sense. 

In context of a numerical approximation, a violation of such a constraint has dire consequences: it almost surely leads to catastrophic failure of the numerical scheme, loss of hyperbolicity, and overall, loss of well-posedness of the (discrete) problem. It would also mean that we have computed something that can not be interpreted physically. (For example, what are we to make of a computed solution with a negative density?) In the following we will formulate a scheme that ensures that the discrete approximation of  [2.x.40]  remains in  [2.x.41] . 


[1.x.57][1.x.58] 


Following  [2.x.42] ,  [2.x.43] ,  [2.x.44] , and  [2.x.45] , at this point it might look tempting to base a discretization of Euler's equations on a (semi-discrete) variational formulation: 

[1.x.59] 

Here,  [2.x.46]  is an appropriate finite element space, and  [2.x.47]  is some linear stabilization method (possibly complemented with some ad-hoc shock-capturing technique, see for instance Chapter 5 of  [2.x.48]  and references therein). Most time-dependent discretization approaches described in the deal.II tutorials are based on such a (semi-discrete) variational approach. Fundamentally, from an analysis perspective, variational discretizations are conceived to provide some notion of global (integral) stability, meaning an estimate of the form 

[1.x.60] 

holds true, where  [2.x.49]  could represent the  [2.x.50] -norm or, more generally, some discrete (possibly mesh dependent) energy-norm. Variational discretizations of hyperbolic conservation laws have been very popular since the mid eighties, in particular combined with SUPG-type stabilization and/or upwinding techniques (see the early work of  [2.x.51]  and  [2.x.52] ). They have proven to be some of the best approaches for simulations in the subsonic shockless regime and similarly benign situations. 

<!-- In particular, tutorial  [2.x.53]  focuses on Euler's equation of gas dynamics in the subsonic regime using dG techniques. --> 

However, in the transonic and supersonic regimes, and shock-hydrodynamics applications the use of variational schemes might be questionable. In fact, at the time of this writing, most shock-hydrodynamics codes are still firmly grounded on finite volume methods. The main reason for failure of variational schemes in such extreme regimes is the lack of pointwise stability. This stems from the fact that [1.x.61] bounds on integrated quantities (e.g. integrals of moments) have in general no implications on pointwise properties of the solution. While some of these problems might be alleviated by the (perpetual) chase of the right shock capturing scheme, finite difference-like and finite volume schemes still have an edge in many regards. 

In this tutorial step we therefore depart from variational schemes. We will present a completely algebraic formulation (with the flavor of a collocation-type scheme) that preserves constraints pointwise, i.e., 

[1.x.62] 

Contrary to finite difference/volume schemes, the scheme implemented in this step maximizes the use of finite element software infrastructure, works on any mesh, in any space dimension, and is theoretically guaranteed to always work, all the time, no exception. This illustrates that deal.II can be used far beyond the context of variational schemes in Hilbert spaces and that a large number of classes, modules and namespaces from deal.II can be adapted for such a purpose. 


[1.x.63][1.x.64] 


Let  [2.x.54]  be scalar-valued finite dimensional space spanned by a basis  [2.x.55]  where:  [2.x.56]  and  [2.x.57]  is the set of all indices (nonnegative integers) identifying each scalar Degree of Freedom (DOF) in the mesh. Therefore a scalar finite element functional  [2.x.58]  can be written as  [2.x.59]  with  [2.x.60] . We introduce the notation for vector-valued approximation spaces  [2.x.61] . Let  [2.x.62] , then it can be written as  [2.x.63]  where  [2.x.64]  and  [2.x.65]  is a scalar-valued shape function. 

 [2.x.66]  We purposely refrain from using vector-valued finite element spaces in our notation. Vector-valued finite element spaces are natural for variational formulations of PDE systems (e.g. Navier-Stokes). In such context, the interactions that have to be computed describe [1.x.65]: with proper renumbering of the vector-valued DoFHandler (i.e. initialized with an FESystem) it is possible to compute the block-matrices (required in order to advance the solution) with relative ease. However, the interactions that have to be computed in the context of time-explicit collocation-type schemes (such as finite differences and/or the scheme presented in this tutorial) can be better described as [1.x.66] (not between DOFs). In addition, in our case we do not solve a linear equation in order to advance the solution. This leaves very little reason to use vector-valued finite element spaces both in theory and/or practice. 

We will use the usual Lagrange finite elements: let  [2.x.67]  denote the set of all support points (see  [2.x.68]  "this glossary entry"), where  [2.x.69] . Then each index  [2.x.70]  uniquely identifies a support point  [2.x.71] , as well as a scalar-valued shape function  [2.x.72] . With this notation at hand we can define the (explicit time stepping) scheme as: 

[1.x.67] 

where 

  -  [2.x.73]      is the lumped mass matrix 

  -  [2.x.74]  is the time step size 

  -  [2.x.75]  (note that  [2.x.76] )     is a vector-valued matrix that was used to approximate the divergence     of the flux in a weak sense. 

  -  [2.x.77]  is the adjacency list     containing all degrees of freedom coupling to the index  [2.x.78] . In other     words  [2.x.79]  contains all nonzero column indices for row     index i.  [2.x.80]  will also be called a "stencil". 

  -  [2.x.81]  is the flux  [2.x.82]  of the     hyperbolic system evaluated for the state  [2.x.83]  associated     with support point  [2.x.84] . 

  -  [2.x.85]  if  [2.x.86]  is the so     called [1.x.68]. The graph viscosity serves as a     stabilization term, it is somewhat the discrete counterpart of      [2.x.87]  that appears in the notion of viscosity     solution described above. We will base our construction of  [2.x.88]  on     an estimate of the maximal local wavespeed  [2.x.89]  that     will be explained in detail in a moment. 

  - the diagonal entries of the viscosity matrix are defined as      [2.x.90] . 

  -  [2.x.91]  is a     normalization of the  [2.x.92]  matrix that enters the     approximate Riemann solver with which we compute an the approximations      [2.x.93]  on the local wavespeed. (This will be explained     further down below). 

The definition of  [2.x.94]  is far from trivial and we will postpone the precise definition in order to focus first on some algorithmic and implementation questions. We note that 

  -  [2.x.95]  and  [2.x.96]  do not evolve in time (provided we keep the     discretization fixed). It thus makes sense to assemble these     matrices/vectors once in a so called [1.x.69] and reuse     them in every time step. They are part of what we are going to call     off-line data. 

  - At every time step we have to evaluate  [2.x.97]  and      [2.x.98] , which will     constitute the bulk of the computational cost. 

Consider the following pseudo-code, illustrating a possible straight forward strategy for computing the solution  [2.x.99]  at a new time  [2.x.100]  given a known state  [2.x.101]  at time  [2.x.102] : 

[1.x.70] 



We note here that: 

- This "assembly" does not require any form of quadrature or cell-loops. 

- Here  [2.x.103]  and  [2.x.104]  are a global matrix and a global vector containing all the vectors  [2.x.105]  and all the states  [2.x.106]  respectively. 

-  [2.x.107] ,  [2.x.108] , and  [2.x.109]  are hypothetical implementations that either collect (from) or write (into) global matrices and vectors. 

- If we assume a Cartesian mesh in two space dimensions, first-order polynomial space  [2.x.110] , and that  [2.x.111]  is an interior node (i.e.  [2.x.112]  is not on the boundary of the domain) then:  [2.x.113]  should contain nine state vector elements (i.e. all the states in the patch/macro element associated to the shape function  [2.x.114] ). This is one of the major differences with the usual cell-based loop where the gather functionality (encoded in FEValuesBase<dim, spacedim>.get_function_values() in the case of deal.II) only collects values for the local cell (just a subset of the patch). 

The actual implementation will deviate from above code in one key aspect: the time-step size  [2.x.115]  has to be chosen subject to a CFL condition 

[1.x.71] 

where  [2.x.116]  is a chosen constant. This will require to compute all  [2.x.117]  in a separate step prior to actually performing above update. The core principle remains unchanged, though: we do not loop over cells but rather over all edges of the sparsity graph. 

 [2.x.118]  It is not uncommon to encounter such fully-algebraic schemes (i.e. no bilinear forms, no cell loops, and no quadrature) outside of the finite element community in the wider CFD community. There is a rich history of application of this kind of schemes, also called [1.x.72] or [1.x.73] finite element schemes (see for instance  [2.x.119]  for a historical overview). However, it is important to highlight that the algebraic structure of the scheme (presented in this tutorial) and the node-loops are not just a performance gimmick. Actually, the structure of this scheme was born out of theoretical necessity: the proof of pointwise stability of the scheme hinges on the specific algebraic structure of the scheme. In addition, it is not possible to compute the algebraic viscosities  [2.x.120]  using cell-loops since they depend nonlinearly on information that spans more than one cell (superposition does not hold: adding contributions from separate cells does not lead to the right result). 

[1.x.74][1.x.75] 


In the example considered in this tutorial step we use three different types of boundary conditions: essential-like boundary conditions (we prescribe a state at the left boundary of our domain), outflow boundary conditions (also called "do-nothing" boundary conditions) at the right boundary of the domain, and "reflecting" boundary conditions  [2.x.121]  (also called "slip" boundary conditions) at the top, bottom, and surface of the obstacle. We will not discuss much about essential and "do-nothing" boundary conditions since their implementation is relatively easy and the reader will be able to pick-up the implementation directly from the (documented) source code. In this portion of the introduction we will focus only on the "reflecting" boundary conditions which are somewhat more tricky. 

 [2.x.122]  At the time of this writing (early 2020) it is not unreasonable to say that both analysis and implementation of stable boundary conditions for hyperbolic systems of conservation laws is an open issue. For the case of variational formulations, stable boundary conditions are those leading to a well-posed (coercive) bilinear form. But for general hyperbolic systems of conservation laws (and for the algebraic formulation used in this tutorial) coercivity has no applicability and/or meaning as a notion of stability. In this tutorial step we will use preservation of the invariant set as our main notion of stability which (at the very least) guarantees well-posedness of the discrete problem. 

For the case of the reflecting boundary conditions we will proceed as follows: 

- For every time step advance in time satisfying no boundary condition at all. 

- Let  [2.x.123]  be the portion of the boundary where we want to   enforce reflecting boundary conditions. At the end of the time step we enforce   reflecting boundary conditions strongly in a post-processing step where we   execute the projection     [1.x.76] 

  that removes the normal component of  [2.x.124] . This is a somewhat   naive idea that preserves a few fundamental properties of the PDE as we   explain below. 

This is approach is usually called "explicit treatment of boundary conditions". The well seasoned finite element person might find this approach questionable. No doubt, when solving parabolic, or elliptic equations, we typically enforce essential (Dirichlet-like) boundary conditions by making them part of the approximation space  [2.x.125] , and treat natural (e.g. Neumann) boundary conditions as part of the variational formulation. We also know that explicit treatment of boundary conditions (in the context of parabolic PDEs) almost surely leads to catastrophic consequences. However, in the context of nonlinear hyperbolic equations we have that: 

- It is relatively easy to prove that (for the case of reflecting boundary conditions) explicit treatment of boundary conditions is not only conservative but also guarantees preservation of the property  [2.x.126]  for all  [2.x.127]  (well-posedness). This is perhaps the most important reason to use explicit enforcement of boundary conditions. 

- To the best of our knowledge: we are not aware of any mathematical result proving that it is possible to guarantee the property  [2.x.128]  for all  [2.x.129]  when using either direct enforcement of boundary conditions into the approximation space, or weak enforcement using the Nitsche penalty method (which is for example widely used in discontinuous Galerkin schemes). In addition, some of these traditional ideas lead to quite restrictive time step constraints. 

- There is enough numerical evidence suggesting that explicit treatment of Dirichlet-like boundary conditions is stable under CFL conditions and does not introduce any loss in accuracy. 

If  [2.x.130]  represents Euler's equation with reflecting boundary conditions on the entirety of the boundary (i.e.  [2.x.131] ) and we integrate in space and time  [2.x.132]  we would obtain 

[1.x.77] 

Note that momentum is NOT a conserved quantity (interaction with walls leads to momentum gain/loss): however  [2.x.133]  has to satisfy a momentum balance. Even though we will not use reflecting boundary conditions in the entirety of the domain, we would like to know that our implementation of reflecting boundary conditions is consistent with the conservation properties mentioned above. In particular, if we use the projection  [2.x.134]  in the entirety of the domain the following discrete mass-balance can be guaranteed: 

[1.x.78] 

where  [2.x.135]  is the pressure at the nodes that lie at the boundary. Clearly  [2.x.136]  is the discrete counterpart of  [2.x.137] . The proof of identity  [2.x.138]  is omitted, but we briefly mention that it hinges on the definition of the [1.x.79]  [2.x.139]  provided in  [2.x.140] . We also note that this enforcement of reflecting boundary conditions is different from the one originally advanced in  [2.x.141] . [1.x.80] [1.x.81] 


[1.x.82]  [1.x.83] 




The set of include files is quite standard. The most intriguing part is the fact that we will rely solely on deal.II data structures for MPI parallelization, in particular  [2.x.142]  and  [2.x.143]  included through  [2.x.144]  and  [2.x.145] . Instead of a Trilinos, or PETSc specific matrix class, we will use a non-distributed  [2.x.146]  ( [2.x.147] ) to store the local part of the  [2.x.148] ,  [2.x.149]  and  [2.x.150]  matrices. 

[1.x.84] 



In addition to above deal.II specific includes, we also include four boost headers. The first two are for binary archives that we will use for implementing a check-pointing and restart mechanism. 

[1.x.85] 



The last two header files are for creating custom iterator ranges over integer intervals. 

[1.x.86] 



For  [2.x.151]   [2.x.152]   [2.x.153]   [2.x.154]  and  [2.x.155]  

[1.x.87] 




[1.x.88]  [1.x.89] 




We begin our actual implementation by declaring all classes with their data structures and methods upfront. In contrast to previous example steps we use a more fine-grained encapsulation of concepts, data structures, and parameters into individual classes. A single class thus usually centers around either a single data structure (such as the Triangulation) in the  [2.x.156]  class, or a single method (such as the  [2.x.157]  function of the  [2.x.158]  class). We typically declare parameter variables and scratch data object `private` and make methods and data structures used by other classes `public`. 




 [2.x.159]  A cleaner approach would be to guard access to all data structures by [1.x.90]. For the sake of brevity, we refrain from that approach, though. 




We also note that the vast majority of classes is derived from ParameterAcceptor. This facilitates the population of all the global parameters into a single (global) ParameterHandler. More explanations about the use of inheritance from ParameterAcceptor as a global subscription mechanism can be found in  [2.x.160] . 

[1.x.91] 



We start with defining a number of  [2.x.161]  constants used throughout the tutorial step. This allows us to refer to boundary types by a mnemonic (such as  [2.x.162] ) rather than a numerical value. 







[1.x.92] 




[1.x.93]  [1.x.94]    


The class  [2.x.163]  contains all data structures concerning the mesh (triangulation) and discretization (mapping, finite element, quadrature) of the problem. As mentioned, we use the ParameterAcceptor class to automatically populate problem-specific parameters, such as the geometry information ( [2.x.164] , etc.) or the refinement level ( [2.x.165] ) from a parameter file. This requires us to split the initialization of data structures into two functions: We initialize everything that does not depend on parameters in the constructor, and defer the creation of the mesh to the  [2.x.166]  method that can be called once all parameters are read in via  [2.x.167]  

[1.x.95] 




[1.x.96]  [1.x.97]    


The class  [2.x.168]  contains pretty much all components of the discretization that do not evolve in time, in particular, the DoFHandler, SparsityPattern, boundary maps, the lumped mass matrix,  [2.x.169]  and  [2.x.170]  matrices. Here, the term [1.x.98] refers to the fact that all the class members of  [2.x.171]  have well-defined values independent of the current time step. This means that they can be initialized ahead of time (at [1.x.99]) and are not meant to be modified at any later time step. For instance, the sparsity pattern should not change as we advance in time (we are not doing any form of adaptivity in space). Similarly, the entries of the lumped mass matrix should not be modified as we advance in time either.    


We also compute and store a  [2.x.172]  that contains a map from a global index of type  [2.x.173]  of a boundary degree of freedom to a tuple consisting of a normal vector, the boundary id, and the position associated with the degree of freedom. We have to compute and store this geometric information in this class because we won't have access to geometric (or cell-based) information later on in the algebraic loops over the sparsity pattern.    




 [2.x.174]  Even though this class currently does not have any parameters that could be read in from a parameter file we nevertheless derive from ParameterAcceptor and follow the same idiom of providing a  [2.x.175] ) method as for the class Discretization. 

[1.x.100] 




[1.x.101]  [1.x.102]    


The member functions of this class are utility functions and data structures specific to Euler's equations: 

- The type alias  [2.x.176]  is used for the states  [2.x.177]  

- The type alias  [2.x.178]  is used for the fluxes  [2.x.179] . 

- The  [2.x.180]  function extracts  [2.x.181]  out of the state vector  [2.x.182]  and stores it in a  [2.x.183] . 

- The  [2.x.184]  function computes  [2.x.185]  from a given state vector  [2.x.186] .    


The purpose of the class members  [2.x.187] ,  [2.x.188]  is evident from their names. We also provide a function  [2.x.189] , that computes the wave speed estimate mentioned above,  [2.x.190] , which is used in the computation of the  [2.x.191]  matrix.    




 [2.x.192]  The  [2.x.193]  macro expands to a (compiler specific) pragma that ensures that the corresponding function defined in this class is always inlined, i.e., the function body is put in place for every invocation of the function, and no call (and code indirection) is generated. This is stronger than the  [2.x.194]  keyword, which is more or less a (mild) suggestion to the compiler that the programmer thinks it would be beneficial to inline the function.  [2.x.195]  should only be used rarely and with caution in situations such as this one, where we actually know (due to benchmarking) that inlining the function in question improves performance.    


Finally, we observe that this is the only class in this tutorial step that is tied to a particular "physics" or "hyperbolic conservation law" (in this case Euler's equations). All the other classes are primarily "discretization" classes, very much agnostic of the particular physics being solved. 

[1.x.103] 




[1.x.104]  [1.x.105]    


The class  [2.x.196] 's only public data attribute is a  [2.x.197]   [2.x.198]  that computes the initial state of a given point and time. This function is used for populating the initial flow field as well as setting Dirichlet boundary conditions (at inflow boundaries) explicitly in every time step.    


For the purpose of this example step we simply implement a homogeneous uniform flow field for which the direction and a 1D primitive state (density, velocity, pressure) are read from the parameter file.    


It would be desirable to initialize the class in a single shot: initialize/set the parameters and define the class members that depend on these default parameters. However, since we do not know the actual values for the parameters, this would be sort of meaningless and unsafe in general (we would like to have mechanisms to check the consistency of the input parameters). Instead of defining another  [2.x.199]  method to be called (by-hand) after the call to  [2.x.200]  we provide an "implementation" for the class member  [2.x.201]  which is automatically called when invoking  [2.x.202]  for every class that inherits from ParameterAceptor. 

[1.x.106] 



We declare a private callback function that will be wired up to the  [2.x.203]  signal. 

[1.x.107] 




[1.x.108]  [1.x.109]    


With the  [2.x.204]  classes at hand we can now implement the explicit time-stepping scheme that was introduced in the discussion above. The main method of the  [2.x.205]  class is <code>make_one_step(vector_type &U, double t)</code> that takes a reference to a state vector  [2.x.206]  (as input arguments) computes the updated solution, stores it in the vector  [2.x.207] , and returns the chosen  [2.x.208] size  [2.x.209] .    


The other important method is  [2.x.210]  which primarily sets the proper partition and sparsity pattern for the temporary vector  [2.x.211]  respectively. 

[1.x.110] 




[1.x.111]  [1.x.112]    


At its core, the Schlieren class implements the class member  [2.x.212] . The main purpose of this class member is to compute an auxiliary finite element field  [2.x.213] , that is defined at each node by [1.x.113] where  [2.x.214]  can in principle be any scalar quantity. In practice though, the density is a natural candidate, viz.  [2.x.215] . [1.x.114] postprocessing is a standard method for enhancing the contrast of a visualization inspired by actual experimental X-ray and shadowgraphy techniques of visualization. (See  [2.x.216]  for another example where we create a Schlieren plot.) 

[1.x.115] 




[1.x.116]  [1.x.117]    


Now, all that is left to do is to chain the methods implemented in the  [2.x.217] , and  [2.x.218]  classes together. We do this in a separate class  [2.x.219]  that contains an object of every class and again reads in a number of parameters with the help of the ParameterAcceptor class. 

[1.x.118] 




[1.x.119]  [1.x.120] 





[1.x.121]  [1.x.122] 




The first major task at hand is the typical triplet of grid generation, setup of data structures, and assembly. A notable novelty in this example step is the use of the ParameterAcceptor class that we use to populate parameter values: we first initialize the ParameterAcceptor class by calling its constructor with a string  [2.x.220]  denoting the correct subsection in the parameter file. Then, in the constructor body every parameter value is initialized to a sensible default value and registered with the ParameterAcceptor class with a call to  [2.x.221]  

[1.x.123] 



Note that in the previous constructor we only passed the MPI communicator to the  [2.x.222]  but we still have not initialized the underlying geometry/mesh. As mentioned earlier, we have to postpone this task to the  [2.x.223]  function that gets called after the  [2.x.224]  function has populated all parameter variables with the final values read from the parameter file.    


The  [2.x.225]  function is the last class member that has to be implemented. It creates the actual triangulation that is a benchmark configuration consisting of a channel with a disk obstacle, see  [2.x.226] . We construct the geometry by modifying the mesh generated by  [2.x.227]  We refer to  [2.x.228] ,  [2.x.229] , and  [2.x.230]  for an overview how to create advanced meshes. We first create 4 temporary (non distributed) coarse triangulations that we stitch together with the  [2.x.231]  function. We center the disk at  [2.x.232]  with a diameter of  [2.x.233] . The lower left corner of the channel has coordinates ( [2.x.234] ) and the upper right corner has ( [2.x.235] ,  [2.x.236] ). 

[1.x.124] 



We have to fix up the left edge that is currently located at  [2.x.237]  [2.x.238]  and has to be shifted to  [2.x.239]  [2.x.240] . As a last step the boundary has to be colorized with  [2.x.241]  on the right,  [2.x.242]  on the upper and lower outer boundaries and the obstacle. 







[1.x.125] 




[1.x.126]  [1.x.127] 




Not much is done in the constructor of  [2.x.243]  other than initializing the corresponding class members in the initialization list. 

[1.x.128] 



Now we can initialize the DoFHandler, extract the IndexSet objects for locally owned and locally relevant DOFs, and initialize a  [2.x.244]  object that is needed for distributed vectors. 

[1.x.129] 




[1.x.130]  [1.x.131] 




We are now in a position to create the sparsity pattern for our matrices. There are quite a few peculiarities that need a detailed explanation. We avoid using a distributed matrix class (as for example provided by Trilinos or PETSc) and instead rely on deal.II's own SparseMatrix object to store the local part of all matrices. This design decision is motivated by the fact that (a) we actually never perform a matrix-vector multiplication, and (b) we can always assemble the local part of a matrix exclusively on a given MPI rank. Instead, we will compute nonlinear updates while iterating over (the local part) of a connectivity stencil; a task for which deal.II's own SparsityPattern is specifically optimized for.      


This design consideration has a caveat, though. What makes the deal.II SparseMatrix class fast is the [1.x.132] used in the SparsityPattern (see  [2.x.245] ). This, unfortunately, does not play nicely with a global distributed index range because a sparsity pattern with CSR cannot contain "holes" in the index range. The distributed matrices offered by deal.II avoid this by translating from a global index range into a contiguous local index range. But this is precisely the type of index manipulation we want to avoid in our iteration over the stencil because it creates a measurable overhead.      


The  [2.x.246]  class already implements the translation from a global index range to a contiguous local (per MPI rank) index range: we don't have to reinvent the wheel. We just need to use that translation capability (once and only once) in order to create a "local" sparsity pattern for the contiguous index range  [2.x.247]  [2.x.248]  [2.x.249] . That capability can be invoked by the  [2.x.250]  function. Once the sparsity pattern is created using local indices, all that is left to do is to ensure that (when implementing our scatter and gather auxiliary functions) we always access elements of a distributed vector by a call to  [2.x.251]  This way we avoid index translations altogether and operate exclusively with local indices. 







[1.x.133] 



We have to create the "local" sparsity pattern by hand. We therefore loop over all locally owned and ghosted cells (see  [2.x.252] ) and extract the (global)  [2.x.253]  associated with the cell DOFs and renumber them using  [2.x.254] .        




 [2.x.255]  In the case of a locally owned dof, such renumbering consist of applying a shift (i.e. we subtract an offset) such that now they will become a number in the integer interval  [2.x.256]  [2.x.257]  [2.x.258] . However, in the case of a ghosted dof (i.e. not locally owned) the situation is quite different, since the global indices associated with ghosted DOFs will not be (in general) a contiguous set of integers. 







[1.x.134] 



This concludes the setup of the DoFHandler and SparseMatrix objects. Next, we have to assemble various matrices. We define a number of helper functions and data structures in an anonymous namespace. 







[1.x.135] 



 [2.x.259]  class that will be used to assemble the offline data matrices using WorkStream. It acts as a container: it is just a struct where WorkStream stores the local cell contributions. Note that it also contains a class member  [2.x.260]  used to store the local contributions required to compute the normals at the boundary. 







[1.x.136] 



Next we introduce a number of helper functions that are all concerned about reading and writing matrix and vector entries. They are mainly motivated by providing slightly more efficient code and [1.x.137] for otherwise somewhat tedious code. 




The first function we introduce,  [2.x.261] , will be used to read the value stored at the entry pointed by a SparsityPattern iterator  [2.x.262] . The function works around a small deficiency in the SparseMatrix interface: The SparsityPattern is concerned with all index operations of the sparse matrix stored in CRS format. As such the iterator already knows the global index of the corresponding matrix entry in the low-level vector stored in the SparseMatrix object. Due to the lack of an interface in the SparseMatrix for accessing the element directly with a SparsityPattern iterator, we unfortunately have to create a temporary SparseMatrix iterator. We simply hide this in the  [2.x.263]  function. 







[1.x.138] 



The  [2.x.264]  helper is the inverse operation of  [2.x.265] : Given an iterator and a value, it sets the entry pointed to by the iterator in the matrix. 







[1.x.139] 



 [2.x.266] : we note that  [2.x.267] . If  [2.x.268]  then  [2.x.269] . Which basically implies that we need one matrix per space dimension to store the  [2.x.270]  vectors. Similar observation follows for the matrix  [2.x.271] . The purpose of  [2.x.272]  is to retrieve those entries and store them into a  [2.x.273]  for our convenience. 







[1.x.140] 



 [2.x.274]  (first interface): this first function signature, having three input arguments, will be used to retrieve the individual components  [2.x.275]  of a matrix. The functionality of  [2.x.276]  and  [2.x.277]  is very much the same, but their context is different: the function  [2.x.278]  does not rely on an iterator (that actually knows the value pointed to) but rather on the indices  [2.x.279]  of the entry in order to retrieve its actual value. We should expect  [2.x.280]  to be slightly more expensive than  [2.x.281] . The use of  [2.x.282]  will be limited to the task of computing the algebraic viscosity  [2.x.283]  in the particular case that when both  [2.x.284]  and  [2.x.285]  lie at the boundary.      




 [2.x.286]  The reader should be aware that accessing an arbitrary  [2.x.287]  entry of a matrix (say for instance Trilinos or PETSc matrices) is in general unacceptably expensive. Here is where we might want to keep an eye on complexity: we want this operation to have constant complexity, which is the case of the current implementation using deal.II matrices. 







[1.x.141] 



 [2.x.288]  (second interface): this second function signature having two input arguments will be used to gather the state at a node  [2.x.289]  and return it as a  [2.x.290]  for our convenience. 







[1.x.142] 



 [2.x.291] : this function has three input arguments, the first one is meant to be a "global object" (say a locally owned or locally relevant vector), the second argument which could be a  [2.x.292] , and the last argument which represents a index of the global object. This function will be primarily used to write the updated nodal values, stored as  [2.x.293] , into the global objects. 







[1.x.143] 



We are now in a position to assemble all matrices stored in  [2.x.294] : the lumped mass entries  [2.x.295] , the vector-valued matrices  [2.x.296]  and  [2.x.297] , and the boundary normals  [2.x.298] .    


In order to exploit thread parallelization we use the WorkStream approach detailed in the  [2.x.299]  "Parallel computing with multiple processors" accessing shared memory. As customary this requires definition of 

- Scratch data (i.e. input info required to carry out computations): in this case it is  [2.x.300] . 

- The worker: in our case this is the  [2.x.301]  function that actually computes the local (i.e. current cell) contributions from the scratch data. 

- A copy data: a struct that contains all the local assembly contributions, in this case  [2.x.302] . 

- A copy data routine: in this case it is  [2.x.303]  in charge of actually coping these local contributions into the global objects (matrices and/or vectors)    


Most of the following lines are spent in the definition of the worker  [2.x.304]  and the copy data routine  [2.x.305] . There is not much to say about the WorkStream framework since the vast majority of ideas are reasonably well-documented in  [2.x.306] ,  [2.x.307]  and  [2.x.308]  among others.    


Finally, assuming that  [2.x.309]  is a support point at the boundary, the (nodal) normals are defined as:    




[1.x.144] 

   


We will compute the numerator of this expression first and store it in  [2.x.310] . We will normalize these vectors in a posterior loop. 







[1.x.145] 



What follows is the initialization of the scratch data required by WorkStream 







[1.x.146] 



We compute the local contributions for the lumped mass matrix entries  [2.x.311]  and and vectors  [2.x.312]  in the usual fashion: 

[1.x.147] 



Now we have to compute the boundary normals. Note that the following loop does not do much unless the element has faces on the boundary of the domain. 

[1.x.148] 



Note that "normal" will only represent the contributions from one of the faces in the support of the shape function phi_j. So we cannot normalize this local contribution right here, we have to take it "as is", store it and pass it to the copy data routine. The proper normalization requires an additional loop on nodes. This is done in the copy function below. 

[1.x.149] 



Last, we provide a copy_local_to_global function as required for the WorkStream 

[1.x.150] 



At this point in time we are done with the computation of  [2.x.313]  and  [2.x.314] , but so far the matrix  [2.x.315]  contains just a copy of the matrix  [2.x.316] . That's not what we really want: we have to normalize its entries. In addition, we have not filled the entries of the matrix  [2.x.317]   and the vectors stored in the map  [2.x.318]  are not normalized.      


In principle, this is just offline data, it doesn't make much sense to over-optimize their computation, since their cost will get amortized over the many time steps that we are going to use. However, computing/storing the entries of the matrix  [2.x.319]  are perfect to illustrate thread-parallel node-loops: 

- we want to visit every node  [2.x.320]  in the mesh/sparsity graph, 

- and for every such node we want to visit to every  [2.x.321]  such that  [2.x.322] .      


From an algebraic point of view, this is equivalent to: visiting every row in the matrix and for each one of these rows execute a loop on the columns. Node-loops is a core theme of this tutorial step (see the pseudo-code in the introduction) that will repeat over and over again. That's why this is the right time to introduce them.      


We have the thread parallelization capability  [2.x.323]  that is somehow more general than the WorkStream framework. In particular,  [2.x.324]  can be used for our node-loops. This functionality requires four input arguments which we explain in detail (for the specific case of our thread-parallel node loops): 

- The iterator  [2.x.325]  points to a row index. 

- The iterator  [2.x.326]  points to a numerically higher row index. 

- The function  [2.x.327]  and  [2.x.328]  define a sub-range within the range spanned by the end and begin iterators defined in the two previous bullets) applies an operation to every iterator in such subrange. We may as well call  [2.x.329]  the "worker". 

- Grainsize: minimum number of iterators (in this case representing rows) processed by each thread. We decided for a minimum of 4096 rows.      


A minor caveat here is that the iterators  [2.x.330]  and  [2.x.331]  supplied to  [2.x.332]  have to be random access iterators: internally,  [2.x.333]  will break the range defined by the  [2.x.334]  and  [2.x.335]  iterators into subranges (we want to be able to read any entry in those subranges with constant complexity). In order to provide such iterators we resort to  [2.x.336]       


The bulk of the following piece of code is spent defining the "worker"  [2.x.337] : i.e. the  operation applied at each row of the sub-range. Given a fixed  [2.x.338]  we want to visit every column/entry in such row. In order to execute such columns-loops we use [1.x.151] from the standard library, where: 

-  [2.x.339]  gives us an iterator starting at the first column of the row, 

-  [2.x.340]  is an iterator pointing at the last column of the row, 

- the last argument required by  [2.x.341]  is the operation applied at each nonzero entry (a lambda expression in this case) of such row.      


We note that,  [2.x.342]  will operate on disjoint sets of rows (the subranges) and our goal is to write into these rows. Because of the simple nature of the operations we want to carry out (computation and storage of normals, and normalization of the  [2.x.343]  of entries) threads cannot conflict attempting to write the same entry (we do not need a scheduler). 

[1.x.152] 



First column-loop: we compute and store the entries of the matrix norm_matrix and write normalized entries into the matrix nij_matrix: 

[1.x.153] 



Finally, we normalize the vectors stored in  [2.x.344] . This operation has not been thread parallelized as it would neither illustrate any important concept nor lead to any noticeable speed gain. 

[1.x.154] 



At this point we are very much done with anything related to offline data. 





[1.x.155]  [1.x.156] 




In this section we describe the implementation of the class members of the  [2.x.345]  class. Most of the code here is specific to the compressible Euler's equations with an ideal gas law. If we wanted to re-purpose  [2.x.346]  for a different conservation law (say for: instance the shallow water equation) most of the implementation of this class would have to change. But most of the other classes (in particular those defining loop structures) would remain unchanged.    


We start by implementing a number of small member functions for computing  [2.x.347] ,  [2.x.348] , and the flux  [2.x.349]  of the system. The functionality of each one of these functions is self-explanatory from their names. 







[1.x.157] 



Now we discuss the computation of  [2.x.350] . The analysis and derivation of sharp upper-bounds of maximum wavespeeds of Riemann problems is a very technical endeavor and we cannot include an advanced discussion about it in this tutorial. In this portion of the documentation we will limit ourselves to sketch the main functionality of our implementation functions and point to specific academic references in order to help the (interested) reader trace the source (and proper mathematical justification) of these ideas.    


In general, obtaining a sharp guaranteed upper-bound on the maximum wavespeed requires solving a quite expensive scalar nonlinear problem. This is typically done with an iterative solver. In order to simplify the presentation in this example step we decided not to include such an iterative scheme. Instead, we will just use an initial guess as a guess for an upper bound on the maximum wavespeed. More precisely, equations (2.11) (3.7), (3.8) and (4.3) of  [2.x.351]  are enough to define a guaranteed upper bound on the maximum wavespeed. This estimate is returned by a call to the function  [2.x.352] . At its core the construction of such an upper bound uses the so-called two-rarefaction approximation for the intermediate pressure  [2.x.353] , see for instance Equation (4.46), page 128 in  [2.x.354] .    


The estimate returned by  [2.x.355]  is guaranteed to be an upper bound, it is in general quite sharp, and overall sufficient for our purposes. However, for some specific situations (in particular when one of states is close to vacuum conditions) such an estimate will be overly pessimistic. That's why we used a second estimate to avoid this degeneracy that will be invoked by a call to the function  [2.x.356] . The most important function here is  [2.x.357]  which takes the minimum between the estimates returned by  [2.x.358]  and  [2.x.359] .    


We start again by defining a couple of helper functions:    


The first function takes a state  [2.x.360]  and a unit vector  [2.x.361]  and computes the [1.x.158] 1D state in direction of the unit vector. 

[1.x.159] 



For this, we have to change the momentum to  [2.x.362]  and have to subtract the kinetic energy of the perpendicular part from the total energy: 

[1.x.160] 



We return the 1D state in [1.x.161] variables instead of conserved quantities. The return array consists of density  [2.x.363] , velocity  [2.x.364] , pressure  [2.x.365]  and local speed of sound  [2.x.366] : 







[1.x.162] 



At this point we also define two small functions that return the positive and negative part of a double. 







[1.x.163] 



Next, we need two local wavenumbers that are defined in terms of a primitive state  [2.x.367]  and a given pressure  [2.x.368]   [2.x.369]   Eqn. (3.7): 

[1.x.164] 

Here, the  [2.x.370]  denotes the positive part of the given argument. 







[1.x.165] 



Analougously  [2.x.371]  Eqn. (3.8): 

[1.x.166] 









[1.x.167] 



All that is left to do is to compute the maximum of  [2.x.372]  and  [2.x.373]  computed from the left and right primitive state ( [2.x.374]  Eqn. (2.11)), where  [2.x.375]  is given by  [2.x.376]  Eqn (4.3): 







[1.x.168] 



We compute the second upper bound of the maximal wavespeed that is, in general, not as sharp as the two-rarefaction estimate. But it will save the day in the context of near vacuum conditions when the two-rarefaction approximation might attain extreme values: 

[1.x.169] 



 [2.x.377]  The constant 5.0 multiplying the maximum of the sound speeds is [1.x.170] an ad-hoc constant, [1.x.171] a tuning parameter. It defines an upper bound for any  [2.x.378] . Do not play with it! 







[1.x.172] 



The following is the main function that we are going to call in order to compute  [2.x.379] . We simply compute both maximal wavespeed estimates and return the minimum. 







[1.x.173] 



We conclude this section by defining static arrays  [2.x.380]  that contain strings describing the component names of our state vector. We have template specializations for dimensions one, two and three, that are used later in DataOut for naming the corresponding components: 







[1.x.174] 




[1.x.175]  [1.x.176] 




The last preparatory step, before we discuss the implementation of the forward Euler scheme, is to briefly implement the `InitialValues` class.    


In the constructor we initialize all parameters with default values, declare all parameters for the `ParameterAcceptor` class and connect the  [2.x.381]  slot to the respective signal.    


The  [2.x.382]  slot will be invoked from ParameterAceptor after the call to  [2.x.383]  In that regard, its use is appropriate for situations where the parameters have to be postprocessed (in some sense) or some consistency condition between the parameters has to be checked. 







[1.x.177] 



So far the constructor of  [2.x.384]  has defined default values for the two private members  [2.x.385]  and added them to the parameter list. But we have not defined an implementation of the only public member that we really care about, which is  [2.x.386]  (the function that we are going to call to actually evaluate the initial solution at the mesh nodes). At the top of the function, we have to ensure that the provided initial direction is not the zero vector.    




 [2.x.387]  As commented, we could have avoided using the method  [2.x.388]  and defined a class member  [2.x.389]  in order to define the implementation of  [2.x.390] . But for illustrative purposes we want to document a different way here and use the call back signal from ParameterAcceptor. 







[1.x.178] 



Next, we implement the  [2.x.391]  function object with a lambda function computing a uniform flow field. For this we have to translate a given primitive 1d state (density  [2.x.392] , velocity  [2.x.393] , and pressure  [2.x.394] ) into a conserved n-dimensional state (density  [2.x.395] , momentum  [2.x.396] , and total energy  [2.x.397] ). 







[1.x.179] 




[1.x.180]  [1.x.181] 




The constructor of the  [2.x.398]  class does not contain any surprising code: 







[1.x.182] 



In the class member  [2.x.399]  we initialize the temporary vector  [2.x.400] . The vector  [2.x.401]  will be used to store the solution update temporarily before its contents is swapped with the old vector. 







[1.x.183] 



It is now time to implement the forward Euler step. Given a (writable reference) to the old state  [2.x.402]  at time  [2.x.403]  we update the state  [2.x.404]  in place and return the chosen time-step size. We first declare a number of read-only references to various different variables and data structures. We do this is mainly to have shorter variable names (e.g.,  [2.x.405]  instead of  [2.x.406] ). 







[1.x.184] 



[1.x.185]: Computing the  [2.x.407]  graph viscosity matrix.      


It is important to highlight that the viscosity matrix has to be symmetric, i.e.,  [2.x.408] . In this regard we note here that  [2.x.409]  (or equivalently  [2.x.410] ) provided either  [2.x.411]  or  [2.x.412]  is a support point located away from the boundary. In this case we can check that  [2.x.413]  by construction, which guarantees the property  [2.x.414] .      


However, if both support points  [2.x.415]  or  [2.x.416]  happen to lie on the boundary, then, the equalities  [2.x.417]  and  [2.x.418]  do not necessarily hold true. The only mathematically safe solution for this dilemma is to compute both of them  [2.x.419]  and  [2.x.420]  and take the maximum.      


Overall, the computation of  [2.x.421]  is quite expensive. In order to save some computing time we exploit the fact that the viscosity matrix has to be symmetric (as mentioned above): we only compute the upper-triangular entries of  [2.x.422]  and copy the corresponding entries to the lower-triangular counterpart.      


We use again  [2.x.423]  for thread-parallel for loops. Pretty much all the ideas for parallel traversal that we introduced when discussing the assembly of the matrix  [2.x.424]  and the normalization of  [2.x.425]  above are used here again.      


We define again a "worker" function  [2.x.426]  that computes the viscosity  [2.x.427]  for a subrange [i1, i2) of column indices: 

[1.x.186] 



For a given column index i we iterate over the columns of the sparsity pattern from  [2.x.428]  to  [2.x.429] : 

[1.x.187] 



We only compute  [2.x.430]  if  [2.x.431]  (upper triangular entries) and later copy the values over to  [2.x.432] . 

[1.x.188] 



If both support points happen to be at the boundary we have to compute  [2.x.433]  as well and then take  [2.x.434] . After this we can finally set the upper triangular and lower triangular entries. 

[1.x.189] 



[1.x.190]: Compute diagonal entries  [2.x.435]  and  [2.x.436] . 




So far we have computed all off-diagonal entries of the matrix  [2.x.437] . We still have to fill its diagonal entries defined as  [2.x.438] . We use again  [2.x.439]  for this purpose. While computing the  [2.x.440] s we also determine the largest admissible time-step, which is defined as [1.x.191] Note that the operation  [2.x.441]  is intrinsically global, it operates on all nodes: first we have to take the minimum over all threads (of a given node) and then we have to take the minimum over all MPI processes. In the current implementation: 

- We store   [2.x.442]  (per node) as [1.x.192]. The internal implementation of  [2.x.443]  will take care of guarding any possible race condition when more than one thread attempts to read and/or write  [2.x.444]  at the same time. 

- In order to take the minimum over all MPI process we use the utility function  [2.x.445] . 







[1.x.193] 



on_subranges() will be executed on every thread individually. The variable  [2.x.446]  is thus stored thread locally. 







[1.x.194] 



We store the negative sum of the d_ij entries at the diagonal position 

[1.x.195] 



and compute the maximal local time-step size  [2.x.447] : 

[1.x.196] 



 [2.x.448]  contains the largest possible time-step size computed for the (thread local) subrange. At this point we have to synchronize the value over all threads. This is were we use the [1.x.197] [1.x.198] update mechanism: 

[1.x.199] 



After all threads have finished we can simply synchronize the value over all MPI processes: 







[1.x.200] 



This is a good point to verify that the computed  [2.x.449]  is indeed a valid floating point number. 

[1.x.201] 



[1.x.202]: Perform update. 




At this point, we have computed all viscosity coefficients  [2.x.450]  and we know the maximal admissible time-step size  [2.x.451] . This means we can now compute the update:      


[1.x.203]      


This update formula is slightly different from what was discussed in the introduction (in the pseudo-code). However, it can be shown that both equations are algebraically equivalent (they will produce the same numerical values). We favor this second formula since it has natural cancellation properties that might help avoid numerical artifacts. 







[1.x.204] 



[1.x.205]: Fix up boundary states. 




As a last step in the Forward Euler method, we have to fix up all boundary states. As discussed in the intro we 

- advance in time satisfying no boundary condition at all, 

- at the end of the time step enforce boundary conditions strongly in a post-processing step.      


Here, we compute the correction [1.x.206] which removes the normal component of  [2.x.452] . 







[1.x.207] 



We only iterate over the locally owned subset: 

[1.x.208] 



On free slip boundaries we remove the normal component of the momentum: 

[1.x.209] 



On Dirichlet boundaries we enforce initial conditions strongly: 

[1.x.210] 



[1.x.211]: We now update the ghost layer over all MPI ranks, swap the temporary vector with the solution vector  [2.x.453]  (that will get returned by reference) and return the chosen time-step size  [2.x.454] : 







[1.x.212] 




[1.x.213]  [1.x.214]    


At various intervals we will output the current state  [2.x.455]  of the solution together with a so-called Schlieren plot. The constructor of the  [2.x.456]  class again contains no surprises. We simply supply default values to and register two parameters: 

- schlieren_beta: is an ad-hoc positive amplification factor in order to enhance the contrast in the visualization. Its actual value is a matter of taste. 

- schlieren_index: is an integer indicating which component of the state  [2.x.457]  are we going to use in order to generate the visualization. 







[1.x.215] 



Again, the  [2.x.458]  function initializes two temporary the vectors ( [2.x.459] ). 







[1.x.216] 



We now discuss the implementation of the class member  [2.x.460] , which basically takes a component of the state vector  [2.x.461]  and computes the Schlieren indicator for such component (the formula of the Schlieren indicator can be found just before the declaration of the class  [2.x.462] ). We start by noting that this formula requires the "nodal gradients"  [2.x.463] . However, nodal values of gradients are not defined for  [2.x.464]  finite element functions. More generally, pointwise values of gradients are not defined for  [2.x.465]  functions. The simplest technique we can use to recover gradients at nodes is weighted-averaging i.e.    


[1.x.217]    


where  [2.x.466]  is the support of the shape function  [2.x.467] , and  [2.x.468]  is the weight. The weight could be any positive function such as  [2.x.469]  (that would allow us to recover the usual notion of mean value). But as usual, the goal is to reuse the off-line data as much as possible. In this sense, the most natural choice of weight is  [2.x.470] . Inserting this choice of weight and the expansion  [2.x.471]  into  [2.x.472]  we get :    


[1.x.218]    


Using this last formula we can recover averaged nodal gradients without resorting to any form of quadrature. This idea aligns quite well with the whole spirit of edge-based schemes (or algebraic schemes) where we want to operate on matrices and vectors as directly as it could be possible avoiding by all means assembly of bilinear forms, cell-loops, quadrature, or any other intermediate construct/operation between the input arguments (the state from the previous time-step) and the actual matrices and vectors required to compute the update.    


The second thing to note is that we have to compute global minimum and maximum  [2.x.473]  and  [2.x.474] . Following the same ideas used to compute the time step size in the class member  [2.x.475]  we define  [2.x.476]  and  [2.x.477]  as atomic doubles in order to resolve any conflicts between threads. As usual, we use  [2.x.478]  and  [2.x.479]  to find the global maximum/minimum among all MPI processes.    


Finally, it is not possible to compute the Schlieren indicator in a single loop over all nodes. The entire operation requires two loops over nodes: 

   




- The first loop computes  [2.x.480]  for all  [2.x.481]  in the mesh, and the bounds  [2.x.482]  and  [2.x.483] . 

- The second loop finally computes the Schlieren indicator using the formula    


[1.x.219]    


This means that we will have to define two workers  [2.x.484]  for each one of these stages. 







[1.x.220] 



We define the r_i_max and r_i_min in the current MPI process as atomic doubles in order to avoid race conditions between threads: 

[1.x.221] 



First loop: compute the averaged gradient at each node and the global maxima and minima of the gradients. 

[1.x.222] 



We fix up the gradient r_i at free slip boundaries similarly to how we fixed up boundary states in the forward Euler step. This avoids sharp, artificial gradients in the Schlieren plot at free slip boundaries and is a purely cosmetic choice. 







[1.x.223] 



We remind the reader that we are not interested in the nodal gradients per se. We only want their norms in order to compute the Schlieren indicator (weighted with the lumped mass matrix  [2.x.485] ): 

[1.x.224] 



We compare the current_r_i_max and current_r_i_min (in the current subrange) with r_i_max and r_i_min (for the current MPI process) and update them if necessary: 







[1.x.225] 



And synchronize  [2.x.486]  over all MPI processes. 







[1.x.226] 



Second loop: we now have the vector  [2.x.487]  and the scalars  [2.x.488]  at our disposal. We are thus in a position to actually compute the Schlieren indicator. 







[1.x.227] 



And finally, exchange ghost elements. 

[1.x.228] 




[1.x.229]  [1.x.230]    


With all classes implemented it is time to create an instance of  [2.x.489] ,  [2.x.490] , and  [2.x.491] , and run the forward Euler step in a loop.    


In the constructor of  [2.x.492]  we now initialize an instance of all classes, and declare a number of parameters controlling output. Most notable, we declare a boolean parameter  [2.x.493]  that will control whether the program attempts to restart from an interrupted computation, or not. 







[1.x.231] 



We start by implementing a helper function  [2.x.494]  in an anonymous namespace that is used to output messages in the terminal with some nice formatting. 







[1.x.232] 



With  [2.x.495]  in place it is now time to implement the  [2.x.496]  that contains the main loop of our program. 







[1.x.233] 



We start by reading in parameters and initializing all objects. We note here that the call to  [2.x.497]  reads in all parameters from the parameter file (whose name is given as a string argument). ParameterAcceptor handles a global ParameterHandler that is initialized with subsections and parameter declarations for all class instances that are derived from ParameterAceptor. The call to initialize enters the subsection for each each derived class, and sets all variables that were added using  [2.x.498]  







[1.x.234] 



Next we create the triangulation, assemble all matrices, set up scratch space, and initialize the DataOut<dim> object: 







[1.x.235] 



We will store the current time and state in the variable  [2.x.499] : 







[1.x.236] 




[1.x.237]  [1.x.238]      


By default the boolean  [2.x.500]  is set to false, i.e. the following code snippet is not run. However, if  [2.x.501]  we indicate that we have indeed an interrupted computation and the program shall restart by reading in an old state consisting of  [2.x.502] ,  [2.x.503]  from a checkpoint file. These checkpoint files will be created in the  [2.x.504]  routine discussed below. 







[1.x.239] 



We use a  [2.x.505]  to store and read in the contents the checkpointed state. 







[1.x.240] 



 [2.x.506]  iterates over all components of the state vector  [2.x.507] . We read in every entry of the component in sequence and update the ghost layer afterwards: 

[1.x.241] 



With either the initial state set up, or an interrupted state restored it is time to enter the main loop: 







[1.x.242] 



We first print an informative status message 







[1.x.243] 



and then perform a single forward Euler step. Note that the state vector  [2.x.508]  is updated in place and that  [2.x.509]  returns the chosen step size. 







[1.x.244] 



Post processing, generating output and writing out the current state is a CPU and IO intensive task that we cannot afford to do every time step - in particular with explicit time stepping. We thus only schedule output by calling the  [2.x.510]  function if we are past a threshold set by  [2.x.511] . 







[1.x.245] 



We wait for any remaining background output thread to finish before printing a summary and exiting. 

[1.x.246] 



The  [2.x.512]  takes an initial time "t" as input argument and populates a state vector  [2.x.513]  with the help of the  [2.x.514]  object. 







[1.x.247] 



The function signature of  [2.x.515]  is not quite right for  [2.x.516]  We work around this issue by, first, creating a lambda function that for a given position  [2.x.517]  returns just the value of the  [2.x.518] th component. This lambda in turn is converted to a  [2.x.519]  with the help of the ScalarFunctionFromFunctionObject wrapper. 







[1.x.248] 




[1.x.249]  [1.x.250]    


Writing out the final vtk files is quite an IO intensive task that can stall the main loop for a while. In order to avoid this we use an [1.x.251] strategy by creating a background thread that will perform IO while the main loop is allowed to continue. In order for this to work we have to be mindful of two things: 

- Before running the  [2.x.520]  thread, we have to create a copy of the state vector  [2.x.521] . We store it in the vector  [2.x.522] . 

- We have to avoid any MPI communication in the background thread, otherwise the program might deadlock. This implies that we have to run the postprocessing outside of the worker thread. 







[1.x.252] 



If the asynchronous writeback option is set we launch a background thread performing all the slow IO to disc. In that case we have to make sure that the background thread actually finished running. If not, we have to wait to for it to finish. We launch said background thread with [1.x.253] that returns a [1.x.254] object. This  [2.x.523]  object contains the return value of the function, which is in our case simply  [2.x.524] . 







[1.x.255] 



At this point we make a copy of the state vector, run the schlieren postprocessor, and run  [2.x.525]  The actual output code is standard: We create a DataOut instance, attach all data vectors we want to output and call  [2.x.526]  There is one twist, however. In order to perform asynchronous IO on a background thread we create the DataOut<dim> object as a shared pointer that we pass on to the worker thread to ensure that once we exit this function and the worker thread finishes the DataOut<dim> object gets destroyed again. 







[1.x.256] 



Next we create a lambda function for the background thread. We [1.x.257] the  [2.x.527]  pointer as well as most of the arguments of the output function by value so that we have access to them inside the lambda function. 

[1.x.258] 



We checkpoint the current state by doing the precise inverse operation to what we discussed for the [1.x.259]: 







[1.x.260] 



If the asynchronous writeback option is set we launch a new background thread with the help of [1.x.261] function. The function returns a [1.x.262] object that we can use to query the status of the background thread. At this point we can return from the  [2.x.528]  function and resume with the time stepping in the main loop - the thread will run in the background. 

[1.x.263] 



And finally, the main function. 







[1.x.264] 

[1.x.265] [1.x.266][1.x.267] 


Running the program with default parameters in release mode takes about 1 minute on a 4 core machine (with hyperthreading): 

[1.x.268] 



One thing that becomes evident is the fact that the program spends two thirds of the execution time computing the graph viscosity d_ij and about a third of the execution time in performing the update, where computing the flux  [2.x.529]  is the expensive operation. The preset default resolution is about 37k gridpoints, which amounts to about 148k spatial degrees of freedom in 2D. An animated schlieren plot of the solution looks as follows: 

 [2.x.530]  

It is evident that 37k gridpoints for the first-order method is nowhere near the resolution needed to resolve any flow features. For comparison, here is a "reference" computation with a second-order method and about 9.5M gridpoints ([1.x.269]): 

 [2.x.531]  

So, we give the first-order method a second chance and run it with about 2.4M gridpoints on a small compute server: 

[1.x.270] 



And with the following result: 

 [2.x.532]  

That's substantially better, although of course at the price of having run the code for roughly 2 hours on 16 cores. 




[1.x.271] [1.x.272][1.x.273] 


The program showcased here is really only first-order accurate, as discussed above. The pictures above illustrate how much diffusion that introduces and how far the solution is from one that actually resolves the features we care about. 

This can be fixed, but it would exceed what a *tutorial* is about. Nevertheless, it is worth showing what one can achieve by adding a second-order scheme. For example, here is a video computed with [1.x.274] that shows (with a different color scheme) a 2d simulation that corresponds to the cases shown above: 

[1.x.275] 



This simulation was done with 38 million degrees of freedom (continuous  [2.x.533]  finite elements) per component of the solution vector. The exquisite detail of the solution is remarkable for these kinds of simulations, including in the sub-sonic region behind the obstacle. 

One can also with relative ease further extend this to the 3d case: 

[1.x.276] 



Solving this becomes expensive, however: The simulation was done with 1,817 million degrees of freedom (continuous  [2.x.534]  finite elements) per component (for a total of 9.09 billion spatial degrees of freedom) and ran on 30,720 MPI ranks. The code achieved an average througput of 969M grid points per second (0.04M gridpoints per second per CPU). The front and back wall show a "Schlieren plot": the magnitude of the gradient of the density on an exponential scale from white (low) to black (high). All other cutplanes and the surface of the obstacle show the magnitude of the vorticity on a white (low) - yellow (medium) - red (high) scale. (The scales of the individual cutplanes have been adjusted for a nicer visualization.) [1.x.277] [1.x.278]  [2.x.535]  

 [2.x.536] 
 [2.x.0]   [2.x.1]  

This tutorial depends on  [2.x.2] ,  [2.x.3] ,  [2.x.4] . 

[1.x.0][1.x.1][1.x.2][1.x.3][1.x.4][1.x.5][1.x.6][1.x.7][1.x.8][1.x.9][1.x.10][1.x.11][1.x.12][1.x.13][1.x.14][1.x.15][1.x.16][1.x.17][1.x.18][1.x.19][1.x.20][1.x.21][1.x.22][1.x.23][1.x.24][1.x.25][1.x.26][1.x.27][1.x.28] 

 [2.x.5]  

[1.x.29] 

 [2.x.6]  


[1.x.30][1.x.31] 


[1.x.32][1.x.33] 


In this tutorial we consider a mixing problem in the laminar flow regime. Such problems occur in a wide range of applications ranging from chemical engineering to power generation (e.g. turbomachinery). Mixing problems are particularly hard to solve numerically, because they often involve a container (with fixed boundaries, and possibly complex geometries such as baffles), represented by the domain  [2.x.7] , and one (or more) immersed and rotating impellers (represented by the domain  [2.x.8] ). The domain in which we would like to solve the flow equations is the (time dependent) difference between the two domains, namely:  [2.x.9] . 

For rotating impellers, the use of Arbitrary Lagrangian Eulerian formulations (in which the fluid domain -- along with the mesh! -- is smoothly deformed to follow the deformations of the immersed solid) is not possible, unless only small times (i.e., small fluid domain deformations) are considered. If one wants to track the evolution of the flow across multiple rotations of the impellers, the resulting deformed grid would simply be too distorted to be useful. 

In this case, a viable alternative strategy would be to use non-matching methods (similarly to what we have done in  [2.x.10] ), where a background fixed grid (that may or may not be locally refined in time to better capture the solid motion) is coupled with a rotating, independent, grid. 

In order to maintain the same notations used in  [2.x.11] , we use  [2.x.12]  to denote the domain in  [2.x.13]  representing the container of both the fluid and the impeller, and we use  [2.x.14]  in  [2.x.15]  to denote either the full impeller (when its `spacedim` measure is non-negligible, i.e., when we can represent it as a grid of dimension `dim` equal to `spacedim`), a co-dimension one representation of a thin impeller, or just the boundary of the full impeller. 

The domain  [2.x.16]  is embedded in  [2.x.17]  ( [2.x.18] ) and it is non-matching: It does not, in general, align with any of the features of the volume mesh. We solve a partial differential equation on  [2.x.19] , enforcing some conditions on the solution of the problem on the embedded domain  [2.x.20]  by some penalization techniques. In the current case, the condition is that the velocity of the fluid at points on  [2.x.21]  equal the velocity of the solid impeller at that point. 

The technique we describe here is presented in the literature using one of many names: the [1.x.34] and the [1.x.35] among others.  The main principle is that the discretization of the two grids are kept completely independent. In the present tutorial, this approach is used to solve for the motion of a viscous fluid, described by the Stokes equation, that is agitated by a rigid non-deformable impeller. 

Thus, the equations solved in  [2.x.22]  are the Stokes equations for a creeping flow (i.e. a flow where  [2.x.23] ) and a no-slip boundary condition is applied on the moving *embedded domain*  [2.x.24]  associated with the impeller. However, this tutorial could be readily extended to other equations (e.g. the Navier-Stokes equations, linear elasticity equation, etc.). It can be seen as a natural extension of  [2.x.25]  that enables the solution of large problems using a distributed parallel computing architecture via MPI. 

However, contrary to  [2.x.26] , the Dirichlet boundary conditions on  [2.x.27]  are imposed weakly instead of through the use of Lagrange multipliers, and we concentrate on dealing with the coupling of two fully distributed triangulations (a combination that was not possible in the implementation of  [2.x.28] ). 

There are two interesting scenarios that occur when one wants to enforce conditions on the embedded domain  [2.x.29] : 

- The geometrical dimension `dim` of the embedded domain  [2.x.30]  is the same of the domain  [2.x.31]  (`spacedim`), that is, the spacedim-dimensional measure of  [2.x.32]  is not zero. In this case, the imposition of the Dirichlet boundary boundary condition on  [2.x.33]  is done through a volumetric penalization. If the applied penalization only depends on the velocity, this is often referred to as  [2.x.34]  penalization whereas if the penalization depends on both the velocity and its gradient, it is an  [2.x.35]  penalization. The case of the  [2.x.36]  penalization is very similar to a Darcy-type approach. Both  [2.x.37]  and  [2.x.38]  penalizations have been analyzed extensively (see, for example,  [2.x.39] ). 

- The embedded domain  [2.x.40]  has an intrinsic dimension `dim` which is smaller than that of  [2.x.41]  (`spacedim`), thus its spacedim-dimensional measure is zero; for example it is a curve embedded in a two dimensional domain, or a surface embedded in a three-dimensional domain. This is of course physically impossible, but one may consider very thin sheets of metal moving in a fluid as essentially lower-dimensional if the thickness of the sheet is negligible. In this case, the boundary condition is imposed weakly on  [2.x.42]  by applying the [1.x.36] method (see  [2.x.43] ). 

Both approaches have very similar requirements and result in highly similar formulations. Thus, we treat them almost in the same way. 

In this tutorial program we are not interested in further details on  [2.x.44] : we assume that the dimension of the embedded domain (`dim`) is always smaller by one or equal with respect to the dimension of the embedding domain  [2.x.45]  (`spacedim`). 

We are going to solve the following differential problem: given a sufficiently regular function  [2.x.46]  on  [2.x.47] , find the solution  [2.x.48]  to 

[1.x.37] 



This equation, which we have normalized by scaling the time units in such a way that the viscosity has a numerical value of 1, describes slow, viscous flow such as honey or lava. The main goal of this tutorial is to show how to impose the velocity field condition  [2.x.49]  on a non-matching  [2.x.50]  in a weak way, using a penalization method. A more extensive discussion of the Stokes problem including body forces, different boundary conditions, and solution strategies can be found in  [2.x.51] . 

Let us start by considering the Stokes problem alone, in the entire domain  [2.x.52] . We look for a velocity field  [2.x.53]  and a pressure field  [2.x.54]  that satisfy the Stokes equations with homogeneous boundary conditions on  [2.x.55] . 

The weak form of the Stokes equations is obtained by first writing it in vector form as [1.x.38] 

forming the dot product from the left with a vector-valued test function  [2.x.56] , and integrating over the domain  [2.x.57] , yielding the following set of equations: [1.x.39] 

which has to hold for all test functions  [2.x.58] . 


Integrating by parts and exploiting the boundary conditions on  [2.x.59] , we obtain the following variational problem: [1.x.40] 



where  [2.x.60]  represents the  [2.x.61]  scalar product. This is the same variational form used in  [2.x.62] . 

This variational formulation does not take into account the embedded domain. Contrary to  [2.x.63] , we do not enforce strongly the constraints of  [2.x.64]  on  [2.x.65] , but enforce them weakly via a penalization term. 

The analysis of this weak imposition of the boundary condition depends on the spacedim-dimensional measure of  [2.x.66]  as either positive (if `dim` is equal to `spacedim`) or zero (if `dim` is smaller than `spacedim`). We discuss both scenarios. 


[1.x.41][1.x.42] 


In this case, we assume that  [2.x.67]  is the boundary of the actual impeller, that is, a closed curve embedded in a two-dimensional domain or a closed surface in a three-dimensional domain. The idea of this method starts by considering a weak imposition of the Dirichlet boundary condition on  [2.x.68] , following the Nitsche method. This is achieved by using the following modified formulation on the fluid domain, where no strong conditions on the test functions on  [2.x.69]  are imposed: 

[1.x.43] 



The integrals over  [2.x.70]  are lower-dimensional integrals. It can be shown (see  [2.x.71] ) that there exists a positive constant  [2.x.72]  so that if  [2.x.73] , the weak imposition of the boundary will be consistent and stable. The first two additional integrals on  [2.x.74]  (the second line in the equation above) appear naturally after integrating by parts, when one does not assume that  [2.x.75]  is zero on  [2.x.76] . 

The third line in the equation above contains two terms that are added to ensure consistency of the weak form, and a stabilization term, that is there to enforce the boundary condition with an error which is consistent with the approximation error. The consistency terms and the stabilization term are added to the right hand side with the actual boundary data  [2.x.77] . 

When  [2.x.78]  satisfies the condition  [2.x.79]  on  [2.x.80] , all the consistency and stability integrals on  [2.x.81]  cancel out, and one is left with the usual weak form of Stokes flow, that is, the above formulation is consistent. 

We note that an alternative (non-symmetric) formulation can be used : 

[1.x.44] 

Note the different sign of the first terms on the third and fourth lines. In this case, the stability and consistency conditions become  [2.x.82] . In the symmetric case, the value of  [2.x.83]  is dependent on  [2.x.84] , and it is in general chosen such that  [2.x.85]  with  [2.x.86]  a measure of size of the face being integrated and  [2.x.87]  a constant such that  [2.x.88] . This is as one usually does with the Nitsche penalty method to enforcing Dirichlet boundary conditions. 

The non-symmetric approach, on the other hand, is related to how one enforced continuity for the non-symmetric interior penalty method for discontinuous Galerkin methods (the "NIPG" method  [2.x.89] ). Even if the non-symmetric case seems advantageous w.r.t. possible choices of stabilization parameters, we opt for the symmetric discretization, since in this case it can be shown that the dual problem is also consistent, leading to a solution where not only the energy norm of the solution converges with the correct order, but also its  [2.x.90]  norm. Furthermore, the resulting matrix remains symmetric. 

The above formulation works under the assumption that the domain is discretized exactly. However, if the deformation of the impeller is a rigid body motion, it is possible to artificially extend the solution of the Stokes problem inside the propeller itself, since a rigid body motion is also a solution to the Stokes problem. The idea is then to solve the same problem, inside  [2.x.91] , imposing the same boundary conditions on  [2.x.92] , using the same penalization technique, and testing with test functions  [2.x.93]  which are globally continuous over  [2.x.94] . 

This results in the following (intermediate) formulation: [1.x.45] 

where the jump terms, denoted with  [2.x.95] , are computed with respect to a fixed orientation of the normal vector  [2.x.96] . The factor of 2 appears in front of  [2.x.97]  since we see every part of  [2.x.98]  twice, once from within the fluid and once from within the obstacle moving around in it. (For all of the other integrals over  [2.x.99] , we visit each part of  [2.x.100]  twice, but with opposite signs, and consequently get the jump terms.) 

Here we notice that, unlike in discontinuous Galerkin methods, the test and trial functions are continuous across  [2.x.101] . Moreover, if  [2.x.102]  is not aligned with cell boundaries, all the jump terms are also zero, since, in general, finite element function spaces are smooth inside each cell, and if  [2.x.103]  cuts through an element intersecting its boundary only at a finite number of points, all the contributions on  [2.x.104] , with the exception of the stabilization ones, can be neglected from the formulation, resulting in the following final form of the variational formulation: 

[1.x.46] 



In  [2.x.105] , the imposition of the constraint required the addition of new variables in the form of Lagrange multipliers. This is not the case for this tutorial program. The imposition of the boundary condition using Nitsche's method only modifies the system matrix and the right-hand side without adding additional unknowns. However, the velocity vector  [2.x.106]  on the embedded domain will not match exactly the prescribed velocity  [2.x.107] , but only up to a numerical error which is in the same order as the interpolation error of the finite element method. Furthermore, as in  [2.x.108] , we still need to integrate over the non-matching embedded grid in order to construct the boundary term necessary to impose the boundary condition over  [2.x.109] . 


[1.x.47][1.x.48] 


In this case,  [2.x.110]  has the same dimension, but is embedded into  [2.x.111] . We can think of this as a thick object moving around in the fluid. In the case of  [2.x.112]  penalization, the additional penalization term can be interpreted as a Darcy term within  [2.x.113] , resulting in: 

[1.x.49] 



Here, integrals over  [2.x.114]  are simply integrals over a part of the volume. The  [2.x.115]  penalization thus consists in adding a volumetric term that constrains the velocity of the fluid to adhere to the velocity of the rigid body within  [2.x.116] . Also in this case,  [2.x.117]  must be chosen sufficiently large in order to ensure that the Dirichlet boundary condition in  [2.x.118]  is sufficiently respected, but not too high in order to maintain the proper conditioning of the system matrix. 

A  [2.x.119]  penalization may be constructed in a similar manner, with the addition of a viscous component to the penalization that dampens the velocity gradient within  [2.x.120] : 

[1.x.50] 



Notice that the  [2.x.121]  penalization (`dim` equal to `spacedim`) and the Nitsche penalization (`dim` equal to `spacedim-1`) result in the exact same numerical implementation, thanks to the dimension independent capabilities of deal.II. 


[1.x.51][1.x.52] 


In this tutorial, both the embedded grid  [2.x.122]  and the embedding grid are described using a  [2.x.123]  These two triangulations can be built from functions in the GridGenerator namespace or by reading a mesh file produced with another application (e.g. GMSH, see the discussion in  [2.x.124] ). This is slightly more general than what was previously done in  [2.x.125] . 

The addition of the immersed boundary method, whether it is in the `dim=spacedim` or `dim<spacedim` case, only introduces additional terms in the system matrix and the right-hand side of the system which result from the integration over  [2.x.126] . This does not modify the number of variables for which the problem must be solved. The challenge is thus related to the integrals that must be carried over  [2.x.127] . 

As usual in finite elements we split this integral into contributions from all cells of the triangulation used to discretize  [2.x.128] , we transform the integral on  [2.x.129]  to an integral on the reference element  [2.x.130] , where  [2.x.131]  is the mapping from  [2.x.132]  to  [2.x.133] , and compute the integral on  [2.x.134]  using a quadrature formula. For example: 

[1.x.53] 

Computing this sum is non-trivial because we have to evaluate  [2.x.135] . In general, if  [2.x.136]  and  [2.x.137]  are not aligned, the point  [2.x.138]  is completely arbitrary with respect to  [2.x.139] , and unless we figure out a way to interpolate all basis functions of  [2.x.140]  on an arbitrary point on  [2.x.141] , we cannot compute the integral needed. 


To evaluate  [2.x.142]  the following steps needs to be taken (as shown in the picture below): 

- For a given cell  [2.x.143]  in  [2.x.144]  compute the real point  [2.x.145] , where  [2.x.146]  is one of the quadrature points used for the integral on  [2.x.147] . This is the easy part:  [2.x.148]  gives us the real-space locations of all quadrature points. 

- Find the cell of  [2.x.149]  in which  [2.x.150]  lies. We shall call this element  [2.x.151] . 

- Find the reference coordinates within  [2.x.152]  of  [2.x.153] . For this, we need the inverse of the mapping  [2.x.154]  that transforms the reference element  [2.x.155]  into the element  [2.x.156] :  [2.x.157] . 

- Evaluate the basis function  [2.x.158]  of the  [2.x.159]  mesh at this   point  [2.x.160] . This is, again, relatively simple using FEValues. 


 [2.x.161]  

In  [2.x.162] , the second through fourth steps above were computed by calling, in turn, 

-  [2.x.163]  followed by 

-  [2.x.164]  We then 

- construct a custom Quadrature formula, containing the point in the reference  cell and then 

- construct an FEValues object, with the given quadrature formula, and  initialized with the cell obtained in the first step. 

Although this approach could work for the present case, it does not lends itself readily to parallel simulations using distributed triangulations. Indeed, since the position of the quadrature points on the cells of the embedded domain  [2.x.165]  do not match that of the embedding triangulation and since  [2.x.166]  is constantly moving, this would require that the triangulation representing  [2.x.167]  be stored in it's entirety for all of the processors. As the number of processor and the number of cells in  [2.x.168]  increases, this leads to a severe bottleneck in terms of memory. Consequently, an alternative strategy is sought in this step. 


[1.x.54][1.x.55] 


Remember that for both the penalization approach ( [2.x.169]  or  [2.x.170] ) and the Nitsche method, we want to compute integrals that are approximated by the quadrature. That is, we need to compute [1.x.56] If you followed the discussion above, then you will recall that  [2.x.171]  and  [2.x.172]  are shape functions defined on the fluid mesh. The only things defined on the solid mesh are:  [2.x.173] , which is the location of a quadrature point on a solid cell that is part of  [2.x.174] ,  [2.x.175]  is the determinant of its Jacobian, and  [2.x.176]  the corresponding quadrature weight. 

The important part to realize is now this:  [2.x.177]  is a property of the quadrature formula and does not change with time. Furthermore, the Jacobian matrix of  [2.x.178]  itself changes as the solid obstacle moves around in the fluid, but because the solid is considered non-deforming (it only translates and rotates, but doesn't dilate), the determinant of the Jacobian remains constant. As a consequence, the product  [2.x.179]  (which we typically denote by `JxW`) remains constant for each quadrature point. So the only thing we need keep track of are the positions  [2.x.180]  -- but these move with the velocity of the solid domain. 

In other words, we don't actually need to keep the solid mesh at all. All we need is the positions  [2.x.181]  and corresponding `JxW` values. Since both of these properties are point-properties (or point-vectors) that are attached to the solid material, they can be idealized as a set of disconnected infinitesimally small "particles", which carry the required `JxW` information with the movement of the solid. deal.II has the ability to distribute and store such a set of particles in large-scale parallel computations in the form of the ParticleHandler class (for details on the implementation see  [2.x.182] ), and we will make use of this functionality in this tutorial. 

Thus, the approach taken in this step is as follows: 

- Create a  [2.x.183]  for the domain  [2.x.184] ; 

- Create  [2.x.185]  at the positions of the quadrature points on  [2.x.186] ; 

- Call the  [2.x.187]  function,   to distribute the particles across processors, *following the solid   triangulation*; 

- Attach the `JxW` values as a "property" to each  [2.x.188]  object. 

This structure is relatively expensive to generate, but must only be generated once per simulation. Once the  [2.x.189]  is generated and the required information is attached to the particle, the integrals over  [2.x.190]  can be carried out by exploiting the fact that particles are grouped cellwise inside ParticleHandler, allowing us to: 

- Looping over all cells of  [2.x.191]  that contain at least one particle 

- Looping over all particles in the given cell 

- Compute the integrals and fill the global matrix. 

Since the  [2.x.192]  can manage the exchange of particles from one processor to the other, the embedded triangulation can be moved or deformed by displacing the particles. The only constraint associated with this displacement is that particles should be displaced by a distance that is no larger than the size of one cell. That's because that is the limit to which  [2.x.193]  can track which cell a particle that leaves its current cell now resides in. 

Once the entire problem (the Stokes problem and the immersed boundary imposition) is assembled, the final saddle point problem is solved by an iterative solver, applied to the Schur complement  [2.x.194]  (whose construction is described, for example, in  [2.x.195] ), and we construct  [2.x.196]  using LinearOperator classes. 


[1.x.57][1.x.58] 


The problem we solve here is a demonstration of the time-reversibility of Stokes flow. This is often illustrated in science education experiments with a Taylor-Couette flow and dye droplets that revert back to their original shape after the fluid has been displaced in a periodic manner. 

[1.x.59] 



In the present problem, a very viscous fluid is agitated by the rotation of an impeller, which, in 2D, is modeled by a rectangular grid. The impeller rotates for a given number of revolutions, after which the flow is reversed such that the same number of revolutions is carried out in the opposite direction. We recall that since the Stokes equations are self-adjoint, creeping flows are reversible. Consequently, if the impeller motion is reversed in the opposite direction, the fluid should return to its original position. In the present case, this is illustrated by inserting a circle of passive tracer particles that are advected by the fluid and which return to their original position, thus demonstrating the time-reversibility of the flow. 


[1.x.60][1.x.61] 


This tutorial program uses a number of techniques on imposing velocity conditions on non-matching interfaces in the interior of the fluid. For more background material, you may want to look up the following references:  [2.x.197] ,  [2.x.198] ,  [2.x.199] ,  [2.x.200] ,  [2.x.201] . [1.x.62] [1.x.63] 


[1.x.64]  [1.x.65] Most of these have been introduced elsewhere, we'll comment only on the new ones. The switches close to the top that allow selecting between PETSc and Trilinos linear algebra capabilities are similar to the ones in  [2.x.202]  and  [2.x.203] . 







[1.x.66] 



These are the only new include files with regard to  [2.x.204] . In this tutorial, the non-matching coupling between the solid and the fluid is computed using an intermediate data structure that keeps track of how the locations of quadrature points of the solid evolve within the fluid mesh. This data structure needs to keep track of the position of the quadrature points on each cell describing the solid domain, of the quadrature weights, and possibly of the normal vector to each point, if the solid domain is of co-dimension one. 




Deal.II offers these facilities in the Particles namespace, through the ParticleHandler class. ParticleHandler is a class that allows you to manage a collection of particles (objects of type  [2.x.205]  representing a collection of points with some attached properties (e.g., an id) floating on a  [2.x.206]  The methods and classes in the namespace Particles allows one to easily implement Particle-In-Cell methods and particle tracing on distributed triangulations. 




We "abuse" this data structure to store information about the location of solid quadrature points embedded in the surrounding fluid grid, including integration weights, and possibly surface normals. The reason why we use this additional data structure is related to the fact that the solid and the fluid grids might be non-overlapping, and if we were using two separate triangulation objects, would be distributed independently among parallel processes. 




In order to couple the two problems, we rely on the ParticleHandler class, storing in each particle the position of a solid quadrature point (which is in general not aligned to any of the fluid quadrature points), its weight, and any other information that may be required to couple the two problems. These locations are then propagated along with the (prescribed) velocity of the solid impeller. 




Ownership of the solid quadrature points is initially inherited from the MPI partitioning on the solid mesh itself. The Particles so generated are later distributed to the fluid mesh using the methods of the ParticleHandler class. This allows transparent exchange of information between MPI processes about the overlapping pattern between fluid cells and solid quadrature points. 

[1.x.67] 



When generating the grids, we allow reading it from a file, and if deal.II has been built with OpenCASCADE support, we also allow reading CAD files and use them as manifold descriptors for the grid (see  [2.x.207]  for a detailed description of the various Manifold descriptors that are available in the OpenCASCADE namespace) 

[1.x.68] 




[1.x.69]  [1.x.70] 




Similarly to what we have done in  [2.x.208] , we set up a class that holds all the parameters of our problem and derive it from the ParameterAcceptor class to simplify the management and creation of parameter files.    


The ParameterAcceptor paradigm requires all parameters to be writable by the ParameterAcceptor methods. In order to avoid bugs that would be very difficult to track down (such as writing things like `time = 0` instead of `time == 0`), we declare all the parameters in an external class, which is initialized before the actual `StokesImmersedProblem` class, and pass it to the main class as a `const` reference.    


The constructor of the class is responsible for the connection between the members of this class and the corresponding entries in the ParameterHandler. Thanks to the use of the  [2.x.209]  method, this connection is trivial, but requires all members of this class to be writeable. 

[1.x.71] 



however, since this class will be passed as a `const` reference to the StokesImmersedProblem class, we have to make sure we can still set the time correctly in the objects derived by the Function class defined herein. In order to do so, we declare both the  [2.x.210]  and  [2.x.211]  members to be `mutable`, and define the following little helper method that sets their time to the correct value. 

[1.x.72] 



The remainder of the class consists largely of member variables that describe the details of the simulation and its discretization. The following parameters are about where output should land, the spatial and temporal discretization (the default is the  [2.x.212]  Taylor-Hood discretization which uses a polynomial degree of 2 for the velocity), and how many time steps should elapse before we generate graphical output again: 

[1.x.73] 



We allow every grid to be refined independently. In this tutorial, no physics is resolved on the solid grid, and its velocity is given as a datum. However it is relatively straightforward to incorporate some elasticity model in this tutorial, and transform it into a fully fledged FSI solver. 

[1.x.74] 



To provide a rough description of the fluid domain, we use the method extract_rtree_level() applied to the tree of bounding boxes of each locally owned cell of the fluid triangulation. The higher the level of the tree, the larger the number of extracted bounding boxes, and the more accurate is the description of the fluid domain. However, a large number of bounding boxes also implies a large communication cost, since the collection of bounding boxes is gathered by all processes. 

[1.x.75] 



The only two numerical parameters used in the equations are the viscosity of the fluid, and the penalty term  [2.x.213]  used in the Nitsche formulation: 

[1.x.76] 



By default, we create a hyper_cube without colorization, and we use homogeneous Dirichlet boundary conditions. In this set we store the boundary ids to use when setting the boundary conditions: 

[1.x.77] 



We illustrate here another way to create a Triangulation from a parameter file, using the method  [2.x.214]  that takes the name of a function in the GridGenerator namespace, and its arguments as a single string representing the arguments as a tuple.      


The mechanism with which the arguments are parsed from and to a string is explained in detail in the  [2.x.215]  class, which is used to translate from strings to most of the basic STL types (vectors, maps, tuples) and basic deal.II types (Point, Tensor, BoundingBox, etc.).      


In general objects that can be represented by rank 1 uniform elements (i.e.,  [2.x.216]  Point<dim>,  [2.x.217]  etc.) are comma separated. Additional ranks take a semicolon, allowing you to parse strings into objects of type  [2.x.218]  or, for example,  [2.x.219]  as `0.0, 0.1; 0.1, 0.2`. This string could be interpreted as a vector of two Point objects, or a vector of vector of doubles.      


When the entries are not uniform, as in the tuple case, we use a colon to separate the various entries. For example, a string like `5: 0.1, 0.2` could be used to parse an object of type  [2.x.220]  Point<2>>` or a  [2.x.221]   [2.x.222]       


In our case most of the arguments are Point objects (representing centers, corners, subdivision elements, etc.), integer values (number of subdivisions), double values (radius, lengths, etc.), or boolean options (such as the `colorize` option that many GridGenerator functions take).      


In the example below, we set reasonable default values, but these can be changed at run time by selecting any other supported function of the GridGenerator namespace. If the GridGenerator function fails, this program will interpret the name of the grid as a vtk grid filename, and the arguments as a map from manifold_id to the CAD files describing the geometry of the domain. Every CAD file will be analyzed and a Manifold of the OpenCASCADE namespace will be generated according to the content of the CAD file itself.      


To be as generic as possible, we do this for each of the generated grids: the fluid grid, the solid grid, but also the tracer particles which are also generated using a triangulation. 

[1.x.78] 



Similarly, we allow for different local refinement strategies. In particular, we limit the maximum number of refinement levels, in order to control the minimum size of the fluid grid, and guarantee that it is compatible with the solid grid. The minimum number of refinement levels is also controlled to ensured sufficient accuracy in the bulk of the flow. Additionally, we perform local refinement based on standard error estimators on the fluid velocity field.      


We permit the user to choose between the two most common refinement strategies, namely `fixed_number` or `fixed_fraction`, that refer to the methods  [2.x.223]  and  [2.x.224]       


Refinement may be done every few time steps, instead of continuously, and we control this value by the `refinement_frequency` parameter: 

[1.x.79] 



Finally, the following two function objects are used to control the source term of Stokes flow and the angular velocity at which we move the solid body. In a more realistic simulation, the solid velocity or its deformation would come from the solution of an auxiliary problem on the solid domain. In this example step we leave this part aside, and simply impose a fixed rotational velocity field along the z-axis on the immersed solid, governed by a function that can be specified in the parameter file: 

[1.x.80] 



There remains the task of declaring what run-time parameters we can accept in input files. We split the parameters in various categories, by putting them in different sections of the ParameterHandler class. We begin by declaring all the global parameters used by StokesImmersedProblem in the global scope: 

[1.x.81] 



Next section is dedicated to the parameters used to create the various grids. We will need three different triangulations: `Fluid grid` is used to define the fluid domain, `Solid grid` defines the solid domain, and `Particle grid` is used to distribute some tracer particles, that are advected with the velocity and only used as passive tracers. 

[1.x.82] 



The final task is to correct the default dimension for the right hand side function and define a meaningful default angular velocity instead of zero. 

[1.x.83] 



Once the angular velocity is provided as a Function object, we reconstruct the pointwise solid velocity through the following class which derives from the Function class. It provides the value of the velocity of the solid body at a given position by assuming that the body rotates around the origin (or the  [2.x.225]  axis in 3d) with a given angular velocity. 

[1.x.84] 



We assume that the angular velocity is directed along the z-axis, i.e., we model the actual angular velocity as if it was a two-dimensional rotation, irrespective of the actual value of `spacedim`. 

[1.x.85] 



Similarly, we assume that the solid position can be computed explicitly at each time step, exploiting the knowledge of the angular velocity. We compute the exact position of the solid particle assuming that the solid is rotated by an amount equal to the time step multiplied by the angular velocity computed at the point `p`: 

[1.x.86] 




[1.x.87]  [1.x.88] 




We are now ready to introduce the main class of our tutorial program. As usual, other than the constructor, we leave a single public entry point: the `run()` method. Everything else is left `private`, and accessed through the run method itself. 

[1.x.89] 



The next section contains the `private` members of the class. The first method is similar to what is present in previous example. However it not only takes care of generating the grid for the fluid, but also the grid for the solid. The second computes the largest time step that guarantees that each particle moves of at most one cell. This is important to ensure that the  [2.x.226]  can find which cell a particle ends up in, as it can only look from one cell to its immediate neighbors (because, in a parallel setting, every MPI process only knows about the cells it owns as well as their immediate neighbors). 

[1.x.90] 



The next two functions initialize the  [2.x.227]  objects used in this class. We have two such objects: One represents passive tracers, used to plot the trajectories of fluid particles, while the the other represents material particles of the solid, which are placed at quadrature points of the solid grid. 

[1.x.91] 



The remainder of the set up is split in two parts: The first of the following two functions creates all objects that are needed once per simulation, whereas the other sets up all objects that need to be reinitialized at every refinement step. 

[1.x.92] 



The assembly routine is very similar to other Stokes assembly routines, with the exception of the Nitsche restriction part, which exploits one of the particle handlers to integrate on a non-matching part of the fluid domain, corresponding to the position of the solid. We split these two parts into two separate functions. 

[1.x.93] 



The remaining functions solve the linear system (which looks almost identical to the one in  [2.x.228] ) and then postprocess the solution: The refine_and_transfer() method is called only every `refinement_frequency` steps to adapt the mesh and also make sure that all the fields that were computed on the time step before refinement are transferred correctly to the new grid. This includes vector fields, as well as particle information. Similarly, we call the two output methods only every `output_frequency` steps. 

[1.x.94] 



Let us then move on to the member functions of the class. The first deals with run-time parameters that are read from a parameter file. As noted before, we make sure we cannot modify this object from within this class, by making it a `const` reference. 

[1.x.95] 



Then there is also the MPI communicator object that we will use to let processes send information across the network if the program runs in parallel, along with the `pcout` object and timer information that has also been employed by  [2.x.229] , for example: 

[1.x.96] 



Next is one of the main novelties with regard to  [2.x.230] . Here we assume that both the solid and the fluid are fully distributed triangulations. This allows the problem to scale to a very large number of degrees of freedom, at the cost of communicating all the overlapping regions between non matching triangulations. This is especially tricky, since we make no assumptions on the relative position or distribution of the various subdomains of the two triangulations. In particular, we assume that every process owns only a part of the `solid_tria`, and only a part of the `fluid_tria`, not necessarily in the same physical region, and not necessarily overlapping.      


We could in principle try to create the initial subdivisions in such a way that each process's subdomains overlap between the solid and the fluid regions. However, this overlap would be destroyed during the simulation, and we would have to redistribute the DoFs again and again. The approach we follow in this tutorial is more flexible, and not much more expensive. We make two all-to-all communications at the beginning of the simulation to exchange information about an (approximate) information of the geometrical occupancy of each processor (done through a collection of bounding boxes).      


This information is used by the  [2.x.231]  class to exchange (using a some-to-some communication pattern) all particles, so that every process knows about the particles that live on the region occupied by the fluid subdomain that it owns.      


In order to couple the overlapping regions, we exploit the facilities implemented in the ParticleHandler class. 

[1.x.97] 



Next come descriptions of the finite elements in use, along with appropriate quadrature formulas and the corresponding DoFHandler objects. For the current implementation, only `fluid_fe` is really necessary. For completeness, and to allow easy extension, we also keep the `solid_fe` around, which is however initialized to a FE_Nothing finite element space, i.e., one that has no degrees of freedom.      


We declare both finite element spaces as  [2.x.232]  objects rather than regular member variables, to allow their generation after `StokesImmersedProblemParameters` has been initialized. In particular, they will be initialized in the `initial_setup()` method. 

[1.x.98] 



Similarly to how things are done in  [2.x.233] , we use a block system to treat the Stokes part of the problem, and follow very closely what was done there. 

[1.x.99] 



Using this partitioning of degrees of freedom, we can then define all of the objects necessary to describe the linear systems in question: 

[1.x.100] 



Let us move to the particles side of this program. There are two  [2.x.234]  objects used to couple the solid with the fluid, and to describe the passive tracers. These, in many ways, play a role similar to the DoFHandler class used in the discretization, i.e., they provide for an enumeration of particles and allow querying information about each particle. 

[1.x.101] 



For every tracer particle, we need to compute the velocity field in its current position, and update its position using a discrete time stepping scheme. We do this using distributed linear algebra objects that store the coordinates of each particle's location or velocity. That is, these vectors have `tracer_particle_handler.n_global_particles() * spacedim` entries that we will store in a way so that parts of the vector are partitioned across all processes. (Implicitly, we here make the assumption that the `spacedim` coordinates of each particle are stored in consecutive entries of the vector.) Thus, we need to determine who the owner of each vector entry is. We set this owner to be equal to the process that generated that particle at time  [2.x.235] . This information is stored for every process in the `locally_owned_tracer_particle_coordinates` IndexSet.      


Once the particles have been distributed around to match the process that owns the region where the particle lives, we will need read access from that process to the corresponding velocity field. We achieve this by filling a read only velocity vector field that contains the relevant information in ghost entries. This is achieved using the `locally_relevant_tracer_particle_coordinates` IndexSet, that keeps track of how things change during the simulation, i.e., it keeps track of where particles that the current process owns have ended up being, and who owns the particles that ended up in my subdomain.      


While this is not the most efficient strategy, we keep it this way to illustrate how things would work in a real fluid-structure interaction (FSI) problem. If a particle is linked to a specific solid degree of freedom, we are not free to choose who owns it, and we have to communicate this information around. We illustrate this here, and show that the communication pattern is point-to-point, and negligible in terms of total cost of the algorithm.      


The vectors defined based on these subdivisions are then used to store the particles velocities (read-only, with ghost entries) and their displacement (read/write, no ghost entries). 

[1.x.102] 



One of the key points of this tutorial program is the coupling between two independent  [2.x.236]  objects, one of which may be moving and deforming (with possibly large deformations) with respect to the other. When both the fluid and the solid triangulations are of type  [2.x.237]  every process has access only to its fraction of locally owned cells of each of the two triangulations. As mentioned above, in general, the locally owned domains are not overlapping.      


In order to allow for the efficient exchange of information between non-overlapping  [2.x.238]  objects, some algorithms of the library require the user to provide a rough description of the area occupied by the locally owned part of the triangulation, in the form of a collection of axis-aligned bounding boxes for each process, that provide a full covering of the locally owned part of the domain. This kind of information can then be used in situations where one needs to send information to the owner of the cell surrounding a known location, without knowing who that owner may in fact be. But, if one knows a collection of bounding boxes for the geometric area or volume each process owns, then we can determine a subset of all processes that might possibly own the cell in which that location lies: namely, all of those processes whose bounding boxes contain that point. Instead of sending the information associated to that location to all processes, one can then get away with only sending it to a small subset of the processes with point-to-point communication primitives. (You will notice that this also allows for the typical time-vs-memory trade-off: The more data we are willing to store about each process's owned area -- in the form of more refined bounding box information -- the less communication we have to perform.)      


We construct this information by gathering a vector (of length  [2.x.239]  of vectors of BoundingBox objects. We fill this vector using the extract_rtree_level() function, and allow the user to select what level of the tree to extract. The "level" corresponds to how coarse/fine the overlap of the area with bounding boxes should be.      


As an example, this is what would be extracted by the extract_rtree_level() function applied to a two dimensional hyper ball, distributed over three processes. Each image shows in green the bounding boxes associated to the locally owned cells of the triangulation on each process, and in violet the bounding boxes extracted from the rtree:      


 [2.x.240]   [2.x.241]   [2.x.242]       


We store these boxes in a global member variable, which is updated at every refinement step: 

[1.x.103] 




[1.x.104]  [1.x.105] 





[1.x.106]  [1.x.107] 




In the constructor, we create the mpi_communicator as well as the triangulations and dof_handler for both the fluid and the solid. Using the mpi_communicator, both the ConditionalOStream and TimerOutput object are constructed. 

[1.x.108] 



In order to generate the grid, we first try to use the functions in the deal.II GridGenerator namespace, by leveraging the  [2.x.243]  If this function fails, then we use the following method, where the name is interpreted as a filename, and the arguments are interpreted as a map from manifold ids to CAD files, and are converted to Manifold descriptors using the OpenCASCADE namespace facilities. At the top, we read the file into a triangulation: 

[1.x.109] 



If we got to this point, then the Triangulation has been read, and we are ready to attach to it the correct manifold descriptions. We perform the next lines of code only if deal.II has been built with OpenCASCADE support. For each entry in the map, we try to open the corresponding CAD file, we analyze it, and according to its content, opt for either a  [2.x.244]  (if the CAD file contains a single `TopoDS_Edge` or a single `TopoDS_Wire`) or a  [2.x.245]  if the file contains a single face. Notice that if the CAD files do not contain single wires, edges, or faces, an assertion will be throw in the generation of the Manifold.      


We use the  [2.x.246]  class to do the conversion from the string to a map between manifold ids and file names for us: 

[1.x.110] 



Now we check how many faces are contained in the `Shape`. OpenCASCADE is intrinsically 3D, so if this number is zero, we interpret this as a line manifold, otherwise as a  [2.x.247]  in `spacedim` = 3, or  [2.x.248]  in `spacedim` = 2. 

[1.x.111] 



We use this trick, because  [2.x.249]  is only implemented for spacedim = 3. The check above makes sure that things actually work correctly. 

[1.x.112] 



We also allow surface descriptions in two dimensional spaces based on single NURBS patches. For this to work, the CAD file must contain a single `TopoDS_Face`. 

[1.x.113] 



Now let's put things together, and make all the necessary grids. As mentioned above, we first try to generate the grid internally, and if we fail (i.e., if we end up in the `catch` clause), then we proceed with the above function.    


We repeat this pattern for both the fluid and the solid mesh. 

[1.x.114] 




[1.x.115]  [1.x.116] 




Once the solid and fluid grids have been created, we start filling the  [2.x.250]  objects. The first one we take care of is the one we use to keep track of passive tracers in the fluid. These are simply transported along, and in some sense their locations are unimportant: We just want to use them to see where flow is being transported. We could use any way we choose to determine where they are initially located. A convenient one is to create the initial locations as the vertices of a mesh in a shape of our choice -- a choice determined by one of the run-time parameters in the parameter file.    


In this implementation, we create tracers using the support points of a FE_Q finite element space defined on a temporary grid, which is then discarded. Of this grid, we only keep around the  [2.x.251]  objects (stored in a  [2.x.252]  class) associated to the support points.    


The  [2.x.253]  class offers the possibility to insert a set of particles that live physically in the part of the domain owned by the active process. However, in this case this function would not suffice. The particles generated as the locally owned support points of an FE_Q object on an arbitrary grid (non-matching with regard to the fluid grid) have no reasons to lie in the same physical region of the locally owned subdomain of the fluid grid. In fact this will almost never be the case, especially since we want to keep track of what is happening to the particles themselves.    


In particle-in-cell methods (PIC), it is often customary to assign ownership of the particles to the process where the particles lie. In this tutorial we illustrate a different approach, which is useful if one wants to keep track of information related to the particles (for example, if a particle is associated to a given degree of freedom, which is owned by a specific process and not necessarily the same process that owns the fluid cell where the particle happens to be at any given time). In the approach used here, ownership of the particles is assigned once at the beginning, and one-to-one communication happens whenever the original owner needs information from the process that owns the cell where the particle lives. We make sure that we set ownership of the particles using the initial particle distribution, and keep the same ownership throughout the execution of the program.    


With this overview out of the way, let us see what the function does. At the top, we create a temporary triangulation and DoFHandler object from which we will take the node locations for initial particle locations: 

[1.x.117] 



This is where things start to get complicated. Since we may run this program in a parallel environment, every parallel process will now have created these temporary triangulations and DoFHandlers. But, in fully distributed triangulations, the active process only knows about the locally owned cells, and has no idea of how other processes have distributed their own cells. This is true for both the temporary triangulation created above as well as the fluid triangulation into which we want to embed the particles below. On the other hand, these locally known portions of the two triangulations will, in general, not overlap. That is, the locations of the particles we will create from the node locations of the temporary mesh are arbitrary, and may fall within a region of the fluid triangulation that the current process doesn't have access to (i.e., a region of the fluid domain where cells are artificial). In order to understand who to send those particles to, we need to have a (rough) idea of how the fluid grid is distributed among processors.      


We construct this information by first building an index tree of boxes bounding the locally owned cells, and then extracting one of the first levels of the tree: 

[1.x.118] 



Each process now has a collection of bounding boxes that completely enclose all locally owned processes (but that may overlap the bounding boxes of other processes). We then exchange this information between all participating processes so that every process knows the bounding boxes of all other processes.      


Equipped with this knowledge, we can then initialize the `tracer_particle_handler` to the fluid mesh and generate the particles from the support points of the (temporary) tracer particles triangulation. This function call uses the `global_bounding_boxes` object we just constructed to figure out where to send the particles whose locations were derived from the locally owned part of the `particles_dof_handler`. At the end of this call, every particle will have been distributed to the correct process (i.e., the process that owns the fluid cell where the particle lives). We also output their number to the screen at this point. 

[1.x.119] 



Each particle so created has a unique ID. At some point in the algorithm below, we will need vectors containing position and velocity information for each particle. This vector will have size `n_particles * spacedim`, and we will have to store the elements of this vector in a way so that each parallel process "owns" those elements that correspond to coordinates of the particles it owns. In other words, we have to partition the index space between zero and `n_particles * spacedim` among all processes. We can do this by querying the `tracer_particle_handler` for the IDs of its locally relevant particles, and construct the indices that would be needed to store in a (parallel distributed) vector of the position and velocity of all particles where we implicitly assume that we store the coordinates of each location or velocity in `spacedim` successive vector elements (this is what the  [2.x.254]  function does). 

[1.x.120] 



At the beginning of the simulation, all particles are in their original position. When particles move, they may traverse to a part of the domain which is owned by another process. If this happens, the current process keeps formally "ownership" of the particles, but may need read access from the process where the particle has landed. We keep this information in another index set, which stores the indices of all particles that are currently on the current process's subdomain, independently if they have always been here or not.      


Keeping this index set around allows us to leverage linear algebra classes for all communications regarding positions and velocities of the particles. This mimics what would happen in the case where another problem was solved in the solid domain (as in fluid-structure interaction. In this latter case, additional DOFs on the solid domain would be coupled to what is occurring in the fluid domain. 

[1.x.121] 



Finally, we make sure that upon refinement, particles are correctly transferred. When performing local refinement or coarsening, particles will land in another cell. We could in principle redistribute all particles after refining, however this would be overly expensive.      


The  [2.x.255]  class has a way to transfer information from a cell to its children or to its parent upon refinement, without the need to reconstruct the entire data structure. This is done by registering two callback functions to the triangulation. These functions will receive a signal when refinement is about to happen, and when it has just happened, and will take care of transferring all information to the newly refined grid with minimal computational cost. 

[1.x.122] 



Similarly to what we have done for passive tracers, we next set up the particles that track the quadrature points of the solid mesh. The main difference here is that we also want to attach a weight value (the "JxW" value of the quadrature point) to each of particle, so that we can compute integrals even without direct access to the original solid grid.    


This is achieved by leveraging the "properties" concept of the  [2.x.256]  class. It is possible to store (in a memory efficient way) an arbitrary number of `double` numbers for each of the  [2.x.257]  objects inside a  [2.x.258]  object. We use this possibility to store the JxW values of the quadrature points of the solid grid.    


In our case, we only need to store one property per particle: the JxW value of the integration on the solid grid. This is passed at construction time to the solid_particle_handler object as the last argument 

[1.x.123] 



The number of particles that we generate locally is equal to the total number of locally owned cells times the number of quadrature points used in each cell. We store all these points in a vector, and their corresponding properties in a vector of vectors: 

[1.x.124] 



We proceed in the same way we did with the tracer particles, reusing the computed bounding boxes. However, we first check that the `global_fluid_bounding_boxes` object has been actually filled. This should certainly be the case here, since this method is called after the one that initializes the tracer particles. However, we want to make sure that if in the future someone decides (for whatever reason) to initialize first the solid particle handler, or to copy just this part of the tutorial, a meaningful exception is thrown when things don't work as expected      


Since we have already stored the position of the quadrature points, we can use these positions to insert the particles directly using the `solid_particle_handler` instead of having to go through a  [2.x.259]  function: 

[1.x.125] 



As in the previous function, we end by making sure that upon refinement, particles are correctly transferred: 

[1.x.126] 




[1.x.127]  [1.x.128] 




We set up the finite element space and the quadrature formula to be used throughout the step. For the fluid, we use Taylor-Hood elements (e.g.  [2.x.260] ). Since we do not solve any equation on the solid domain, an empty finite element space is generated. A natural extension of this program would be to solve a fluid structure interaction problem, which would require that the `solid_fe` use more useful FiniteElement class.    


Like for many other functions, we store the time necessary to carry out the operations we perform here. The current function puts its timing information into a section with label "Initial setup". Numerous other calls to this timer are made in various functions. They allow to monitor the absolute and relative cost of each individual function to identify bottlenecks. 

[1.x.129] 



We next construct the distributed block matrices and vectors which are used to solve the linear equations that arise from the problem. This function is adapted from  [2.x.261]  and we refer to this step for a thorough explanation. 

[1.x.130] 




[1.x.131]  [1.x.132] 




We assemble the system matrix, the preconditioner matrix, and the right hand side. The code is adapted from  [2.x.262] , which is essentially what  [2.x.263]  also has, and is pretty standard if you know what the Stokes equations look like. 

[1.x.133] 



The following method is then the one that deals with the penalty terms that result from imposing the velocity on the impeller. It is, in a sense, the heart of the tutorial, but it is relatively straightforward. Here we exploit the `solid_particle_handler` to compute the Nitsche restriction or the penalization in the embedded domain. 

[1.x.134] 



We loop over all the local particles. Although this could be achieved directly by looping over all the cells, this would force us to loop over numerous cells which do not contain particles. Consequently, we loop over all the particles, but, we get the reference of the cell in which the particle lies and then loop over all particles within that cell. This enables us to skip the cells which do not contain particles, yet to assemble the local matrix and rhs of each cell to apply the Nitsche restriction. Once we are done with all particles on one cell, we advance the `particle` iterator to the particle past the end of the ones on the current cell (this is the last line of the `while` loop's body). 

[1.x.135] 



We get an iterator to the cell within which the particle lies from the particle itself. We can then assemble the additional terms in the system matrix and the right hand side as we would normally. 

[1.x.136] 



So then let us get the collection of cells that are located on this cell and iterate over them. From each particle we gather the location and the reference location of the particle as well as the additional information that is attached to the particle. In the present case, this information is the "JxW" of the quadrature points which were used to generate the particles.          


Using this information, we can add the contribution of the quadrature point to the local_matrix and local_rhs. We can evaluate the value of the shape function at the position of each particle easily by using its reference location. 

[1.x.137] 




[1.x.138]  [1.x.139] 




This function solves the linear system with FGMRES with a block diagonal preconditioner and an algebraic multigrid (AMG) method for the diagonal blocks. The preconditioner applies a V cycle to the  [2.x.264]  (i.e., the velocity-velocity) block and a CG with the mass matrix for the  [2.x.265]  block (which is our approximation to the Schur complement: the pressure mass matrix assembled above). 

[1.x.140] 




[1.x.141]  [1.x.142] 




We deal with mesh refinement in a completely standard way: 

[1.x.143] 




[1.x.144]  [1.x.145] 




We output the results (velocity and pressure) on the fluid domain using the standard parallel capabilities of deal.II. A single compressed vtu file is written that agglomerates the information of all processors. An additional `.pvd` record is written to associate the physical time to the vtu files. 

[1.x.146] 



Similarly, we write the particles (either from the solid or the tracers) as a single compressed vtu file through the  [2.x.266]  object. This simple object does not write the additional information attached as "properties" to the particles, but only writes their id -- but then, we don't care about the "JxW" values of these particle locations anyway, so no information that we may have wanted to visualize is lost. 

[1.x.147] 




[1.x.148]  [1.x.149] 




This function now orchestrates the entire simulation. It is very similar to the other time dependent tutorial programs -- take  [2.x.267]  or  [2.x.268]  as an example. At the beginning, we output some status information and also save all current parameters to a file in the output directory, for reproducibility. 

[1.x.150] 



We then start the time loop. We initialize all the elements of the simulation in the first cycle 

[1.x.151] 



After the first time step, we displace the solid body at the beginning of each time step to take into account the fact that is has moved. 

[1.x.152] 



In order to update the state of the system, we first interpolate the fluid velocity at the position of the tracer particles and, with a naive explicit Euler scheme, advect the massless tracer particles. 

[1.x.153] 



Using these new locations, we can then assemble the Stokes system and solve it. 

[1.x.154] 



With the appropriate frequencies, we then write the information of the solid particles, the tracer particles, and the fluid domain into files for visualization, and end the time step by adapting the mesh. 

[1.x.155] 




[1.x.156]  [1.x.157] 




The remainder of the code, the `main()` function, is standard, with the exception of the handling of input parameter files. We allow the user to specify an optional parameter file as an argument to the program. If nothing is specified, we use the default file "parameters.prm", which is created if non existent. The file name is scanned for the the string "23" first, and "3" afterwards. If the filename contains the string "23", the problem classes are instantiated with template arguments 2 and 3 respectively. If only the string "3" is found, then both template arguments are set to 3, otherwise both are set to 2. 




If the program is called without any command line arguments (i.e., `argc==1`), then we just use "parameters.prm" by default. 

[1.x.158] 

[1.x.159][1.x.160] 


The directory in which this program is run contains a number of sample parameter files that you can use to reproduce the results presented in this section. If you do not specify a parameter file as an argument on the command line, the program will try to read the file "`parameters.prm`" by default, and will execute the two dimensional version of the code. As explained in the discussion of the source code, if your file name contains the string "23", then the program will run a three dimensional problem, with immersed solid of co-dimension one. If it contains the string "3", it will run a three dimensional problem, with immersed solid of co-dimension zero, otherwise it will run a two dimensional problem with immersed solid of co-dimension zero. 

Regardless of the specific parameter file name, if the specified file does not exist, when you execute the program you will get an exception that no such file can be found: 

[1.x.161] 



However, as the error message already states, the code that triggers the exception will also generate the specified file ("`parameters.prm`" in this case) that simply contains the default values for all parameters this program cares about (for the correct dimension and co-dimension, according to the whether a string "23" or "3" is contained in the file name). By inspection of the default parameter file, we see the following: 

[1.x.162] 



If you now run the program, you will get a file called `parameters_22.prm` in the directory specified by the parameter `Output directory` (which defaults to the current directory) containing a shorter version of the above parameters (without comments and documentation), documenting all parameters that were used to run your program: 

[1.x.163] 



The rationale behind creating first `parameters.prm` file (the first time the program is run) and then a `output/parameters_22.prm` (every time you run the program with an existing input file), is because you may want to leave most parameters to their default values, and only modify a handful of them, while still beeing able to reproduce the results and inspect what parameters were used for a specific simulation. It is generally good scientific practice to store the parameter file you used for a simulation along with the simulation output so that you can repeat the exact same run at a later time if necessary. 

Another reason is because the input file may only contain those parameters that differ from their defaults. For example, you could use the following (perfectly valid) parameter file with this tutorial program: 

[1.x.164] 

and you would run the program with Q3/Q2 Taylor-Hood finite elements, for 101 steps, using a Nitsche penalty of `10`, and leaving all the other parameters to their default value. The output directory then contains a record of not just these parameters, but indeed all parameters used in the simulation. You can inspect all the other parameters in the produced file `parameters_22.prm`. 


[1.x.165][1.x.166] 


The default problem generates a co-dimension zero impeller, consisting of a rotating rectangular grid, where the rotation is for half a time unit in one direction, and half a time unit in the opposite direction, with constant angular velocity equal to  [2.x.269] . Consequently, the impeller does half a rotation and returns to its original position. The following animation displays the velocity magnitude, the motion of the solid impeller and of the tracer particles. 


 [2.x.270]  

On one core, the output of the program will look like the following: 

[1.x.167] 



You may notice that assembling the coupling system is more expensive than assembling the Stokes part. This depends highly on the number of Gauss points (solid particles) that are used to apply the Nitsche restriction. In the present case, a relatively low number of tracer particles are used. Consequently, tracking their motion is relatively cheap. 

The following movie shows the evolution of the solution over time: 

[1.x.168] 



The movie shows the rotating obstacle in gray (actually a superposition of the solid particles plotted with large enough dots that they overlap), [1.x.169] in light colors (including the corner vertices that form at specific times during the simulation), and the tracer particles in bluish tones. 

The simulation shows that at the end time, the tracer particles have somewhat returned to their original position, although they have been distorted by the flow field. The following image compares the initial and the final position of the particles after one time unit of flow. 

 [2.x.271]  

In this case, we see that the tracer particles that were outside of the swept volume of the impeller have returned very close to their initial position, whereas those in the swept volume were slightly more deformed. This deformation is non-physical. It is caused by the numerical error induced by the explicit Euler scheme used to advect the particles, by the loss of accuracy due to the fictitious domain and, finally, by the discretization error on the Stokes equations. The first two errors are the leading cause of this deformation and they could be alleviated by the use of a finer mesh and a lower time step. 


[1.x.170][1.x.171] 


To play around a little bit, we complicate the fictitious domain (taken from https://grabcad.com/library/lungstors-blower-1), and run a co-dimension one simulation in three space dimensions, using the following "`parameters_23.prm`" file: 

[1.x.172] 



In this case, the timing outputs are a bit different: 

[1.x.173] 



Now, the solver is taking most of the solution time in three dimensions, and the particle motion and Nitsche assembly remain relatively unimportant as far as run time is concerned. 




[1.x.174] 




[1.x.175] [1.x.176][1.x.177] 


The current tutorial program shows a one-way coupling between the fluid and the solid, where the solid motion is imposed (and not solved for), and read in the solid domain by exploiting the location and the weights of the solid quadrature points. 

The structure of the code already allows one to implement a two-way coupling, by exploiting the possibility to read values of the fluid velocity on the quadrature points of the solid grid. For this to be more efficient in terms of MPI communication patterns, one should maintain ownership of the quadrature points on the solid processor that owns the cells where they have been created. In the current code, it is sufficient to define the IndexSet of the vectors used to exchange information of the quadrature points by using the solid partition instead of the initial fluid partition. 

This allows the combination of the technique used in this tutorial program with those presented in the tutorial  [2.x.272]  to solve a fluid structure interaction problem with distributed Lagrange multipliers, on  [2.x.273]  objects. 

The timings above show that the current preconditioning strategy does not work well for Nitsche penalization, and we should come up with a better preconditioner if we want to aim at larger problems. Moreover, a checkpoint restart strategy should be implemented to allow for longer simulations to be interrupted and restored, as it is done for example in the  [2.x.274]  tutorial. [1.x.178] [1.x.179]  [2.x.275]  

 [2.x.276] 
